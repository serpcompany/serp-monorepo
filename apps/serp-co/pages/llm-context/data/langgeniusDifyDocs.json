[
  {
    "owner": "langgenius",
    "repo": "dify-docs",
    "content": "TITLE: Conversational API call with Python\nDESCRIPTION: This Python script demonstrates calling the conversational API endpoint (`/v1/chat-messages`). It requires the `requests` and `json` libraries and includes headers for `Authorization` and `Content-Type`. The payload includes `query`, `response_mode`, `conversation_id`, and `user`. The `conversation_id` is crucial for continuing existing conversations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-publishing/developing-with-apis.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport requests\nimport json\n\nurl = 'https://api.dify.ai/v1/chat-messages'\nheaders = {\n    'Authorization': 'Bearer ENTER-YOUR-SECRET-KEY',\n    'Content-Type': 'application/json',\n}\ndata = {\n    \"inputs\": {},\n    \"query\": \"eh\",\n    \"response_mode\": \"streaming\",\n    \"conversation_id\": \"1c7e55fb-1ba2-4e10-81b5-30addcea2276\",\n    \"user\": \"abc-123\"\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nprint(response.text())\n```\n\n----------------------------------------\n\nTITLE: Invoking a Large Language Model (LLM) (Python)\nDESCRIPTION: This code snippet demonstrates the core method for invoking a Large Language Model (LLM) within the Dify platform. It inherits from the `__base.large_language_model.LargeLanguageModel` base class. This method supports both streaming and synchronous responses, using prompt messages, model parameters, tools, and stop sequences.  It's crucial for interacting with language models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              prompt_messages: list[PromptMessage], model_parameters: dict,\n              tools: Optional[list[PromptMessageTool]] = None, stop: Optional[list[str]] = None,\n              stream: bool = True, user: Optional[str] = None) \\\n          -> Union[LLMResult, Generator]:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param model_parameters: model parameters\n    :param tools: tools for tool calling\n    :param stop: stop words\n    :param stream: is stream response\n    :param user: unique user id\n    :return: full response or stream response chunk generator result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: LLM Invocation in Python\nDESCRIPTION: This Python code snippet shows the implementation of the `_invoke` method for LLM invocation. This method is the core of LLM interaction, supporting both streaming and synchronous responses. It takes model details, credentials, prompt messages, model parameters, tools, stop words, streaming flag, and user information as input and returns either a full response or a stream response chunk generator.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              prompt_messages: list[PromptMessage], model_parameters: dict,\n              tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,\n              stream: bool = True, user: Optional[str] = None) \\\n          -> Union[LLMResult, Generator]:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param model_parameters: model parameters\n    :param tools: tools for tool calling\n    :param stop: stop words\n    :param stream: is stream response\n    :param user: unique user id\n    :return: full response or stream response chunk generator result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: POST Request to Completion Messages API with Python\nDESCRIPTION: This Python script sends a POST request to the `/v1/completion-messages` endpoint using the `requests` library for text completion. It includes the authorization header with a bearer token, the content type header, and a JSON payload with input text, response mode, and user ID.  It depends on the `requests` and `json` libraries. The `ENTER-YOUR-SECRET-KEY` should be replaced with the actual API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-publishing/developing-with-apis.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport requests\nimport json\n\nurl = \"https://api.dify.ai/v1/completion-messages\"\n\nheaders = {\n    'Authorization': 'Bearer ENTER-YOUR-SECRET-KEY',\n    'Content-Type': 'application/json',\n}\n\ndata = {\n    \"inputs\": {\"text\": 'Hello, how are you?'},\n    \"response_mode\": \"streaming\",\n    \"user\": \"abc-123\"\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Creating Agent Log Messages in Python\nDESCRIPTION: This code snippet defines a function for creating log messages within an agent's execution flow. It allows for creating a log message with a label, data, status, and a parent node to establish a log tree. This supports tracking the agent's thought process by allowing the display of in-progress logs that can be updated upon task completion.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/agent.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef create_log_message(\n    self,\n    label: str,\n    data: Mapping[str, Any],\n    status: AgentInvokeMessage.LogMessage.LogStatus = AgentInvokeMessage.LogMessage.LogStatus.SUCCESS,\n    parent: AgentInvokeMessage | None = None,\n) -> AgentInvokeMessage\n```\n\n----------------------------------------\n\nTITLE: LLM Request Interface (_invoke)\nDESCRIPTION: Defines the core interface `_invoke` for requesting a Large Language Model (LLM). Supports both streaming and synchronous responses. Key parameters include model name, credentials, prompt messages, model parameters, tools, stop words, stream flag, and user ID. The return type is either a full `LLMResult` or a stream response chunk generator.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/predefined-model.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            prompt_messages: list[PromptMessage], model_parameters: dict,\n            tools: Optional[list[PromptMessageTool]] = None, stop: Optional[list[str]] = None,\n            stream: bool = True, user: Optional[str] = None) \\\n        -> Union[LLMResult, Generator]:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param model_parameters: model parameters\n    :param tools: tools for tool calling\n    :param stop: stop words\n    :param stream: is stream response\n    :param user: unique user id\n    :return: full response or stream response chunk generator result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking a Large Language Model in Dify (Python)\nDESCRIPTION: This method invokes a large language model to generate a response based on the provided prompt messages and parameters. It supports both streaming and synchronous responses. The `prompt_messages` parameter contains a list of `PromptMessage` objects, and the `model_parameters` parameter contains model-specific parameters defined in the model's YAML configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            prompt_messages: list[PromptMessage], model_parameters: dict,\n            tools: Optional[list[PromptMessageTool]] = None, stop: Optional[list[str]] = None,\n            stream: bool = True, user: Optional[str] = None) \\\n        -> Union[LLMResult, Generator]:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param model_parameters: model parameters\n    :param tools: tools for tool calling\n    :param stop: stop words\n    :param stream: is stream response\n    :param user: unique user id\n    :return: full response or stream response chunk generator result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Conversational API call with cURL\nDESCRIPTION: This cURL command illustrates how to call the conversational API endpoint (`/v1/chat-messages`). It includes parameters like `query`, `response_mode`, `conversation_id`, and `user`. A `conversation_id` is essential for maintaining context in ongoing conversations. Replace `ENTER-YOUR-SECRET-KEY` with your API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-publishing/developing-with-apis.md#_snippet_2\n\nLANGUAGE: cURL\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/chat-messages' \\\n--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"inputs\": {},\n    \"query\": \"eh\",\n    \"response_mode\": \"streaming\",\n    \"conversation_id\": \"1c7e55fb-1ba2-4e10-81b5-30addcea2276\",\n    \"user\": \"abc-123\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Adding Metadata Fields to Knowledge Base via Dify API\nDESCRIPTION: Adds new metadata fields to a dataset within the Dify knowledge base.  Requires the dataset ID as a path parameter, a valid API key in the Authorization header, and a JSON payload containing the metadata field's type and name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer {api_key}' \\\n--data '{\n    \"type\":\"string\",\n    \"name\":\"test\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Create Document from File - Dify API (cURL)\nDESCRIPTION: This cURL command creates a new document in an existing Dify knowledge base by uploading a file. It requires the dataset ID and API key. The request uses `multipart/form-data` to send the file and the configuration data, including indexing technique and processing rules, as a JSON string within the `data` field.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/document/create-by-file' \\\n--header 'Authorization: Bearer {api_key}' \\\n--form 'data=\"{\"indexing_technique\":\"high_quality\",\"process_rule\":{\"rules\":{\"pre_processing_rules\":[{\"id\":\"remove_extra_spaces\",\"enabled\":true},{\"id\":\"remove_urls_emails\",\"enabled\":true}],\"segmentation\":{\"separator\":\"###\",\"max_tokens\":500}},\"mode\":\"custom\"}}\";type=text/plain' \\\n--form 'file=@\"/path/to/file\"'\n```\n\n----------------------------------------\n\nTITLE: Define LLMResult Class in Python\nDESCRIPTION: This code defines the `LLMResult` class, inheriting from `BaseModel`.  It includes attributes for `model` (the model used), `prompt_messages` (a list of PromptMessage objects), `message` (an AssistantPromptMessage representing the response), `usage` (LLMUsage information), and an optional `system_fingerprint`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResult(BaseModel):\n    \"\"\"\n    Model class for llm result.\n    \"\"\"\n    model: str  # Actual used modele\n    prompt_messages: list[PromptMessage]  # prompt messages\n    message: AssistantPromptMessage  # response message\n    usage: LLMUsage  # usage info\n    system_fingerprint: Optional[str] = None  # request fingerprint, refer to OpenAI definition\n```\n\n----------------------------------------\n\nTITLE: Calling Text Completion API with cURL\nDESCRIPTION: This cURL command demonstrates how to call the `/v1/completion-messages` endpoint to generate text completions. It sets the `Authorization` header with a Bearer token, specifies the `Content-Type` as `application/json`, and sends a JSON payload with input parameters, response mode, and user identifier. Replace `ENTER-YOUR-SECRET-KEY` with your actual API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-publishing/developing-with-apis.md#_snippet_0\n\nLANGUAGE: cURL\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/completion-messages' \\\n--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"inputs\": {},\n    \"response_mode\": \"streaming\",\n    \"user\": \"abc-123\"\n}'\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Workflow for Automated Plugin Publishing\nDESCRIPTION: This GitHub Actions workflow automates the process of packaging, pushing, and creating pull requests for plugin updates. It is triggered when code is pushed to the main branch of the plugin source repository, and requires a Personal Access Token (PAT) with write permissions to a forked repository.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/plugin-auto-publish-pr.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# .github/workflows/auto-pr.yml\nname: Auto Create PR on Main Push\n\non:\n  push:\n    branches: [ main ]  # Trigger on push to main\n\njobs:\n  create_pr: # Renamed job for clarity\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Print working directory # Kept for debugging\n        run: |\n          pwd\n          ls -la\n\n      - name: Download CLI tool\n        run: |\n          # Create bin directory in runner temp\n          mkdir -p $RUNNER_TEMP/bin\n          cd $RUNNER_TEMP/bin\n\n          # Download CLI tool\n          wget https://github.com/langgenius/dify-plugin-daemon/releases/download/0.0.6/dify-plugin-linux-amd64\n          chmod +x dify-plugin-linux-amd64\n\n          # Show download location and file\n          echo \"CLI tool location:\"\n          pwd\n          ls -la dify-plugin-linux-amd64\n\n      - name: Get basic info from manifest # Changed step name and content\n        id: get_basic_info\n        run: |\n          PLUGIN_NAME=$(grep \"^name:\" manifest.yaml | cut -d' ' -f2)\n          echo \"Plugin name: $PLUGIN_NAME\"\n          echo \"plugin_name=$PLUGIN_NAME\" >> $GITHUB_OUTPUT\n\n          VERSION=$(grep \"^version:\" manifest.yaml | cut -d' ' -f2)\n          echo \"Plugin version: $VERSION\"\n          echo \"version=$VERSION\" >> $GITHUB_OUTPUT\n\n          # If the author's name is not your github username, you can change the author here\n          AUTHOR=$(grep \"^author:\" manifest.yaml | cut -d' ' -f2)\n          echo \"Plugin author: $AUTHOR\"\n          echo \"author=$AUTHOR\" >> $GITHUB_OUTPUT\n\n      - name: Package Plugin\n        id: package\n        run: |\n          # Use the downloaded CLI tool to package\n          cd $GITHUB_WORKSPACE\n          # Use variables for package name\n          PACKAGE_NAME=\"${{ steps.get_basic_info.outputs.plugin_name }}-${{ steps.get_basic_info.outputs.version }}.difypkg\"\n          # Use CLI from runner temp\n          $RUNNER_TEMP/bin/dify-plugin-linux-amd64 plugin package . -o \"$PACKAGE_NAME\"\n\n          # Show packaging result\n          echo \"Package result:\"\n          ls -la \"$PACKAGE_NAME\"\n          echo \"package_name=$PACKAGE_NAME\" >> $GITHUB_OUTPUT\n\n          # Show full file path and directory structure (kept for debugging)\n          echo \"\\\\nFull file path:\"\n          pwd\n          echo \"\\\\nDirectory structure:\"\n          tree || ls -R\n\n      - name: Checkout target repo\n        uses: actions/checkout@v3\n        with:\n          # Use author variable for repository\n          repository: ${{steps.get_basic_info.outputs.author}}/dify-plugins\n          path: dify-plugins\n          token: ${{ secrets.PLUGIN_ACTION }}\n          fetch-depth: 1 # Fetch only the last commit to speed up checkout\n          persist-credentials: true # Persist credentials for subsequent git operations\n\n      - name: Prepare and create PR\n        run: |\n          # Debug info (kept)\n          echo \"Debug: Current directory $(pwd)\"\n          # Use variable for package name\n          PACKAGE_NAME=\"${{ steps.get_basic_info.outputs.plugin_name }}-${{ steps.get_basic_info.outputs.version }}.difypkg\"\n          echo \"Debug: Package name: $PACKAGE_NAME\"\n          ls -la\n\n          # Move the packaged file to the target directory using variables\n          mkdir -p dify-plugins/${{ steps.get_basic_info.outputs.author }}/${{ steps.get_basic_info.outputs.plugin_name }}\n          mv \"$PACKAGE_NAME\" dify-plugins/${{ steps.get_basic_info.outputs.author }}/${{ steps.get_basic_info.outputs.plugin_name }}/\n\n          # Enter the target repository directory\n          cd dify-plugins\n\n          # Configure git\n          git config user.name \"GitHub Actions\"\n          git config user.email \"actions@github.com\"\n\n          # Ensure we are on the latest main branch\n          git fetch origin main\n          git checkout main\n          git pull origin main\n\n          # Create and switch to a new branch using variables and new naming convention\n          BRANCH_NAME=\"bump-${{ steps.get_basic_info.outputs.plugin_name }}-plugin-${{ steps.get_basic_info.outputs.version }}\"\n          git checkout -b \"$BRANCH_NAME\"\n\n          # Add and commit changes (using git add .)\n          git add .\n          git status # for debugging\n          # Use variables in commit message\n          git commit -m \"bump ${{ steps.get_basic_info.outputs.plugin_name }} plugin to version ${{ steps.get_basic_info.outputs.version }}\"\n\n          # Push to remote (use force just in case the branch existed before from a failed run)\n          git push -u origin \"$BRANCH_NAME\" --force\n\n          # Confirm branch has been pushed and wait for sync (GitHub API might need a moment)\n          git branch -a\n          echo \"Waiting for branch to sync...\"\n          sleep 10  # Wait 10 seconds for branch sync\n\n      - name: Create PR via GitHub API\n        env:\n          GH_TOKEN: ${{ secrets.PLUGIN_ACTION }} # Use the provided token for authentication\n        run: |\n          gh pr create \\\n            --repo langgenius/dify-plugins \\\n            --head \"${{ steps.get_basic_info.outputs.author }}:${{ steps.get_basic_info.outputs.plugin_name }}-${{ steps.get_basic_info.outputs.version }}\" \\\n            --base main \\\n            --title \"bump ${{ steps.get_basic_info.outputs.plugin_name }} plugin to version ${{ steps.get_basic_info.outputs.version }}\" \\\n            --body \"bump ${{ steps.get_basic_info.outputs.plugin_name }} plugin package to version ${{ steps.get_basic_info.outputs.version }}\n\n            Changes:\n            - Updated plugin package file\" || echo \"PR already exists or creation skipped.\" # Handle cases where PR already exists\n\n      - name: Print environment info # Kept for debugging\n        run: |\n          echo \"GITHUB_WORKSPACE: $GITHUB_WORKSPACE\"\n          echo \"Current directory contents:\"\n          ls -R\n```\n\n----------------------------------------\n\nTITLE: Define Invoke Error Mapping (Python)\nDESCRIPTION: This Python code defines a mapping between model invocation errors and unified error types, allowing Dify to handle different errors appropriately. It uses a dictionary to map specific exceptions raised by the model to generic InvokeError types, such as InvokeConnectionError, InvokeServerUnavailableError, InvokeRateLimitError, InvokeAuthorizationError, and InvokeBadRequestError.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/customizable-model.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@property\n    def _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n        \"\"\"\n        Map model invoke error to unified error\n        The key is the error type thrown to the caller\n        The value is the error type thrown by the model,\n        which needs to be converted into a unified error type for the caller.\n\n        :return: Invoke error mapping\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking Large Language Model in Python\nDESCRIPTION: This method invokes a large language model. It supports both streaming and synchronous returns. It takes the model name, credentials, prompt messages, model parameters, tools, stop words, stream flag, and user ID as input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n                prompt_messages: list[PromptMessage], model_parameters: dict,\n                tools: Optional[list[PromptMessageTool]] = None, stop: Optional[list[str]] = None,\n                stream: bool = True, user: Optional[str] = None) \\\n            -> Union[LLMResult, Generator]:\n        \"\"\"\n        Invoke large language model\n\n        :param model: model name\n        :param credentials: model credentials\n        :param prompt_messages: prompt messages\n        :param model_parameters: model parameters\n        :param tools: tools for tool calling\n        :param stop: stop words\n        :param stream: is stream response\n        :param user: unique user id\n        :return: full response or stream response chunk generator result\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Parsing Tool Invoke Responses in Python\nDESCRIPTION: This code snippet provides a function to parse the response from invoking a tool using the Dify SDK. It iterates through the generator output of the `self.session.tool.invoke()` function and converts each message type (TEXT, LINK, IMAGE_LINK, IMAGE, JSON) into a string. The resulting string represents the consolidated response from the tool invocation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/agent.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom collections.abc import Generator\nfrom typing import cast\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.tool import ToolInvokeMessage\n\ndef parse_invoke_response(tool_invoke_responses: Generator[AgentInvokeMessage]) -> str:\n    result = \"\"\n    for response in tool_invoke_responses:\n        if response.type == ToolInvokeMessage.MessageType.TEXT:\n            result += cast(ToolInvokeMessage.TextMessage, response.message).text\n        elif response.type == ToolInvokeMessage.MessageType.LINK:\n            result += (\n                f\"result link: {cast(ToolInvokeMessage.TextMessage, response.message).text}.\"\n                + \" please tell user to check it.\"\n            )\n        elif response.type in {\n            ToolInvokeMessage.MessageType.IMAGE_LINK,\n            ToolInvokeMessage.MessageType.IMAGE,\n        }:\n            result += (\n                \"image has been created and sent to user already, \"\n                + \"you do not need to create it, just tell the user to check it now.\"\n            )\n        elif response.type == ToolInvokeMessage.MessageType.JSON:\n            text = json.dumps(cast(ToolInvokeMessage.JsonMessage, response.message).json_object, ensure_ascii=False)\n            result += f\"tool response: {text}.\"\n        else:\n            result += f\"tool response: {response.message!r}.\"\n    return result'\n```\n\n----------------------------------------\n\nTITLE: Simplified LLM Invocation with Stream Handling - Python\nDESCRIPTION: This simplified example shows how to handle both stream and sync responses within the `_invoke` method. It checks the `stream` flag and calls the appropriate handler function.  The `_handle_stream_response` function uses `yield` to return a generator, while `_handle_sync_response` returns an `LLMResult` object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/customizable-model.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, stream: bool, **kwargs) \\\n            -> Union[LLMResult, Generator]:\n        if stream:\n              return self._handle_stream_response(**kwargs)\n        return self._handle_sync_response(**kwargs)\n\n    def _handle_stream_response(self, **kwargs) -> Generator:\n        for chunk in response:\n              yield chunk\n    def _handle_sync_response(self, **kwargs) -> LLMResult:\n        return LLMResult(**response)\n```\n\n----------------------------------------\n\nTITLE: Invoking Tools in Python with Dify SDK\nDESCRIPTION: This code snippet showcases how to invoke a tool using the Dify SDK. It retrieves necessary parameters, prepares tool instances, and calls the `self.session.tool.invoke()` function. It emphasizes how to integrate the output from LLM, specifically `tool_call_name` and `tool_call_args`, into the tool invocation process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/agent.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dify_plugin.entities.tool import ToolProviderType\n\nclass FunctionCallingAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        \"\"\"\n        Run FunctionCall agent application\n        \"\"\"\n        fc_params = FunctionCallingParams(**parameters)\n        \n        # tool_call_name and tool_call_args parameter is obtained from the output of LLM\n        tool_instances = {tool.identity.name: tool for tool in fc_params.tools} if fc_params.tools else {}\n        tool_instance = tool_instances[tool_call_name]\n        tool_invoke_responses = self.session.tool.invoke(\n            provider_type=ToolProviderType.BUILT_IN,\n            provider=tool_instance.identity.provider,\n            tool_name=tool_instance.identity.name,\n            # add the default value\n            parameters={**tool_instance.runtime_parameters, **tool_call_args},\n        )\n```\n\n----------------------------------------\n\nTITLE: Handling Stream and Sync Responses\nDESCRIPTION: Demonstrates the implementation of handling both stream and synchronous responses within the _invoke method. It uses separate functions to manage each type of response. Python's 'yield' keyword is used to define the generator function for the stream response, while a standard return is used for the sync response.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/customizable-model.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, stream: bool, **kwargs) \\\n            -> Union[LLMResult, Generator]:\n        if stream:\n              return self._handle_stream_response(**kwargs)\n        return self._handle_sync_response(**kwargs)\n\ndef _handle_stream_response(self, **kwargs) -> Generator:\n    for chunk in response:\n          yield chunk\ndef _handle_sync_response(self, **kwargs) -> LLMResult:\n    return LLMResult(**response)\n```\n\n----------------------------------------\n\nTITLE: Setting a Key-Value Pair in Storage (Python)\nDESCRIPTION: This code snippet shows the `set` method used to store data in the persistent storage. It takes a key (string) and a value (bytes) as input.  The bytes type allows storing any binary data, including files.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/persistent-storage.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef set(self, key: str, val: bytes) -> None:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Deleting a Document using Dify API\nDESCRIPTION: Deletes a specific document from a dataset within the Dify knowledge base.  Requires the dataset ID and document ID as path parameters and a valid API key in the Authorization header.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Extracting Tool Calls from LLM Result Python\nDESCRIPTION: This Python code defines the `extract_tool_calls` method. It extracts tool call information (tool_call_id, tool_call_name, and tool_call_args) from an LLM result chunk. It iterates through the tool calls in `llm_result_chunk.delta.message.tool_calls` and returns a list of tuples, where each tuple represents a tool call.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n    def extract_tool_calls(\n        self, llm_result_chunk: LLMResultChunk\n    ) -> list[tuple[str, str, dict[str, Any]]]:\n        \"\"\"\n        Extract tool calls from llm result chunk\n\n        Returns:\n            List[Tuple[str, str, Dict[str, Any]]]: [(tool_call_id, tool_call_name, tool_call_args)]\n        \"\"\"\n        tool_calls = []\n        for prompt_message in llm_result_chunk.delta.message.tool_calls:\n            args = {}\n\n```\n\n----------------------------------------\n\nTITLE: Implementing the Custom Tool Logic (Python)\nDESCRIPTION: This Python code defines the `WeatherSearch` class, which inherits from `ExternalDataTool`. It includes the `validate_config` method for validating the configuration schema and the `query` method for executing the weather search logic based on user inputs and configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/code-based-extension/external-data-tool.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nfrom core.external_data_tool.base import ExternalDataTool\n\n\nclass WeatherSearch(ExternalDataTool):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"weather_search\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user save the config.\n\n        Example:\n            .. code-block:: python\n                config = {\n                    \"temperature_unit\": \"centigrade\"\n                }\n\n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n\n        if not config.get('temperature_unit'):\n            raise ValueError('temperature unit is required')\n\n    def query(self, inputs: dict, query: Optional[str] = None) -> str:\n        \"\"\"\n        Query the external data tool.\n\n        :param inputs: user inputs\n        :param query: the query of chat app\n        :return: the tool query result\n        \"\"\"\n        city = inputs.get('city')\n        temperature_unit = self.config.get('temperature_unit')\n\n        if temperature_unit == 'fahrenheit':\n            return f'Weather in {city} is 32°F'\n        else:\n            return f'Weather in {city} is 0°C'\n```\n\n----------------------------------------\n\nTITLE: Define provider_credential_schema for Model Name\nDESCRIPTION: This YAML snippet defines the credential schema for 'model_name'. It mandates users to input the name of their chosen model. This parameter is required for identification and invocation purposes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n  - variable: model_name\n    type: text-input\n    label:\n      en_US: Model name\n      zh_Hans: 模型名称\n    required: true\n    placeholder:\n      zh_Hans: 填写模型名称\n      en_US: Input model name\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Arguments in Python\nDESCRIPTION: This code snippet demonstrates how to parse JSON arguments received from a prompt message within a Dify plugin. It checks if the 'arguments' field in the 'function' object of 'prompt_message' is not empty, and then uses the 'json.loads' method to parse the JSON string into a Python dictionary named 'args'. The parsed arguments, along with the message ID and function name, are then appended to a list called 'tool_calls'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nif prompt_message.function.arguments != \"\":\n  args = json.loads(prompt_message.function.arguments)\n\n            tool_calls.append(\n                (\n                    prompt_message.id,\n                    prompt_message.function.name,\n                    args,\n                )\n            )\n```\n\n----------------------------------------\n\nTITLE: Fetch Custom Model Schema in Python\nDESCRIPTION: This Python code snippet presents the `get_customizable_model_schema` method, which allows custom models to fetch their schema.  When the provider supports adding custom LLMs, this method can be implemented. It takes model details and credentials as input and returns the model schema.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> Optional[AIModelEntity]:\n    \"\"\"\n    Get customizable model schema\n\n    :param model: model name\n    :param credentials: model credentials\n    :return: model schema\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Agent Strategy Plugin Implementation with Tool Handling (Python)\nDESCRIPTION: This Python code defines an Agent strategy plugin responsible for invoking language models and handling tool calls. It defines parameter classes, the Agent strategy, and methods for converting tools to prompt messages, checking for tool calls, and extracting tool call information. The strategy handles LLM responses, including text and tool calls, and converts tools to prompt message tools for the LLM. It then invokes the appropriate tools and returns the result.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom collections.abc import Generator\nfrom typing import Any, cast\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.model.llm import LLMModelConfig, LLMResult, LLMResultChunk\nfrom dify_plugin.entities.model.message import (\n    PromptMessageTool,\n    UserPromptMessage,\n)\nfrom dify_plugin.entities.tool import ToolInvokeMessage, ToolParameter, ToolProviderType\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n        chunks: Generator[LLMResultChunk, None, None] | LLMResult = (\n            self.session.model.llm.invoke(\n                model_config=LLMModelConfig(**params.model.model_dump(mode=\"json\")),\n                prompt_messages=[UserPromptMessage(content=params.query)],\n                tools=[\n                    self._convert_tool_to_prompt_message_tool(tool)\n                    for tool in params.tools\n                ],\n                stop=params.model.completion_params.get(\"stop\", [])\n                if params.model.completion_params\n                else [],\n                stream=True,\n            )\n        )\n        response = \"\"\n        tool_calls = []\n        tool_instances = (\n            {tool.identity.name: tool for tool in params.tools} if params.tools else {}\n        )\n\n        for chunk in chunks:\n            # check if there is any tool call\n            if self.check_tool_calls(chunk):\n                tool_calls = self.extract_tool_calls(chunk)\n                tool_call_names = \";\".join([tool_call[1] for tool_call in tool_calls])\n                try:\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls},\n                        ensure_ascii=False,\n                    )\n                except json.JSONDecodeError:\n                    # ensure ascii to avoid encoding error\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls}\n                    )\n                print(tool_call_names, tool_call_inputs)\n            if chunk.delta.message and chunk.delta.message.content:\n                if isinstance(chunk.delta.message.content, list):\n                    for content in chunk.delta.message.content:\n                        response += content.data\n                        print(content.data, end=\"\", flush=True)\n                else:\n                    response += str(chunk.delta.message.content)\n                    print(str(chunk.delta.message.content), end=\"\", flush=True)\n\n            if chunk.delta.usage:\n                # usage of the model\n                usage = chunk.delta.usage\n\n        yield self.create_text_message(\n            text=f\"{response or json.dumps(tool_calls, ensure_ascii=False)}\\n\"\n        )\n        result = \"\"\n        for tool_call_id, tool_call_name, tool_call_args in tool_calls:\n            tool_instance = tool_instances[tool_call_name]\n            tool_invoke_responses = self.session.tool.invoke(\n                provider_type=ToolProviderType.BUILT_IN,\n                provider=tool_instance.identity.provider,\n                tool_name=tool_instance.identity.name,\n                parameters={**tool_instance.runtime_parameters, **tool_call_args},\n            )\n            if not tool_instance:\n                tool_invoke_responses = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": f\"there is not a tool named {tool_call_name}\",\n                }\n            else:\n                # invoke tool\n                tool_invoke_responses = self.session.tool.invoke(\n                    provider_type=ToolProviderType.BUILT_IN,\n                    provider=tool_instance.identity.provider,\n                    tool_name=tool_instance.identity.name,\n                    parameters={**tool_instance.runtime_parameters, **tool_call_args},\n                )\n                result = \"\"\n                for tool_invoke_response in tool_invoke_responses:\n                    if tool_invoke_response.type == ToolInvokeMessage.MessageType.TEXT:\n                        result += cast(\n                            ToolInvokeMessage.TextMessage, tool_invoke_response.message\n                        ).text\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.LINK\n                    ):\n                        result += (\n                            f\"result link: {cast(ToolInvokeMessage.TextMessage, tool_invoke_response.message).text}.\"\n                            + \" please tell user to check it.\"\n                        )\n                    elif tool_invoke_response.type in {\n                        ToolInvokeMessage.MessageType.IMAGE_LINK,\n                        ToolInvokeMessage.MessageType.IMAGE,\n                    }:\n                        result += (\n                            \"image has been created and sent to user already, \"\n                            + \"you do not need to create it, just tell the user to check it now.\"\n                        )\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.JSON\n                    ):\n                        text = json.dumps(\n                            cast(\n                                ToolInvokeMessage.JsonMessage,\n                                tool_invoke_response.message,\n                            ).json_object,\n                            ensure_ascii=False,\n                        )\n                        result += f\"tool response: {text}.\"\n                    else:\n                        result += f\"tool response: {tool_invoke_response.message!r}.\"\n\n                tool_response = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": result,\n                }\n        yield self.create_text_message(result)\n\n    def _convert_tool_to_prompt_message_tool(\n        self, tool: ToolEntity\n    ) -> PromptMessageTool:\n        \"\"\"\n        convert tool to prompt message tool\n        \"\"\"\n        message_tool = PromptMessageTool(\n            name=tool.identity.name,\n            description=tool.description.llm if tool.description else \"\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        )\n\n        parameters = tool.parameters\n        for parameter in parameters:\n            if parameter.form != ToolParameter.ToolParameterForm.LLM:\n                continue\n\n            parameter_type = parameter.type\n            if parameter.type in {\n                ToolParameter.ToolParameterType.FILE,\n                ToolParameter.ToolParameterType.FILES,\n            }:\n                continue\n            enum = []\n            if parameter.type == ToolParameter.ToolParameterType.SELECT:\n                enum = (\n                    [option.value for option in parameter.options]\n                    if parameter.options\n                    else []\n                )\n\n            message_tool.parameters[\"properties\"][parameter.name] = {\n                \"type\": parameter_type,\n                \"description\": parameter.llm_description or \"\",\n            }\n\n            if len(enum) > 0:\n                message_tool.parameters[\"properties\"][parameter.name][\"enum\"] = enum\n\n            if parameter.required:\n                message_tool.parameters[\"required\"].append(parameter.name)\n\n        return message_tool\n\n    def check_tool_calls(self, llm_result_chunk: LLMResultChunk) -> bool:\n        \"\"\"\n        Check if there is any tool call in llm result chunk\n        \"\"\"\n        return bool(llm_result_chunk.delta.message.tool_calls)\n\n    def extract_tool_calls(\n        self, llm_result_chunk: LLMResultChunk\n    ) -> list[tuple[str, str, dict[str, Any]]]:\n        \"\"\"\n        Extract tool calls from llm result chunk\n\n        Returns:\n            List[Tuple[str, str, Dict[str, Any]]]: [(tool_call_id, tool_call_name, tool_call_args)]\n        \"\"\"\n        tool_calls = []\n        for prompt_message in llm_result_chunk.delta.message.tool_calls:\n            args = {}\n            if prompt_message.function.arguments != \"\":\n                args = json.loads(prompt_message.function.arguments)\n\n            tool_calls.append(\n                (\n                    prompt_message.id,\n                    prompt_message.function.name,\n                    args,\n                )\n            )\n\n        return tool_calls\n```\n\n----------------------------------------\n\nTITLE: Retrieve Knowledge from AWS Bedrock (Python)\nDESCRIPTION: This service class uses the boto3 library to interact with AWS Bedrock and retrieve knowledge. The `knowledge_retrieval` method takes retrieval settings, a query, and a knowledge ID as input, calls the Bedrock API, and parses the response to extract relevant information, including metadata, score, title, and content. It requires AWS credentials (access key, secret key, region name) to be configured.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/use-cases/how-to-connect-aws-bedrock.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\n\n\nclass ExternalDatasetService:\n    @staticmethod\n    def knowledge_retrieval(retrieval_setting: dict, query: str, knowledge_id: str):\n        # get bedrock client\n        client = boto3.client(\n            \"bedrock-agent-runtime\",\n            aws_secret_access_key=\"AWS_SECRET_ACCESS_KEY\",\n            aws_access_key_id=\"AWS_ACCESS_KEY_ID\",\n            # example: us-east-1\n            region_name=\"AWS_REGION_NAME\",\n        )\n        # fetch external knowledge retrieval\n        response = client.retrieve(\n            knowledgeBaseId=knowledge_id,\n            retrievalConfiguration={\n                \"vectorSearchConfiguration\": {\"numberOfResults\": retrieval_setting.get(\"top_k\"), \"overrideSearchType\": \"HYBRID\"}\n            },\n            retrievalQuery={\"text\": query},\n        )\n        # parse response\n        results = []\n        if response.get(\"ResponseMetadata\") and response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") == 200:\n            if response.get(\"retrievalResults\"):\n                retrieval_results = response.get(\"retrievalResults\")\n                for retrieval_result in retrieval_results:\n                    # filter out results with score less than threshold\n                    if retrieval_result.get(\"score\") < retrieval_setting.get(\"score_threshold\", .0):\n                        continue\n                    result = {\n                        \"metadata\": retrieval_result.get(\"metadata\"),\n                        \"score\": retrieval_result.get(\"score\"),\n                        \"title\": retrieval_result.get(\"metadata\").get(\"x-amz-bedrock-kb-source-uri\"),\n                        \"content\": retrieval_result.get(\"content\").get(\"text\"),\n                    }\n                    results.append(result)\n        return {\n            \"records\": results\n        }\n```\n\n----------------------------------------\n\nTITLE: Celery Broker URL Configuration (Redis Direct)\nDESCRIPTION: This snippet demonstrates the format for configuring the Celery broker URL using a direct connection to Redis.  It includes placeholders for the Redis username, password, host, port, and database number.  The database number should differ from those used for Session Redis and the Redis cache.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/environments.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nredis://<redis_username>:<redis_password>@<redis_host>:<redis_port>/<redis_database>\n```\n\n----------------------------------------\n\nTITLE: Image Message Creation in Dify (Python)\nDESCRIPTION: This Python code snippet defines the interface for creating an image message within the Dify tool plugin framework. It takes an image URL as input and returns a ToolInvokeMessage object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/tool.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef create_image_message(self, image: str) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Direct Reply Node Markdown Guide\nDESCRIPTION: Demonstrates how to use Markdown formatting within a direct reply node in Dify to provide a helpful message directing users to the help documentation. This includes a link to the documentation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/workshop/intermediate/customer-service-bot.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nI'm sorry, I can't answer your question. If you need more help, please check the [help documentation](https://docs.dify.ai).\n```\n\n----------------------------------------\n\nTITLE: Model Credential Validation in Python\nDESCRIPTION: This Python code snippet illustrates how to validate model credentials. The `validate_credentials` method receives the model name and a dictionary of credentials.  It should raise an exception if the validation fails. The credentials are defined by either the `provider_credential_schema` or `model_credential_schema`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n    \"\"\"\n    Validate model credentials\n\n    :param model: model name\n    :param credentials: model credentials\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Dynamic Model Parameter Schema - Python\nDESCRIPTION: This function dynamically generates a parameter schema for the model.  It defines customizable parameters such as temperature, top_p, and max_tokens. It adapts the schema based on the model's capabilities, such as adding top_k only for certain models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> AIModelEntity | None:\n    \"\"\"\n        used to define customizable model schema\n    \"\"\"\n    rules = [\n        ParameterRule(\n            name='temperature', type=ParameterType.FLOAT,\n            use_template='temperature',\n            label=I18nObject(\n                zh_Hans='温度', en_US='Temperature'\n            )\n        ),\n        ParameterRule(\n            name='top_p', type=ParameterType.FLOAT,\n            use_template='top_p',\n            label=I18nObject(\n                zh_Hans='Top P', en_US='Top P'\n            )\n        ),\n        ParameterRule(\n            name='max_tokens', type=ParameterType.INT,\n            use_template='max_tokens',\n            min=1,\n            default=512,\n            label=I18nObject(\n                zh_Hans='最大生成长度', en_US='Max Tokens'\n            )\n        )\n    ]\n\n    # if model is A, add top_k to rules\n    if model == 'A':\n        rules.append(\n            ParameterRule(\n                name='top_k', type=ParameterType.INT,\n                use_template='top_k',\n                min=1,\n                default=50,\n                label=I18nObject(\n                    zh_Hans='Top K', en_US='Top K'\n                )\n            )\n        )\n\n    \"\"\"\n        some NOT IMPORTANT code here\n    \"\"\"\n\n    entity = AIModelEntity(\n        model=model,\n        label=I18nObject(\n            en_US=model\n        ),\n        fetch_from=FetchFrom.CUSTOMIZABLE_MODEL,\n        model_type=model_type,\n        model_properties={ \n            ModelPropertyKey.MODE:  ModelType.LLM,\n        },\n        parameter_rules=rules\n    )\n\n    return entity\n```\n\n----------------------------------------\n\nTITLE: Dynamic Model Parameter Schema Generation - Python\nDESCRIPTION: This snippet demonstrates how to dynamically generate the model parameter schema using `get_customizable_model_schema`. It creates `ParameterRule` objects for common parameters like `temperature`, `top_p`, and `max_tokens`, and conditionally adds parameters like `top_k` based on the model being used.  The schema is then used to create an `AIModelEntity`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/customizable-model.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> AIModelEntity | None:\n        \"\"\"\n            Used to define customizable model schema\n        \"\"\"\n        rules = [\n            ParameterRule(\n                name='temperature', type=ParameterType.FLOAT,\n                use_template='temperature',\n                label=I18nObject(\n                    zh_Hans='温度', en_US='Temperature'\n                )\n            ),\n            ParameterRule(\n                name='top_p', type=ParameterType.FLOAT,\n                use_template='top_p',\n                label=I18nObject(\n                    zh_Hans='Top P', en_US='Top P'\n                )\n            ),\n            ParameterRule(\n                name='max_tokens', type=ParameterType.INT,\n                use_template='max_tokens',\n                min=1,\n                default=512,\n                label=I18nObject(\n                    zh_Hans='最大生成长度', en_US='Max Tokens'\n                )\n            )\n        ]\n\n        # if model is A, add top_k to rules\n        if model == 'A':\n            rules.append(\n                ParameterRule(\n                    name='top_k', type=ParameterType.INT,\n                    use_template='top_k',\n                    min=1,\n                    default=50,\n                    label=I18nObject(\n                        zh_Hans='Top K', en_US='Top K'\n                    )\n                )\n            )\n\n        \"\"\"\n            some NOT IMPORTANT code here\n        \"\"\"\n\n        entity = AIModelEntity(\n            model=model,\n            label=I18nObject(\n                en_US=model\n            ),\n            fetch_from=FetchFrom.CUSTOMIZABLE_MODEL,\n            model_type=model_type,\n            model_properties={ \n                ModelPropertyKey.MODE:  ModelType.LLM,\n            },\n            parameter_rules=rules\n        )\n\n        return entity\n```\n\n----------------------------------------\n\nTITLE: LLM Invocation Method\nDESCRIPTION: Implements the core LLM invocation method, supporting both streaming and synchronous responses. This method takes model credentials, prompt messages, model parameters, and other optional parameters as input. It returns either a full response or a stream response chunk generator result.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/customizable-model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            prompt_messages: list[PromptMessage], model_parameters: dict,\n            tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,\n            stream: bool = True, user: Optional[str] = None) \\\n            -> Union[LLMResult, Generator]:\n        \"\"\"\n        Invoke large language model\n\n        :param model: model name\n        :param credentials: model credentials\n        :param prompt_messages: prompt messages\n        :param model_parameters: model parameters\n        :param tools: tools for tool calling\n        :param stop: stop words\n        :param stream: is stream response\n        :param user: unique user id\n        :return: full response or stream response chunk generator result\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: JSON Validation Code in Dify Workflow (Code Node)\nDESCRIPTION: This code snippet shows Python code for validating JSON content within a Dify workflow's code node. The `main` function takes a JSON string as input, parses it using `json.loads`, and returns the parsed object within a dictionary labeled `result`.  This is used to verify if the generated JSON is in correct format. If not an error handling mechanism will be triggered.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef main(json_str: str) -> dict:\n    obj = json.loads(json_str)\n    return {'result': obj}\n```\n\n----------------------------------------\n\nTITLE: Function Calling Agent Strategy Invoke Method in Python\nDESCRIPTION: This code snippet demonstrates the `_invoke` method within a `FunctionCallingAgentStrategy` class. It first creates and yields a 'Thinking' log message. Then, it invokes an LLM to process a query, finishes the 'Thinking' log, and yields the LLM's response as a text message.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/agent.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass FunctionCallingAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        thinking_log = self.create_log_message(\n            data={\"Query\": parameters.get(\"query\")},\n            label=\"Thinking\",\n            status=AgentInvokeMessage.LogMessage.LogStatus.START,\n        )\n\n        yield thinking_log\n\n        llm_response = self.session.model.llm.invoke(\n            model_config=LLMModelConfig(\n                provider=\"openai\",\n                model=\"gpt-4o-mini\",\n                mode=\"chat\",\n                completion_params={},\n            ),\n            prompt_messages=[\n                SystemPromptMessage(content=\"you are a helpful assistant\"),\n                UserPromptMessage(content=parameters.get(\"query\")),\n            ],\n            stream=False,\n            tools=[],\n        )\n\n        thinking_log = self.finish_log_message(log=thinking_log)\n        yield thinking_log\n        yield self.create_text_message(text=llm_response.message.content)\n```\n\n----------------------------------------\n\nTITLE: System Prompt Template for Chat Models (Conversational Apps)\nDESCRIPTION: This prompt template is used for chat models to build conversational applications. It includes context, pre-prompt, and instructs the model on how to respond to user queries, including handling unknown information and language consistency. The template uses XML tags for context and placeholders for dynamic content.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/prompt-engineering/prompt-template.md#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nUse the following context as your learned knowledge, inside <context></context> XML tags.\n\n<context>\n{{#context#}}\n</context>\n\nWhen answer to user:\n- If you don't know, just say that you don't know.\n- If you don't know when you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n{{pre_prompt}}\n```\n\n----------------------------------------\n\nTITLE: Implement External Data Tool Class Template - Python\nDESCRIPTION: Provides a code template for implementing a custom external data tool class in Python.  It highlights the structure with `validate_config` for validating form parameters and `query` for handling data retrieval. The example shows how to stub the methods, allowing developers to insert their custom logic. This includes handling user inputs and potentially incorporating the current conversation context.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/code-based-extension/external-data-tool.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nfrom core.external_data_tool.base import ExternalDataTool\n\n\nclass WeatherSearch(ExternalDataTool):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"weather_search\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user save the config.\n\n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n\n        # implement your own logic here\n\n    def query(self, inputs: dict, query: Optional[str] = None) -> str:\n        \"\"\"\n        Query the external data tool.\n\n        :param inputs: user inputs\n        :param query: the query of chat app\n        :return: the tool query result\n        \"\"\"\n       \n        # implement your own logic here\n        return \"your own data.\"\n```\n\n----------------------------------------\n\nTITLE: Request Body Example\nDESCRIPTION: This snippet shows the JSON format for the request body. It includes the knowledge_id, query, and retrieval_setting properties. The retrieval_setting object includes top_k (maximum results) and score_threshold (relevance score).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/external-knowledge-api-documentation.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"knowledge_id\": \"your-knowledge-id\",\n    \"query\": \"your question\",\n    \"retrieval_setting\":{\n        \"top_k\": 2,\n        \"score_threshold\": 0.5\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: OpenAI Model Credential Schema (YAML)\nDESCRIPTION: Defines the model credential schema for the OpenAI family of models in a YAML file.  It includes fields for the API key, organization ID, and API base URL, allowing users to configure these credentials for each model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_credential_schema:\nmodel:\n label:\n   en_US: Model Name\n placeholder:\n   en_US: Enter your model name\n credential_form_schemas:\n   - variable: openai_api_key\n     label:\n       en_US: API Key\n     type: secret-input\n     required: true\n     placeholder:\n       en_US: Enter your API Key\n   - variable: openai_organization\n     label:\n       en_US: Organization\n     type: text-input\n     required: false\n     placeholder:\n       en_US: Enter your Organization ID\n   - variable: openai_api_base\n     label:\n       en_US: API Base\n     type: text-input\n     required: false\n     placeholder:\n       en_US: Enter your API Base\n```\n\n----------------------------------------\n\nTITLE: Install FastAPI Dependencies (Shell)\nDESCRIPTION: Command to install FastAPI and Uvicorn, used for building the example API. Uvicorn is required for running the example.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/README.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\npip install 'fastapi[all]' uvicorn\n```\n\n----------------------------------------\n\nTITLE: Implement External Data Tool Class - Python\nDESCRIPTION: Implements the `WeatherSearch` class, extending `ExternalDataTool`, to define the business logic for the custom tool. It includes the `validate_config` method for validating user inputs and the `query` method for fetching and processing data. The `name` attribute must match the directory and file name, ensuring Dify correctly identifies the tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/code-based-extension/external-data-tool.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nfrom core.external_data_tool.base import ExternalDataTool\n\n\nclass WeatherSearch(ExternalDataTool):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"weather_search\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user save the config.\n\n        Example:\n            .. code-block:: python\n                config = {\n                    \"temperature_unit\": \"centigrade\"\n                }\n\n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n\n        if not config.get('temperature_unit'):\n            raise ValueError('temperature unit is required')\n\n    def query(self, inputs: dict, query: Optional[str] = None) -> str:\n        \"\"\"\n        Query the external data tool.\n\n        :param inputs: user inputs\n        :param query: the query of chat app\n        :return: the tool query result\n        \"\"\"\n        city = inputs.get('city')\n        temperature_unit = self.config.get('temperature_unit')\n\n        if temperature_unit == 'fahrenheit':\n            return f'Weather in {city} is 32°F'\n        else:\n            return f'Weather in {city} is 0°C'\n```\n\n----------------------------------------\n\nTITLE: Creating Link Message in Dify (Python)\nDESCRIPTION: This snippet shows how to create a link message in Dify's `Tool` class. It accepts a URL and returns a `ToolInvokeMessage`. The optional `save_as` parameter does not appear to apply to links and should be ignored.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/advanced-tool-integration.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    def create_link_message(self, link: str, save_as: str = '') -> ToolInvokeMessage:\n        \"\"\"\n            create a link message\n\n            :param link: the url of the link\n            :return: the link message\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Content Moderation Output Response Overridden Example\nDESCRIPTION: Shows the response format from the `app.moderation.output` extension point when the `action` is `overridden`.  The `text` field contains the modified LLM output to replace the original response. This allows for sanitizing or filtering of inappropriate language.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"overridden\",\n    \"text\": \"I will *** you.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Text Generation API call with cURL\nDESCRIPTION: This cURL command demonstrates how to call the text generation API endpoint (`/v1/completion-messages`). It sends a POST request with the `Authorization` and `Content-Type` headers, along with a JSON payload containing input parameters, the response mode, and a user identifier. Replace `ENTER-YOUR-SECRET-KEY` with your actual API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-publishing/developing-with-apis.md#_snippet_0\n\nLANGUAGE: cURL\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/completion-messages' \\\n--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"inputs\": {},\n    \"response_mode\": \"streaming\",\n    \"user\": \"abc-123\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Create Empty Knowledge Base - Dify API (cURL)\nDESCRIPTION: This cURL command creates an empty knowledge base in Dify. It requires the API key for authentication and authorization. The request body includes the name and permission settings for the new knowledge base.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"name\": \"name\", \"permission\": \"only_me\"}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Dify Chatbot with JavaScript\nDESCRIPTION: This JavaScript code snippet configures the Dify chatbot, including the token, development mode, base URL, container properties, draggable behavior, drag axis, system variables, and input variables.  It demonstrates how to customize the chatbot's behavior and appearance.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-publishing/embedding-in-websites.md#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.difyChatbotConfig = {\n    // 必填项，由 Dify 自动生成\n    token: 'YOUR_TOKEN',\n    // 可选项，默认为 false\n    isDev: false,\n    // 可选项，当 isDev 为 true 时，默认为 '[https://dev.udify.app](https://dev.udify.app)'，否则默认为 '[https://udify.app](https://udify.app)'\n    baseUrl: 'YOUR_BASE_URL',\n    // 可选项，可以接受除 `id` 以外的任何有效的 HTMLElement 属性，例如 `style`、`className` 等\n    containerProps: {},\n    // 可选项，是否允许拖动按钮，默认为 `false`\n    draggable: false,\n    // 可选项，允许拖动按钮的轴，默认为 `both`，可以是 `x`、`y`、`both`\n    dragAxis: 'both',\n    // 可选项，在 dify 聊天机器人中设置的系统变量对象\n    systemVariables: {\n        // 键是系统变量名\n        // 例如：\n        // user_id: \"YOU CAN DEFINE USER ID HERE\",\n        // conversation_id: \"YOU CAN DEFINE CONVERSATION ID HERE, IT MUST BE A VALID UUID\"\n    },\n    // 可选项，在 dify 聊天机器人中设置的输入对象\n    inputs: {\n        // 键是变量名\n        // 例如：\n        // name: \"NAME\"\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Invoking Text Embedding Model in Python\nDESCRIPTION: This Python code defines the `_invoke` method for invoking a text embedding model.  It takes the model name, credentials, a list of texts, and user identifier as input and returns a `TextEmbeddingResult`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            texts: list[str], user: Optional[str] = None) \\\n        -> TextEmbeddingResult:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param texts: texts to embed\n    :param user: unique user id\n    :return: embeddings result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Mapping Model Invoke Errors in Python\nDESCRIPTION: This Python code defines the `_invoke_error_mapping` property for mapping model invocation errors to a unified error type.  The dictionary maps `InvokeError` types to lists of `Exception` types that the model might throw.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Plugin Signature (bash)\nDESCRIPTION: This command verifies that the plugin has been correctly signed using the specified public key. It takes the signed plugin file and the public key file as input. If the public key is omitted, the Dify Marketplace public key will be used. Requires dify CLI tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndify signature verify your_plugin_project.signed.difypkg -p your_key_pair.public.pem\n```\n\n----------------------------------------\n\nTITLE: Text Generation API call with Python\nDESCRIPTION: This Python script demonstrates how to call the text generation API endpoint (`/v1/completion-messages`) using the `requests` library. It constructs the necessary headers, including the `Authorization` token and `Content-Type`, and sends a POST request with a JSON payload. The script requires the `requests` and `json` libraries.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-publishing/developing-with-apis.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport requests\nimport json\n\nurl = \"https://api.dify.ai/v1/completion-messages\"\n\nheaders = {\n    'Authorization': 'Bearer ENTER-YOUR-SECRET-KEY',\n    'Content-Type': 'application/json',\n}\n\ndata = {\n    \"inputs\": {\"text\": 'Hello, how are you?'},\n    \"response_mode\": \"streaming\",\n    \"user\": \"abc-123\"\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Implementing Weather Search Template Logic Python\nDESCRIPTION: This Python code defines the `WeatherSearch` class template which can be implemented by the user. It provides a structure for defining the custom data tool, including methods for `validate_config` and `query`. The `validate_config` method is intended for validating the configuration schema, and the `query` method for performing the external data lookup based on user inputs. The template emphasizes the 'name' field's uniqueness and the importance of matching the directory and file names.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/code-based-extension/external-data-tool.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nfrom core.external_data_tool.base import ExternalDataTool\n\n\nclass WeatherSearch(ExternalDataTool):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"weather_search\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user save the config.\n\n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n\n        # implement your own logic here\n\n    def query(self, inputs: dict, query: Optional[str] = None) -> str:\n        \"\"\"\n        Query the external data tool.\n\n        :param inputs: user inputs\n        :param query: the query of chat app\n        :return: the tool query result\n        \"\"\"\n       \n        # implement your own logic here\n        return \"your own data.\"\n```\n\n----------------------------------------\n\nTITLE: LLM Invocation Method - Python\nDESCRIPTION: This is the core method for invoking the large language model. It supports both streaming and synchronous responses. Key parameters include 'model', 'credentials', 'prompt_messages', 'model_parameters', 'tools', 'stop', 'stream', and 'user'. The function returns a full response or a chunk generator, depending on the stream parameter.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(\n    self,\n    model: str,\n    credentials: dict,\n    prompt_messages: list[PromptMessage],\n    model_parameters: dict,\n    tools: Optional[list[PromptMessageTool]] = None,\n    stop: Optional[list[str]] = None,\n    stream: bool = True,\n    user: Optional[str] = None\n) -> Union[LLMResult, Generator]:\n    \"\"\"\n    Invoke the large language model.\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param model_parameters: model parameters\n    :param tools: tools for tool calling\n    :param stop: stop words\n    :param stream: determines if response is streamed\n    :param user: unique user id\n    :return: full response or a chunk generator\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invocation Error Mapping in Python\nDESCRIPTION: This Python code defines a property, `_invoke_error_mapping`, which maps model invocation errors to unified error types. This mapping is crucial for Dify's error handling, allowing it to take appropriate follow-up actions based on the type of error encountered during model invocation.  The key is the error type to be thrown to the caller, and the value is the error type thrown by the model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining LLMUsage Model in Python\nDESCRIPTION: Defines the `LLMUsage` model, providing details about the usage of an LLM. It includes token counts (prompt, completion, total), pricing information (unit price, price unit, cost), currency, and request latency.  Requires Decimal and ModelUsage.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nclass LLMUsage(ModelUsage):\n    \"\"\"\n    Model class for LLM usage.\n    \"\"\"\n    prompt_tokens: int  # Tokens used for prompt\n    prompt_unit_price: Decimal  # Unit price for prompt\n    prompt_price_unit: Decimal  # Price unit for prompt, i.e., the unit price based on how many tokens\n    prompt_price: Decimal  # Cost for prompt\n    completion_tokens: int  # Tokens used for response\n    completion_unit_price: Decimal  # Unit price for response\n    completion_price_unit: Decimal  # Price unit for response, i.e., the unit price based on how many tokens\n    completion_price: Decimal  # Cost for response\n    total_tokens: int  # Total number of tokens used\n    total_price: Decimal  # Total cost\n    currency: str  # Currency unit\n    latency: float  # Request latency (s)\n```\n\n----------------------------------------\n\nTITLE: Extracting Data from JSON using Python in Dify\nDESCRIPTION: This Python code snippet demonstrates how to extract the `data.name` field from a JSON string received from an HTTP node within a Dify workflow. It uses the `json` library to parse the string and returns the specified field in a dictionary with the key `result`. The function expects a string as input and returns a dictionary containing the extracted data.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_66\n\nLANGUAGE: Python\nCODE:\n```\ndef main(http_response: str) -> str:\n    import json\n    data = json.loads(http_response)\n    return {\n        # Note to declare 'result' in the output variables\n        'result': data['data']['name']\n    }\n```\n\n----------------------------------------\n\nTITLE: Calling Chat Messages API with Python\nDESCRIPTION: This Python script uses the `requests` library to interact with the `/v1/chat-messages` endpoint for conversational applications. It sets the necessary headers (Authorization and Content-Type) and sends a JSON payload including `inputs`, a `query`, the `response_mode`, a `conversation_id`, and a `user` identifier. The `conversation_id` is crucial for maintaining context across multiple turns. Replace `ENTER-YOUR-SECRET-KEY` with your actual API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-publishing/developing-with-apis.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport requests\nimport json\n\nurl = 'https://api.dify.ai/v1/chat-messages'\nheaders = {\n    'Authorization': 'Bearer ENTER-YOUR-SECRET-KEY',\n    'Content-Type': 'application/json',\n}\ndata = {\n    \"inputs\": {},\n    \"query\": \"eh\",\n    \"response_mode\": \"streaming\",\n    \"conversation_id\": \"1c7e55fb-1ba2-4e10-81b5-30addcea2276\",\n    \"user\": \"abc-123\"\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Update Document by File with Dify API\nDESCRIPTION: This snippet demonstrates how to update an existing document in a Dify knowledge base by uploading a file via the API. It requires the `dataset_id` and `document_id` of the document to be updated, along with a valid API key. The request uses the `--form` option to send both metadata (name, indexing technique, processing rules) and the updated file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/update-by-file' \\\n--header 'Authorization: Bearer {api_key}' \\\n--form 'data=\"{\"name\":\"Dify\",\"indexing_technique\":\"high_quality\",\"process_rule\":{\"rules\":{\"pre_processing_rules\":[{\"id\":\"remove_extra_spaces\",\"enabled\":true},{\"id\":\"remove_urls_emails\",\"enabled\":true}],\"segmentation\":{\"separator\":\"###\",\"max_tokens\":500}},\"mode\":\"custom\"}}\";type=text/plain' \\\n--form 'file=@\"/path/to/file\"'\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Parameter Class (Python)\nDESCRIPTION: This Python code defines a Pydantic model class `BasicParams` to represent the agent parameters. It includes fields for `maximum_iterations` (integer), `model` (AgentModelConfig), `tools` (list of ToolEntity), and `query` (string). This class is used to validate and structure the input parameters received by the plugin.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n```\n\n----------------------------------------\n\nTITLE: Defining Model Type Credential Schema\nDESCRIPTION: This YAML snippet defines the credential schema for the `model_type` parameter, allowing the user to select the type of model (text generation, embeddings, or reranking).  It uses a select input type with predefined options and labels in both English and Simplified Chinese. The `required` field ensures that this parameter is mandatory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/customizable-model.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprovider_credential_schema:\n  credential_form_schemas:\n  - variable: model_type\n    type: select\n    label:\n      en_US: Model type\n      zh_Hans: 模型类型\n    required: true\n    options:\n    - value: text-generation\n      label:\n        en_US: Language Model\n        zh_Hans: 语言模型\n    - value: embeddings\n      label:\n        en_US: Text Embedding\n    - value: reranking\n      label:\n        en_US: Rerank\n```\n\n----------------------------------------\n\nTITLE: LLMResult Class Definition in Python\nDESCRIPTION: Defines the `LLMResult` class as a Pydantic BaseModel representing the result of a language model operation. It includes the model name, prompt messages, the response message, usage information, and an optional system fingerprint.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResult(BaseModel):\n    \"\"\"\n    Model class for llm result.\n    \"\"\"\n    model: str  # 实际使用模型\n    prompt_messages: list[PromptMessage]  # prompt 消息列表\n    message: AssistantPromptMessage  # 回复消息\n    usage: LLMUsage  # 使用的 tokens 及费用信息\n    system_fingerprint: Optional[str] = None  # 请求指纹，可参考 OpenAI 该参数定义\n```\n\n----------------------------------------\n\nTITLE: Invoke Model using Agent Strategy (Python)\nDESCRIPTION: This code defines the `BasicAgentAgentStrategy` class, which inherits from `AgentStrategy`. It implements the `_invoke` method to call an LLM, format prompts, and handle streaming responses. It also extracts tool calls and their parameters to pass into the tool invocation process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom collections.abc import Generator\nfrom typing import Any, cast\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.model.llm import LLMModelConfig, LLMResult, LLMResultChunk\nfrom dify_plugin.entities.model.message import (\n    PromptMessageTool,\n    UserPromptMessage,\n)\nfrom dify_plugin.entities.tool import ToolInvokeMessage, ToolParameter, ToolProviderType\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n        chunks: Generator[LLMResultChunk, None, None] | LLMResult = (\n            self.session.model.llm.invoke(\n                model_config=LLMModelConfig(**params.model.model_dump(mode=\"json\")),\n                prompt_messages=[UserPromptMessage(content=params.query)],\n                tools=[\n                    self._convert_tool_to_prompt_message_tool(tool)\n                    for tool in params.tools\n                ],\n                stop=params.model.completion_params.get(\"stop\", [])\n                if params.model.completion_params\n                else [],\n                stream=True,\n            )\n        )\n        response = \"\"\n        tool_calls = []\n        tool_instances = (\n            {tool.identity.name: tool for tool in params.tools} if params.tools else {}\n        )\n\n        for chunk in chunks:\n            # check if there is any tool call\n            if self.check_tool_calls(chunk):\n                tool_calls = self.extract_tool_calls(chunk)\n                tool_call_names = \";\".join([tool_call[1] for tool_call in tool_calls])\n                try:\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls},\n                        ensure_ascii=False,\n                    )\n                except json.JSONDecodeError:\n                    # ensure ascii to avoid encoding error\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls}\n                    )\n                print(tool_call_names, tool_call_inputs)\n            if chunk.delta.message and chunk.delta.message.content:\n                if isinstance(chunk.delta.message.content, list):\n                    for content in chunk.delta.message.content:\n                        response += content.data\n                        print(content.data, end=\"\", flush=True)\n                else:\n                    response += str(chunk.delta.message.content)\n                    print(str(chunk.delta.message.content), end=\"\", flush=True)\n\n            if chunk.delta.usage:\n                # usage of the model\n                usage = chunk.delta.usage\n\n        yield self.create_text_message(\n            text=f\"{response or json.dumps(tool_calls, ensure_ascii=False)}\\n\"\n        )\n        result = \"\"\n        for tool_call_id, tool_call_name, tool_call_args in tool_calls:\n            tool_instance = tool_instances[tool_call_name]\n            tool_invoke_responses = self.session.tool.invoke(\n                provider_type=ToolProviderType.BUILT_IN,\n                provider=tool_instance.identity.provider,\n                tool_name=tool_instance.identity.name,\n                parameters={**tool_instance.runtime_parameters, **tool_call_args},\n            )\n            if not tool_instance:\n                tool_invoke_responses = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": f\"there is not a tool named {tool_call_name}\",\n                }\n            else:\n                # invoke tool\n                tool_invoke_responses = self.session.tool.invoke(\n                    provider_type=ToolProviderType.BUILT_IN,\n                    provider=tool_instance.identity.provider,\n                    tool_name=tool_instance.identity.name,\n                    parameters={**tool_instance.runtime_parameters, **tool_call_args},\n                )\n                result = \"\"\n                for tool_invoke_response in tool_invoke_responses:\n                    if tool_invoke_response.type == ToolInvokeMessage.MessageType.TEXT:\n                        result += cast(\n                            ToolInvokeMessage.TextMessage, tool_invoke_response.message\n                        ).text\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.LINK\n                    ):\n                        result += (\n                            f\"result link: {cast(ToolInvokeMessage.TextMessage, tool_invoke_response.message).text}.\"\n                            + \" please tell user to check it.\"\n                        )\n                    elif tool_invoke_response.type in {\n                        ToolInvokeMessage.MessageType.IMAGE_LINK,\n                        ToolInvokeMessage.MessageType.IMAGE,\n                    }:\n                        result += (\n                            \"image has been created and sent to user already, \"\n                            + \"you do not need to create it, just tell the user to check it now.\"\n                        )\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.JSON\n                    ):\n                        text = json.dumps(\n                            cast(\n                                ToolInvokeMessage.JsonMessage,\n                                tool_invoke_response.message,\n                            ).json_object,\n                            ensure_ascii=False,\n                        )\n                        result += f\"tool response: {text}.\"\n                    else:\n                        result += f\"tool response: {tool_invoke_response.message!r}.\"\n\n                tool_response = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": result,\n                }\n        yield self.create_text_message(result)\n```\n\n----------------------------------------\n\nTITLE: Successful Retrieval API Response Example\nDESCRIPTION: This snippet illustrates the JSON response structure for a successful retrieval request (HTTP 200).  It includes a list of records, each containing metadata, a score, a title, and the content of the retrieved document.  The metadata provides additional context about the document, like its path and description.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/external-knowledge-api-documentation.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\nHTTP/1.1 200\nContent-type: application/json\n{\n    \"records\": [{\n                    \"metadata\": {\n                            \"path\": \"s3://dify/knowledge.txt\",\n                            \"description\": \"dify 知识文档\"\n                    },\n                    \"score\": 0.98,\n                    \"title\": \"knowledge.txt\",\n                    \"content\": \"这是外部知识的文档。\"\n            },\n            {\n                    \"metadata\": {\n                            \"path\": \"s3://dify/introduce.txt\",\n                            \"description\": \"dify 介绍\"\n                    },\n                    \"score\": 0.66,\n                    \"title\": \"introduce.txt\",\n                    \"content\": \"GenAI 应用程序的创新引擎\"\n            }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Stream and Sync Responses\nDESCRIPTION: Shows example implementations for handling both streaming and synchronous LLM responses within the `_invoke` method. It routes the request to either `_handle_stream_response` or `_handle_sync_response` based on the `stream` parameter. The streaming handler yields response chunks, while the synchronous handler returns a complete `LLMResult`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/predefined-model.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, stream: bool, **kwargs) -> Union[LLMResult, Generator]:\n   \"\"\"Call the corresponding processing function based on return type.\"\"\"\n   if stream:\n       return self._handle_stream_response(**kwargs)\n   return self._handle_sync_response(**kwargs) \n\ndef _handle_stream_response(self, **kwargs) -> Generator:\n   \"\"\"Handle streaming response logic.\"\"\"\n   for chunk in response: # Assume response is a streaming data iterator\n       yield chunk\n\ndef _handle_sync_response(self, **kwargs) -> LLMResult:\n   \"\"\"Handle synchronous response logic.\"\"\" \n   return LLMResult(**response) # Assume response is a complete response dictionary\n```\n\n----------------------------------------\n\nTITLE: FastAPI API Extension Example\nDESCRIPTION: This Python code snippet provides a complete example of a FastAPI-based API extension for Dify. It includes endpoint definition, request data parsing, authentication, and handling of the `ping` and `app.external_data_tool.query` extension points.  It requires the fastapi and pydantic libraries.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI, Body, HTTPException, Header\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass InputData(BaseModel):\n    point: str\n    params: dict = {}\n\n\n@app.post(\"/api/dify/receive\")\nasync def dify_receive(data: InputData = Body(...), authorization: str = Header(None)):\n    \"\"\"\n    Receive API query data from Dify.\n    \"\"\"\n    expected_api_key = \"123456\"  # TODO Your API key of this API\n    auth_scheme, _, api_key = authorization.partition(' ')\n\n    if auth_scheme.lower() != \"bearer\" or api_key != expected_api_key:\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n\n    point = data.point\n\n    # for debug\n    print(f\"point: {point}\")\n\n    if point == \"ping\":\n        return {\n            \"result\": \"pong\"\n        }\n    if point == \"app.external_data_tool.query\":\n        return handle_app_external_data_tool_query(params=data.params)\n    # elif point == \"{point name}\":\n        # TODO other point implementation here\n\n    raise HTTPException(status_code=400, detail=\"Not implemented\")\n\n\n\ndef handle_app_external_data_tool_query(params: dict):\n    app_id = params.get(\"app_id\")\n    tool_variable = params.get(\"tool_variable\")\n    inputs = params.get(\"inputs\")\n    query = params.get(\"query\")\n\n    # for debug\n    print(f\"app_id: {app_id}\")\n    print(f\"tool_variable: {tool_variable}\")\n    print(f\"inputs: {inputs}\")\n    print(f\"query: {query}\")\n\n    # TODO your external data tool query implementation here, \n    #  return must be a dict with key \"result\", and the value is the query result\n    if inputs.get(\"location\") == \"London\":\n        return {\n            \"result\": \"City: London\\nTemperature: 10°C\\nRealFeel®: 8°C\\nAir Quality: Poor\\nWind Direction: ENE\\nWind Speed: 8 km/h\\nWind Gusts: 14 km/h\\nPrecipitation: Light rain\"\n        }\n    else:\n        return {\"result\": \"Unknown city\"}\n\n```\n\n----------------------------------------\n\nTITLE: Rerank Model Invocation Interface\nDESCRIPTION: Defines the `_invoke` method for reranking documents based on a query. It takes a model name, credentials, query, list of documents, an optional score threshold, top_n, and user identifier. It returns a `RerankResult` entity.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              query: str, docs: list[str], score_threshold: Optional[float] = None, top_n: Optional[int] = None,\n              user: Optional[str] = None) \\\n          -> RerankResult:\n      \"\"\"\n      Invoke rerank model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param query: search query\n      :param docs: docs for reranking\n      :param score_threshold: score threshold\n      :param top_n: top n\n      :param user: unique user id\n      :return: rerank result\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: BasicAgentAgentStrategy Implementation Python\nDESCRIPTION: This code snippet implements the `BasicAgentAgentStrategy` class, which extends `AgentStrategy`. It defines the `_invoke` method, responsible for orchestrating the interaction between the model and tools. It handles model invocation, tool calls, and logging. The code also includes helper methods for converting tools to prompt message tools and extracting tool calls from LLM results.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport time\nfrom collections.abc import Generator\nfrom typing import Any, cast\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.model.llm import LLMModelConfig, LLMResult, LLMResultChunk\nfrom dify_plugin.entities.model.message import (\n    PromptMessageTool,\n    UserPromptMessage,\n)\nfrom dify_plugin.entities.tool import ToolInvokeMessage, ToolParameter, ToolProviderType\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n        function_call_round_log = self.create_log_message(\n            label=\"Function Call Round1 \",\n            data={},\n            metadata={},\n        )\n        yield function_call_round_log\n        model_started_at = time.perf_counter()\n        model_log = self.create_log_message(\n            label=f\"{params.model.model} Thought\",\n            data={},\n            metadata={\"start_at\": model_started_at, \"provider\": params.model.provider},\n            status=ToolInvokeMessage.LogMessage.LogStatus.START,\n            parent=function_call_round_log,\n        )\n        yield model_log\n        chunks: Generator[LLMResultChunk, None, None] | LLMResult = (\n            self.session.model.llm.invoke(\n                model_config=LLMModelConfig(**params.model.model_dump(mode=\"json\")),\n                prompt_messages=[UserPromptMessage(content=params.query)],\n                tools=[\n                    self._convert_tool_to_prompt_message_tool(tool)\n                    for tool in params.tools\n                ],\n                stop=params.model.completion_params.get(\"stop\", [])\n                if params.model.completion_params\n                else [],\n                stream=True,\n            )\n        )\n        response = \"\"\n        tool_calls = []\n        tool_instances = (\n            {tool.identity.name: tool for tool in params.tools} if params.tools else {}\n        )\n        tool_call_names = \"\"\n        tool_call_inputs = \"\"\n        for chunk in chunks\n            # check if there is any tool call\n            if self.check_tool_calls(chunk):\n                tool_calls = self.extract_tool_calls(chunk)\n                tool_call_names = \";\".join([tool_call[1] for tool_call in tool_calls])\n                try:\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls},\n                        ensure_ascii=False,\n                    )\n                except json.JSONDecodeError:\n                    # ensure ascii to avoid encoding error\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls}\n                    )\n                print(tool_call_names, tool_call_inputs)\n            if chunk.delta.message and chunk.delta.message.content:\n                if isinstance(chunk.delta.message.content, list):\n                    for content in chunk.delta.message.content:\n                        response += content.data\n                        print(content.data, end=\"\", flush=True)\n                else:\n                    response += str(chunk.delta.message.content)\n                    print(str(chunk.delta.message.content), end=\"\", flush=True)\n\n            if chunk.delta.usage:\n                # usage of the model\n                usage = chunk.delta.usage\n\n        yield self.finish_log_message(\n            log=model_log,\n            data={\n                \"output\": response,\n                \"tool_name\": tool_call_names,\n                \"tool_input\": tool_call_inputs,\n            },\n            metadata={\n                \"started_at\": model_started_at,\n                \"finished_at\": time.perf_counter(),\n                \"elapsed_time\": time.perf_counter() - model_started_at,\n                \"provider\": params.model.provider,\n            },\n        )\n        yield self.create_text_message(\n            text=f\"{response or json.dumps(tool_calls, ensure_ascii=False)}\\n\"\n        )\n        result = \"\"\n        for tool_call_id, tool_call_name, tool_call_args in tool_calls:\n            tool_instance = tool_instances[tool_call_name]\n            tool_invoke_responses = self.session.tool.invoke(\n                provider_type=ToolProviderType.BUILT_IN,\n                provider=tool_instance.identity.provider,\n                tool_name=tool_instance.identity.name,\n                parameters={**tool_instance.runtime_parameters, **tool_call_args},\n            )\n            if not tool_instance:\n                tool_invoke_responses = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": f\"there is not a tool named {tool_call_name}\",\n                }\n            else:\n                # invoke tool\n                tool_invoke_responses = self.session.tool.invoke(\n                    provider_type=ToolProviderType.BUILT_IN,\n                    provider=tool_instance.identity.provider,\n                    tool_name=tool_instance.identity.name,\n                    parameters={**tool_instance.runtime_parameters, **tool_call_args},\n                )\n                result = \"\"\n                for tool_invoke_response in tool_invoke_responses:\n                    if tool_invoke_response.type == ToolInvokeMessage.MessageType.TEXT:\n                        result += cast(\n                            ToolInvokeMessage.TextMessage, tool_invoke_response.message\n                        ).text\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.LINK\n                    ):\n                        result += (\n                            f\"result link: {cast(ToolInvokeMessage.TextMessage, tool_invoke_response.message).text}.\"\n                            + \" please tell user to check it.\"\n                        )\n                    elif tool_invoke_response.type in {\n                        ToolInvokeMessage.MessageType.IMAGE_LINK,\n                        ToolInvokeMessage.MessageType.IMAGE,\n                    }:\n                        result += (\n                            \"image has been created and sent to user already, \"\n                            + \"you do not need to create it, just tell the user to check it now.\"\n                        )\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.JSON\n                    ):\n                        text = json.dumps(\n                            cast(\n                                ToolInvokeMessage.JsonMessage,\n                                tool_invoke_response.message,\n                            ).json_object,\n                            ensure_ascii=False,\n                        )\n                        result += f\"tool response: {text}.\"\n                    else:\n                        result += f\"tool response: {tool_invoke_response.message!r}.\"\n\n                tool_response = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": result,\n                }\n        yield self.create_text_message(result)\n\n```\n\n----------------------------------------\n\nTITLE: Update Document with Text - Dify API (cURL)\nDESCRIPTION: This cURL command updates a document in an existing Dify knowledge base using the provided text. It requires the dataset ID, document ID and API key for authentication and authorization. The request body includes the document name and text content.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/update_by_text' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"name\": \"name\",\"text\": \"text\"}'\n```\n\n----------------------------------------\n\nTITLE: Getting Customizable Model Schema in Python\nDESCRIPTION: This Python code defines the `get_customizable_model_schema` method for retrieving a customizable model schema.  It takes the model name and credentials as input and returns an `AIModelEntity` object, or None if not supported.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> Optional[AIModelEntity]:\n    \"\"\"\n    Get customizable model schema\n\n    :param model: model name\n    :param credentials: model credentials\n    :return: model schema\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define Agent Strategy Class in Python\nDESCRIPTION: This code defines the `BasicAgentAgentStrategy` class, which inherits from `AgentStrategy`. It implements the `_invoke` method to handle agent logic, including model invocation, tool interaction, and logging. It uses the `pydantic` library to define data models for input parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport time\nfrom collections.abc import Generator\nfrom typing import Any, cast\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.model.llm import LLMModelConfig, LLMResult, LLMResultChunk\nfrom dify_plugin.entities.model.message import (\n    PromptMessageTool,\n    UserPromptMessage,\n)\nfrom dify_plugin.entities.tool import ToolInvokeMessage, ToolParameter, ToolProviderType\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n        function_call_round_log = self.create_log_message(\n            label=\"Function Call Round1 \",\n            data={},\n            metadata={},\n        )\n        yield function_call_round_log\n        model_started_at = time.perf_counter()\n        model_log = self.create_log_message(\n            label=f\"{params.model.model} Thought\",\n            data={},\n            metadata={\"start_at\": model_started_at, \"provider\": params.model.provider},\n            status=ToolInvokeMessage.LogMessage.LogStatus.START,\n            parent=function_call_round_log,\n        )\n        yield model_log\n        chunks: Generator[LLMResultChunk, None, None] | LLMResult = (\n            self.session.model.llm.invoke(\n                model_config=LLMModelConfig(**params.model.model_dump(mode=\"json\")),\n                prompt_messages=[UserPromptMessage(content=params.query)],\n                tools=[\n                    self._convert_tool_to_prompt_message_tool(tool)\n                    for tool in params.tools\n                ],\n                stop=params.model.completion_params.get(\"stop\", [])\n                if params.model.completion_params\n                else [],\n                stream=True,\n            )\n        )\n        response = \"\"\n        tool_calls = []\n        tool_instances = (\n            {tool.identity.name: tool for tool in params.tools} if params.tools else {}\n        )\n        tool_call_names = \"\"\n        tool_call_inputs = \"\"\n        for chunk in chunks\n            # check if there is any tool call\n            if self.check_tool_calls(chunk):\n                tool_calls = self.extract_tool_calls(chunk)\n                tool_call_names = \";\".join([tool_call[1] for tool_call in tool_calls])\n                try:\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls},\n                        ensure_ascii=False,\n                    )\n                except json.JSONDecodeError:\n                    # ensure ascii to avoid encoding error\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls}\n                    )\n                print(tool_call_names, tool_call_inputs)\n            if chunk.delta.message and chunk.delta.message.content:\n                if isinstance(chunk.delta.message.content, list):\n                    for content in chunk.delta.message.content:\n                        response += content.data\n                        print(content.data, end=\"\", flush=True)\n                else:\n                    response += str(chunk.delta.message.content)\n                    print(str(chunk.delta.message.content), end=\"\", flush=True)\n\n            if chunk.delta.usage:\n                # usage of the model\n                usage = chunk.delta.usage\n\n        yield self.finish_log_message(\n            log=model_log,\n            data={\n                \"output\": response,\n                \"tool_name\": tool_call_names,\n                \"tool_input\": tool_call_inputs,\n            },\n            metadata={\n                \"started_at\": model_started_at,\n                \"finished_at\": time.perf_counter(),\n                \"elapsed_time\": time.perf_counter() - model_started_at,\n                \"provider\": params.model.provider,\n            },\n        )\n        yield self.create_text_message(\n            text=f\"{response or json.dumps(tool_calls, ensure_ascii=False)}\\n\"\n        )\n        result = \"\"\n        for tool_call_id, tool_call_name, tool_call_args in tool_calls:\n            tool_instance = tool_instances[tool_call_name]\n            tool_invoke_responses = self.session.tool.invoke(\n                provider_type=ToolProviderType.BUILT_IN,\n                provider=tool_instance.identity.provider,\n                tool_name=tool_instance.identity.name,\n                parameters={**tool_instance.runtime_parameters, **tool_call_args},\n            )\n            if not tool_instance:\n                tool_invoke_responses = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": f\"there is not a tool named {tool_call_name}\",\n                }\n            else:\n                # invoke tool\n                tool_invoke_responses = self.session.tool.invoke(\n                    provider_type=ToolProviderType.BUILT_IN,\n                    provider=tool_instance.identity.provider,\n                    tool_name=tool_instance.identity.name,\n                    parameters={**tool_instance.runtime_parameters, **tool_call_args},\n                )\n                result = \"\"\n                for tool_invoke_response in tool_invoke_responses:\n                    if tool_invoke_response.type == ToolInvokeMessage.MessageType.TEXT:\n                        result += cast(\n                            ToolInvokeMessage.TextMessage, tool_invoke_response.message\n                        ).text\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.LINK\n                    ):\n                        result += (\n                            f\"result link: {cast(ToolInvokeMessage.TextMessage, tool_invoke_response.message).text}.\"\n                            + \" please tell user to check it.\"\n                        )\n                    elif tool_invoke_response.type in {\n                        ToolInvokeMessage.MessageType.IMAGE_LINK,\n                        ToolInvokeMessage.MessageType.IMAGE,\n                    }:\n                        result += (\n                            \"image has been created and sent to user already, \"\n                            + \"you do not need to create it, just tell the user to check it now.\"\n                        )\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.JSON\n                    ):\n                        text = json.dumps(\n                            cast(\n                                ToolInvokeMessage.JsonMessage,\n                                tool_invoke_response.message,\n                            ).json_object,\n                            ensure_ascii=False,\n                        )\n                        result += f\"tool response: {text}.\"\n                    else:\n                        result += f\"tool response: {tool_invoke_response.message!r}.\"\n\n                tool_response = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": result,\n                }\n        yield self.create_text_message(result)\n```\n\n----------------------------------------\n\nTITLE: app.moderation.output API Response Example (JSON)\nDESCRIPTION: This JSON payload showcases example responses from the `app.moderation.output` API, with the action being either `direct_output` or `overridden`. It includes a flagged status, action type, preset response (when applicable), and the modified LLM output text (when applicable).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/moderation.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"direct_output\",\n    \"preset_response\": \"Your content violates our usage policy.\"\n}\n```\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"overridden\",\n    \"text\": \"I will *** you.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Define Frontend Form Schema - JSON\nDESCRIPTION: Defines the JSON schema for the frontend form of the custom external data tool. This schema dictates the available options, labels, variables, and defaults for the user interface.  It includes a `select` component to define temperature units, which is crucial for collecting necessary parameters from the user before querying external data.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/code-based-extension/external-data-tool.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"label\": {\n        \"en-US\": \"Weather Search\",\n        \"zh-Hans\": \"天气查询\"\n    },\n    \"form_schema\": [\n        {\n            \"type\": \"select\",\n            \"label\": {\n                \"en-US\": \"Temperature Unit\",\n                \"zh-Hans\": \"温度单位\"\n            },\n            \"variable\": \"temperature_unit\",\n            \"required\": true,\n            \"options\": [\n                {\n                    \"label\": {\n                        \"en-US\": \"Fahrenheit\",\n                        \"zh-Hans\": \"华氏度\"\n                    },\n                    \"value\": \"fahrenheit\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Centigrade\",\n                        \"zh-Hans\": \"摄氏度\"\n                    },\n                    \"value\": \"centigrade\"\n                }\n            ],\n            \"default\": \"centigrade\",\n            \"placeholder\": \"Please select temperature unit\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Tool Code Implementation: google_search.py\nDESCRIPTION: This Python code defines the GoogleSearchTool class, which inherits from the Tool class. It uses the requests library to perform a Google SERP search via the SerpAPI. The _invoke method sends the request and returns a JSON formatted message with the search results. It depends on the `requests` and `dify_plugin` libraries.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom collections.abc import Generator\nfrom typing import Any\n\nimport requests\n\nfrom dify_plugin import Tool\nfrom dify_plugin.entities.tool import ToolInvokeMessage\n\nSERP_API_URL = \"https://serpapi.com/search\"\n\nclass GoogleSearchTool(Tool):\n    def _parse_response(self, response: dict) -> dict:\n        result = {}\n        if \"knowledge_graph\" in response:\n            result[\"title\"] = response[\"knowledge_graph\"].get(\"title\", \"\")\n            result[\"description\"] = response[\"knowledge_graph\"].get(\"description\", \"\")\n        if \"organic_results\" in response:\n            result[\"organic_results\"] = [\n                {\n                    \"title\": item.get(\"title\", \"\"),\n                    \"link\": item.get(\"link\", \"\"),\n                    \"snippet\": item.get(\"snippet\", \"\"),\n                }\n                for item in response[\"organic_results\"]\n            ]\n        return result\n\n    def _invoke(self, tool_parameters: dict[str, Any]) -> Generator[ToolInvokeMessage]:\n        params = {\n            \"api_key\": self.runtime.credentials[\"serpapi_api_key\"],\n            \"q\": tool_parameters[\"query\"],\n            \"engine\": \"google\",\n            \"google_domain\": \"google.com\",\n            \"gl\": \"us\",\n            \"hl\": \"en\",\n        }\n\n        response = requests.get(url=SERP_API_URL, params=params, timeout=5)\n        response.raise_for_status()\n        valuable_res = self._parse_response(response.json())\n        \n        yield self.create_json_message(valuable_res)\n```\n\n----------------------------------------\n\nTITLE: Mapping Model Invoke Errors (Python)\nDESCRIPTION: This code snippet shows how to map model invocation errors to unified error types in Dify. This is a general interface that all model types need to implement. This is crucial for Dify to handle different errors appropriately, allowing for consistent error handling across different models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Tools via self.session.tool in Python\nDESCRIPTION: This snippet shows the entry point for accessing tools within a Dify plugin using `self.session.tool`. This allows plugins to request and use other implemented tools.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/tool.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.session.tool\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Configuration for SearXNG\nDESCRIPTION: This YAML configuration defines the services for SearXNG, Redis, and Caddy within a Docker Compose setup. It specifies the images, ports, volumes, and networks for each service to enable a complete SearXNG environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3'\n\nservices:\n  searxng:\n    image: searxng/searxng:latest\n    ports:\n      - \"8081:8080\"\n    volumes:\n      - ./searxng:/etc/searxng\n    networks:\n      - searxng_network\n\n  redis:\n    image: valkey/valkey:8-alpine\n    ports:\n      - \"6379:6379\"\n    networks:\n      - searxng_network\n\n  caddy:\n    image: caddy:2-alpine\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    networks:\n      - searxng_network\n\nnetworks:\n  searxng_network:\n    driver: bridge\n```\n\n----------------------------------------\n\nTITLE: Creating a JSON Message in Dify (Python)\nDESCRIPTION: This code shows how to create a JSON message to return structured data in a Dify tool plugin. It takes a Python dictionary (`json`) as input which Dify automatically serializes to JSON. JSON messages are suitable for workflow node communication and for use with large language models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/tool.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef create_json_message(self, json: dict) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Initializing Plugin Project (Bash)\nDESCRIPTION: This command initializes a new Dify plugin project using the plugin scaffolding tool. It is assumed the `dify-plugin-darwin-arm64` binary is in the current directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./dify-plugin-darwin-arm64 plugin init\n```\n\n----------------------------------------\n\nTITLE: Getting the Number of Tokens in Python\nDESCRIPTION: This function calculates the number of tokens for the given prompt messages using a tokenizer appropriate for the given model. It accepts the model name, credentials, and prompt messages as input. If the model does not provide its own tokenizer, it defaults to using the `_get_num_tokens_by_gpt2` method from the AIModel base class.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                   tools: Optional[list[PromptMessageTool]] = None) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param tools: tools for tool calling\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Dify API Endpoint Implementation (FastAPI Python)\nDESCRIPTION: Python code implementing a FastAPI endpoint for receiving API queries from Dify.  It handles authentication and different extension points, including 'ping' and 'app.external_data_tool.query'.  An example is shown for accessing weather information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/README.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom fastapi import FastAPI, Body, HTTPException, Header\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass InputData(BaseModel):\n    point: str\n    params: dict\n\n\n@app.post(\"/api/dify/receive\")\nasync def dify_receive(data: InputData = Body(...), authorization: str = Header(None)):\n    \"\"\"\n    Receive API query data from Dify.\n    \"\"\"\n    expected_api_key = \"123456\"  # TODO Your API key of this API\n    auth_scheme, _, api_key = authorization.partition(' ')\n\n    if auth_scheme.lower() != \"bearer\" or api_key != expected_api_key:\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n\n    point = data.point\n\n    # for debug\n    print(f\"point: {point}\")\n\n    if point == \"ping\":\n        return {\n            \"result\": \"pong\"\n        }\n    if point == \"app.external_data_tool.query\":\n        return handle_app_external_data_tool_query(params=data.params)\n    # elif point == \"{point name}\":\n        # TODO other point implementation here\n\n    raise HTTPException(status_code=400, detail=\"Not implemented\")\n\n\ndef handle_app_external_data_tool_query(params: dict):\n    app_id = params.get(\"app_id\")\n    tool_variable = params.get(\"tool_variable\")\n    inputs = params.get(\"inputs\")\n    query = params.get(\"query\")\n\n    # for debug\n    print(f\"app_id: {app_id}\")\n    print(f\"tool_variable: {tool_variable}\")\n    print(f\"inputs: {inputs}\")\n    print(f\"query: {query}\")\n\n    # TODO your external data tool query implementation here, \n    #  return must be a dict with key \"result\", and the value is the query result\n    if inputs.get(\"location\") == \"London\":\n        return {\n            \"result\": \"City: London\\nTemperature: 10°C\\nRealFeel®: 8°C\\nAir Quality: Poor\\nWind Direction: ENE\\nWind \"\n                      \"Speed: 8 km/h\\nWind Gusts: 14 km/h\\nPrecipitation: Light rain\"\n        }\n    else:\n        return {\"result\": \"Unknown city\"}\n\n```\n\n----------------------------------------\n\nTITLE: Cloudflare Worker with Content Moderation\nDESCRIPTION: This code snippet demonstrates a Cloudflare Worker that implements content moderation using keyword matching. It uses Hono for routing and middleware, Zod for schema validation, and performs keyword filtering on both user input (`app.moderation.input`) and LLM output (`app.moderation.output`).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation.md#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Hono } from \"hono\";\nimport { bearerAuth } from \"hono/bearer-auth\";\nimport { z } from \"zod\";\nimport { zValidator } from \"@hono/zod-validator\";\nimport { generateSchema } from '@anatine/zod-openapi';\n\ntype Bindings = {\n  TOKEN: string;\n};\n\nconst app = new Hono<{ Bindings: Bindings }>();\n\n// API format validation ⬇️\nconst schema = z.object({\n  point: z.union([\n    z.literal(\"ping\"),\n    z.literal(\"app.external_data_tool.query\"),\n    z.literal(\"app.moderation.input\"),\n    z.literal(\"app.moderation.output\"),\n  ]), // Restricts 'point' to specific values\n  params: z\n    .object({\n      app_id: z.string().optional(),\n      tool_variable: z.string().optional(),\n      inputs: z.record(z.any()).optional(),\n      query: z.any(),\n      text: z.any()\n    })\n    .optional(),\n});\n\n// Generate OpenAPI schema\napp.get(\"/\", (c) => {\n  return c.json(generateSchema(schema));\n});\n\napp.post(\n  \"/\",\n  (c, next) => {\n    const auth = bearerAuth({ token: c.env.TOKEN });\n    return auth(c, next);\n  },\n  zValidator(\"json\", schema),\n  async (c) => {\n    const { point, params } = c.req.valid(\"json\");\n    if (point === \"ping\") {\n      return c.json({\n        result: \"pong\",\n      });\n    }\n    // ⬇️ implement your logic here ⬇️\n    // point === \"app.external_data_tool.query\"\n    else if (point === \"app.moderation.input\"){\n    // Input check ⬇️\n    const inputkeywords = [\"input filter test1\", \"input filter test2\", \"input filter test3\"];\n\n    if (inputkeywords.some(keyword => params.query.includes(keyword)))\n      {\n      return c.json({\n        \"flagged\": true,\n        \"action\": \"direct_output\",\n        \"preset_response\": \"Input contains prohibited content, please try a different question!\"\n      });\n    } else {\n      return c.json({\n        \"flagged\": false,\n        \"action\": \"direct_output\",\n        \"preset_response\": \"Input is normal\"\n      });\n    }\n    // Input check completed \n    }\n    \n    else {\n      // Output check ⬇️\n      const outputkeywords = [\"output filter test1\", \"output filter test2\", \"output filter test3\"]; \n\n  if (outputkeywords.some(keyword => params.text.includes(keyword)))\n    {\n      return c.json({\n        \"flagged\": true,\n        \"action\": \"direct_output\",\n        \"preset_response\": \"Output contains sensitive content and has been filtered by the system. Please ask a different question!\"\n      });\n    }\n  \n  else {\n    return c.json({\n      \"flagged\": false,\n      \"action\": \"direct_output\",\n      \"preset_response\": \"Output is normal\"\n    });\n  };\n    }\n    // Output check completed \n  }\n);\n\nexport default app;\n```\n\n----------------------------------------\n\nTITLE: LLM Request Example (Python)\nDESCRIPTION: This code demonstrates how to make an LLM request using the Dify plugin.  It constructs an `LLMModelConfig` to specify the model, mode, and provider.  It then creates prompt messages and invokes the `session.model.llm.invoke` method to get the LLM's response. The response is streamed, and each chunk's message content is yielded as a ToolInvokeMessage.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom typing import Any\n\nfrom dify_plugin import Tool\nfrom dify_plugin.entities.model.llm import LLMModelConfig\nfrom dify_plugin.entities.tool import ToolInvokeMessage\nfrom dify_plugin.entities.model.message import SystemPromptMessage, UserPromptMessage\n\nclass LLMTool(Tool):\n    def _invoke(self, tool_parameters: dict[str, Any]) -> Generator[ToolInvokeMessage]:\n        response = self.session.model.llm.invoke(\n            model_config=LLMModelConfig(\n                provider='openai',\n                model='gpt-4o-mini',\n                mode='chat',\n                completion_params={}\n            ),\n            prompt_messages=[\n                SystemPromptMessage(\n                    content='you are a helpful assistant'\n                ),\n                UserPromptMessage(\n                    content=tool_parameters.get('query')\n                )\n            ],\n            stream=True\n        )\n\n        for chunk in response:\n            if chunk.delta.message:\n                assert isinstance(chunk.delta.message.content, str)\n                yield self.create_text_message(text=chunk.delta.message.content)\n```\n\n----------------------------------------\n\nTITLE: Customizable Model Schema in Python\nDESCRIPTION: Demonstrates how to dynamically generate a model parameter schema based on the specific model being used. It creates an `AIModelEntity` with parameter rules for `temperature`, `top_p`, and `max_tokens`, and conditionally adds `top_k` based on the model name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> AIModelEntity | None:\n      \"\"\"\n          used to define customizable model schema\n      \"\"\"\n      rules = [\n          ParameterRule(\n              name='temperature', type=ParameterType.FLOAT,\n              use_template='temperature',\n              label=I18nObject(\n                  zh_Hans='温度', en_US='Temperature'\n              )\n          ),\n          ParameterRule(\n              name='top_p', type=ParameterType.FLOAT,\n              use_template='top_p',\n              label=I18nObject(\n                  zh_Hans='Top P', en_US='Top P'\n              )\n          ),\n          ParameterRule(\n              name='max_tokens', type=ParameterType.INT,\n              use_template='max_tokens',\n              min=1,\n              default=512,\n              label=I18nObject(\n                  zh_Hans='最大生成长度', en_US='Max Tokens'\n              )\n          )\n      ]\n\n      # if model is A, add top_k to rules\n      if model == 'A':\n          rules.append(\n              ParameterRule(\n                  name='top_k', type=ParameterType.INT,\n                  use_template='top_k',\n                  min=1,\n                  default=50,\n                  label=I18nObject(\n                      zh_Hans='Top K', en_US='Top K'\n                  )\n              )\n          )\n\n      \"\"\"\n          some NOT IMPORTANT code here\n      \"\"\"\n\n      entity = AIModelEntity(\n          model=model,\n          label=I18nObject(\n              en_US=model\n          ),\n          fetch_from=FetchFrom.CUSTOMIZABLE_MODEL,\n          model_type=model_type,\n          model_properties={ \n              ModelPropertyKey.MODE:  ModelType.LLM,\n          },\n          parameter_rules=rules\n      )\n\n      return entity\n```\n\n----------------------------------------\n\nTITLE: Delete Document with Dify API\nDESCRIPTION: This snippet demonstrates how to delete a document from a Dify knowledge base using the API. It requires the `dataset_id` and `document_id` of the document to be deleted, along with a valid API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Rerank Request Entry Point (Python)\nDESCRIPTION: This code shows the entry point to request text reranking. It uses the session model's `rerank` property.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.rerank\n```\n\n----------------------------------------\n\nTITLE: File (Blob) Message Creation in Dify (Python)\nDESCRIPTION: This Python code snippet defines the interface for creating a file (blob) message within the Dify tool plugin framework. It takes raw file data (bytes) and optional metadata (mime_type) as input and returns a ToolInvokeMessage object. Dify uses `octet/stream` as the default `mime_type` if one is not provided.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/tool.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef create_blob_message(self, blob: bytes, meta: dict = None) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Updating a Chunk in a Document using Dify API\nDESCRIPTION: Updates an existing chunk (segment) within a document in a dataset of the Dify knowledge base. It requires the dataset ID, document ID, and segment ID as path parameters, a valid API key in the Authorization header, and a JSON payload containing the updated segment data.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments/{segment_id}' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json'\\\n--data-raw '{\"segment\": {\"content\": \"1\",\"answer\": \"1\", \"keywords\": [\"a\"], \"enabled\": false}}'\n```\n\n----------------------------------------\n\nTITLE: Invoking LLM Model with Dify SDK in Python\nDESCRIPTION: This snippet demonstrates how to invoke a language model using the Dify SDK's `session.model.llm.invoke()` function. It initializes prompt messages and tools before calling the model with a provided `model_config`, `prompt_messages`, `tools`, `stream` and `stop` arguments. The `_init_prompt_tools` method is used to prepare the tools for the model invocation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/agent.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.model.llm import LLMModelConfig\nfrom dify_plugin.entities.model.message import (\n    PromptMessageTool,\n    SystemPromptMessage,\n    UserPromptMessage,\n)\nfrom dify_plugin.entities.tool import ToolParameter\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\n\nclass FunctionCallingParams(BaseModel):\n    query: str\n    instruction: str | None\n    model: AgentModelConfig\n    tools: list[ToolEntity] | None\n    maximum_iterations: int = 3\n\nclass FunctionCallingAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        \"\"\"\n        Run FunctionCall agent application\n        \"\"\"\n        # init params\n        fc_params = FunctionCallingParams(**parameters)\n        query = fc_params.query\n        model = fc_params.model\n        stop = fc_params.model.completion_params.get(\"stop\", []) if fc_params.model.completion_params else []\n        prompt_messages = [\n            SystemPromptMessage(content=\"your system prompt message\"),\n            UserPromptMessage(content=query),\n        ]\n        tools = fc_params.tools\n        prompt_messages_tools = self._init_prompt_tools(tools)\n\n        # invoke llm\n        chunks = self.session.model.llm.invoke(\n            model_config=LLMModelConfig(**model.model_dump(mode=\"json\")),\n            prompt_messages=prompt_messages,\n            stream=True,\n            stop=stop,\n            tools=prompt_messages_tools,\n        )\n\n    def _init_prompt_tools(self, tools: list[ToolEntity] | None) -> list[PromptMessageTool]:\n        \"\"\"\n        Init tools\n        \"\"\"\n\n        prompt_messages_tools = []\n        for tool in tools or []:\n            try:\n                prompt_tool = self._convert_tool_to_prompt_message_tool(tool)\n            except Exception:\n                # api tool may be deleted\n                continue\n\n            # save prompt tool\n            prompt_messages_tools.append(prompt_tool)\n\n        return prompt_messages_tools\n\n    def _convert_tool_to_prompt_message_tool(self, tool: ToolEntity) -> PromptMessageTool:\n        \"\"\"\n        convert tool to prompt message tool\n        \"\"\"\n        message_tool = PromptMessageTool(\n            name=tool.identity.name,\n            description=tool.description.llm if tool.description else \"\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        )\n\n        parameters = tool.parameters\n        for parameter in parameters:\n            if parameter.form != ToolParameter.ToolParameterForm.LLM:\n                continue\n\n            parameter_type = parameter.type\n            if parameter.type in {\n                ToolParameter.ToolParameterType.FILE,\n                ToolParameter.ToolParameterType.FILES,\n            }:\n                continue\n            enum = []\n            if parameter.type == ToolParameter.ToolParameterType.SELECT:\n                enum = [option.value for option in parameter.options] if parameter.options else []\n\n            message_tool.parameters[\"properties\"][parameter.name] = {\n                \"type\": parameter_type,\n                \"description\": parameter.llm_description or \"\",\n            }\n\n            if len(enum) > 0:\n                message_tool.parameters[\"properties\"][parameter.name][\"enum\"] = enum\n\n            if parameter.required:\n                message_tool.parameters[\"required\"].append(parameter.name)\n\n        return message_tool\n```\n\n----------------------------------------\n\nTITLE: Web Page Crawling Tool in Dify (Python)\nDESCRIPTION: This snippet shows how to use Dify's web page crawling tool. It accepts a `url` and an optional `user_agent`.  If no `user_agent` is provided, Dify uses a default one. The tool returns the crawled content of the web page as a string.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/advanced-tool-integration.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    def get_url(self, url: str, user_agent: str = None) -> str:\n        \"\"\"\n            get url\n        \"\"\" the crawled result\n```\n\n----------------------------------------\n\nTITLE: Invoking Speech-to-Text Model in Dify (Python)\nDESCRIPTION: This method invokes a speech-to-text model to transcribe an audio file into text. It takes an audio file stream as input and returns the transcribed text as a string. The model and credentials parameters are used for authentication and model selection.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            file: IO[bytes], user: Optional[str] = None) \\\n        -> str:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param file: audio file\n    :param user: unique user id\n    :return: text for given audio file\n    \"\"\"        \n```\n\n----------------------------------------\n\nTITLE: Workflow Interface Invoke Method Definition (Python)\nDESCRIPTION: Defines the `invoke` method for the Workflow Interface, enabling plugins to interact with Workflow applications.  It specifies the parameters `app_id`, `inputs`, `response_mode`, and `files`. The return type is either a generator of dictionaries for streaming or a single dictionary for blocking mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self,\n    app_id: str,\n    inputs: dict,\n    response_mode: Literal[\"streaming\", \"blocking\"],\n    files: list,\n) -> Generator[dict, None, None] | dict:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Validating Provider Credentials in Python\nDESCRIPTION: This function validates the credentials provided for a model provider. It receives a dictionary of credentials and raises an exception if validation fails, using a CredentialsValidateFailedError. This function must be fully implemented for pre-defined models; custom providers can use a simple pass.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Process Slack Messages in Python\nDESCRIPTION: This Python code defines an endpoint to handle Slack messages. It verifies the Slack URL, processes app mentions, invokes the Dify app to generate responses, and sends the response back to Slack. It uses the `slack_sdk` library to interact with the Slack API. The `allow_retry` setting prevents duplicate message processing.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport traceback\nfrom typing import Mapping\nfrom werkzeug import Request, Response\nfrom dify_plugin import Endpoint\nfrom slack_sdk import WebClient\nfrom slack_sdk.errors import SlackApiError\n\n\nclass SlackEndpoint(Endpoint):\n    def _invoke(self, r: Request, values: Mapping, settings: Mapping) -> Response:\n        \"\"\"\n        Invokes the endpoint with the given request.\n        \"\"\"\n        retry_num = r.headers.get(\"X-Slack-Retry-Num\")\n        if (not settings.get(\"allow_retry\") and (r.headers.get(\"X-Slack-Retry-Reason\") == \"http_timeout\" or ((retry_num is not None and int(retry_num) > 0)))): \n            return Response(status=200, response=\"ok\")\n        data = r.get_json()\n\n        # Handle Slack URL verification challenge\n        if data.get(\"type\") == \"url_verification\":\n            return Response(\n                response=json.dumps({\"challenge\": data.get(\"challenge\")}),\n                status=200,\n                content_type=\"application/json\"\n            )\n        \n        if (data.get(\"type\") == \"event_callback\"):\n            event = data.get(\"event\")\n            if (event.get(\"type\") == \"app_mention\"):\n                message = event.get(\"text\", \"\")\n                if message.startswith(\"<@\"):\n                    message = message.split(\"> \", 1)[1] if \"> \" in message else message\n                    channel = event.get(\"channel\", \"\")\n                    blocks = event.get(\"blocks\", [])\n                    blocks[0][\"elements\"][0][\"elements\"] = blocks[0].get(\"elements\")[0].get(\"elements\")[1:]\n                    token = settings.get(\"bot_token\")\n                    client = WebClient(token=token)\n                    try: \n                        response = self.session.app.chat.invoke(\n                            app_id=settings[\"app\"][\"app_id\"],\n                            query=message,\n                            inputs={},\n                            response_mode=\"blocking\",\n                        )\n                        try:\n                            blocks[0][\"elements\"][0][\"elements\"][0][\"text\"] = response.get(\"answer\")\n                            result = client.chat_postMessage(\n                                channel=channel,\n                                text=response.get(\"answer\"),\n                                blocks=blocks\n                            )\n                            return Response(\n                                status=200,\n                                response=json.dumps(result),\n                                content_type=\"application/json\"\n                            )\n                        except SlackApiError as e:\n                            raise e\n                    except Exception as e:\n                        err = traceback.format_exc()\n                        return Response(\n                            status=200,\n                            response=\"Sorry, I'm having trouble processing your request. Please try again later.\" + str(err),\n                            content_type=\"text/plain\",\n                        )\n                else:\n                    return Response(status=200, response=\"ok\")\n            else:\n                return Response(status=200, response=\"ok\")\n        else:\n            return Response(status=200, response=\"ok\")\n```\n\n----------------------------------------\n\nTITLE: Embedding Invocation in Python\nDESCRIPTION: This Python code snippet displays the implementation of the `_invoke` method for text embedding. This method takes model details, credentials, a list of texts to embed, and user information as input and returns the embeddings result as a `TextEmbeddingResult` entity.  It is capable of batch processing texts.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              texts: list[str], user: Optional[str] = None) \\\n          -> TextEmbeddingResult:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param texts: texts to embed\n    :param user: unique user id\n    :return: embeddings result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define LLM Result Model (Python)\nDESCRIPTION: Defines a Pydantic model `LLMResult` representing the result of a language model (LLM) call. It includes attributes for the `model` used, the `prompt_messages` sent, the `message` received (as an `AssistantPromptMessage`), the `usage` information (as an `LLMUsage` object), and an optional `system_fingerprint`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResult(BaseModel):\n    \"\"\"\n    Model class for llm result.\n    \"\"\"\n    model: str  # Actual used modele\n    prompt_messages: list[PromptMessage]  # prompt messages\n    message: AssistantPromptMessage  # response message\n    usage: LLMUsage  # usage info\n    system_fingerprint: Optional[str] = None  # request fingerprint, refer to OpenAI definition\n```\n\n----------------------------------------\n\nTITLE: Implement Moderation Model Invocation in Python\nDESCRIPTION: This code snippet illustrates the `_invoke` method for a moderation model, inheriting from `ModerationModel`. It takes `model` (model name), `credentials` (model credentials), `text` (text to moderate), and `user` (unique user id) as parameters. The function returns a boolean: `False` if the text is safe, `True` otherwise.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            text: str, user: Optional[str] = None) \\\n        -> bool:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param text: text to moderate\n    :param user: unique user id\n    :return: false if text is safe, true otherwise\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Tool Configuration: google_search.yaml\nDESCRIPTION: This YAML file defines the configuration for the GoogleSearch tool, including its identity (name, author, labels, description), parameters (query), and extra information (Python source file). The parameters section specifies the input required by the tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_6\n\nLANGUAGE: YAML\nCODE:\n```\nidentity:\n  name: google_search\n  author: Dify\n  label:\n    en_US: GoogleSearch\n    zh_Hans: Google Search\n    pt_BR: GoogleSearch\ndescription:\n  human:\n    en_US: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.\n    zh_Hans: A tool for performing Google SERP search and extracting snippets and webpages. Input should be a search query.\n    pt_BR: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.\n  llm: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.\nparameters:\n  - name: query\n    type: string\n    required: true\n    label:\n      en_US: Query string\n      zh_Hans: Query string\n      pt_BR: Query string\n    human_description:\n      en_US: used for searching\n      zh_Hans: used for searching webpage content\n      pt_BR: used for searching\n    llm_description: key words for searching\n    form: llm\nextra:\n  python:\n    source: tools/google_search.py\n```\n\n----------------------------------------\n\nTITLE: Define Assistant Prompt Message Model (Python)\nDESCRIPTION: Defines a Pydantic model `AssistantPromptMessage` representing a message returned by the model, typically used for `few-shots` or inputting chat history. It includes a nested `ToolCall` model for representing tool calls made by the assistant, including the tool ID, type, and function. The `tool_calls` attribute is a list of `ToolCall` objects.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nclass AssistantPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for assistant prompt message.\n    \"\"\"\n    class ToolCall(BaseModel):\n        \"\"\"\n        Model class for assistant prompt message tool call.\n        \"\"\"\n        class ToolCallFunction(BaseModel):\n            \"\"\"\n            Model class for assistant prompt message tool call function.\n            \"\"\"\n            name: str  # tool name\n            arguments: str  # tool arguments\n\n        id: str  # Tool ID, effective only in OpenAI tool calls. It's the unique ID for tool invocation and the same tool can be called multiple times.\n        type: str  # default: function\n        function: ToolCallFunction  # tool call information\n\n    role: PromptMessageRole = PromptMessageRole.ASSISTANT\n    tool_calls: list[ToolCall] = []  # The result of tool invocation in response from the model (returned only when tools are input and the model deems it necessary to invoke a tool).\n```\n\n----------------------------------------\n\nTITLE: Invoking an LLM Model in Python\nDESCRIPTION: This code snippet demonstrates how to invoke a Language Model (LLM) using the Dify SDK. It includes defining classes for handling parameters and configuring the LLM, creating prompt messages, and calling the model's `invoke` method. It showcases how to set up the necessary components and parameters to interact with an LLM within the Dify plugin framework.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/agent.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.model.llm import LLMModelConfig\nfrom dify_plugin.entities.model.message import (\n    PromptMessageTool,\n    SystemPromptMessage,\n    UserPromptMessage,\n)\nfrom dify_plugin.entities.tool import ToolParameter\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\n\nclass FunctionCallingParams(BaseModel):\n    query: str\n    instruction: str | None\n    model: AgentModelConfig\n    tools: list[ToolEntity] | None\n    maximum_iterations: int = 3\n\nclass FunctionCallingAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        \"\"\"\n        Run FunctionCall agent application\n        \"\"\"\n        # init params\n        fc_params = FunctionCallingParams(**parameters)\n        query = fc_params.query\n        model = fc_params.model\n        stop = fc_params.model.completion_params.get(\"stop\", []) if fc_params.model.completion_params else []\n        prompt_messages = [\n            SystemPromptMessage(content=\"your system prompt message\"),\n            UserPromptMessage(content=query),\n        ]\n        tools = fc_params.tools\n        prompt_messages_tools = self._init_prompt_tools(tools)\n\n        # invoke llm\n        chunks = self.session.model.llm.invoke(\n            model_config=LLMModelConfig(**model.model_dump(mode=\"json\")),\n            prompt_messages=prompt_messages,\n            stream=True,\n            stop=stop,\n            tools=prompt_messages_tools,\n        )\n\n    def _init_prompt_tools(self, tools: list[ToolEntity] | None) -> list[PromptMessageTool]:\n        \"\"\"\n        Init tools\n        \"\"\"\n\n        prompt_messages_tools = []\n        for tool in tools or []:\n            try:\n                prompt_tool = self._convert_tool_to_prompt_message_tool(tool)\n            except Exception:\n                # api tool may be deleted\n                continue\n\n            # save prompt tool\n            prompt_messages_tools.append(prompt_tool)\n\n        return prompt_messages_tools\n\n    def _convert_tool_to_prompt_message_tool(self, tool: ToolEntity) -> PromptMessageTool:\n        \"\"\"\n        convert tool to prompt message tool\n        \"\"\"\n        message_tool = PromptMessageTool(\n            name=tool.identity.name,\n            description=tool.description.llm if tool.description else \"\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        )\n\n        parameters = tool.parameters\n        for parameter in parameters:\n            if parameter.form != ToolParameter.ToolParameterForm.LLM:\n                continue\n\n            parameter_type = parameter.type\n            if parameter.type in {\n                ToolParameter.ToolParameterType.FILE,\n                ToolParameter.ToolParameterType.FILES,\n            }:\n                continue\n            enum = []\n            if parameter.type == ToolParameter.ToolParameterType.SELECT:\n                enum = [option.value for option in parameter.options] if parameter.options else []\n\n            message_tool.parameters[\"properties\"][parameter.name] = {\n                \"type\": parameter_type,\n                \"description\": parameter.llm_description or \"\",\n            }\n\n            if len(enum) > 0:\n                message_tool.parameters[\"properties\"][parameter.name][\"enum\"] = enum\n\n            if parameter.required:\n                message_tool.parameters[\"required\"].append(parameter.name)\n\n        return message_tool\n```\n\n----------------------------------------\n\nTITLE: Model Credential Validation Interface in Python\nDESCRIPTION: Defines the interface for validating model credentials. It takes the model name and a dictionary of credentials as input, raising an exception if validation fails. The credentials form is defined in either the `provider_credential_schema` or `model_credential_schema`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n    \"\"\"\n    Validate model credentials\n\n    :param model: model name\n    :param credentials: model credentials\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Provider (YAML)\nDESCRIPTION: Illustrates how to create an `agent.yaml` file to define basic agent provider information, including identity details (author, name, label, description, icon) and a reference to the specific agent strategy YAML file. This file acts as a central configuration point for the agent.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/agent.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: langgenius\n  name: agent\n  label:\n    en_US: Agent\n    zh_Hans: Agent\n    pt_BR: Agent\n  description:\n    en_US: Agent\n    zh_Hans: Agent\n    pt_BR: Agent\n  icon: icon.svg\nstrategies:\n  - strategies/function_calling.yaml\n```\n\n----------------------------------------\n\nTITLE: Create Document from Text - Dify API (cURL)\nDESCRIPTION: This cURL command creates a new document in an existing Dify knowledge base using the provided text. It requires the dataset ID and API key for authentication and authorization. The request body includes the document name, text content, indexing technique, and processing rule.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/document/create_by_text' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"name\": \"text\",\"text\": \"text\",\"indexing_technique\": \"high_quality\",\"process_rule\": {\"mode\": \"automatic\"}}'\n```\n\n----------------------------------------\n\nTITLE: Logging Model Calls in Python\nDESCRIPTION: This snippet illustrates how to create and finish log messages before and after a model call. It uses `self.create_log_message` to initiate a log with a label, data, and metadata and `self.finish_log_message` to finalize the log with the output, tool names, and tool inputs, as well as timing information. This helps in tracking the execution and debugging the agent's reasoning process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel_log = self.create_log_message(\n            label=f\"{params.model.model} Thought\",\n            data={},\n            metadata={\"start_at\": model_started_at, \"provider\": params.model.provider},\n            status=ToolInvokeMessage.LogMessage.LogStatus.START,\n        )\nyield model_log\nself.session.model.llm.invoke(...)\nyield self.finish_log_message(\n    log=model_log,\n    data={\n        \"output\": response,\n        \"tool_name\": tool_call_names,\n        \"tool_input\": tool_call_inputs,\n    },\n    metadata={\n        \"started_at\": model_started_at,\n        \"finished_at\": time.perf_counter(),\n        \"elapsed_time\": time.perf_counter() - model_started_at,\n        \"provider\": params.model.provider,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Output Schema for a Dify Tool (YAML)\nDESCRIPTION: This YAML snippet defines the output schema for a Dify tool, using JSON schema format. The `output_schema` section specifies the structure and data types of the variables that the tool can output. This is crucial for enabling workflow applications to understand and utilize the tool's outputs.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/tool.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: author\n  name: tool\n  label:\n    en_US: label\n    zh_Hans: 标签\n    ja_JP: レベル\n    pt_BR: etiqueta\ndescription:\n  human:\n    en_US: description\n    zh_Hans: 描述\n    ja_JP: 説明\n    pt_BR: descrição\n  llm: description\noutput_schema:\n  type: object\n  properties:\n    name:\n      type: string\n```\n\n----------------------------------------\n\nTITLE: Example App Request in Endpoint Python\nDESCRIPTION: Demonstrates how to request a chat-type App within an Endpoint in Dify using the `self.session.app.workflow.invoke` method. It sets up a generator to yield responses, converts data to JSON, and returns it as text/html.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom typing import Mapping\nfrom werkzeug import Request, Response\nfrom dify_plugin import Endpoint\n\nclass Duck(Endpoint):\n    def _invoke(self, r: Request, values: Mapping, settings: Mapping) -> Response:\n        \"\"\"\n        与えられたリクエストでエンドポイントを呼び出します。\n        \"\"\"\n        app_id = values[\"app_id\"]\n        def generator():\n            response = self.session.app.workflow.invoke(\n                app_id=app_id, inputs={}, response_mode=\"streaming\", files=[]\n            )\n            for data in response:\n                yield f\"{json.dumps(data)} <br>\"\n        return Response(generator(), status=200, content_type=\"text/html\")\n```\n\n----------------------------------------\n\nTITLE: JSON Validation - Python Code Node\nDESCRIPTION: This Python code validates a JSON string by parsing it using `json.loads()`. The `main` function takes a string `json_str` as input, attempts to parse it as JSON, and returns a dictionary containing the parsed JSON object under the key 'result'. If the JSON string is invalid, the `json.loads()` function will raise an exception, which is expected to be handled by the workflow's error handling mechanism.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_74\n\nLANGUAGE: python\nCODE:\n```\ndef main(json_str: str) -> dict:\n    obj = json.loads(json_str)\n    return {'result': obj}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Document List from Dify Knowledge Base\nDESCRIPTION: Retrieves a list of documents associated with a specific dataset from the Dify knowledge base. It requires the dataset ID as a path parameter and a valid API key in the Authorization header. The response includes document metadata, indexing status, and other relevant information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Defining BasicAgentAgentStrategy for Model Invocation in Python\nDESCRIPTION: This code snippet defines the `BasicAgentAgentStrategy` class, which inherits from `AgentStrategy`. It includes the `_invoke` method, responsible for invoking the LLM, handling tool calls, and creating appropriate responses. The class also includes helper methods for converting tools to prompt messages, checking for tool calls, and extracting tool call information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom collections.abc import Generator\nfrom typing import Any, cast\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.model.llm import LLMModelConfig, LLMResult, LLMResultChunk\nfrom dify_plugin.entities.model.message import (\n    PromptMessageTool,\n    UserPromptMessage,\n)\nfrom dify_plugin.entities.tool import ToolInvokeMessage, ToolParameter, ToolProviderType\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n        chunks: Generator[LLMResultChunk, None, None] | LLMResult = (\n            self.session.model.llm.invoke(\n                model_config=LLMModelConfig(**params.model.model_dump(mode=\"json\")),\n                prompt_messages=[UserPromptMessage(content=params.query)],\n                tools=[\n                    self._convert_tool_to_prompt_message_tool(tool)\n                    for tool in params.tools\n                ],\n                stop=params.model.completion_params.get(\"stop\", [])\n                if params.model.completion_params\n                else [],\n                stream=True,\n            )\n        )\n        response = \"\"\n        tool_calls = []\n        tool_instances = (\n            {tool.identity.name: tool for tool in params.tools} if params.tools else {}\n        )\n\n        for chunk in chunks:\n            # check if there is any tool call\n            if self.check_tool_calls(chunk):\n                tool_calls = self.extract_tool_calls(chunk)\n                tool_call_names = \";\".join([tool_call[1] for tool_call in tool_calls])\n                try:\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls},\n                        ensure_ascii=False,\n                    )\n                except json.JSONDecodeError:\n                    # ensure ascii to avoid encoding error\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls}\n                    )\n                print(tool_call_names, tool_call_inputs)\n            if chunk.delta.message and chunk.delta.message.content:\n                if isinstance(chunk.delta.message.content, list):\n                    for content in chunk.delta.message.content:\n                        response += content.data\n                        print(content.data, end=\"\", flush=True)\n                else:\n                    response += str(chunk.delta.message.content)\n                    print(str(chunk.delta.message.content), end=\"\", flush=True)\n\n            if chunk.delta.usage:\n                # usage of the model\n                usage = chunk.delta.usage\n\n        yield self.create_text_message(\n            text=f\"{response or json.dumps(tool_calls, ensure_ascii=False)}\\n\"\n        )\n        result = \"\"\n        for tool_call_id, tool_call_name, tool_call_args in tool_calls:\n            tool_instance = tool_instances[tool_call_name]\n            tool_invoke_responses = self.session.tool.invoke(\n                provider_type=ToolProviderType.BUILT_IN,\n                provider=tool_instance.identity.provider,\n                tool_name=tool_instance.identity.name,\n                parameters={**tool_instance.runtime_parameters, **tool_call_args},\n            )\n            if not tool_instance:\n                tool_invoke_responses = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": f\"there is not a tool named {tool_call_name}\",\n                }\n            else:\n                # invoke tool\n                tool_invoke_responses = self.session.tool.invoke(\n                    provider_type=ToolProviderType.BUILT_IN,\n                    provider=tool_instance.identity.provider,\n                    tool_name=tool_instance.identity.name,\n                    parameters={**tool_instance.runtime_parameters, **tool_call_args},\n                )\n                result = \"\"\n                for tool_invoke_response in tool_invoke_responses:\n                    if tool_invoke_response.type == ToolInvokeMessage.MessageType.TEXT:\n                        result += cast(\n                            ToolInvokeMessage.TextMessage, tool_invoke_response.message\n                        ).text\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.LINK\n                    ):\n                        result += (\n                            f\"result link: {cast(ToolInvokeMessage.TextMessage, tool_invoke_response.message).text}.\"\n                            + \" please tell user to check it.\"\n                        )\n                    elif tool_invoke_response.type in {\n                        ToolInvokeMessage.MessageType.IMAGE_LINK,\n                        ToolInvokeMessage.MessageType.IMAGE,\n                    }:\n                        result += (\n                            \"image has been created and sent to user already, \"\n                            + \"you do not need to create it, just tell the user to check it now.\"\n                        )\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.JSON\n                    ):\n                        text = json.dumps(\n                            cast(\n                                ToolInvokeMessage.JsonMessage,\n                                tool_invoke_response.message,\n                            ).json_object,\n                            ensure_ascii=False,\n                        )\n                        result += f\"tool response: {text}.\"\n                    else:\n                        result += f\"tool response: {tool_invoke_response.message!r}.\"\n\n                tool_response = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": result,\n                }\n        yield self.create_text_message(result)\n\n    def _convert_tool_to_prompt_message_tool(\n        self, tool: ToolEntity\n    ) -> PromptMessageTool:\n        \"\"\"\n        convert tool to prompt message tool\n        \"\"\"\n        message_tool = PromptMessageTool(\n            name=tool.identity.name,\n            description=tool.description.llm if tool.description else \"\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        )\n\n        parameters = tool.parameters\n        for parameter in parameters:\n            if parameter.form != ToolParameter.ToolParameterForm.LLM:\n                continue\n\n            parameter_type = parameter.type\n            if parameter.type in {\n                ToolParameter.ToolParameterType.FILE,\n                ToolParameter.ToolParameterType.FILES,\n            }:\n                continue\n            enum = []\n            if parameter.type == ToolParameter.ToolParameterType.SELECT:\n                enum = (\n                    [option.value for option in parameter.options]\n                    if parameter.options\n                    else []\n                )\n\n            message_tool.parameters[\"properties\"][parameter.name] = {\n                \"type\": parameter_type,\n                \"description\": parameter.llm_description or \"\",\n            }\n\n            if len(enum) > 0:\n                message_tool.parameters[\"properties\"][parameter.name][\"enum\"] = enum\n\n            if parameter.required:\n                message_tool.parameters[\"required\"].append(parameter.name)\n\n        return message_tool\n\n    def check_tool_calls(self, llm_result_chunk: LLMResultChunk) -> bool:\n        \"\"\"\n        Check if there is any tool call in llm result chunk\n        \"\"\"\n        return bool(llm_result_chunk.delta.message.tool_calls)\n\n    def extract_tool_calls(\n        self, llm_result_chunk: LLMResultChunk\n    ) -> list[tuple[str, str, dict[str, Any]]]:\n        \"\"\"\n        Extract tool calls from llm result chunk\n\n        Returns:\n            List[Tuple[str, str, Dict[str, Any]]]: [(tool_call_id, tool_call_name, tool_call_args)]\n        \"\"\"\n        tool_calls = []\n        for prompt_message in llm_result_chunk.delta.message.tool_calls:\n            args = {}\n            if prompt_message.function.arguments != \"\":\n                args = json.loads(prompt_message.function.arguments)\n\n            tool_calls.append(\n                (\n                    prompt_message.id,\n                    prompt_message.function.name,\n                    args,\n                )\n            )\n\n        return tool_calls\n```\n\n----------------------------------------\n\nTITLE: Tool Label Enumeration (Python)\nDESCRIPTION: This Python code defines an enumeration `ToolLabelEnum` for categorizing tools with predefined labels such as `SEARCH`, `IMAGE`, `VIDEOS`, etc. These tags are used to help users quickly search for plugins by category in the front-end.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass ToolLabelEnum(Enum):\n  SEARCH = 'search'\n  IMAGE = 'image'\n  VIDEOS = 'videos'\n  WEATHER = 'weather'\n  FINANCE = 'finance'\n  DESIGN = 'design'\n  TRAVEL = 'travel'\n  SOCIAL = 'social'\n  NEWS = 'news'\n  MEDICAL = 'medical'\n  PRODUCTIVITY = 'productivity'\n  EDUCATION = 'education'\n  BUSINESS = 'business'\n  ENTERTAINMENT = 'entertainment'\n  UTILITIES = 'utilities'\n  OTHER = 'other'\n```\n\n----------------------------------------\n\nTITLE: Audio to Text Model Invocation Python\nDESCRIPTION: This Python snippet defines the `_invoke` method for an audio-to-text model. It takes a model name, credentials, an audio file, and an optional user ID as input. It returns the converted text from the audio file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            file: IO[bytes], user: Optional[str] = None) \\\n        -> str:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param file: audio file\n    :param user: unique user id\n    :return: text for given audio file\n    \"\"\"        \n```\n\n----------------------------------------\n\nTITLE: Anthropic Provider Configuration (YAML)\nDESCRIPTION: Defines the Anthropic model provider's configuration using a YAML file. This includes basic information, supported model types, configuration methods, and credential rules.  It specifies the provider name, labels, descriptions, icons, background color, help URL, supported model types (llm), configuration methods (predefined-model) and the schema for the API key credential.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprovider: anthropic\nlabel:\n en_US: Anthropic\ndescription:\n en_US: Anthropic's powerful models, such as Claude 3.\nicon_small:\n en_US: icon_s_en.svg\nicon_large:\n en_US: icon_l_en.svg\nbackground: \"#F0F0EB\"\nhelp:\n title:\n   en_US: Get your API Key from Anthropic\n url:\n   en_US: https://console.anthropic.com/account/keys\nsupported_model_types:\n - llm\nconfigurate_methods:\n - predefined-model\nprovider_credential_schema:\n credential_form_schemas:\n   - variable: anthropic_api_key\n     label:\n       en_US: API Key\n     type: secret-input\n     required: true\n     placeholder:\n       en_US: Enter your API Key\n   - variable: anthropic_api_url\n     label:\n       en_US: API URL\n     type: text-input\n     required: false\n     placeholder:\n       en_US: Enter your API URL\nmodels:\n llm:\n   predefined:\n     - \"models/llm/*.yaml\"\n   position: \"models/llm/_position.yaml\"\nextra:\n python:\n   provider_source: provider/anthropic.py\n   model_sources:\n     - \"models/llm/llm.py\"\n```\n\n----------------------------------------\n\nTITLE: Google Search Tool Implementation (Python)\nDESCRIPTION: This Python code implements the GoogleSearch tool, making requests to the SerpApi API and returning the results in JSON format.  It defines a class `GoogleSearchTool` inheriting from `Tool`, which overrides the `_invoke` method to perform the search. It requires the `requests` and `dify_plugin` libraries.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom typing import Any\n\nimport requests\n\nfrom dify_plugin import Tool\nfrom dify_plugin.entities.tool import ToolInvokeMessage\n\nSERP_API_URL = \"https://serpapi.com/search\"\n\nclass GoogleSearchTool(Tool):\n    def _parse_response(self, response: dict) -> dict:\n        result = {}\n        if \"knowledge_graph\" in response:\n            result[\"title\"] = response[\"knowledge_graph\"].get(\"title\", \"\")\n            result[\"description\"] = response[\"knowledge_graph\"].get(\"description\", \"\")\n        if \"organic_results\" in response:\n            result[\"organic_results\"] = [\n                {\n                    \"title\": item.get(\"title\", \"\"),\n                    \"link\": item.get(\"link\", \"\"),\n                    \"snippet\": item.get(\"snippet\", \"\"),\n                }\n                for item in response[\"organic_results\"]\n            ]\n        return result\n\n    def _invoke(self, tool_parameters: dict[str, Any]) -> Generator[ToolInvokeMessage]:\n        params = {\n            \"api_key\": self.runtime.credentials[\"serpapi_api_key\"],\n            \"q\": tool_parameters[\"query\"],\n            \"engine\": \"google\",\n            \"google_domain\": \"google.com\",\n            \"gl\": \"us\",\n            \"hl\": \"en\",\n        }\n\n        response = requests.get(url=SERP_API_URL, params=params, timeout=5)\n        response.raise_for_status()\n        valuable_res = self._parse_response(response.json())\n        \n        yield self.create_json_message(valuable_res)\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for OpenAI Model Credentials\nDESCRIPTION: This YAML configuration demonstrates how to define model-specific credentials using `model_credential_schema` for providers like OpenAI, which offer fine-tuned models.  It includes fields for model name, API key, organization ID, and API base URL.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/new-provider.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nmodel_credential_schema:\n  model: # Fine-tuned model name\n    label:\n      en_US: Model Name\n      zh_Hans: 模型名称\n    placeholder:\n      en_US: Enter your model name\n      zh_Hans: 输入模型名称\n  credential_form_schemas:\n  - variable: openai_api_key\n    label:\n      en_US: API Key\n    type: secret-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入你的 API Key\n      en_US: Enter your API Key\n  - variable: openai_organization\n    label:\n        zh_Hans: 组织 ID\n        en_US: Organization\n    type: text-input\n    required: false\n    placeholder:\n      zh_Hans: 在此输入你的组织 ID\n      en_US: Enter your Organization ID\n  - variable: openai_api_base\n    label:\n      zh_Hans: API Base\n      en_US: API Base\n    type: text-input\n    required: false\n    placeholder:\n      zh_Hans: 在此输入你的 API Base\n      en_US: Enter your API Base\n```\n\n----------------------------------------\n\nTITLE: Invoking Tools from Agent Strategy (Python)\nDESCRIPTION: This code demonstrates how to invoke tools from within an Agent strategy using `session.tool.invoke()`. It shows the structure of parameters that are required: `provider_type`, `provider`, `tool_name`, and `parameters`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n def invoke(\n        self,\n        provider_type: ToolProviderType,\n        provider: str,\n        tool_name: str,\n        parameters: dict[str, Any],\n    ) -> Generator[ToolInvokeMessage, None, None]:...\n```\n\n----------------------------------------\n\nTITLE: LLM Invocation Function\nDESCRIPTION: This Python code represents the core method for invoking a large language model, handling both streaming and synchronous responses. It takes parameters like model name, credentials, prompt messages, model parameters, and optional tools and stop words. The return value is either a full `LLMResult` or a generator for streaming responses.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/customizable-model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n                prompt_messages: list[PromptMessage], model_parameters: dict,\n                tools: Optional[list[PromptMessageTool]] = None, stop: Optional[list[str]] = None,\n                stream: bool = True, user: Optional[str] = None) \\\n            -> Union[LLMResult, Generator]:\n        \"\"\"\n        Invoke large language model\n\n        :param model: model name\n        :param credentials: model credentials\n        :param prompt_messages: prompt messages\n        :param model_parameters: model parameters\n        :param tools: tools for tool calling\n        :param stop: stop words\n        :param stream: is stream response\n        :param user: unique user id\n        :return: full response or stream response chunk generator result\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Google Search Tool Implementation (Python)\nDESCRIPTION: This Python code implements the Google Search tool using the SerpApi to perform the search and extract the results. It defines the `GoogleSearchTool` class, which inherits from `Tool`, and includes methods for parsing the response and invoking the tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom typing import Any\n\nimport requests\n\nfrom dify_plugin import Tool\nfrom dify_plugin.entities.tool import ToolInvokeMessage\n\nSERP_API_URL = \"https://serpapi.com/search\"\n\nclass GoogleSearchTool(Tool):\n    def _parse_response(self, response: dict) -> dict:\n        result = {}\n        if \"knowledge_graph\" in response:\n            result[\"title\"] = response[\"knowledge_graph\"].get(\"title\", \"\")\n            result[\"description\"] = response[\"knowledge_graph\"].get(\"description\", \"\")\n        if \"organic_results\" in response:\n            result[\"organic_results\"] = [\n                {\n                    \"title\": item.get(\"title\", \"\"),\n                    \"link\": item.get(\"link\", \"\"),\n                    \"snippet\": item.get(\"snippet\", \"\"),\n                }\n                for item in response[\"organic_results\"]\n            ]\n        return result\n\n    def _invoke(self, tool_parameters: dict[str, Any]) -> Generator[ToolInvokeMessage]:\n        params = {\n            \"api_key\": self.runtime.credentials[\"serpapi_api_key\"],\n            \"q\": tool_parameters[\"query\"],\n            \"engine\": \"google\",\n            \"google_domain\": \"google.com\",\n            \"gl\": \"us\",\n            \"hl\": \"en\",\n        }\n\n        response = requests.get(url=SERP_API_URL, params=params, timeout=5)\n        response.raise_for_status()\n        valuable_res = self._parse_response(response.json())\n        \n        yield self.create_json_message(valuable_res)\n```\n\n----------------------------------------\n\nTITLE: Moderation Input Request Body JSON\nDESCRIPTION: Defines the structure for the request body of the `app.moderation.input` extension point. It includes the extension point name, application ID, input variables, and the user query.  The `app_id` is a string representing the application's unique ID. `inputs` is a dictionary of input variables, and `query` is the user's input text or null if no query exists. Flagged content needs specific attention.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation-extension.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"point\": \"app.moderation.input\",\n    \"app_id\": string,\n    \"inputs\": {\n        \"var_1\": \"value_1\",\n        \"var_2\": \"value_2\",\n        ...\n    },\n    \"query\": string | null\n}\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Role Enum\nDESCRIPTION: Defines the `PromptMessageRole` enumeration, specifying possible roles for prompt messages: SYSTEM, USER, ASSISTANT, and TOOL. These roles help categorize and structure conversations with language models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageRole(Enum):\n    \"\"\"\n    Enum class for prompt message.\n    \"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    TOOL = \"tool\"\n```\n\n----------------------------------------\n\nTITLE: Request Body Structure (JSON)\nDESCRIPTION: Describes the structure of the request body for Dify API extensions, including the 'point' (extension point identifier) and 'params' (module-specific parameters).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/README.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"point\":  string, // Extension point, different modules may contain multiple extension points\n    \"params\": {\n        ...  // Parameters passed to each module's extension point\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing CloudServiceModeration Class (Python)\nDESCRIPTION: This Python code defines the `CloudServiceModeration` class, which extends the `Moderation` base class, implementing custom moderation logic. It includes methods for validating configuration (`validate_config`), moderating inputs (`moderation_for_inputs`), and moderating outputs (`moderation_for_outputs`).  The `name` attribute must match the directory and file name.  Dependencies include 'core.moderation.base'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/code-based-extension/moderation.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom core.moderation.base import Moderation, ModerationAction, ModerationInputsResult, ModerationOutputsResult\n\nclass CloudServiceModeration(Moderation):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"cloud_service\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user save the config.\n\n        Example:\n            .. code-block:: python\n                config = {\n                    \"cloud_provider\": \"GoogleCloud\",\n                    \"api_endpoint\": \"https://api.example.com\",\n                    \"api_keys\": \"123456\",\n                    \"inputs_config\": {\n                        \"enabled\": True,\n                        \"preset_response\": \"Your content violates our usage policy. Please revise and try again.\"\n                    },\n                    \"outputs_config\": {\n                        \"enabled\": True,\n                        \"preset_response\": \"Your content violates our usage policy. Please revise and try again.\"\n                    }\n                }\n\n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n\n        cls._validate_inputs_and_outputs_config(config, True)\n\n        if not config.get(\"cloud_provider\"):\n            raise ValueError(\"cloud_provider is required\")\n\n        if not config.get(\"api_endpoint\"):\n            raise ValueError(\"api_endpoint is required\")\n\n        if not config.get(\"api_keys\"):\n            raise ValueError(\"api_keys is required\")\n\n    def moderation_for_inputs(self, inputs: dict, query: str = \"\") -> ModerationInputsResult:\n        \"\"\"\n        Moderation for inputs.\n\n        :param inputs: user inputs\n        :param query: the query of chat app, there is empty if is completion app\n        :return: the moderation result\n        \"\"\"\n        flagged = False\n        preset_response = \"\"\n\n        if self.config['inputs_config']['enabled']:\n            preset_response = self.config['inputs_config']['preset_response']\n\n            if query:\n                inputs['query__'] = query\n            flagged = self._is_violated(inputs)\n\n        # return ModerationInputsResult(flagged=flagged, action=ModerationAction.overridden, inputs=inputs, query=query)\n        return ModerationInputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)\n\n    def moderation_for_outputs(self, text: str) -> ModerationOutputsResult:\n        \"\"\"\n        Moderation for outputs.\n\n        :param text: the text of LLM response\n        :return: the moderation result\n        \"\"\"\n        flagged = False\n        preset_response = \"\"\n\n        if self.config['outputs_config']['enabled']:\n            preset_response = self.config['outputs_config']['preset_response']\n\n            flagged = self._is_violated({'text': text})\n\n        # return ModerationOutputsResult(flagged=flagged, action=ModerationAction.overridden, text=text)\n        return ModerationOutputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)\n\n    def _is_violated(self, inputs: dict):\n        \"\"\"\n        The main logic of moderation.\n\n        :param inputs:\n        :return: the moderation result\n        \"\"\"\n        return False\n```\n\n----------------------------------------\n\nTITLE: Agent Log Example: Thinking Process (Python)\nDESCRIPTION: This example illustrates how to use the logging functions to track the agent's 'thinking' process. It creates a 'Thinking' log message with a 'start' status, invokes the LLM, finishes the log message, and yields both the log messages and the LLM's text response as part of the agent's overall response.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/agent.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass FunctionCallingAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        thinking_log = self.create_log_message(\n            data={\n                \"Query\": parameters.get(\"query\"),\n            },\n            label=\"Thinking\",\n            status=AgentInvokeMessage.LogMessage.LogStatus.START,\n        )\n\n        yield thinking_log\n\n        llm_response = self.session.model.llm.invoke(\n            model_config=LLMModelConfig(\n                provider=\"openai\",\n                model=\"gpt-4o-mini\",\n                mode=\"chat\",\n                completion_params={},\n            ),\n            prompt_messages=[\n                SystemPromptMessage(content=\"you are a helpful assistant\"),\n                UserPromptMessage(content=parameters.get(\"query\")),\n            ],\n            stream=False,\n            tools=[],\n        )\n\n        thinking_log = self.finish_log_message(\n            log=thinking_log,\n        )\n\n        yield thinking_log\n\n        yield self.create_text_message(text=llm_response.message.content)\n```\n\n----------------------------------------\n\nTITLE: Implementing Google Search Tool Logic in Python\nDESCRIPTION: This Python code implements the Google Search tool's logic within the `_invoke` method. It retrieves the query and result_type from tool_Parameters and uses the SerpAPI to perform the search. It returns a text or link message based on the result_type.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/quick-tool-integration.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom core.tools.tool.builtin_tool import BuiltinTool\nfrom core.tools.entities.tool_entities import ToolInvokeMessage\n\nfrom typing import Any, Dict, List, Union\n\nclass GoogleSearchTool(BuiltinTool):\n    def _invoke(self, \n                user_id: str,\n               tool_Parameters: Dict[str, Any], \n        ) -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:\n        \"\"\"\n            invoke tools\n        \"\"\"\n        query = tool_Parameters['query']\n        result_type = tool_Parameters['result_type']\n        api_key = self.runtime.credentials['serpapi_api_key']\n        # TODO: search with serpapi\n        result = SerpAPI(api_key).run(query, result_type=result_type)\n\n        if result_type == 'text':\n            return self.create_text_message(text=result)\n        return self.create_link_message(link=result)\n```\n\n----------------------------------------\n\nTITLE: Model YAML Configuration for claude-2.1\nDESCRIPTION: Defines the YAML configuration for the claude-2.1 model. This configuration includes model identifier, display name, model type, supported features, model properties (like mode and context size), parameter rules (including templates and custom configurations), and pricing information. It specifies how the model integrates with the Dify platform.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/predefined-model.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodel: claude-2.1  # Model identifier\n# Model display name, can be set in en_US English and zh_Hans Chinese. If zh_Hans is not set, it will default to en_US.\n# You can also not set a label, in which case the model identifier will be used.\nlabel:\n  en_US: claude-2.1\nmodel_type: llm  # Model type, claude-2.1 is an LLM\nfeatures:  # Supported features, agent-thought supports Agent reasoning, vision supports image understanding\n- agent-thought\nmodel_properties:  # Model properties\n  mode: chat  # LLM mode, complete for text completion model, chat for dialogue model\n  context_size: 200000  # Maximum context size supported\nparameter_rules:  # Model invocation parameter rules, only LLM needs to provide\n- name: temperature  # Invocation parameter variable name\n  # There are 5 preset variable content configuration templates: temperature/top_p/max_tokens/presence_penalty/frequency_penalty\n  # You can set the template variable name directly in use_template, and it will use the default configuration in entities.defaults.PARAMETER_RULE_TEMPLATE\n  # If additional configuration parameters are set, they will override the default configuration\n  use_template: temperature\n- name: top_p\n  use_template: top_p\n- name: top_k\n  label:  # Invocation parameter display name\n    zh_Hans: 取样数量\n    en_US: Top k\n  type: int  # Parameter type, supports float/int/string/boolean\n  help:  # Help information, describes the parameter's function\n    zh_Hans: 仅从每个后续标记的前 K 个选项中采样。\n    en_US: Only sample from the top K options for each subsequent token.\n  required: false  # Whether it is required, can be omitted\n- name: max_tokens_to_sample\n  use_template: max_tokens\n  default: 4096  # Default parameter value\n  min: 1  # Minimum parameter value, only applicable to float/int\n  max: 4096  # Maximum parameter value, only applicable to float/int\npricing:  # Pricing information\n  input: '8.00'  # Input unit price, i.e., Prompt unit price\n  output: '24.00'  # Output unit price, i.e., return content unit price\n  unit: '0.000001'  # Price unit, the above price is per 100K\n  currency: USD  # Price currency\n```\n\n----------------------------------------\n\nTITLE: Model Parameters Schema Definition (Python)\nDESCRIPTION: This Python code shows how to dynamically define the model parameters schema, which is required if the YAML file does not predefine the parameters. It demonstrates how to add parameter rules for parameters like `temperature`, `top_p`, and `max_tokens`, and how to conditionally add parameters based on the specific model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n  def get_customizable_model_schema(self, model: str, credentials: dict) -> AIModelEntity | None:\n      \"\"\"\n          used to define customizable model schema\n      \"\"\"\n      rules = [\n          ParameterRule(\n              name='temperature', type=ParameterType.FLOAT,\n              use_template='temperature',\n              label=I18nObject(\n                  zh_Hans='温度', en_US='Temperature'\n              )\n          ),\n          ParameterRule(\n              name='top_p', type=ParameterType.FLOAT,\n              use_template='top_p',\n              label=I18nObject(\n                  zh_Hans='Top P', en_US='Top P'\n              )\n          ),\n          ParameterRule(\n              name='max_tokens', type=ParameterType.INT,\n              use_template='max_tokens',\n              min=1,\n              default=512,\n              label=I18nObject(\n                  zh_Hans='最大生成长度', en_US='Max Tokens'\n              )\n          )\n      ]\n\n      # if model is A, add top_k to rules\n      if model == 'A':\n          rules.append(\n              ParameterRule(\n                  name='top_k', type=ParameterType.INT,\n                  use_template='top_k',\n                  min=1,\n                  default=50,\n                  label=I18nObject(\n                      zh_Hans='Top K', en_US='Top K'\n                  )\n              )\n          )\n\n      \"\"\"\n          some NOT IMPORTANT code here\n      \"\"\"\n\n      entity = AIModelEntity(\n          model=model,\n          label=I18nObject(\n              en_US=model\n          ),\n          fetch_from=FetchFrom.CUSTOMIZABLE_MODEL,\n          model_type=model_type,\n          model_properties={\n              ModelPropertyKey.MODE:  ModelType.LLM,\n          },\n          parameter_rules=rules\n      )\n\n      return entity\n```\n\n----------------------------------------\n\nTITLE: LLM Invocation with Streaming and Synchronous Returns - Python\nDESCRIPTION: This code snippet demonstrates the core method for LLM invocation, supporting both streaming and synchronous responses.  It uses two internal functions, `_handle_stream_response` for streaming and `_handle_sync_response` for synchronous results, due to Python's `yield` keyword fixing the return type to `Generator` when streaming.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/customizable-model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n                prompt_messages: list[PromptMessage], model_parameters: dict,\n                tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,\n                stream: bool = True, user: Optional[str] = None) \\\n            -> Union[LLMResult, Generator]:\n        \"\"\"\n        Invoke large language model\n\n        :param model: model name\n        :param credentials: model credentials\n        :param prompt_messages: prompt messages\n        :param model_parameters: model parameters\n        :param tools: tools for tool calling\n        :param stop: stop words\n        :param stream: is stream response\n        :param user: unique user id\n        :return: full response or stream response chunk generator result\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Parsing Tool Invoke Response in Dify Plugin (Python)\nDESCRIPTION: This Python code snippet demonstrates how to parse the response from a tool invocation within a Dify plugin.  The `parse_invoke_response` function iterates through the generator returned by the `self.session.tool.invoke()` function and extracts the relevant information from each `ToolInvokeMessage` based on its `MessageType`. It supports extracting text, links, image links/images, and JSON responses, concatenating them into a single result string.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/agent.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom collections.abc import Generator\nfrom typing import cast\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.tool import ToolInvokeMessage\n\ndef parse_invoke_response(tool_invoke_responses: Generator[AgentInvokeMessage]) -> str:\n    result = \"\"\n    for response in tool_invoke_responses:\n        if response.type == ToolInvokeMessage.MessageType.TEXT:\n            result += cast(ToolInvokeMessage.TextMessage, response.message).text\n        elif response.type == ToolInvokeMessage.MessageType.LINK:\n            result += (\n                f\"result link: {cast(ToolInvokeMessage.TextMessage, response.message).text}.\"\n                + \" please tell user to check it.\"\n            )\n        elif response.type in {\n            ToolInvokeMessage.MessageType.IMAGE_LINK,\n            ToolInvokeMessage.MessageType.IMAGE,\n        }:\n            result += (\n                \"image has been created and sent to user already, \"\n                + \"you do not need to create it, just tell the user to check it now.\"\n            )\n        elif response.type == ToolInvokeMessage.MessageType.JSON:\n            text = json.dumps(cast(ToolInvokeMessage.JsonMessage, response.message).json_object, ensure_ascii=False)\n            result += f\"tool response: {text}.\".\n        else:\n            result += f\"tool response: {response.message!r}.\".\n    return result\n```\n\n----------------------------------------\n\nTITLE: Retrieval API Request Example\nDESCRIPTION: This snippet demonstrates a sample JSON payload for a POST request to the /retrieval endpoint. It includes the knowledge_id, query, and retrieval_setting, which specifies the top_k and score_threshold for the retrieval process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/external-knowledge-api-documentation.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\nPOST <your-endpoint>/retrieval HTTP/1.1\n-- 请求头\nContent-Type: application/json\nAuthorization: Bearer your-api-key\n-- 数据\n{\n    \"knowledge_id\": \"your-knowledge-id\",\n    \"query\": \"你的问题\",\n    \"retrieval_setting\":{\n        \"top_k\": 2,\n        \"score_threshold\": 0.5\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Validation in Code Node - Python\nDESCRIPTION: This Python code snippet demonstrates how to validate a JSON string in a code node. It takes a JSON string as input, parses it using `json.loads`, and returns the parsed object in a dictionary with the key 'result'. This code relies on the `json` library being available.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_74\n\nLANGUAGE: python\nCODE:\n```\ndef main(json_str: str) -> dict:\n    obj = json.loads(json_str)\n    return {'result': obj}\n```\n\n----------------------------------------\n\nTITLE: Modify Metadata for a Single Document via Dify API\nDESCRIPTION: Modifies the metadata associated with a single document within a dataset in the Dify knowledge base. Requires the dataset ID as a path parameter, a valid API key in the Authorization header, and a JSON payload containing the document ID and a list of metadata updates.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/metadata' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer {api_key}'\n--data '{\n    \"operation_data\":[\n        {\n            \"document_id\": \"3e928bc4-65ea-4201-87c8-cbcc5871f525\",\n            \"metadata_list\": [\n                    {\n                    \"id\": \"1887f5ec-966f-4c93-8c99-5ad386022f46\",\n                    \"value\": \"dify\",\n                    \"name\": \"test\"\n                }\n            ]\n        }\n    ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Running SearXNG Docker Container\nDESCRIPTION: This command runs a SearXNG Docker container, mapping port 8080 inside the container to port 8081 on the host, and mounting the SearXNG configuration directory. This makes the SearXNG service accessible on port 8081.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd dify\ndocker run --rm -d -p 8081:8080 -v \"${PWD}/api/core/tools/provider/builtin/searxng/docker:/etc/searxng\" searxng/searxng\n```\n\n----------------------------------------\n\nTITLE: Speech-to-Text Model Invocation Interface in Python\nDESCRIPTION: This code snippet defines the `_invoke` method for a speech-to-text model. It takes an audio file as input and returns the transcribed text. It requires model name, credentials, the audio file stream, and optionally a user ID.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, file: IO[bytes], user: Optional[str] = None) -> str:\n      \"\"\"\n      Invoke large language model\n  \n      :param model: model name\n      :param credentials: model credentials\n      :param file: audio file\n      :param user: unique user id\n      :return: text for given audio file\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking Built-in Tools in Python\nDESCRIPTION: This snippet defines the endpoint for invoking built-in tools (marketplace tool plugins) within a Dify plugin.  It takes a provider, tool name, and parameters as input and yields ToolInvokeMessage objects. `provider` is the plugin ID plus tool provider name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/tool.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef invoke_builtin_tool(\n    self, provider: str, tool_name: str, parameters: dict[str, Any]\n) -> Generator[ToolInvokeMessage, None, None]:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Initializing Directory Structure for Cloud Service Moderation\nDESCRIPTION: This snippet shows the directory structure required for implementing the 'Cloud Service' content moderation type in Dify.  It includes the `__init__.py`, `cloud_service.py`, and `schema.json` files, which define the module, implementation, and frontend configuration, respectively.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/code-based-extension/moderation.md#_snippet_0\n\nLANGUAGE: Plain\nCODE:\n```\n.\\n└── api\\n    └── core\\n        └── moderation\\n            └── cloud_service\\n                ├── __init__.py\\n                ├── cloud_service.py\\n                └── schema.json\n```\n\n----------------------------------------\n\nTITLE: Getting Customizable Model Schema in Python\nDESCRIPTION: This function retrieves the customizable model schema for a given model, allowing vendors to support custom LLM additions. It returns an AIModelEntity object representing the model schema or None if no schema is available. It allows to expose model parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> Optional[AIModelEntity]:\n    \"\"\"\n    Get customizable model schema\n\n    :param model: model name\n    :param credentials: model credentials\n    :return: model schema\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Input Tokens\nDESCRIPTION: Defines the `get_num_tokens` method for pre-calculating the number of input tokens for a given prompt. It takes the model name, credentials, prompt messages, and tools as input. Returns an integer representing the number of tokens; returns 0 if the model doesn't support token pre-calculation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/predefined-model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                   tools: Optional[list[PromptMessageTool]] = None) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param tools: tools for tool calling\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Text2speech Model Invocation Interface\nDESCRIPTION: Defines the `_invoke` method for converting text to speech. It takes a model name, credentials, text content, a boolean indicating streaming output, and an optional user identifier. It returns the translated audio file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, content_text: str, streaming: bool, user: Optional[str] = None):\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param content_text: text content to be translated\n      :param streaming: output is streaming\n      :param user: unique user id\n      :return: translated audio file\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking a Large Language Model in Python\nDESCRIPTION: This function invokes a large language model to generate a response based on the provided prompt messages, model parameters, and other optional settings such as tools, stop words, and user identification. It supports both streaming and non-streaming output, returning either a generator of LLMResultChunk objects or a complete LLMResult object, respectively.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            prompt_messages: list[PromptMessage], model_parameters: dict,\n            tools: Optional[list[PromptMessageTool]] = None, stop: Optional[list[str]] = None,\n            stream: bool = True, user: Optional[str] = None) \\\n        -> Union[LLMResult, Generator]:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param model_parameters: model parameters\n    :param tools: tools for tool calling\n    :param stop: stop words\n    :param stream: is stream response\n    :param user: unique user id\n    :return: full response or stream response chunk generator result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Convert Tool to Prompt Message Tool in Python\nDESCRIPTION: This function `_convert_tool_to_prompt_message_tool` converts a `ToolEntity` to a `PromptMessageTool`, which is used for prompting the language model. It iterates through the tool parameters and creates a structured representation suitable for the LLM. It handles different parameter types and their properties, including required fields and enums.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n    def _convert_tool_to_prompt_message_tool(\n        self, tool: ToolEntity\n    ) -> PromptMessageTool:\n        \"\"\"\n        convert tool to prompt message tool\n        \"\"\"\n        message_tool = PromptMessageTool(\n            name=tool.identity.name,\n            description=tool.description.llm if tool.description else \"\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        )\n\n        parameters = tool.parameters\n        for parameter in parameters:\n            if parameter.form != ToolParameter.ToolParameterForm.LLM:\n                continue\n\n            parameter_type = parameter.type\n            if parameter.type in {\n                ToolParameter.ToolParameterType.FILE,\n                ToolParameter.ToolParameterType.FILES,\n            }:\n                continue\n            enum = []\n            if parameter.type == ToolParameter.ToolParameterType.SELECT:\n                enum = (\n                    [option.value for option in parameter.options]\n                    if parameter.options\n                    else []\n                )\n\n            message_tool.parameters[\"properties\"][parameter.name] = {\n                \"type\": parameter_type,\n                \"description\": parameter.llm_description or \"\",\n            }\n\n            if len(enum) > 0:\n                message_tool.parameters[\"properties\"][parameter.name][\"enum\"] = enum\n\n            if parameter.required:\n                message_tool.parameters[\"required\"].append(parameter.name)\n\n        return message_tool\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Role Enum Definition in Python\nDESCRIPTION: This code defines an Enum for Prompt Message Roles. It supports 'SYSTEM', 'USER', 'ASSISTANT', and 'TOOL' roles, defining the source and purpose of the message.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageRole(Enum):\n    \"\"\"\n    Enum class for prompt message.\n    \"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    TOOL = \"tool\"\n```\n\n----------------------------------------\n\nTITLE: Handle Stream/Sync LLM Responses\nDESCRIPTION: This Python code snippet demonstrates how to handle both synchronous and streaming responses from a language model.  It checks the `stream` parameter and calls either `_handle_stream_response` for streaming or `_handle_sync_response` for synchronous execution, ensuring that the data is appropriately processed based on the response type.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/customizable-model.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, stream: bool, **kwargs) \\\n            -> Union[LLMResult, Generator]:\n        if stream:\n              return self._handle_stream_response(**kwargs)\n        return self._handle_sync_response(**kwargs)\n\n    def _handle_stream_response(self, **kwargs) -> Generator:\n        for chunk in response:\n              yield chunk\n    def _handle_sync_response(self, **kwargs) -> LLMResult:\n        return LLMResult(**response)\n```\n\n----------------------------------------\n\nTITLE: LLM Invocation Method Signature in Python\nDESCRIPTION: Defines the signature for the `_invoke` method in the `AnthropicLargeLanguageModel` class, which is responsible for invoking the LLM.  It supports streaming and synchronous responses. Parameters include model name, credentials, prompt messages, model parameters, tools, stop words, stream flag, and user identifier. The return type is either a full LLMResult or a stream response chunk generator.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/predefined-model.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n                prompt_messages: list[PromptMessage], model_parameters: dict,\n                tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,\n                stream: bool = True, user: Optional[str] = None) \\\n            -> Union[LLMResult, Generator]:\n        \"\"\"\n        Invoke large language model\n\n        :param model: model name\n        :param credentials: model credentials\n        :param prompt_messages: prompt messages\n        :param model_parameters: model parameters\n        :param tools: tools for tool calling\n        :param stop: stop words\n        :param stream: is stream response\n        :param user: unique user id\n        :return: full response or stream response chunk generator result\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: JSON Verification Code Node in Python\nDESCRIPTION: This Python code snippet is used in the Code Node to validate the JSON string. It attempts to parse the JSON string using `json.loads()` and returns the parsed object within a dictionary.  If the JSON is invalid, an exception will be raised, triggering the error handling mechanism.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/error-handling/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef main(json_str: str) -> dict:\n    obj = json.loads(json_str)\n    return {'result': obj}\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for Text Embedding (Python)\nDESCRIPTION: This code snippet demonstrates how to get the number of tokens for a list of texts when using a Text Embedding model within Dify.  Similar to LLMs, this helps estimate costs and manage input lengths.  It inherits from `__base.text_embedding_model.TextEmbeddingModel`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, texts: list[str]) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param texts: texts to embed\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Parameter Validation Class (Python)\nDESCRIPTION: This Python code defines a Pydantic model `BasicParams` to validate the incoming parameters for the Agent Strategy plugin. It includes fields for `maximum_iterations` (int), `model` (AgentModelConfig), `tools` (list of ToolEntity), and `query` (str).  These classes are imported from `dify_plugin.entities.agent` and `dify_plugin.interfaces.agent`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n```\n\n----------------------------------------\n\nTITLE: Generic JSON Schema Template\nDESCRIPTION: This snippet provides a generic JSON Schema template to define the structure of the expected JSON output. It includes properties with different data types like string, number, array, and object, along with descriptions and constraints.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/how-to-use-json-schema-in-dify.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"template_schema\",\n    \"description\": \"A generic template for JSON Schema\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"field1\": {\n                \"type\": \"string\",\n                \"description\": \"Description of field1\"\n            },\n            \"field2\": {\n                \"type\": \"number\",\n                \"description\": \"Description of field2\"\n            },\n            \"field3\": {\n                \"type\": \"array\",\n                \"description\": \"Description of field3\",\n                \"items\": {\n                    \"type\": \"string\"\n                }\n            },\n            \"field4\": {\n                \"type\": \"object\",\n                \"description\": \"Description of field4\",\n                \"properties\": {\n                    \"subfield1\": {\n                        \"type\": \"string\",\n                        \"description\": \"Description of subfield1\"\n                    }\n                },\n                \"required\": [\"subfield1\"],\n                \"additionalProperties\": false\n            }\n        },\n        \"required\": [\"field1\", \"field2\", \"field3\", \"field4\"],\n        \"additionalProperties\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Precompute Input Tokens Method in Python\nDESCRIPTION: Defines the method `get_num_tokens` to precompute the number of tokens for given prompt messages. This function is used to estimate the cost of invoking the model. If the model does not natively support precomputing tokens, the method should return 0.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/predefined-model.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                       tools: Optional[list[PromptMessageTool]] = None) -> int:\n        \"\"\"\n        Get number of tokens for given prompt messages\n\n        :param model: model name\n        :param credentials: model credentials\n        :param prompt_messages: prompt messages\n        :param tools: tools for tool calling\n        :return:\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking Rerank Model in Python\nDESCRIPTION: This Python code defines the `_invoke` method for invoking a rerank model. It takes the model name, credentials, query, a list of documents, score threshold, top n, and user identifier as input and returns a `RerankResult`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            query: str, docs: list[str], score_threshold: Optional[float] = None, top_n: Optional[int] = None,\n            user: Optional[str] = None) \\\n        -> RerankResult:\n    \"\"\"\n    Invoke rerank model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param query: search query\n    :param docs: docs for reranking\n    :param score_threshold: score threshold\n    :param top_n: top n\n    :param user: unique user id\n    :return: rerank result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Adding Chunks to a Document via Dify API\nDESCRIPTION: Adds new chunks (segments) to a specific document within a dataset in the Dify knowledge base. It requires the dataset ID and document ID as path parameters, a valid API key in the Authorization header, and a JSON payload containing the segments to be added.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"segments\": [{\"content\": \"1\",\"answer\": \"1\",\"keywords\": [\"a\"]}]}'\n```\n\n----------------------------------------\n\nTITLE: Moderation Invoke Method Definition in Python\nDESCRIPTION: Defines the `_invoke` method for moderation, using a language model to assess the safety of a given text. It takes the model name, credentials, text to moderate, and an optional user ID as input, and returns a boolean indicating whether the text is safe (False) or not (True).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            text: str, user: Optional[str] = None) \\\n        -> bool:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param text: text to moderate\n    :param user: unique user id\n    :return: false if text is safe, true otherwise\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking Text Embedding Model (Python)\nDESCRIPTION: This code snippet demonstrates how to invoke a Text Embedding model within the Dify platform. It inherits from `__base.text_embedding_model.TextEmbeddingModel`. It takes a list of texts as input and returns a `TextEmbeddingResult` entity containing the embeddings.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              texts: list[str], user: Optional[str] = None) \\\n          -> TextEmbeddingResult:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param texts: texts to embed\n    :param user: unique user id\n    :return: embeddings result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Extracting JSON Field in Dify Workflow (Python)\nDESCRIPTION: This Python snippet demonstrates how to extract a specific field (`data.name`) from a JSON string returned by an HTTP node in a Dify workflow. It uses the `json` library to parse the JSON string and returns the desired field as the 'result' output variable.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/code.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef main(http_response: str) -> str:\n    import json\n    data = json.loads(http_response)\n    return {\n        # Note to declare 'result' in the output variables\n        'result': data['data']['name'] \n    }\n```\n\n----------------------------------------\n\nTITLE: Chat Interface Invoke Method Definition (Python)\nDESCRIPTION: Defines the `invoke` method for the Chat Interface, which allows plugins to interact with Chatbot, Agent, or Chatflow type applications. It specifies the parameters for `app_id`, `inputs`, `response_mode`, `conversation_id`, and `files`. The return type is either a generator of dictionaries for streaming or a single dictionary for blocking mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self,\n    app_id: str,\n    inputs: dict,\n    response_mode: Literal[\"streaming\", \"blocking\"],\n    conversation_id: str,\n    files: list,\n) -> Generator[dict, None, None] | dict:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Starting LiteLLM Proxy with Docker\nDESCRIPTION: This Docker command starts the LiteLLM proxy server. It mounts the `litellm_config.yaml` file to `/app/config.yaml` inside the container, maps port 4000 on the host to port 4000 in the container, and specifies the image `ghcr.io/berriai/litellm:main-latest`. The command also passes configuration and debugging flags to the LiteLLM proxy process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_76\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n    -v $(pwd)/litellm_config.yaml:/app/config.yaml \\\n    -p 4000:4000 \\\n    ghcr.io/berriai/litellm:main-latest \\\n    --config /app/config.yaml --detailed_debug\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for LLM (Python)\nDESCRIPTION: This code snippet demonstrates how to retrieve the number of tokens for given prompt messages when interacting with a Large Language Model (LLM) in Dify. It's essential for cost estimation and managing context window limits. If the underlying model doesn't provide a tokenization interface, a default method like `_get_num_tokens_by_gpt2` can be used.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                     tools: Optional[list[PromptMessageTool]] = None) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param tools: tools for tool calling\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Model Selector Configuration YAML\nDESCRIPTION: This YAML configuration defines the parameters for the LLM tool, including a `model-selector` type for the `model` parameter. It specifies that the model scope is `llm`, allowing users to select from the `llm` model type in the UI. This configuration allows users to select a model through the UI rather than hardcoding it.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: llm\n  author: Dify\n  label:\n    en_US: LLM\n    zh_Hans: LLM\n    pt_BR: LLM\ndescription:\n  human:\n    en_US: A tool for invoking a large language model\n    zh_Hans: 用于调用大型语言模型的工具\n    pt_BR: A tool for invoking a large language model\n  llm: A tool for invoking a large language model\nparameters:\n  - name: prompt\n    type: string\n    required: true\n    label:\n      en_US: Prompt string\n      zh_Hans: 提示字符串\n      pt_BR: Prompt string\n    human_description:\n      en_US: used for searching\n      zh_Hans: 用于搜索网页内容\n      pt_BR: used for searching\n    llm_description: key words for searching\n    form: llm\n  - name: model\n    type: model-selector\n    scope: llm\n    required: true\n    label:\n      en_US: Model\n      zh_Hans: 使用的模型\n      pt_BR: Model\n    human_description:\n      en_US: Model\n      zh_Hans: 使用的模型\n      pt_BR: Model\n    llm_description: which Model to invoke\n    form: form\nextra:\n  python:\n    source: tools/llm.py\n```\n\n----------------------------------------\n\nTITLE: Tool Prompt Message Class Definition\nDESCRIPTION: This code snippet defines the ToolPromptMessage class, representing a message from a tool. It includes the 'tool_call_id' attribute and allows passing the tool execution results in the 'content' attribute. It inherits from PromptMessage and sets the role to TOOL.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nclass ToolPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for tool prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.TOOL\n    tool_call_id: str  # ツール呼び出しID。OpenAI のツール呼び出しをサポートしない場合は、ツール名を渡すこともできます。\n```\n\n----------------------------------------\n\nTITLE: Define Tool Configuration YAML for Google Search\nDESCRIPTION: This YAML file defines the configuration for the Google Search tool within the Dify platform. It includes the tool's identity (name, author, labels, and descriptions), parameters (query and result_type), and their properties (type, required status, options, labels, descriptions, and form type). This configuration is used by Dify to understand the tool's capabilities and how to interact with it.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/quick-tool-integration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nidentity: # Basic information of the tool\n  name: google_search # Tool name, unique, no duplication with other tools\n  author: Dify # Author\n  label: # Label for frontend display\n    en_US: GoogleSearch # English label\n    zh_Hans: 谷歌搜索 # Chinese label\n    ja_JP: Google検索 # Japanese label\n    pt_BR: Pesquisa Google # Portuguese label\ndescription: # Description for frontend display\n  human: # Introduction for frontend display, supports multiple languages\n    en_US: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.\n    zh_Hans: 一个用于执行 Google SERP 搜索并提取片段和网页的工具。输入应该是一个搜索查询。\n    ja_JP: Google SERP 検索を実行し、スニペットと Web ページを抽出するためのツール。入力は検索クエリである必要があります。\n    pt_BR: Uma ferramenta para realizar pesquisas no Google SERP e extrair snippets e páginas da web. A entrada deve ser uma consulta de pesquisa.\n  llm: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query. # Introduction passed to LLM, in order to make LLM better understand this tool, we suggest to write as detailed information about this tool as possible here, so that LLM can understand and use this tool\nparameters: # Parameter list\n  - name: query # Parameter name\n    type: string # Parameter type\n    required: true # Required or not\n    label: # Parameter label\n      en_US: Query string # English label\n      zh_Hans: 查询语句 # Chinese label\n      ja_JP: クエリステートメント # Japanese label\n      pt_BR: Declaração de consulta # Portuguese label\n    human_description: # Introduction for frontend display, supports multiple languages\n      en_US: used for searching\n      zh_Hans: 用于搜索网页内容\n      ja_JP: ネットの検索に使用する\n      pt_BR: usado para pesquisar\n    llm_description: key words for searching # Introduction passed to LLM, similarly, in order to make LLM better understand this parameter, we suggest to write as detailed information about this parameter as possible here, so that LLM can understand this parameter\n    form: llm # Form type, llm means this parameter needs to be inferred by Agent, the frontend will not display this parameter\n  - name: result_type\n    type: select # Parameter type\n    required: true\n    options: # Drop-down box options\n      - value: text\n        label:\n          en_US: text\n          zh_Hans: 文本\n          ja_JP: テキスト\n          pt_BR: texto\n      - value: link\n        label:\n          en_US: link\n          zh_Hans: 链接\n          ja_JP: リンク\n          pt_BR: link\n    default: link\n    label:\n      en_US: Result type\n      zh_Hans: 结果类型\n      ja_JP: 結果タイプ\n      pt_BR: tipo de resultado\n    human_description:\n      en_US: used for selecting the result type, text or link\n      zh_Hans: 用于选择结果类型，使用文本还是链接进行展示\n      ja_JP: 結果の種類、テキスト、リンクを選択するために使用されます\n      pt_BR: usado para selecionar o tipo de resultado, texto ou link\n    form: form # Form type, form means this parameter needs to be filled in by the user on the frontend before the conversation starts\n```\n\n----------------------------------------\n\nTITLE: Define Google Search Tool YAML\nDESCRIPTION: This YAML configuration defines the Google Search tool, including its identity, descriptions, and parameters. The `parameters` section defines the inputs required for the tool, such as the search query (`query`) and the result type (`result_type`). The `form` field specifies whether the parameter is inferred by the agent (`llm`) or entered by the user (`form`).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/quick-tool-integration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nidentity: # ツールの基本情報\n  name: google_search # ツール名、唯一無二で、他のツールと重複してはいけません\n  author: Dify # 著者\n  label: # ラベル、前端表示用\n    en_US: GoogleSearch # 英語ラベル\n    zh_Hans: 谷歌搜索 # 中国語ラベル\n    ja_JP: Google検索 # 日本語ラベル\n    pt_BR: Pesquisa Google # プルトガル語ラベル\ndescription: # 説明、前端表示用\n  human: # 前端表示用の紹介文、複数言語対応\n    en_US: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.\n    zh_Hans: 一个用于执行 Google SERP 搜索并提取片段和网页的工具。输入应该是一个搜索查询。\n    ja_JP: Google SERP 検索を実行し、スニペットと Web ページを抽出するためのツール。入力は検索クエリである必要があります。\n    pt_BR: Uma ferramenta para realizar pesquisas no Google SERP e extrair snippets e páginas da web. A entrada deve ser uma consulta de pesquisa.\n  llm: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query. # LLMに渡す紹介文。LLMがこのツールをよりよく理解して使用できるように、ここにはできるだけ詳細な情報を書いておくことをお勧めします。\nparameters: # パラメーターリスト\n  - name: query # パラメーター名\n    type: string # パラメータータイプ\n    required: true # 必須かどうか\n    label: # パラメーターラベル\n      en_US: Query string # 英語ラベル\n      zh_Hans: 查询语句 # 中国語ラベル\n      ja_JP: クエリステートメント # 日本語ラベル\n      pt_BR: Declaração de consulta # プルトガル語ラベル\n    human_description: # 前端表示用の紹介文、複数言語対応\n      en_US: used for searching\n      zh_Hans: 用于搜索网页内容\n      ja_JP: ネットの検索に使用する\n      pt_BR: usado para pesquisar\n    llm_description: key words for searching # LLMに渡す紹介文。同上、LLMがこのパラメーターをよりよく理解できるように、できるだけ詳細な情報を書いておくことをお勧めします。\n    form: llm # フォームタイプ。llmはこのパラメーターがエージェントによって推論されるべきであることを示します。前端はこのパラメーターを表示しません。\n  - name: result_type\n    type: select # パラメータータイプ\n    required: true\n    options: # 選択ボックスオプション\n      - value: text\n        label:\n          en_US: text\n          zh_Hans: 文本\n          ja_JP: テキスト\n          pt_BR: texto\n      - value: link\n        label:\n          en_US: link\n          zh_Hans: 链接\n          ja_JP: リンク\n          pt_BR: link\n    default: link\n    label:\n      en_US: Result type\n      zh_Hans: 结果类型\n      ja_JP: 結果タイプ\n      pt_BR: tipo de resultado\n    human_description:\n      en_US: used for selecting the result type, text or link\n      zh_Hans: 用于选择结果类型，使用文本还是链接进行展示\n      ja_JP: 結果の種類、テキスト、リンクを選択するために使用されます\n      pt_BR: usado para selecionar o tipo de resultado, texto ou link\n    form: form # フォームタイプ。formはこのパラメーターが対話開始前にユーザーによって前端で入力されるべきであることを示します。\n```\n\n----------------------------------------\n\nTITLE: Validating Provider Credentials in Dify Model Provider (Python)\nDESCRIPTION: This method validates the credentials provided for a model provider. The credentials dictionary is defined in the `provider_credential_schema` of the provider's YAML configuration file.  It should raise a `CredentialsValidateFailedError` if validation fails.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Workflow Interface Specification\nDESCRIPTION: Defines the signature of the `invoke` method for Workflow interface calls. It outlines the expected input parameters: `app_id` (application identifier), `inputs` (input data dictionary), `response_mode` (specifies streaming or blocking response), and `files` (list of file objects). It returns a generator for streaming (`Generator[dict, None, None]`) or a dictionary (`dict`).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    def invoke(\n        self,\n        app_id: str,\n        inputs: dict,\n        response_mode: Literal[\"streaming\", \"blocking\"],\n        files: list,\n    ) -> Generator[dict, None, None] | dict:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Chat Model Conversational App Template ASSISTANT\nDESCRIPTION: This is the ASSISTANT prompt template for building conversational applications using chat models in Dify. It represents the expected output from the model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n\"\" \n```\n\n----------------------------------------\n\nTITLE: LLM Prompt for Question Generation in Dify\nDESCRIPTION: This prompt is designed for an LLM node in Dify to generate questions based on the summarized content from the structure extraction node. It assists readers by prompting them with meaningful and valuable questions related to each part of the article, encouraging in-depth thought.  It takes the structure extraction output as input and outputs a series of numbered questions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/workshop/intermediate/article-reader.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nRead the following article content and perform the task\n{{Output of the structure extraction}}\n# Task\n\n- **Main Objective**: Thoroughly read the above text, and propose as many questions as possible for each part of the article.\n- **Requirements**: Questions should be meaningful and valuable, worthy of consideration.\n- **Restrictions**: No specific restrictions.\n- **Expected Output**: A series of questions for each part of the article, each question should have depth and thinking value.\n\n# Reasoning Order\n\n- **Reasoning Part**: Thoroughly read the article, analyze the content of each part, and consider the deep questions each part may raise.\n- **Conclusion Part**: Pose meaningful and valuable questions, ensuring they provoke in-depth thought.\n\n# Output Format\n\n- **Format**: Each question should be listed separately, numbered.\n- **Content**: Propose questions for each part of the article (such as introduction, background, methods, results, discussion, conclusion, etc.).\n- **Quantity**: As many as possible, but each question should be meaningful and valuable.\n```\n\n----------------------------------------\n\nTITLE: Configuring Dify Chatbot Bubble Button with JavaScript\nDESCRIPTION: This JavaScript code snippet shows how to configure the Dify Chatbot Bubble Button using the `window.difyChatbotConfig` object. It includes options for setting the token, development mode, base URL, container properties, draggable behavior, system variables, and inputs. The `token` property is required, and other properties are optional, allowing customization of the button's appearance and behavior.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-publishing/embedding-in-websites.md#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.difyChatbotConfig = {\n    // Required, automatically generated by Dify\n    token: 'YOUR_TOKEN',\n    // Optional, default is false\n    isDev: false,\n    // Optional, when isDev is true, default is 'https://dev.udify.app', otherwise default is 'https://udify.app'\n    baseUrl: 'YOUR_BASE_URL',\n    // Optional, It can accept any valid HTMLElement attribute other than `id`, such as `style`, `className`, etc\n    containerProps: {},\n    // Optional, If or not the button is allowed to be dragged, default is `false`\n    draggable: false,\n    // Optional, The axis along which the button is allowed to be dragged, default is `both`, can be `x`, `y`, `both`\n    dragAxis: 'both',\n    // Optional, An object of system variables that set in the dify chatbot\n    systemVariables: {\n        // key is the system variable name\n        // e.g.\n        // user_id: \"YOU CAN DEFINE USER ID HERE\",\n        // conversation_id: \"YOU CAN DEFINE CONVERSATION ID HERE, IT MUST BE A VALID UUID\"\n    },\n    // Optional, An object of inputs that set in the dify chatbot\n    inputs: {\n        // key is the variable name\n        // e.g.\n        // name: \"NAME\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: LLM Invocation Interface in Python\nDESCRIPTION: Defines the interface for invoking a large language model. It supports both streaming and synchronous returns. Parameters include model name, credentials, prompt messages, model parameters, tools, stop words, stream flag, and user identifier.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            prompt_messages: list[PromptMessage], model_parameters: dict,\n            tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,\n            stream: bool = True, user: Optional[str] = None) \\\n        -> Union[LLMResult, Generator]:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param model_parameters: model parameters\n    :param tools: tools for tool calling\n    :param stop: stop words\n    :param stream: is stream response\n    :param user: unique user id\n    :return: full response or stream response chunk generator result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Tool Invocation Definition\nDESCRIPTION: This Python code snippet defines the `invoke` method for invoking tools. It specifies the parameters required for tool invocation, including provider type, provider name, tool name, and parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n def invoke(\n        self,\n        provider_type: ToolProviderType,\n        provider: str,\n        tool_name: str,\n        parameters: dict[str, Any],\n    ) -> Generator[ToolInvokeMessage, None, None]:...\n```\n\n----------------------------------------\n\nTITLE: Text2Speech _invoke method definition in Python\nDESCRIPTION: Defines the _invoke method for Text2SpeechModel, which converts text content to speech using a specified model and credentials. It accepts the model name, credentials, text content to be translated, a boolean indicating if the output should be streaming, and an optional user ID. The method returns the translated audio file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, content_text: str, streaming: bool, user: Optional[str] = None):\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param content_text: text content to be translated\n    :param streaming: output is streaming\n    :param user: unique user id\n    :return: translated audio file\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Agent Strategy Implementation (Python)\nDESCRIPTION: This Python code defines the `BasicAgentAgentStrategy` class, which inherits from `AgentStrategy`. The `_invoke` method takes a dictionary of parameters, validates them using the `BasicParams` model, and initializes the main processing logic. It uses Generator for messages.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n```\n\n----------------------------------------\n\nTITLE: Validating Model Credentials in Python\nDESCRIPTION: This Python code defines the `validate_credentials` method for validating individual model credentials. It takes the model name and credentials as input.  A failed validation should raise a `CredentialsValidateFailedError` exception.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n    \"\"\"\n    Validate model credentials\n\n    :param model: model name\n    :param credentials: model credentials\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: LLM Usage Class Definition\nDESCRIPTION: This code snippet defines the LLMUsage class, which extends ModelUsage, representing the usage information for a large language model. It includes details on token counts, prices, and latency.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nclass LLMUsage(ModelUsage):\n    \"\"\"\n    Model class for llm usage.\n    \"\"\"\n    prompt_tokens: int  # プロンプト使用トークン数\n    prompt_unit_price: Decimal  # プロンプト単価\n    prompt_price_unit: Decimal  # プロンプト価格単位（単価が適用されるトークン数）\n    prompt_price: Decimal  # プロンプト料金\n    completion_tokens: int  # 回答使用トークン数\n    completion_unit_price: Decimal  # 回答単価\n    completion_price_unit: Decimal  # 回答価格単位（単価が適用されるトークン数）\n    completion_price: Decimal  # 回答料金\n    total_tokens: int  # 総使用トークン数\n    total_price: Decimal  # 総料金\n    currency: str  # 通貨単位\n    latency: float  # リクエスト処理時間（秒）\n```\n\n----------------------------------------\n\nTITLE: Defining Server URL Credential Schema\nDESCRIPTION: This YAML snippet defines the credential schema for the `server_url` parameter, which specifies the address of the locally deployed Xinference server. It uses a text input field with labels and placeholders in both English and Simplified Chinese, providing guidance to the user. The `required` field makes it a mandatory parameter.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/customizable-model.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n  - variable: server_url\n    label:\n      zh_Hans: 服务器URL\n      en_US: Server url\n    type: text-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入Xinference的服务器地址，如 https://example.com/xxx\n      en_US: Enter the url of your Xinference, for example https://example.com/xxx\n```\n\n----------------------------------------\n\nTITLE: Endpoint Implementation Example Python\nDESCRIPTION: Demonstrates the implementation of a Dify plugin endpoint in Python, inheriting from `dify_plugin.Endpoint` and implementing the `_invoke` method to handle requests. It uses `werkzeug` for request and response handling. It showcases how to access path parameters and configuration settings, and returns a streaming response.  Requires `werkzeug` and `dify_plugin`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/endpoint.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom typing import Mapping\nfrom werkzeug import Request, Response\nfrom dify_plugin import Endpoint\n\nclass Duck(Endpoint):\n    def _invoke(self, r: Request, values: Mapping, settings: Mapping) -> Response:\n        \"\"\"\n        Invokes the endpoint with the given request.\n        \"\"\"\n        app_id = values[\"app_id\"]\n        def generator():\n            yield f\"{app_id} <br>\"\n        return Response(generator(), status=200, content_type=\"text/html\")\n```\n\n----------------------------------------\n\nTITLE: Agent Plugin Parameter Definition (YAML)\nDESCRIPTION: This YAML file defines the parameters for the Agent Strategy plugin, including model selection, tools list, query input, and maximum iterations.  It specifies the parameter types, scope, requirement status, labels (including translations), default values, and the corresponding Python source file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: basic_agent # the name of the agent_strategy\n  author: novice # the author of the agent_strategy\n  label:\n    en_US: BasicAgent # the engilish label of the agent_strategy\ndescription:\n  en_US: BasicAgent # the english description of the agent_strategy\nparameters:\n  - name: model # the name of the model parameter\n    type: model-selector # model-type\n    scope: tool-call&llm # the scope of the parameter\n    required: true\n    label:\n      en_US: Model\n      zh_Hans: 模型\n      pt_BR: Model\n  - name: tools # the name of the tools parameter\n    type: array[tools] # the type of tool parameter\n    required: true\n    label:\n      en_US: Tools list\n      zh_Hans: 工具列表\n      pt_BR: Tools list\n  - name: query # the name of the query parameter\n    type: string # the type of query parameter\n    required: true\n    label:\n      en_US: Query\n      zh_Hans: 查询\n      pt_BR: Query\n  - name: maximum_iterations\n    type: number\n    required: false\n    default: 5\n    label:\n      en_US: Maxium Iterations\n      zh_Hans: 最大迭代次数\n      pt_BR: Maxium Iterations\n    max: 50 # if you set the max and min value, the display of the parameter will be a slider\n    min: 1\nextra:\n  python:\n    source: strategies/basic_agent.py\n```\n\n----------------------------------------\n\nTITLE: LLM Invoke Endpoint (Python)\nDESCRIPTION: This code defines the signature of the `invoke` method for making LLM requests.  It accepts a `model_config`, `prompt_messages`, optional `tools`, `stop` words, and a boolean indicating whether to stream the response. It returns either a generator of LLMResultChunk objects or a single LLMResult.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self,\n    model_config: LLMModelConfig,\n    prompt_messages: list[PromptMessage],\n    tools: list[PromptMessageTool] | None = None,\n    stop: list[str] | None = None,\n    stream: bool = True,\n) -> Generator[LLMResultChunk, None, None] | LLMResult:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configure .env file for Remote Debugging\nDESCRIPTION: This code snippet shows the content of the `.env` file, which is used to configure the remote debugging environment for a Dify plugin. It includes the installation method, remote host, port, and API key needed for connecting to the remote Dify server.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/debug-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: Configure Notion Internal Integration Environment Variables\nDESCRIPTION: This code snippet shows the environment variables to be configured in the .env file when using internal Notion integration with Dify. It specifies the integration type as 'internal' and includes the internal secret key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/create-knowledge-and-upload-documents/import-content-data/sync-from-notion.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nNOTION_INTEGRATION_TYPE = internal or NOTION_INTEGRATION_TYPE = public\nNOTION_INTERNAL_SECRET=you-internal-secret\n```\n\n----------------------------------------\n\nTITLE: Get Knowledge Base List - Dify API (cURL)\nDESCRIPTION: This cURL command retrieves a list of knowledge bases from the Dify API. It requires the API key for authentication and authorization and supports pagination using the `page` and `limit` parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request GET 'https://api.dify.ai/v1/datasets?page=1&limit=20' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Invoking Rerank Model in Python\nDESCRIPTION: This function invokes a rerank model to reorder a list of documents based on their relevance to a given query. The function returns a RerankResult entity representing the rerank results.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            query: str, docs: list[str], score_threshold: Optional[float] = None, top_n: Optional[int] = None,\n            user: Optional[str] = None) \\\n        -> RerankResult:\n    \"\"\"\n    Invoke rerank model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param query: search query\n    :param docs: docs for reranking\n    :param score_threshold: score threshold\n    :param top_n: top n\n    :param user: unique user id\n    :return: rerank result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Launching a Baichuan Model with Xinference CLI\nDESCRIPTION: This command launches a Baichuan-chat model using Xinference's command-line interface, specifying the model format, size, and quantization. This allows for deploying the Baichuan-13B-Chat model with specific configurations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\nxinference launch --model-name baichuan-chat --model-format pytorch --size-in-billions 13 --quantization 4\n```\n\n----------------------------------------\n\nTITLE: Stability AI Prompt for Image Generation\nDESCRIPTION: This prompt instructs the Agent to draw content based on the user's input using the stability_text2image tool. It serves as a basic instruction for generating images from text descriptions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/workshop/basic/build-ai-image-generation-app.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nDraw the specified content according to the user's prompt using stability_text2image.\n```\n\n----------------------------------------\n\nTITLE: Chat Interface Invocation\nDESCRIPTION: Illustrates how to access and invoke a Chat-type application within a Dify plugin. It shows the entry point `self.session.app.chat` and specifies the `invoke` method's parameters: `app_id`, `inputs`, `response_mode`, `conversation_id`, and `files`. The method returns either a generator for streaming or a dictionary for blocking mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    self.session.app.chat\n```\n\n----------------------------------------\n\nTITLE: Example YAML Configuration for Claude-3-5-Sonnet-20240620\nDESCRIPTION: This YAML snippet provides an example configuration for the `claude-3-5-sonnet-20240620` model, defining its properties, features, and pricing. It includes configurations for parameters like temperature, top_p, top_k, max_tokens, and response_format, as well as pricing information in USD.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodel: claude-3-5-sonnet-20240620\nlabel:\n  en_US: claude-3-5-sonnet-20240620\nmodel_type: llm\nfeatures:\n  - agent-thought\n  - vision\n  - tool-call\n  - stream-tool-call\n  - document\nmodel_properties:\n  mode: chat\n  context_size: 200000\nparameter_rules:\n  - name: temperature\n    use_template: temperature\n  - name: top_p\n    use_template: top_p\n  - name: top_k\n    label:\n      zh_Hans: 取样数量\n      en_US: Top k\n    type: int\n    help:\n      zh_Hans: 仅从每个后续标记的前 K 个选项中采样。\n      en_US: Only sample from the top K options for each subsequent token.\n    required: false\n  - name: max_tokens\n    use_template: max_tokens\n    required: true\n    default: 8192\n    min: 1\n    max: 8192\n  - name: response_format\n    use_template: response_format\npricing:\n  input: '3.00'\n  output: '15.00'\n  unit: '0.000001'\n  currency: USD\n```\n\n----------------------------------------\n\nTITLE: Create Document by Text with Dify API\nDESCRIPTION: This snippet demonstrates how to create a new document in an existing Dify knowledge base using text content via the API. It requires the `dataset_id` of the knowledge base and a valid API key. The request includes the document's name, text content, indexing technique, and processing rules.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/document/create-by-text' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"name\": \"text\",\"text\": \"text\",\"indexing_technique\": \"high_quality\",\"process_rule\": {\"mode\": \"automatic\"}}'\n```\n\n----------------------------------------\n\nTITLE: Summary Request Entry Point (Python)\nDESCRIPTION: This code snippet shows the entry point to request a summary of text. It uses the system model in the workspace to summarize the given text.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.summary\n```\n\n----------------------------------------\n\nTITLE: Define Model UID Credential Schema (YAML)\nDESCRIPTION: This YAML configuration defines the credential schema for the model UID, specifying labels, a text input type, requirement status, and placeholders for both English and Chinese. The `model_uid` variable is essential for identifying a specific model instance within Xinference.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/customizable-model.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n  - variable: model_uid\n    label:\n      zh_Hans: 模型 UID\n      en_US: Model uid\n    type: text-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入你的 Model UID\n      en_US: Enter the model uid\n```\n\n----------------------------------------\n\nTITLE: WeatherSearch ExternalDataTool Implementation (Python)\nDESCRIPTION: Implements the 'WeatherSearch' external data tool as a Python class inheriting from ExternalDataTool. It includes methods for validating the configuration (validate_config) and querying external data (query). The class variable 'name' is set to 'weather_search', matching the directory and file name. It retrieves the city and temperature unit from the inputs and configuration, respectively, and returns a formatted weather string.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/external-data-tool.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nfrom core.external_data_tool.base import ExternalDataTool\n\n\nclass WeatherSearch(ExternalDataTool):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"weather_search\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user save the config.\n\n        Example:\n            .. code-block:: python\n                config = {\n                    \"temperature_unit\": \"centigrade\"\n                }\n\n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n\n        if not config.get('temperature_unit'):\n            raise ValueError('temperature unit is required')\n\n    def query(self, inputs: dict, query: Optional[str] = None) -> str:\n        \"\"\"\n        Query the external data tool.\n\n        :param inputs: user inputs\n        :param query: the query of chat app\n        :return: the tool query result\n        \"\"\"\n        city = inputs.get('city')\n        temperature_unit = self.config.get('temperature_unit')\n\n        if temperature_unit == 'fahrenheit':\n            return f'Weather in {city} is 32°F'\n        else:\n            return f'Weather in {city} is 0°C'\n```\n\n----------------------------------------\n\nTITLE: Content Moderation Input Response Overridden Example\nDESCRIPTION: Shows the response format when the `action` is set to `overridden`.  The `inputs` and `query` fields contain the modified values to replace the original user input. This allows for content sanitization or filtering.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"overridden\",\n    \"inputs\": {\n        \"var_1\": \"I will *** you.\",\n        \"var_2\": \"I will *** you.\"\n    },\n    \"query\": \"Happy everydays.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Vectorizer.AI Tool Implementation (Python)\nDESCRIPTION: This code snippet implements the Vectorizer.AI tool, fetching an image from the variable pool and converting it to a vector image using the Vectorizer.AI API. The `_invoke` method retrieves the image binary data using `self.get_variable_file(self.VARIABLE_KEY.IMAGE)`, sends it to the API, and returns the vectorized SVG image as a blob message. The `get_runtime_parameters` method dynamically generates a parameter list with image options from the variable pool, and `is_tool_available` determines if the tool is available based on the presence of images in the variable pool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/advanced-tool-integration.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom core.tools.tool.builtin_tool import BuiltinTool\nfrom core.tools.entities.tool_entities import ToolInvokeMessage, ToolParameter\nfrom core.tools.errors import ToolProviderCredentialValidationError\n\nfrom typing import Any, Dict, List, Union\nfrom httpx import post\nfrom base64 import b64decode\n\nclass VectorizerTool(BuiltinTool):\n    def _invoke(self, user_id: str, tool_Parameters: Dict[str, Any]) \\\n        -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:\n        \"\"\"\n            invoke tools\n        \"\"\"\n        api_key_name = self.runtime.credentials.get('api_key_name', None)\n        api_key_value = self.runtime.credentials.get('api_key_value', None)\n\n        if not api_key_name or not api_key_value:\n            raise ToolProviderCredentialValidationError('Please input api key name and value')\n\n        # Get image_id, the definition of image_id can be found in get_runtime_parameters\n        image_id = tool_Parameters.get('image_id', '')\n        if not image_id:\n            return self.create_text_message('Please input image id')\n\n        # Get the image generated by DallE from the variable pool\n        image_binary = self.get_variable_file(self.VARIABLE_KEY.IMAGE)\n        if not image_binary:\n            return self.create_text_message('Image not found, please request user to generate image firstly.')\n\n        # Generate vector image\n        response = post(\n            'https://vectorizer.ai/api/v1/vectorize',\n            files={ 'image': image_binary },\n            data={ 'mode': 'test' },\n            auth=(api_key_name, api_key_value), \n            timeout=30\n        )\n\n        if response.status_code != 200:\n            raise Exception(response.text)\n        \n        return [\n            self.create_text_message('the vectorized svg is saved as an image.'),\n            self.create_blob_message(blob=response.content,\n                                    meta={'mime_type': 'image/svg+xml'})\n        ]\n    \n    def get_runtime_parameters(self) -> List[ToolParameter]:\n        \"\"\"\n        override the runtime parameters\n        \"\"\"\n        # Here, we override the tool parameter list, define the image_id, and set its option list to all images in the current variable pool. The configuration here is consistent with the configuration in yaml.\n        return [\n            ToolParameter.get_simple_instance(\n                name='image_id',\n                llm_description=f'the image id that you want to vectorize, \\\n                    and the image id should be specified in \\\n                        {[i.name for i in self.list_default_image_variables()]}',\n                type=ToolParameter.ToolParameterType.SELECT,\n                required=True,\n                options=[i.name for i in self.list_default_image_variables()]\n            )\n        ]\n    \n    def is_tool_available(self) -> bool:\n        # Only when there are images in the variable pool, the LLM needs to use this tool\n        return len(self.list_default_image_variables()) > 0\n```\n\n----------------------------------------\n\nTITLE: DallE3 Tool Integration with Dify (Python)\nDESCRIPTION: This snippet illustrates integrating the DallE3 image generation tool within Dify. It receives a prompt, generates an image using the OpenAI API, and saves the image (as a blob) to Dify's variable pool using a predefined key (`self.VARIABLE_KEY.IMAGE.value`). This enables other tools to access the generated image.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/advanced-tool-integration.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, List, Union\nfrom core.tools.entities.tool_entities import ToolInvokeMessage\nfrom core.tools.tool.builtin_tool import BuiltinTool\n\nfrom base64 import b64decode\n\nfrom openai import OpenAI\n\nclass DallE3Tool(BuiltinTool):\n    def _invoke(self, \n                user_id: str, \n               tool_Parameters: Dict[str, Any], \n        ) -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:\n        \"\"\"\n            invoke tools\n        \"\"\"\n        client = OpenAI(\n            api_key=self.runtime.credentials['openai_api_key'],\n        )\n\n        # prompt\n        prompt = tool_Parameters.get('prompt', '')\n        if not prompt:\n            return self.create_text_message('Please input prompt')\n\n        # call openapi dalle3\n        response = client.images.generate(\n            prompt=prompt, model='dall-e-3',\n            size='1024x1024', n=1, style='vivid', quality='standard',\n            response_format='b64_json'\n        )\n\n        result = []\n        for image in response.data:\n            # 将所有图片通过 save_as 参数保存到变量池中，变量名为 self.VARIABLE_KEY.IMAGE.value，如果如果后续有新的图片生成，那么将会覆盖之前的图片\n            result.append(self.create_blob_message(blob=b64decode(image.b64_json), \n                                                   meta={ 'mime_type': 'image/png' },\n                                                    save_as=self.VARIABLE_KEY.IMAGE.value))\n\n        return result\n```\n\n----------------------------------------\n\nTITLE: Text Completion Generation Template - Dify\nDESCRIPTION: This template is designed for constructing a text generation application using a text completion model in Dify. It integrates context, a pre-prompt, and the user's query.  Placeholders are provided for context, pre-prompt, and the user's query.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nUse the following context as your learned knowledge, inside <context></context> XML tags.\n\n<context>\n{{#context#}}\n</context>\n\nWhen answer to user:\n- If you don't know, just say that you don't know.\n- If you don't know when you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n\n{{pre_prompt}}\n{{query}}\n```\n\n----------------------------------------\n\nTITLE: Deploying the API Extension (Bash)\nDESCRIPTION: These commands install the dependencies and deploy the API extension to Cloudflare Workers using npm.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\nnpm run deploy\n```\n\n----------------------------------------\n\nTITLE: Text-to-Speech Model Invocation Interface in Python\nDESCRIPTION: This code snippet defines the `_invoke` method for a text-to-speech model. It takes text as input and returns an audio stream. It expects model name, credentials, text content, a flag indicating whether the output should be streamed, and optionally a user ID.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, content_text: str, streaming: bool, user: Optional[str] = None):\n      \"\"\"\n      Invoke large language model\n  \n      :param model: model name\n      :param credentials: model credentials\n      :param content_text: text content to be translated\n      :param streaming: output is streaming\n      :param user: unique user id\n      :return: translated audio file\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Moderation Request Entry (Python)\nDESCRIPTION: This code snippet defines the entry point to perform content moderation using the Dify plugin session.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.moderation\n```\n\n----------------------------------------\n\nTITLE: Validating Provider Credentials in Dify (Python)\nDESCRIPTION: This code snippet demonstrates how to validate provider credentials within the Dify platform. It inherits from the `__base.model_provider.ModelProvider` base class and requires implementing the `validate_provider_credentials` interface.  The credentials are defined in the supplier's YAML configuration file under `provider_credential_schema`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking Installed Tools\nDESCRIPTION: This code snippet defines the endpoint for invoking installed tools from other plugins. The `provider` identifies the plugin and tool provider, `tool_name` specifies the tool, and `parameters` are the input values for the tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/tool.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef invoke_builtin_tool(\n    self, provider: str, tool_name: str, parameters: dict[str, Any]\n) -> Generator[ToolInvokeMessage, None, None]:\n    pass\n```\n\n----------------------------------------\n\nTITLE: LiteLLM Configuration YAML\nDESCRIPTION: This YAML configuration file defines the models to be used with the LiteLLM proxy. It specifies the model name and the corresponding parameters for accessing the models through the Azure OpenAI service, including the API base URL, API version, and API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_75\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_list:\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/chatgpt-v-2\n      api_base: https://openai-gpt-4-test-v-1.openai.azure.com/\n      api_version: \"2023-05-15\"\n      api_key:\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/gpt-4\n      api_key:\n      api_base: https://openai-gpt-4-test-v-2.openai.azure.com/\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/gpt-4\n      api_key:\n      api_base: https://openai-gpt-4-test-v-2.openai.azure.com/\n```\n\n----------------------------------------\n\nTITLE: Ping Check Expected Response (JSON)\nDESCRIPTION: Defines the expected JSON response for the 'ping' endpoint check, confirming API availability.  If the request point is 'ping' this should be the expected response.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/README.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"result\": \"pong\"\n}\n```\n\n----------------------------------------\n\nTITLE: Model Credentials Validation Method in Python\nDESCRIPTION: Defines the `validate_credentials` method for validating the model's credentials. This method ensures that the provided credentials are valid before invoking the model. It takes the model name and credentials as input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/predefined-model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n        \"\"\"\n        Validate model credentials\n\n        :param model: model name\n        :param credentials: model credentials\n        :return:\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Agent Log Messages in Python\nDESCRIPTION: This snippet shows how to create a log message within a Dify agent plugin to track the agent's thinking process. It includes methods for creating a log message with a specified label, data, and status, and for finishing a log message, potentially updating its status and adding an error message. The `create_log_message` method creates a new log entry and `finish_log_message` updates an existing one.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/agent.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n    def create_log_message(\n        self,\n        label: str,\n        data: Mapping[str, Any],\n        status: AgentInvokeMessage.LogMessage.LogStatus = AgentInvokeMessage.LogMessage.LogStatus.SUCCESS,\n        parent: AgentInvokeMessage | None = None,\n    ) -> AgentInvokeMessage\n\n    def finish_log_message(\n        self,\n        log: AgentInvokeMessage,\n        status: AgentInvokeMessage.LogMessage.LogStatus = AgentInvokeMessage.LogMessage.LogStatus.SUCCESS,\n        error: Optional[str] = None,\n    ) -> AgentInvokeMessage\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for Embedding Texts (Python)\nDESCRIPTION: This method calculates the number of tokens in the input texts for a text embedding model. It is similar to the token calculation for LLMs and uses an appropriate tokenizer for the specified model. If a specific tokenizer is not available, it falls back to the `_get_num_tokens_by_gpt2` method.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, texts: list[str]) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param texts: texts to embed\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Xinference Server URL and Model UID in YAML\nDESCRIPTION: This YAML snippet configures input fields for the Xinference server URL and model UID within the provider credential schema. These fields are essential for connecting to and identifying the specific Xinference model being used.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n<strong>  - variable: server_url\n</strong>    label:\n      zh_Hans: 服务器 URL\n      en_US: Server url\n    type: text-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入 Xinference 的服务器地址，如 https://example.com/xxx\n      en_US: Enter the url of your Xinference, for example https://example.com/xxx\n<strong>  - variable: model_uid\n</strong>    label:\n      zh_Hans: 模型 UID\n      en_US: Model uid\n    type: text-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入您的 Model UID\n      en_US: Enter the model uid\n```\n\n----------------------------------------\n\nTITLE: Summary Invoke Entry Point Python\nDESCRIPTION: This snippet shows how to access the summary functionality in the Dify plugin session.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.summary\n```\n\n----------------------------------------\n\nTITLE: Defining Model UID Credential Schema\nDESCRIPTION: This YAML snippet defines the credential schema for the `model_uid` parameter, which represents the unique identifier for the model.  It uses a text input field with labels and placeholders in both English and Simplified Chinese. The `required` field indicates that this parameter is mandatory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/customizable-model.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n  - variable: model_uid\n    label:\n      zh_Hans: 模型 UID\n      en_US: Model uid\n    type: text-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入你的 Model UID\n      en_US: Enter the model uid\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for LLM Prompt in Python\nDESCRIPTION: This method gets the number of tokens for given prompt messages. It takes the model name, credentials, prompt messages, and tools as input. If the model doesn't provide a tokenizer, the `_get_num_tokens_by_gpt2` method can be used.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                       tools: Optional[list[PromptMessageTool]] = None) -> int:\n        \"\"\"\n        Get number of tokens for given prompt messages\n\n        :param model: model name\n        :param credentials: model credentials\n        :param prompt_messages: prompt messages\n        :param tools: tools for tool calling\n        :return:\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining LLMUsage Data Model in Python\nDESCRIPTION: Defines a `LLMUsage` class that inherits from `ModelUsage`, representing LLM usage details such as prompt tokens, completion tokens, prices, total tokens, and latency. It utilizes `Decimal` for precise monetary values and includes a currency field.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nclass LLMUsage(ModelUsage):\n    \"\"\"\n    Model class for LLM usage.\n    \"\"\"\n    prompt_tokens: int  # Tokens used for prompt\n    prompt_unit_price: Decimal  # Unit price for prompt\n    prompt_price_unit: Decimal  # Price unit for prompt, i.e., the unit price based on how many tokens\n    prompt_price: Decimal  # Cost for prompt\n    completion_tokens: int  # Tokens used for response\n    completion_unit_price: Decimal  # Unit price for response\n    completion_price_unit: Decimal  # Price unit for response, i.e., the unit price based on how many tokens\n    completion_price: Decimal  # Cost for response\n    total_tokens: int  # Total number of tokens used\n    total_price: Decimal  # Total cost\n    currency: str  # Currency unit\n    latency: float  # Request latency (s)\n```\n\n----------------------------------------\n\nTITLE: Extracting Data from JSON String with Python\nDESCRIPTION: This code snippet demonstrates how to extract a specific field (`data.name`) from a JSON string using the `json` library in Python within a Dify workflow code node. It takes a JSON string as input and returns a dictionary containing the extracted value under the key 'result'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\ndef main(http_response: str) -> str:\n    import json\n    data = json.loads(http_response)\n    return {\n        # Note to declare 'result' in the output variables\n        'result': data['data']['name']\n    }\n```\n\n----------------------------------------\n\nTITLE: Convert Tool to Prompt Message Tool (Python)\nDESCRIPTION: The `_convert_tool_to_prompt_message_tool` method transforms a `ToolEntity` into a `PromptMessageTool`. This involves mapping tool parameters to the prompt message format, filtering out irrelevant parameters, and constructing the required properties for the LLM to use the tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n    def _convert_tool_to_prompt_message_tool(\n        self, tool: ToolEntity\n    ) -> PromptMessageTool:\n        \"\"\"\n        convert tool to prompt message tool\n        \"\"\"\n        message_tool = PromptMessageTool(\n            name=tool.identity.name,\n            description=tool.description.llm if tool.description else \"\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        )\n\n        parameters = tool.parameters\n        for parameter in parameters:\n            if parameter.form != ToolParameter.ToolParameterForm.LLM:\n                continue\n\n            parameter_type = parameter.type\n            if parameter.type in {\n                ToolParameter.ToolParameterType.FILE,\n                ToolParameter.ToolParameterType.FILES,\n            }:\n                continue\n            enum = []\n            if parameter.type == ToolParameter.ToolParameterType.SELECT:\n                enum = (\n                    [option.value for option in parameter.options]\n                    if parameter.options\n                    else []\n                )\n\n            message_tool.parameters[\"properties\"][parameter.name] = {\n                \"type\": parameter_type,\n                \"description\": parameter.llm_description or \"\",\n            }\n\n            if len(enum) > 0:\n                message_tool.parameters[\"properties\"][parameter.name][\"enum\"] = enum\n\n            if parameter.required:\n                message_tool.parameters[\"required\"].append(parameter.name)\n\n        return message_tool\n```\n\n----------------------------------------\n\nTITLE: Defining AssistantPromptMessage Model (Python)\nDESCRIPTION: Defines the `AssistantPromptMessage` model, which represents a message returned by a model. This message can contain tool calls, including the tool's ID, type, and function name with arguments.  It inherits from `PromptMessage` and includes fields for `role` and `tool_calls`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nclass AssistantPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for assistant prompt message.\n    \"\"\"\n    class ToolCall(BaseModel):\n        \"\"\"\n        Model class for assistant prompt message tool call.\n        \"\"\"\n        class ToolCallFunction(BaseModel):\n            \"\"\"\n            Model class for assistant prompt message tool call function.\n            \"\"\"\n            name: str  # tool name\n            arguments: str  # tool arguments\n\n        id: str  # Tool ID, effective only in OpenAI tool calls. It's the unique ID for tool invocation and the same tool can be called multiple times.\n        type: str  # default: function\n        function: ToolCallFunction  # tool call information\n\n    role: PromptMessageRole = PromptMessageRole.ASSISTANT\n    tool_calls: list[ToolCall] = []  # The result of tool invocation in response from the model (returned only when tools are input and the model deems it necessary to invoke a tool).\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Anthropic Provider\nDESCRIPTION: This YAML configuration defines the basic information for the Anthropic provider, including its identifier, display name, supported model types, configuration methods, and credential rules.  It demonstrates the use of `provider_credential_schema` for predefined models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/new-provider.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nprovider: anthropic  # Provider identifier\nlabel:  # Provider display name, can be set in en_US English and zh_Hans Chinese. If zh_Hans is not set, en_US will be used by default.\n  en_US: Anthropic\nicon_small:  # Small icon of the provider, stored in the _assets directory under the corresponding provider implementation directory, same language strategy as label\n  en_US: icon_s_en.png\nicon_large:  # Large icon of the provider, stored in the _assets directory under the corresponding provider implementation directory, same language strategy as label\n  en_US: icon_l_en.png\nsupported_model_types:  # Supported model types, Anthropic only supports LLM\n- llm\nconfigurate_methods:  # Supported configuration methods, Anthropic only supports predefined models\n- predefined-model\nprovider_credential_schema:  # Provider credential rules, since Anthropic only supports predefined models, unified provider credential rules need to be defined\n  credential_form_schemas:  # Credential form item list\n  - variable: anthropic_api_key  # Credential parameter variable name\n    label:  # Display name\n      en_US: API Key\n    type: secret-input  # Form type, secret-input here represents an encrypted information input box, only displaying masked information when editing.\n    required: true  # Whether it is required\n    placeholder:  # PlaceHolder information\n      zh_Hans: 在此输入你的 API Key\n      en_US: Enter your API Key\n  - variable: anthropic_api_url\n    label:\n      en_US: API URL\n    type: text-input  # Form type, text-input here represents a text input box\n    required: false\n    placeholder:\n      zh_Hans: 在此输入你的 API URL\n      en_US: Enter your API URL\n```\n\n----------------------------------------\n\nTITLE: Array to Text Conversion with Template Node in Django\nDESCRIPTION: This Django template code snippet illustrates how to convert an array of strings (articleSections) into a single string, joining the array elements with newline characters. This allows the iteration node output, which is in array format, to be converted back to a string for final output using the Template node.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/iteration.md#_snippet_1\n\nLANGUAGE: django\nCODE:\n```\n{{ articleSections | join(\"/n\") }}\n```\n\n----------------------------------------\n\nTITLE: Manifest File Example YAML\nDESCRIPTION: This is an example of a Dify plugin Manifest file. It demonstrates the structure and required fields, including version, type, author, label, creation time, icon, resource limits, permissions, plugins, meta information, and privacy policy.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/manifest.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nversion: 0.0.1\ntype: \"plugin\"\nauthor: \"Yeuoly\"\nname: \"neko\"\nlabel:\n  en_US: \"Neko\"\ncreated_at: \"2024-07-12T08:03:44.658609186Z\"\nicon: \"icon.svg\"\nresource:\n  memory: 1048576\n  permission:\n    tool:\n      enabled: true\n    model:\n      enabled: true\n      llm: true\n    endpoint:\n      enabled: true\n    app:\n      enabled: true\n    storage: \n      enabled: true\n      size: 1048576\nplugins:\n  endpoints:\n    - \"provider/neko.yaml\"\nmeta:\n  version: 0.0.1\n  arch:\n    - \"amd64\"\n    - \"arm64\"\n  runner:\n    language: \"python\"\n    version: \"3.11\"\n    entrypoint: \"main\"\nprivacy: \"./privacy.md\"\n```\n\n----------------------------------------\n\nTITLE: Defining LLMUsage Model (Python)\nDESCRIPTION: Defines the `LLMUsage` model, representing the usage information for a Language Model.  It inherits from `ModelUsage` and includes fields for token counts (`prompt_tokens`, `completion_tokens`, `total_tokens`), unit prices, costs, currency, and latency.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nclass LLMUsage(ModelUsage):\n    \"\"\"\n    Model class for LLM usage.\n    \"\"\"\n    prompt_tokens: int  # Tokens used for prompt\n    prompt_unit_price: Decimal  # Unit price for prompt\n    prompt_price_unit: Decimal  # Price unit for prompt, i.e., the unit price based on how many tokens\n    prompt_price: Decimal  # Cost for prompt\n    completion_tokens: int  # Tokens used for response\n    completion_unit_price: Decimal  # Unit price for response\n    completion_price_unit: Decimal  # Price unit for response, i.e., the unit price based on how many tokens\n    completion_price: Decimal  # Cost for response\n    total_tokens: int  # Total number of tokens used\n    total_price: Decimal  # Total cost\n    currency: str  # Currency unit\n    latency: float  # Request latency (s)\n```\n\n----------------------------------------\n\nTITLE: LLM Invoke Endpoint Definition Python\nDESCRIPTION: This snippet defines the `invoke` endpoint for making LLM requests. It takes a `model_config`, `prompt_messages`, optional `tools` and `stop` parameters, and a `stream` flag. It returns either a generator of `LLMResultChunk` or an `LLMResult`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self,\n    model_config: LLMModelConfig,\n    prompt_messages: list[PromptMessage],\n    tools: list[PromptMessageTool] | None = None,\n    stop: list[str] | None = None,\n    stream: bool = True,\n) -> Generator[LLMResultChunk, None, None] | LLMResult:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Invoking Tools in Python\nDESCRIPTION: This code snippet illustrates how to invoke a tool within the Dify Agent Plugin using `self.session.tool.invoke()`. It specifies the required parameters such as `provider_type`, `provider`, `tool_name`, and `parameters`. The `tool_name` and `parameters` are typically generated by the LLM during a function call.  The example emphasizes extracting these parameters and passing them to the tool invocation function.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/agent.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dify_plugin.entities.tool import ToolProviderType\n\nclass FunctionCallingAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        \"\"\"\n        Run FunctionCall agent application\n        \"\"\"\n        fc_params = FunctionCallingParams(**parameters)\n        \n        # tool_call_name と tool_call_args パラメータはLLMの出力から取得されます。\n        tool_instances = {tool.identity.name: tool for tool in fc_params.tools} if fc_params.tools else {}\n        tool_instance = tool_instances[tool_call_name]\n        tool_invoke_responses = self.session.tool.invoke(\n            provider_type=ToolProviderType.BUILT_IN,\n            provider=tool_instance.identity.provider,\n            tool_name=tool_instance.identity.name,\n            # デフォルト値を追加\n            parameters={**tool_instance.runtime_parameters, **tool_call_args},\n        )\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: These commands install the required Python packages for the plugin to function correctly.  It includes `werkzeug`, `flask`, and `dify-plugin`.  These packages provide functionalities for request handling, web framework, and Dify plugin integration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npip install werkzeug\npip install flask\npip install dify-plugin\n```\n\n----------------------------------------\n\nTITLE: Validating Model Credentials in Dify (Python)\nDESCRIPTION: This method validates the credentials for a specific model.  The `model` parameter specifies the model name, and the `credentials` parameter contains the credentials dictionary defined in the `provider_credential_schema` or `model_credential_schema` of the provider's YAML configuration file. It should raise a `CredentialsValidateFailedError` if validation fails.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n    \"\"\"\n    Validate model credentials\n\n    :param model: model name\n    :param credentials: model credentials\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for LLM in Python\nDESCRIPTION: This code snippet calculates the number of tokens for given prompt messages. It takes model name, credentials, prompt messages, and tools as input. If the model doesn't provide a pre-calculated tokens interface, it can directly return 0.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                     tools: Optional[list[PromptMessageTool]] = None) -> int:\n      \"\"\"\n      Get number of tokens for given prompt messages\n\n      :param model: model name\n      :param credentials: model credentials\n      :param prompt_messages: prompt messages\n      :param tools: tools for tool calling\n      :return:\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Completion Invoke Specification Python\nDESCRIPTION: Defines the invoke method for completion-type applications in Dify, with parameters for app_id, inputs, response_mode, and files. The method returns a generator of dictionaries if response_mode is streaming or a single dictionary otherwise.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self,\n    app_id: str,\n    inputs: dict,\n    response_mode: Literal[\"streaming\", \"blocking\"],\n    files: list,\n) -> Generator[dict, None, None] | dict:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Creating Log Messages in Agent Strategy (Python)\nDESCRIPTION: This Python code demonstrates how to create log messages within an Agent strategy using `create_log_message` and `finish_log_message`.  It highlights logging the start and finish of a model call, aiding in debugging and analysis.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel_log = self.create_log_message(\n            label=f\"{params.model.model} Thought\",\n            data={},\n            metadata={\"start_at\": model_started_at, \"provider\": params.model.provider},\n            status=ToolInvokeMessage.LogMessage.LogStatus.START,\n        )\nyield model_log\nself.session.model.llm.invoke(...)\nyield self.finish_log_message(\n    log=model_log,\n    data={\n        \"output\": response,\n        \"tool_name\": tool_call_names,\n        \"tool_input\": tool_call_inputs,\n    },\n    metadata={\n        \"started_at\": model_started_at,\n        \"finished_at\": time.perf_counter(),\n        \"elapsed_time\": time.perf_counter() - model_started_at,\n        \"provider\": params.model.provider,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Render an HTML Form\nDESCRIPTION: This HTML code defines a simple form with various input fields, including text, password, textarea, date, time, datetime, select, and checkbox. The form includes attributes for data formatting, options, and tooltips, demonstrating how to use HTML elements to create interactive forms.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/template.md#_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<form data-format=\"json\"> // Default to text\n  <label for=\"username\">Username:</label>\n  <input type=\"text\" name=\"username\" />\n  <label for=\"password\">Password:</label>\n  <input type=\"password\" name=\"password\" />\n  <label for=\"content\">Content:</label>\n  <textarea name=\"content\"></textarea>\n  <label for=\"date\">Date:</label>\n  <input type=\"date\" name=\"date\" />\n  <label for=\"time\">Time:</label>\n  <input type=\"time\" name=\"time\" />\n  <label for=\"datetime\">Datetime:</label>\n  <input type=\"datetime\" name=\"datetime\" />\n  <label for=\"select\">Select:</label>\n  <input type=\"select\" name=\"select\" data-options='[\"hello\",\"world\"]'/>\n  <input type=\"checkbox\" name=\"check\" data-tip=\"By checking this means you agreed\"/>\n  <button data-size=\"small\" data-variant=\"primary\">Login</button>\n</form>\n```\n\n----------------------------------------\n\nTITLE: Query Document Segments with Dify API\nDESCRIPTION: This snippet shows how to retrieve the segments of a document from a Dify knowledge base using the API. It requires the `dataset_id` and `document_id` of the document and a valid API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Dify Tool Output Schema Definition (YAML)\nDESCRIPTION: This YAML code snippet defines the structure for the output schema of a Dify tool plugin. It specifies the metadata (author, name, label, description) and defines a single output property 'name' of type string. This schema allows the tool's output to be referenced in workflow applications.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/tool.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: author\n  name: tool\n  label:\n    en_US: label\n    zh_Hans: 标签\n    ja_JP: ラベル\n    pt_BR: etiqueta\ndescription:\n  human:\n    en_US: description\n    zh_Hans: 描述\n    ja_JP: 説明\n    pt_BR: descrição\n  llm: description\noutput_schema:\n  type: object\n  properties:\n    name:\n      type: string\n```\n\n----------------------------------------\n\nTITLE: LLM Invocation Method Signature (Python)\nDESCRIPTION: This Python code defines the signature for the `_invoke` method, which is the core method for invoking a large language model. It takes parameters like model name, credentials, prompt messages, model parameters, tools, stop words, stream flag, and user ID. It returns either a full LLMResult or a stream response chunk generator, depending on the `stream` parameter.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/predefined-model.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            prompt_messages: list[PromptMessage], model_parameters: dict,\n            tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,\n            stream: bool = True, user: Optional[str] = None) \\\n    -> Union[LLMResult, Generator]:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param model_parameters: model parameters\n    :param tools: tools for tool calling\n    :param stop: stop words\n    :param stream: is stream response\n    :param user: unique user id\n    :return: full response or stream response chunk generator result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Knowledge with cURL\nDESCRIPTION: This cURL command retrieves knowledge from the Dify knowledge base based on a query. It requires the dataset ID and the query text. The request body includes the query, retrieval model configurations (search method, reranking, weights, top_k, score threshold). It returns the relevant segments and their scores.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/retrieve' \\\n--header 'Authorization: Bearer {api_key}'\\\n--header 'Content-Type: application/json'\\\n--data-raw '{\n    \"query\": \"test\",\n    \"retrieval_model\": {\n        \"search_method\": \"keyword_search\",\n        \"reranking_enable\": false,\n        \"reranking_mode\": null,\n        \"reranking_model\": {\n            \"reranking_provider_name\": \"\",\n            \"reranking_model_name\": \"\"\n        },\n        \"weights\": null,\n        \"top_k\": 1,\n        \"score_threshold_enabled\": false,\n        \"score_threshold\": null\n    }\n}'\n```\n\n----------------------------------------\n\nTITLE: Deleting Metadata Fields in Knowledge Base via Dify API\nDESCRIPTION: Deletes existing metadata fields from a dataset within the Dify knowledge base. Requires the dataset ID and metadata ID as path parameters and a valid API key in the Authorization header.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata/{metadata_id}' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Adding Docker's Official Repository\nDESCRIPTION: This command adds Docker's official repository to the system's list of package sources. This allows the system to find and install Docker packages from Docker's own repository. It also specifies that the repository is signed using the previously added GPG key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n```\n\n----------------------------------------\n\nTITLE: Defining LLMResult Model in Python\nDESCRIPTION: Defines the `LLMResult` model, representing the result of a Language Model (LLM) interaction. It includes the model name, prompt messages, the assistant's response, usage information, and an optional system fingerprint.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResult(BaseModel):\n    \"\"\"\n    Model class for llm result.\n    \"\"\"\n    model: str  # Actual used modele\n    prompt_messages: list[PromptMessage]  # prompt messages\n    message: AssistantPromptMessage  # response message\n    usage: LLMUsage  # usage info\n    system_fingerprint: Optional[str] = None  # request fingerprint, refer to OpenAI definition\n```\n\n----------------------------------------\n\nTITLE: Configuring Stop Sequences for LLM Text Generation\nDESCRIPTION: This code snippet demonstrates how to configure stop sequences to prevent LLMs from generating excessive content. By setting specific words, phrases, or characters as stop sequences, the LLM will halt text generation upon encountering them.  This example sets 'Human1:' as the stop sequence, ensuring the model generates only one sentence instead of additional dialogues.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/prompt-engineering/prompt-engineering-expert-mode.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nHuman1: What color is the sky?\n\nAssistant1: The sky is blue.\n\nHuman1: What color is the fire?\n\nAssistant1: The fire is red.\n\nHuman1: What color is the soil?\n\nAssistant1: \n```\n\n----------------------------------------\n\nTITLE: Rerank Invoke Endpoint (Python)\nDESCRIPTION: This code defines the signature for the `invoke` method that performs the reranking. It takes a RerankModelConfig, a list of documents, and a query string as input and returns a RerankResult object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self, model_config: RerankModelConfig, docs: list[str], query: str\n) -> RerankResult:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Run FastAPI Application (Shell)\nDESCRIPTION: Command to launch the FastAPI service using Uvicorn, enabling auto-reload and binding to all network interfaces.  This makes the application available at http://127.0.0.1:8000/api/dify/receive.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/README.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\nuvicorn main:app --reload --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Mapping Model Invocation Errors in Python\nDESCRIPTION: This code snippet shows how to map model invocation errors to unified error types. It defines a dictionary that maps InvokeError types to the corresponding exception types thrown by the model. This allows Dify to handle different errors with appropriate follow-up actions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Endpoint Interface Definition YAML\nDESCRIPTION: This YAML snippet defines an Endpoint interface, specifying its path, method (HTTP verb), and extra configuration. The 'python' section under 'extra' indicates the path to the Python source code implementing this interface.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/endpoint.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npath: \"/duck/<app_id>\"\nmethod: \"GET\"\nextra:\n  python:\n    source: \"endpoints/duck.py\"\n```\n\n----------------------------------------\n\nTITLE: Vectorizer.AI Tool Interface Definition (Python)\nDESCRIPTION: This code defines the interface for the Vectorizer.AI tool. It includes placeholders for the `_invoke`, `get_runtime_parameters`, and `is_tool_available` methods, which are later implemented to interact with the Vectorizer.AI API and manage tool availability based on the variable pool's content. This snippet primarily outlines the structure and required methods of the `VectorizerTool` class.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/advanced-tool-integration.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom core.tools.tool.builtin_tool import BuiltinTool\nfrom core.tools.entities.tool_entities import ToolInvokeMessage, ToolParameter\nfrom core.tools.errors import ToolProviderCredentialValidationError\n\nfrom typing import Any, Dict, List, Union\nfrom httpx import post\nfrom base64 import b64decode\n\nclass VectorizerTool(BuiltinTool):\n    def _invoke(self, user_id: str, tool_Parameters: Dict[str, Any]) \\\n        -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:\n        \"\"\"\n        Tool invocation, the image variable name needs to be passed in from here, so that we can get the image from the variable pool\n        \"\"\"\n        \n    \n    def get_runtime_parameters(self) -> List[ToolParameter]:\n        \"\"\"\n        Override the tool parameter list, we can dynamically generate the parameter list based on the actual situation in the current variable pool, so that the LLM can generate the form based on the parameter list\n        \"\"\"\n        \n    \n    def is_tool_available(self) -> bool:\n        \"\"\"\n        Whether the current tool is available, if there is no image in the current variable pool, then we don't need to display this tool, just return False here\n        \"\"\"     \n```\n\n----------------------------------------\n\nTITLE: Backend Project Structure Overview\nDESCRIPTION: This snippet shows the directory structure of the Dify backend, which is written in Python using Flask. It provides insights into the organization of the codebase, highlighting key areas such as API controllers, core application logic, database models, and task handling.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_26\n\nLANGUAGE: Text\nCODE:\n```\n[api/]\n├── constants             // Constant settings used throughout code base.\n├── controllers           // API route definitions and request handling logic.\n├── core                  // Core application orchestration, model integrations, and tools.\n├── docker                // Docker & containerization related configurations.\n├── events                // Event handling and processing\n├── extensions            // Extensions with 3rd party frameworks/platforms.\n├── fields                // field definitions for serialization/marshalling.\n├── libs                  // Reusable libraries and helpers.\n├── migrations            // Scripts for database migration.\n├── models                // Database models & schema definitions.\n├── services              // Specifies business logic.\n├── storage               // Private key storage.\n├── tasks                 // Handling of async tasks and background jobs.\n└── tests\n```\n\n----------------------------------------\n\nTITLE: Moderation Output Response Body JSON\nDESCRIPTION: Defines the structure for the API response of the `app.moderation.output` extension point. It indicates whether the content was flagged, the action to take, a preset response, and the LLM's text output. Contains parameters to determine if the output is safe and appropriate.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation-extension.md#_snippet_7\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"flagged\": bool,\n    \"action\": string,\n    \"preset_response\": string,\n    \"text\": string\n\n```\n\n----------------------------------------\n\nTITLE: Assistant Prompt Message Class Definition\nDESCRIPTION: This code snippet defines the AssistantPromptMessage class, representing a message from the assistant. It includes the 'tool_calls' attribute for scenarios where the model interacts with tools.  It inherits from PromptMessage and sets the role to ASSISTANT.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass AssistantPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for assistant prompt message.\n    \"\"\"\n    class ToolCall(BaseModel):\n        \"\"\"\n        Model class for assistant prompt message tool call.\n        \"\"\"\n        class ToolCallFunction(BaseModel):\n            \"\"\"\n            Model class for assistant prompt message tool call function.\n            \"\"\"\n            name: str  # ツールの名前\n            arguments: str  # ツールの引数\n\n        id: str  # ツールID。OpenAI のツール呼び出しでのみ有効で、ツール呼び出しの一意なIDです。同じツールを複数回呼び出すことができます。\n        type: str  # デフォルトは function\n        function: ToolCallFunction  # ツール呼び出し情報\n\n    role: PromptMessageRole = PromptMessageRole.ASSISTANT\n    tool_calls: list[ToolCall] = []  # モデルが応答したツール呼び出しの結果です（tools が渡され、モデルがツールを呼び出す必要があると判断した場合のみ返されます）。\n```\n\n----------------------------------------\n\nTITLE: Invoking LLM Model in Python\nDESCRIPTION: This snippet demonstrates how to invoke an LLM model using the `session.model.llm.invoke()` function within the Dify Agent Plugin. It shows how to pass the model configuration, prompt messages, and tools to the invoke method. The `_init_prompt_tools` and `_convert_tool_to_prompt_message_tool` functions handle the conversion of `ToolEntity` objects into `PromptMessageTool` objects, which are required by the `invoke` method.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/agent.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.model.llm import LLMModelConfig\nfrom dify_plugin.entities.model.message import (\n    PromptMessageTool,\n    SystemPromptMessage,\n    UserPromptMessage,\n)\nfrom dify_plugin.entities.tool import ToolParameter\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\n\nclass FunctionCallingParams(BaseModel):\n    query: str\n    instruction: str | None\n    model: AgentModelConfig\n    tools: list[ToolEntity] | None\n    maximum_iterations: int = 3\n\nclass FunctionCallingAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        \"\"\"\n        Run FunctionCall agent application\n        \"\"\"\n        # init params\n        fc_params = FunctionCallingParams(**parameters)\n        query = fc_params.query\n        model = fc_params.model\n        stop = fc_params.model.completion_params.get(\"stop\", []) if fc_params.model.completion_params else []\n        prompt_messages = [\n            SystemPromptMessage(content=\"your system prompt message\"),\n            UserPromptMessage(content=query),\n        ]\n        tools = fc_params.tools\n        prompt_messages_tools = self._init_prompt_tools(tools)\n\n        # invoke llm\n        chunks = self.session.model.llm.invoke(\n            model_config=LLMModelConfig(**model.model_dump(mode=\"json\")),\n            prompt_messages=prompt_messages,\n            stream=True,\n            stop=stop,\n            tools=prompt_messages_tools,\n        )\n\n    def _init_prompt_tools(self, tools: list[ToolEntity] | None) -> list[PromptMessageTool]:\n        \"\"\"\n        Init tools\n        \"\"\"\n\n        prompt_messages_tools = []\n        for tool in tools or []:\n            try:\n                prompt_tool = self._convert_tool_to_prompt_message_tool(tool)\n            except Exception:\n                # api tool may be deleted\n                continue\n\n            # save prompt tool\n            prompt_messages_tools.append(prompt_tool)\n\n        return prompt_messages_tools\n\n    def _convert_tool_to_prompt_message_tool(self, tool: ToolEntity) -> PromptMessageTool:\n        \"\"\"\n        convert tool to prompt message tool\n        \"\"\"\n        message_tool = PromptMessageTool(\n            name=tool.identity.name,\n            description=tool.description.llm if tool.description else \"\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        )\n\n        parameters = tool.parameters\n        for parameter in parameters:\n            if parameter.form != ToolParameter.ToolParameterForm.LLM:\n                continue\n\n            parameter_type = parameter.type\n            if parameter.type in {\n                ToolParameter.ToolParameterType.FILE,\n                ToolParameter.ToolParameterType.FILES,\n            }:\n                continue\n            enum = []\n            if parameter.type == ToolParameter.ToolParameterType.SELECT:\n                enum = [option.value for option in parameter.options] if parameter.options else []\n\n            message_tool.parameters[\"properties\"][parameter.name] = {\n                \"type\": parameter_type,\n                \"description\": parameter.llm_description or \"\",\n            }\n\n            if len(enum) > 0:\n                message_tool.parameters[\"properties\"][parameter.name][\"enum\"] = enum\n\n            if parameter.required:\n                message_tool.parameters[\"required\"].append(parameter.name)\n\n        return message_tool\n```\n\n----------------------------------------\n\nTITLE: API Response Structure\nDESCRIPTION: This JSON structure defines the response format from the API after querying the external data tool. The 'result' field contains the string-formatted data retrieved from the external source. This data can then be used to augment the prompt for the LLM.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/external-data-tool.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"result\": string\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Message Creation in Dify (Python)\nDESCRIPTION: This Python code snippet defines the interface for creating a JSON message within the Dify tool plugin framework. It takes a Python dictionary as input and returns a ToolInvokeMessage object. This is commonly used for data transfer between workflow nodes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/tool.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef create_json_message(self, json: dict) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: コードノードでのJSON検証コード\nDESCRIPTION: コードノードで使用されるJSON検証コードの例を示します。このコードは、入力されたJSON文字列を解析し、結果を辞書として返します。json.loadsを使用してJSON文字列をオブジェクトに変換します。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/error-handling/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef main(json_str: str) -> dict:\n    obj = json.loads(json_str)\n    return {'result': obj}\n```\n\n----------------------------------------\n\nTITLE: LLM Result Chunk Delta Class\nDESCRIPTION: Defines the `LLMResultChunkDelta` class, representing the delta of a chunk in a streaming LLM result.  It includes the index, the message delta, usage information, and the finish reason.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunkDelta(BaseModel):\n    \"\"\"\n    Model class for llm result chunk delta.\n    \"\"\"\n    index: int  # 序号\n    message: AssistantPromptMessage  # 回复消息\n    usage: Optional[LLMUsage] = None  # 使用的 tokens 及费用信息，仅最后一条返回\n    finish_reason: Optional[str] = None  # 结束原因，仅最后一条返回\n```\n\n----------------------------------------\n\nTITLE: Validating Model Credentials (Python)\nDESCRIPTION: This code snippet demonstrates how to validate model credentials within the Dify platform. This is a general interface that all model types need to implement. The credentials can be defined in either the `provider_credential_schema` or `model_credential_schema` of the supplier's YAML configuration file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n    \"\"\"\n    Validate model credentials\n\n    :param model: model name\n    :param credentials: model credentials\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Tool Provider YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates the structure and configuration for a tool provider, defining its identity (author, name, label, description, icon) for frontend display. The 'identity' field is mandatory and includes basic information about the tool provider.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/quick-tool-integration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nidentity: # Basic information of the tool provider\n  author: Dify # Author\n  name: google # Name, unique, no duplication with other providers\n  label: # Label for frontend display\n    en_US: Google # English label\n    zh_Hans: Google # Chinese label\n    ja_JP: Google # Japanese label\n    pt_BR: Google # Portuguese label\n  description: # Description for frontend display\n    en_US: Google # English description\n    zh_Hans: Google # Chinese description\n    ja_JP: Google # Japanese description\n    pt_BR: Google # Portuguese description\n  icon: icon.svg # Icon, needs to be placed in the _assets folder of the current module\n```\n\n----------------------------------------\n\nTITLE: Implementing API Logic in src/index.ts (TypeScript)\nDESCRIPTION: This TypeScript snippet demonstrates how to implement API logic within the `src/index.ts` file of the Cloudflare Workers project. It fetches a random Breaking Bad quote from an external API, demonstrating interaction with third-party services. It accesses `count` parameter from the `params` object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// ⬇️ implement your logic here ⬇️\n// point === \"app.external_data_tool.query\"\n// https://api.breakingbadquotes.xyz/v1/quotes\nconst count = params?.inputs?.count ?? 1;\nconst url = `https://api.breakingbadquotes.xyz/v1/quotes/${count}`;\nconst result = await fetch(url).then(res => res.text())\n// ⬆️ implement your logic here ⬆️\n```\n\n----------------------------------------\n\nTITLE: Creating Xinference Provider YAML File\nDESCRIPTION: This YAML snippet demonstrates the structure of a provider configuration file for Xinference, defining the provider's identity, labels, icons, help information, supported model types (LLM, Text Embedding, Rerank), configurate methods, and the beginning of the credential schema.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprovider: xinference # 确定供应商标识\nlabel: # 供应商展示名称，可设置 en_US 英文、zh_Hans 中文两种语言，zh_Hans 不设置将默认使用 en_US。\n  en_US: Xorbits Inference\nicon_small: # 小图标，可以参考其他供应商的图标，存储在对应供应商实现目录下的 _assets 目录，中英文策略同 label\n  en_US: icon_s_en.svg\nicon_large: # 大图标\n  en_US: icon_l_en.svg\nhelp: # 帮助\n  title:\n    en_US: How to deploy Xinference\n    zh_Hans: 如何部署 Xinference\n  url:\n    en_US: https://github.com/xorbitsai/inference\n<strong>supported_model_types: # 支持的模型类型，Xinference 同时支持 LLM/Text Embedding/Rerank\n</strong><strong>- llm\n</strong><strong>- text-embedding\n</strong><strong>- rerank\n</strong>configurate_methods: # Xinference 为本地部署的供应商，并且没有预定义模型，需要用什么模型需要根据 Xinference 的文档进行部署，因此此处的方法为自定义模型。\n- customizable-model\nprovider_credential_schema:\n  credential_form_schemas:\n```\n\n----------------------------------------\n\nTITLE: Invoking Speech to Text Model in Python\nDESCRIPTION: This method invokes a speech-to-text model. It takes the model name, credentials, audio file, and user ID as input. It returns the text converted from the audio file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n                file: IO[bytes], user: Optional[str] = None) \\\n            -> str:\n        \"\"\"\n        Invoke large language model\n\n        :param model: model name\n        :param credentials: model credentials\n        :param file: audio file\n        :param user: unique user id\n        :return: text for given audio file\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define Tool Prompt Message Model (Python)\nDESCRIPTION: Defines a Pydantic model `ToolPromptMessage` representing tool messages, used for conveying the results of a tool execution to the model for the next step of processing. It includes attributes for the `role` (set to `PromptMessageRole.TOOL`) and the `tool_call_id` representing the Tool invocation ID.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nclass ToolPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for tool prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.TOOL\n    tool_call_id: str  # Tool invocation ID. If OpenAI tool call is not supported, the name of the tool can also be inputted.\n```\n\n----------------------------------------\n\nTITLE: Tool Invocation with LLM-Generated Parameters in Python\nDESCRIPTION: This code snippet demonstrates how to invoke a tool using parameters generated by a Language Model (LLM). It retrieves the tool instance based on the tool call name and invokes it using `self.session.tool.invoke`, passing the provider type, provider name, tool name, and combined runtime and call-specific parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntool_instances = (\n    {tool.identity.name: tool for tool in params.tools} if params.tools else {}\n)\nfor tool_call_id, tool_call_name, tool_call_args in tool_calls:\n    tool_instance = tool_instances[tool_call_name]\n    self.session.tool.invoke(\n        provider_type=ToolProviderType.BUILT_IN,\n        provider=tool_instance.identity.provider,\n        tool_name=tool_instance.identity.name,\n        parameters={**tool_instance.runtime_parameters, **tool_call_args},\n    )\n```\n\n----------------------------------------\n\nTITLE: Enhanced System Prompt with Style\nDESCRIPTION: This expanded prompt directs the Agent to use the stability_text2image tool to create images according to user input, specifically in an anime style. It adds a style constraint to the system prompt, ensuring all generated images default to this style.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/workshop/basic/build-ai-image-generation-app.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n根据用户的提示，使用工具 stability_text2image 绘画指定内容，画面是二次元风格\n```\n\n----------------------------------------\n\nTITLE: Initializing Dify Plugin Project (CLI, Renamed Binary)\nDESCRIPTION: This command initializes a new Dify plugin project, assuming the binary file has been renamed to `dify` and copied to the `/usr/local/bin` path. This makes the command accessible globally.  It's an alternative to using the full path to the binary.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: Model Selector Configuration (YAML)\nDESCRIPTION: This YAML configuration defines a tool parameter list including a `model` parameter of type `model-selector`.  This allows users to select a desired LLM model in the UI. The `scope` is set to `llm`, limiting the selection to LLM models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: llm\n  author: Dify\n  label:\n    en_US: LLM\n    zh_Hans: LLM\n    pt_BR: LLM\ndescription:\n  human:\n    en_US: A tool for invoking a large language model\n    zh_Hans: 用于调用大型语言模型的工具\n    pt_BR: A tool for invoking a large language model\n  llm: A tool for invoking a large language model\nparameters:\n  - name: prompt\n    type: string\n    required: true\n    label:\n      en_US: Prompt string\n      zh_Hans: 提示字符串\n      pt_BR: Prompt string\n    human_description:\n      en_US: used for searching\n      zh_Hans: 用于搜索网页内容\n      pt_BR: used for searching\n    llm_description: key words for searching\n    form: llm\n  - name: model\n    type: model-selector\n    scope: llm\n    required: true\n    label:\n      en_US: Model\n      zh_Hans: 使用的模型\n      pt_BR: Model\n    human_description:\n      en_US: Model\n      zh_Hans: 使用的模型\n      pt_BR: Model\n    llm_description: which Model to invoke\n    form: form\nextra:\n  python:\n    source: tools/llm.py\n```\n\n----------------------------------------\n\nTITLE: Enable History Messages (YAML)\nDESCRIPTION: This YAML snippet shows how to enable the history messages feature within the agent strategy configuration. By adding 'history-messages' under the 'features' section, the agent will be configured to remember previous interactions in the conversation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: basic_agent  # Agent策略名称\n  author: novice     # 作者\n  label:\n    en_US: BasicAgent  # 英文标签\ndescription:\n  en_US: BasicAgent    # 英文描述\nfeatures:\n  - history-messages   # 启用历史消息功能\n...\n```\n\n----------------------------------------\n\nTITLE: Defining Model Name Credential Schema\nDESCRIPTION: This YAML snippet defines the credential schema for the `model_name` parameter, requiring the user to input the name of the model. It utilizes a text input field with labels and placeholders in both English and Simplified Chinese, ensuring a user-friendly experience. The `required` field mandates that this parameter be provided.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/customizable-model.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n  - variable: model_name\n    type: text-input\n    label:\n      en_US: Model name\n      zh_Hans: 模型名称\n    required: true\n    placeholder:\n      zh_Hans: 填写模型名称\n      en_US: Input model name\n```\n\n----------------------------------------\n\nTITLE: LLM Result Class Definition\nDESCRIPTION: This code snippet defines the LLMResult class, representing the result of a large language model invocation. It contains the model name, prompt messages, the assistant's reply message, usage information, and an optional system fingerprint.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResult(BaseModel):\n    \"\"\"\n    Model class for llm result.\n    \"\"\"\n    model: str  # 使用モデル\n    prompt_messages: list[PromptMessage]  # プロンプトメッセージリスト\n    message: AssistantPromptMessage  # 返信メッセージ\n    usage: LLMUsage  # トークン及び費用情報\n    system_fingerprint: Optional[str] = None  # リクエスト指紋（OpenAIの定義に準拠）\n```\n\n----------------------------------------\n\nTITLE: Running LiteLLM Proxy with Docker\nDESCRIPTION: This Docker command starts the LiteLLM proxy server. It mounts the `litellm_config.yaml` file into the container, maps port 4000 for external access, and enables detailed debugging. The proxy becomes accessible at `http://localhost:4000`, ready to receive API requests routed according to the config file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/litellm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n    -v $(pwd)/litellm_config.yaml:/app/config.yaml \\\n    -p 4000:4000 \\\n    ghcr.io/berriai/litellm:main-latest \\\n    --config /app/config.yaml --detailed_debug\n```\n\n----------------------------------------\n\nTITLE: Anthropic Provider Class (Python)\nDESCRIPTION: Implements the `AnthropicProvider` class, which inherits from the `ModelProvider` base class. This class includes a `validate_provider_credentials` method to validate the provider's credentials. It checks if the credentials are valid by attempting to instantiate and validate an Anthropic model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom dify_plugin.entities.model import ModelType\nfrom dify_plugin.errors.model import CredentialsValidateFailedError\nfrom dify_plugin import ModelProvider\n\nlogger = logging.getLogger(__name__)\n\n\nclass AnthropicProvider(ModelProvider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        \"\"\"\n        Validate provider credentials\n\n        if validate failed, raise exception\n\n        :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n        \"\"\"\n        try:\n            model_instance = self.get_model_instance(ModelType.LLM)\n            model_instance.validate_credentials(model=\"claude-3-opus-20240229\", credentials=credentials)\n        except CredentialsValidateFailedError as ex:\n            raise ex\n        except Exception as ex:\n            logger.exception(f\"{self.get_provider_schema().provider} credentials validate failed\")\n            raise ex\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens in Python\nDESCRIPTION: This Python snippet defines a function to retrieve the number of tokens for given prompt messages. It is designed to return 0 if the model doesn't provide an interface for pre-calculating tokens, offering a way to indicate that the functionality is not applicable or has not been implemented yet.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                   tools: Optional[list[PromptMessageTool]] = None) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param tools: tools for tool calling\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implement Google Search Tool Logic in Python\nDESCRIPTION: This Python code defines the GoogleSearchTool class, which inherits from BuiltinTool. The _invoke method handles the tool's logic, taking user_id and tool_parameters as input. It retrieves the query and result_type from the parameters, interacts with the SerpAPI to perform the search, and returns either a text message or a link message based on the result_type. This code depends on the core.tools and typing modules, as well as a SerpAPI implementation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/quick-tool-integration.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom core.tools.tool.builtin_tool import BuiltinTool\nfrom core.tools.entities.tool_entities import ToolInvokeMessage\n\nfrom typing import Any, Dict, List, Union\n\nclass GoogleSearchTool(BuiltinTool):\n    def _invoke(self, \n                user_id: str,\n               tool_parameters: Dict[str, Any], \n        ) -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:\n        \"\"\"\n            invoke tools\n        \"\"\"\n        query = tool_parameters['query']\n        result_type = tool_parameters['result_type']\n        api_key = self.runtime.credentials['serpapi_api_key']\n        # TODO: search with serpapi\n        result = SerpAPI(api_key).run(query, result_type=result_type)\n\n        if result_type == 'text':\n            return self.create_text_message(text=result)\n        return self.create_link_message(link=result)\n```\n\n----------------------------------------\n\nTITLE: Initializing Dify Plugin Project (Bash)\nDESCRIPTION: Initializes a new Dify plugin project using the CLI tool. This command creates the basic project structure and necessary files.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./dify-plugin-darwin-arm64 plugin init\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Labels (Python Enum)\nDESCRIPTION: This Python enum defines the valid labels (categories/tags) that can be used when defining a Tool Plugin.  It ensures that the tool is categorized correctly and helps users find relevant plugins. The enum provides a set of predefined options for categorizing tools.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass ToolLabelEnum(Enum):\n  SEARCH = 'search'\n  IMAGE = 'image'\n  VIDEOS = 'videos'\n  WEATHER = 'weather'\n  FINANCE = 'finance'\n  DESIGN = 'design'\n  TRAVEL = 'travel'\n  SOCIAL = 'social'\n  NEWS = 'news'\n  MEDICAL = 'medical'\n  PRODUCTIVITY = 'productivity'\n  EDUCATION = 'education'\n  BUSINESS = 'business'\n  ENTERTAINMENT = 'entertainment'\n  UTILITIES = 'utilities'\n  OTHER = 'other'\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Content Type Enum Definition\nDESCRIPTION: This code snippet defines an enumeration for different content types of a prompt message, including TEXT and IMAGE. This allows for multimodal prompts consisting of both text and images.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContentType(Enum):\n    \"\"\"\n    Enum class for prompt message content type.\n    \"\"\"\n    TEXT = 'text'\n    IMAGE = 'image'\n```\n\n----------------------------------------\n\nTITLE: Upgrade Dify on EC2 Instance\nDESCRIPTION: This script upgrades a self-hosted Dify instance running on an AWS EC2 instance. It clones the latest Dify code from GitHub, moves the Docker Compose files to the Dify directory, removes the temporary clone, stops the existing containers, pulls the latest images, and then restarts the containers using Docker Compose.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_63\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git /tmp/dify\nmv -f /tmp/dify/docker/* /dify/\nrm -rf /tmp/dify\ndocker-compose down\ndocker-compose pull\ndocker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Streaming Variable Message Creation in Dify (Python)\nDESCRIPTION: This Python code snippet defines the interface for creating a streaming variable message within the Dify tool plugin framework. It takes a variable name (string) and a variable value (string) as input and returns a ToolInvokeMessage object. This is used for typewriter-effect text output.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/tool.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef create_stream_variable_message(\n    self, variable_name: str, variable_value: str\n) -> ToolInvokeMessage:\n```\n\n----------------------------------------\n\nTITLE: UI Generation JSON Schema Example\nDESCRIPTION: This JSON Schema is used for dynamically generating UI components. It uses recursion ('$ref': '#') to allow for nested UI structures. The schema defines the 'type' of UI component (e.g., 'div', 'button'), a 'label', 'children' (nested components), and 'attributes'. It also supports the use of enums to limit the type of component. The strict parameter ensures the generated schema conforms to the defined format.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/extended-reading/how-to-use-json-schema-in-dify.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"ui\",\n    \"description\": \"動的に生成されたUI\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"type\": {\n                \"type\": \"string\",\n                \"description\": \"UIコンポーネントのタイプ\",\n                \"enum\": [\"div\", \"button\", \"header\", \"section\", \"field\", \"form\"]\n            },\n            \"label\": {\n                \"type\": \"string\",\n                \"description\": \"UIコンポーネントのラベル、ボタンやフォームフィールドに使用\"\n            },\n            \"children\": {\n                \"type\": \"array\",\n                \"description\": \"入れ子のUIコンポーネント\",\n                \"items\": {\n                    \"$ref\": \"#\"\n                }\n            },\n            \"attributes\": {\n                \"type\": \"array\",\n                \"description\": \"UIコンポーネントのための任意の属性、任意の要素に適しています\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"name\": {\n                            \"type\": \"string\",\n                            \"description\": \"属性の名前、例えばonClickやclassName\"\n                        },\n                        \"value\": {\n                            \"type\": \"string\",\n                            \"description\": \"属性の値\"\n                        }\n                    },\n                    \"additionalProperties\": false,\n                    \"required\": [\"name\", \"value\"]\n                }\n            }\n        },\n        \"required\": [\"type\", \"label\", \"children\", \"attributes\"],\n        \"additionalProperties\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Sandbox Service (Bash)\nDESCRIPTION: This bash command starts the Dify sandbox service using Docker Compose. It utilizes the `docker-compose.middleware.yaml` file to define and run the necessary containers in detached mode (-d). This requires Docker and Docker Compose to be installed and configured on the system. This command is for local deployment and ensures secure execution of code nodes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/code.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose -f docker-compose.middleware.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Moderation Input Response - Overridden JSON\nDESCRIPTION: Illustrates an example API response for the `app.moderation.input` extension point where the action is `overridden`.  This means the flagged parts of the content have been replaced or modified.  It shows the overridden input variables and original query. Depends on the moderation service to indicate which parts were flagged and what replacement was appropriate.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation-extension.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"overridden\",\n    \"inputs\": {\n        \"var_1\": \"I will *** you.\",\n        \"var_2\": \"I will *** you.\"\n    },\n    \"query\": \"Happy everydays.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking Large Language Model in Python\nDESCRIPTION: This code snippet demonstrates the core method for invoking a large language model. It supports both streaming and synchronous returns and takes model name, credentials, prompt messages, model parameters, tools, stop words, stream flag, and user identifier as input.  The method returns either a full response or a stream response chunk generator result.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              prompt_messages: list[PromptMessage], model_parameters: dict,\n              tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,\n              stream: bool = True, user: Optional[str] = None) \\\n          -> Union[LLMResult, Generator]:\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param prompt_messages: prompt messages\n      :param model_parameters: model parameters\n      :param tools: tools for tool calling\n      :param stop: stop words\n      :param stream: is stream response\n      :param user: unique user id\n      :return: full response or stream response chunk generator result\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Model Plugin Directory Structure Example\nDESCRIPTION: Illustrates the general directory structure for a model plugin, showing how model providers, model categories, and specific models are organized.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n- Model Provider\n  - Model Category\n    - Specific Models\n```\n\n----------------------------------------\n\nTITLE: Invoking Rerank Model in Dify (Python)\nDESCRIPTION: This method invokes a rerank model to reorder a list of documents based on a given query.  It takes a query string and a list of document strings as input, and returns a `RerankResult` object containing the reordered documents and their scores. Optional parameters include a score threshold and a top-n value to limit the number of returned documents.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            query: str, docs: list[str], score_threshold: Optional[float] = None, top_n: Optional[int] = None,\n            user: Optional[str] = None) \\\n        -> RerankResult:\n    \"\"\"\n    Invoke rerank model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param query: search query\n    :param docs: docs for reranking\n    :param score_threshold: score threshold\n    :param top_n: top n\n    :param user: unique user id\n    :return: rerank result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define LLM Result Chunk Model (Python)\nDESCRIPTION: Defines a Pydantic model `LLMResultChunk` representing a chunk of a streaming LLM result. It includes attributes for the `model` used, the `prompt_messages` sent, the `system_fingerprint`, and the `delta` (as an `LLMResultChunkDelta` object).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunk(BaseModel):\n    \"\"\"\n    Model class for llm result chunk.\n    \"\"\"\n    model: str  # Actual used modele\n    prompt_messages: list[PromptMessage]  # prompt messages\n    system_fingerprint: Optional[str] = None  # request fingerprint, refer to OpenAI definition\n    delta: LLMResultChunkDelta\n```\n\n----------------------------------------\n\nTITLE: Mapping Model Invoke Errors with Direct Errors in Python\nDESCRIPTION: This code provides a concrete example of mapping model invoke errors directly, allowing specific `InvokeError` types to be thrown directly during model invocation. This simplifies the error handling process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@property\n    def _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n        return {\n            InvokeConnectionError: [\n              InvokeConnectionError\n            ],\n            InvokeServerUnavailableError: [\n              InvokeServerUnavailableError\n            ],\n            InvokeRateLimitError: [\n              InvokeRateLimitError\n            ],\n            InvokeAuthorizationError: [\n              InvokeAuthorizationError\n            ],\n            InvokeBadRequestError: [\n              InvokeBadRequestError\n            ],\n        }\n```\n\n----------------------------------------\n\nTITLE: Testing SearXNG with cURL\nDESCRIPTION: This command tests the SearXNG integration by making a sample search request using `curl`. It queries for \"apple\" and requests a JSON response with general categories, verifying that the SearXNG instance is accessible and functioning correctly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"http://<your-linux-vm-ip>:8081/search?q=apple&format=json&categories=general\"\n```\n\n----------------------------------------\n\nTITLE: AssistantPromptMessage Class Definition in Python\nDESCRIPTION: Defines the `AssistantPromptMessage` class, inheriting from `PromptMessage`, representing an assistant's (model) message in a prompt interaction. It includes `tool_calls` for representing calls to external tools.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass AssistantPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for assistant prompt message.\n    \"\"\"\n    class ToolCall(BaseModel):\n        \"\"\"\n        Model class for assistant prompt message tool call.\n        \"\"\"\n        class ToolCallFunction(BaseModel):\n            \"\"\"\n            Model class for assistant prompt message tool call function.\n            \"\"\"\n            name: str  # 工具名称\n            arguments: str  # 工具参数\n\n        id: str  # 工具 ID，仅在 OpenAI tool call 生效，为工具调用的唯一 ID，同一个工具可以调用多次\n        type: str  # 默认 function\n        function: ToolCallFunction  # 工具调用信息\n\n    role: PromptMessageRole = PromptMessageRole.ASSISTANT\n    tool_calls: list[ToolCall] = []  # 模型回复的工具调用结果（仅当传入 tools，并且模型认为需要调用工具时返回）\n```\n\n----------------------------------------\n\nTITLE: Defining Google Tool Provider YAML (With Credentials)\nDESCRIPTION: This YAML file configures the Google tool provider with a required SerpApi API key. It defines the field type, labels, placeholders, help text, and a URL for obtaining the API key. The `type` field specifies the input type (secret-input, text-input, select).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/quick-tool-integration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: Dify\n  name: google\n  label:\n    en_US: Google\n    zh_Hans: Google\n    ja_JP: Google\n    pt_BR: Google\n  description:\n    en_US: Google\n    zh_Hans: Google\n    ja_JP: Google\n    pt_BR: Google\n  icon: icon.svg\ncredentials_for_provider: # 凭据字段\n  serpapi_api_key: # 凭据字段名称\n    type: secret-input # 凭据字段类型\n    required: true # 是否必填\n    label: # 凭据字段标签\n      en_US: SerpApi API key # 英文标签\n      zh_Hans: SerpApi API key # 中文标签\n      ja_JP: SerpApi API key # 日文标签\n      pt_BR: chave de API SerpApi # 葡萄牙文标签\n    placeholder: # 凭据字段占位符\n      en_US: Please input your SerpApi API key # 英文占位符\n      zh_Hans: 请输入你的 SerpApi API key # 中文占位符\n      ja_JP: SerpApi API keyを入力してください # 日文占位符\n      pt_BR: Por favor, insira sua chave de API SerpApi # 葡萄牙文占位符\n    help: # 凭据字段帮助文本\n      en_US: Get your SerpApi API key from SerpApi # 英文帮助文本\n      zh_Hans: 从 SerpApi 获取你的 SerpApi API key # 中文帮助文本\n      ja_JP: SerpApiからSerpApi APIキーを取得する # 日文帮助文本\n      pt_BR: Obtenha sua chave de API SerpApi da SerpApi # 葡萄牙文帮助文本\n    url: https://serpapi.com/manage-api-key # 凭据字段帮助链接\n```\n\n----------------------------------------\n\nTITLE: Deleting Knowledge Base Metadata Field with cURL\nDESCRIPTION: This cURL command deletes a metadata field from the Dify knowledge base. It requires the dataset ID and the metadata ID. The command deletes the specified metadata field. It returns a 200 success status if successful.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata/{metadata_id}' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Stability AI Prompt with Anime Style\nDESCRIPTION: This prompt enhances the basic image generation instruction by specifying that the generated image should be in the anime style. This demonstrates how to influence the style of the generated image through prompt engineering.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/workshop/basic/build-ai-image-generation-app.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nDraw the specified content according to the user's prompt using stability_text2image, the picture is in anime style.\n```\n\n----------------------------------------\n\nTITLE: Update Document with File - Dify API (JSON Response)\nDESCRIPTION: This is the JSON response received after successfully updating a document from a file via the Dify API. It includes details about the updated document, such as its ID, position, data source type, name, creation timestamp, indexing status, and other relevant metadata. A batch ID is also returned for tracking the processing of the document.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"document\": {\n    \"id\": \"\",\n    \"position\": 1,\n    \"data_source_type\": \"upload_file\",\n    \"data_source_info\": {\n      \"upload_file_id\": \"\"\n    },\n    \"dataset_process_rule_id\": \"\",\n    \"name\": \"Dify.txt\",\n    \"created_from\": \"api\",\n    \"created_by\": \"\",\n    \"created_at\": 1695308667,\n    \"tokens\": 0,\n    \"indexing_status\": \"waiting\",\n    \"error\": null,\n    \"enabled\": true,\n    \"disabled_at\": null,\n    \"disabled_by\": null,\n    \"archived\": false,\n    \"display_status\": \"queuing\",\n    \"word_count\": 0,\n    \"hit_count\": 0,\n    \"doc_form\": \"text_model\"\n  },\n  \"batch\": \"20230921150427533684\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Provider (YAML)\nDESCRIPTION: This YAML snippet defines the basic Agent provider information within the `agent.yaml` file. It includes identity details such as author, name, label (in multiple languages), description (in multiple languages), and icon. It also specifies the path to the strategy definition file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/agent.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: langgenius\n  name: agent\n  label:\n    en_US: Agent\n    zh_Hans: Agent\n    pt_BR: Agent\n  description:\n    en_US: Agent\n    zh_Hans: Agent\n    pt_BR: Agent\n  icon: icon.svg\nstrategies:\n  - strategies/function_calling.yaml\n```\n\n----------------------------------------\n\nTITLE: Showing IP Address\nDESCRIPTION: This command displays the IP address(es) of the Linux VM, necessary for accessing the SearXNG service from external networks or other services like Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nip addr show\n```\n\n----------------------------------------\n\nTITLE: Define Bedrock Retrieval API Endpoint in Flask (Python)\nDESCRIPTION: This Flask-based API endpoint handles retrieval requests for the AWS Bedrock Knowledge Base. It parses request parameters, validates the authorization header, and calls the ExternalDatasetService for knowledge retrieval. It expects a Bearer token for authentication and returns the retrieval results.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/use-cases/how-to-connect-aws-bedrock.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flask import request\nfrom flask_restful import Resource, reqparse\n\nfrom bedrock.knowledge_service import ExternalDatasetService\n\n\nclass BedrockRetrievalApi(Resource):\n    # url : <your-endpoint>/retrieval\n    def post(self):\n        parser = reqparse.RequestParser()\n        parser.add_argument(\"retrieval_setting\", nullable=False, required=True, type=dict, location=\"json\")\n        parser.add_argument(\"query\", nullable=False, required=True, type=str,)\n        parser.add_argument(\"knowledge_id\", nullable=False, required=True, type=str)\n        args = parser.parse_args()\n\n        # Authorization check\n        auth_header = request.headers.get(\"Authorization\")\n        if \" \" not in auth_header:\n            return {\n                \"error_code\": 1001,\n                \"error_msg\": \"Invalid Authorization header format. Expected 'Bearer <api-key>' format.\"\n            }, 403\n        auth_scheme, auth_token = auth_header.split(None, 1)\n        auth_scheme = auth_scheme.lower()\n        if auth_scheme != \"bearer\":\n            return {\n                \"error_code\": 1001,\n                \"error_msg\": \"Invalid Authorization header format. Expected 'Bearer <api-key>' format.\"\n            }, 403\n        if auth_token:\n            # process your authorization logic here\n            pass\n\n        # Call the knowledge retrieval service\n        result = ExternalDatasetService.knowledge_retrieval(\n            args[\"retrieval_setting\"], args[\"query\"], args[\"knowledge_id\"]\n        )\n        return result, 200\n```\n\n----------------------------------------\n\nTITLE: Text Embedding Invocation Interface in Python\nDESCRIPTION: Defines the interface for invoking a text embedding model. Parameters include model name, credentials, texts to embed, and user identifier.  Returns a TextEmbeddingResult entity.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            texts: list[str], user: Optional[str] = None) \\\n        -> TextEmbeddingResult:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param texts: texts to embed\n    :param user: unique user id\n    :return: embeddings result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Unauthorized File Access (Python) - Example\nDESCRIPTION: This Python snippet demonstrates a potential security vulnerability by attempting to read the `/etc/passwd` file.  This example highlights the limitations enforced by the Dify sandbox environment which blocks file system access for security reasons. Attempting this operation will be blocked by Cloudflare WAF.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/code.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef main() -> dict:\n    return {\n        \"result\": open(\"/etc/passwd\").read(),\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring wrangler.toml for Cloudflare Workers (TOML)\nDESCRIPTION: This snippet shows the configuration for `wrangler.toml`, including the application name, compatibility date, and a token.  The token should be a random string and passed via environment variables for security.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\nname = \"dify-extension-example\"\ncompatibility_date = \"2023-01-01\"\n\n[vars]\nTOKEN = \"bananaiscool\"\n```\n\n----------------------------------------\n\nTITLE: Output Moderation Response Example (JSON)\nDESCRIPTION: Illustrates the JSON response structure for the `app.moderation.output` extension, containing `flagged`, `action`, `preset_response`, and `text` (the potentially overridden LLM output if `action` is `overridden`). The `action` can either be `direct_output` to directly present a preset message or `overridden` to replace the original LLM output.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/moderation.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"direct_output\",\n    \"preset_response\": \"Your content violates our usage policy.\"\n  }\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"overridden\",\n    \"text\": \"I will *** you.\"\n  }\n```\n\n----------------------------------------\n\nTITLE: Simple Provider Credential Validation in Python\nDESCRIPTION: This function provides a minimal implementation of provider credential validation for custom model suppliers. It takes a dictionary of credentials as input but does not perform any validation, effectively bypassing the credential check.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass XinferenceProvider(Provider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Define Model Name Credential Schema (YAML)\nDESCRIPTION: This YAML configures a text input field for defining the model name, specifying labels, requirement status, and placeholders for both English and Chinese. The `model_name` variable uses the `text-input` type, enabling users to explicitly define the model when connecting to Xinference.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/customizable-model.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n  - variable: model_name\n    type: text-input\n    label:\n      en_US: Model name\n      zh_Hans: 模型名称\n    required: true\n    placeholder:\n      zh_Hans: 填写模型名称\n      en_US: Input model name\n```\n\n----------------------------------------\n\nTITLE: Add Segments to Document with Dify API\nDESCRIPTION: This snippet demonstrates how to add new segments to a document in a Dify knowledge base using the API. It requires the `dataset_id` and `document_id`, along with a valid API key. The request includes the content, answer, and keywords for each segment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"segments\": [{\"content\": \"1\",\"answer\": \"1\",\"keywords\": [\"a\"]}]}'\n```\n\n----------------------------------------\n\nTITLE: Modifying Knowledge Base Metadata Field with cURL\nDESCRIPTION: This cURL command modifies an existing metadata field in the Dify knowledge base. It requires the dataset ID and the metadata ID. The request body contains the updated metadata field name. It returns the ID, type and updated name of the metadata field.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request PATCH 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata/{metadata_id}' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer {api_key}' \\\n--data '{\n    \"name\":\"test\"\n}'\n```\n\n----------------------------------------\n\nTITLE: PromptMessage Abstract Base Class Definition in Python\nDESCRIPTION: Defines an abstract base class `PromptMessage` as a Pydantic BaseModel representing a generic prompt message. It contains the message role, content (which can be a string or a list of `PromptMessageContent`), and an optional name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessage(ABC, BaseModel):\n    \"\"\"\n    Model class for prompt message.\n    \"\"\"\n    role: PromptMessageRole  # 消息角色\n    content: Optional[str | list[PromptMessageContent]] = None  # 支持两种类型，字符串和内容列表，内容列表是为了满足多模态的需要，可详见 PromptMessageContent 说明。\n    name: Optional[str] = None  # 名称，可选。\n```\n\n----------------------------------------\n\nTITLE: Packaging Dify Plugin\nDESCRIPTION: This snippet demonstrates how to package a Dify plugin using the `dify plugin package` command.  Replace `./neko` with the actual path to your plugin project. This command creates a `neko.difypkg` file, which is the plugin package.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Replace ./neko with your actual plugin project path.\n\ndify plugin package ./neko\n```\n\n----------------------------------------\n\nTITLE: Defining LLM _invoke method\nDESCRIPTION: This Python code snippet represents the core method `_invoke` for invoking the LLM. It takes model, credentials, prompt messages, model parameters, tools, stop words, stream flag, and user ID as input. The method is expected to return either a full LLMResult or a generator yielding stream response chunks based on the `stream` parameter.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/predefined-model.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n                prompt_messages: list[PromptMessage], model_parameters: dict,\n                tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,\n                stream: bool = True, user: Optional[str] = None) \\\n            -> Union[LLMResult, Generator]:\n        \"\"\"\n        Invoke large language model\n\n        :param model: model name\n        :param credentials: model credentials\n        :param prompt_messages: prompt messages\n        :param model_parameters: model parameters\n        :param tools: tools for tool calling\n        :param stop: stop words\n        :param stream: is stream response\n        :param user: unique user id\n        :return: full response or stream response chunk generator result\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Invoke Error Mapping in Dify (Python)\nDESCRIPTION: This property defines a mapping between model invocation errors and unified error types used by Dify's runtime.  The key is the error type thrown to the caller, and the value is a list of error types thrown by the model that should be converted into the unified error type.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Validate Provider Credentials Method\nDESCRIPTION: This Python code shows the required signature for the validate_provider_credentials method of the ModelProvider class.  This is used for validating the credentials for the provider.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Endpoint Interface in Python\nDESCRIPTION: This Python code demonstrates how to implement an Endpoint interface by creating a subclass of `dify_plugin.Endpoint` and implementing the `_invoke` method. The method takes a `werkzeug.Request` object, path parameters (`values`), and Endpoint settings (`settings`) as input and returns a `werkzeug.Response` object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/endpoint.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom typing import Mapping\nfrom werkzeug import Request, Response\nfrom dify_plugin import Endpoint\n\nclass Duck(Endpoint):\n    def _invoke(self, r: Request, values: Mapping, settings: Mapping) -> Response:\n        \"\"\"\n        Invokes the endpoint with the given request.\n        \"\"\"\n        app_id = values[\"app_id\"]\n\n        def generator():\n            yield f\"{app_id} <br>\"\n\n        return Response(generator(), status=200, content_type=\"text/html\")\n```\n\n----------------------------------------\n\nTITLE: Running Llama3.2 with Ollama\nDESCRIPTION: This command downloads and runs the Llama3.2 model using Ollama. It starts an API service on port 11434, accessible via `http://localhost:11434`.  The user needs to have Ollama installed to run this command. Running other models is possible and more information is available at [Ollama Models](https://ollama.com/library).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/ollama.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama run llama3.2\n```\n\n----------------------------------------\n\nTITLE: Defining PromptMessageTool Model (Python)\nDESCRIPTION: Defines the `PromptMessageTool` model, representing a tool that can be used in prompt messages. It includes fields for the tool's `name`, `description`, and `parameters`, which are stored as a dictionary.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageTool(BaseModel):\n    \"\"\"\n    Model class for prompt message tool.\n    \"\"\"\n    name: str\n    description: str\n    parameters: dict\n```\n\n----------------------------------------\n\nTITLE: LLM Usage Class\nDESCRIPTION: Defines the `LLMUsage` class, extending `ModelUsage`, which represents the usage information for a language model. It includes token counts, prices, and latency.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nclass LLMUsage(ModelUsage):\n    \"\"\"\n    Model class for llm usage.\n    \"\"\"\n    prompt_tokens: int  # prompt 使用 tokens\n    prompt_unit_price: Decimal  # prompt 单价\n    prompt_price_unit: Decimal  # prompt 价格单位，即单价基于多少 tokens \n    prompt_price: Decimal  # prompt 费用\n    completion_tokens: int  # 回复使用 tokens\n    completion_unit_price: Decimal  # 回复单价\n    completion_price_unit: Decimal  # 回复价格单位，即单价基于多少 tokens \n    completion_price: Decimal  # 回复费用\n    total_tokens: int  # 总使用 token 数\n    total_price: Decimal  # 总费用\n    currency: str  # 货币单位\n    latency: float  # 请求耗时(s)\n```\n\n----------------------------------------\n\nTITLE: Cloning Stable Diffusion WebUI Repository (Bash)\nDESCRIPTION: This command clones the Stable Diffusion WebUI repository from GitHub. It is the first step in setting up the local environment for Stable Diffusion.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/stable-diffusion.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui\n```\n\n----------------------------------------\n\nTITLE: Tool Vendor Code: google.py\nDESCRIPTION: This Python code defines the GoogleProvider class, which inherits from the ToolProvider class. It implements the _validate_credentials method to validate the tool's credentials. If the validation fails, it raises a ToolProviderCredentialValidationError exception. It relies on `dify_plugin` and the `GoogleSearchTool`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Any\n\nfrom dify_plugin import ToolProvider\nfrom dify_plugin.errors.tool import ToolProviderCredentialValidationError\nfrom tools.google_search import GoogleSearchTool\n\nclass GoogleProvider(ToolProvider):\n    def _validate_credentials(self, credentials: dict[str, Any]) -> None:\n        try:\n            for _ in GoogleSearchTool.from_credentials(credentials).invoke(\n                tool_parameters={\"query\": \"test\", \"result_type\": \"link\"},\n            ):\n                pass\n        except Exception as e:\n            raise ToolProviderCredentialValidationError(str(e))\n```\n\n----------------------------------------\n\nTITLE: Question Classifier Node Endpoint Definition\nDESCRIPTION: This code snippet defines the endpoint for invoking the Question Classifier node. It takes a list of ClassConfig objects, a ModelConfig object, a query string, and an optional instruction string as input, and returns a NodeResponse object. The result is stored in `NodeResponse.outputs['class_name']`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self,\n    classes: list[ClassConfig],\n    model: ModelConfig,\n    query: str,\n    instruction: str = \"\",\n) -> NodeResponse:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Delete Knowledge Base - Dify API (cURL)\nDESCRIPTION: This cURL command deletes a knowledge base from the Dify API. It requires the dataset ID and API key for authentication and authorization.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Running the Dify Plugin\nDESCRIPTION: This command starts the Dify plugin using the Python interpreter.  It assumes that `main.py` is the entry point of the plugin application. It's crucial for both debugging and running the plugin after installation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m main\n```\n\n----------------------------------------\n\nTITLE: Token Counting - Python\nDESCRIPTION: This function gets the number of tokens for the given prompt messages. If the model doesn't provide a token-counting interface, it simply returns 0. Alternatively, the base class's GPT-2 tokenizer can be used, though this is an approximation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(\n    self,\n    model: str,\n    credentials: dict,\n    prompt_messages: list[PromptMessage],\n    tools: Optional[list[PromptMessageTool]] = None\n) -> int:\n    \"\"\"\n    Get the number of tokens for the given prompt messages.\n    \"\"\"\n    return 0\n```\n\n----------------------------------------\n\nTITLE: Configure MCP SSE Plugin with Zapier URL in Dify\nDESCRIPTION: This code snippet shows the JSON configuration required to connect the MCP SSE plugin in Dify to a Zapier MCP Server URL. The `url` field specifies the Zapier endpoint, and other fields configure headers and timeouts.  It's used within the Dify plugin's settings page to establish a connection to Zapier.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/best-practice/how-to-use-mcp-zapier.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"server_name\": {\n    \"url\": \"https://actions.zapier.com/mcp/*******/sse\",\n    \"headers\": {},\n    \"timeout\": 5,\n    \"sse_read_timeout\": 300\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Validating Model Credentials (Python)\nDESCRIPTION: This Python code defines the `validate_credentials` method, which validates the credentials for a specific model. This is similar to provider credential validation but is performed at the model level.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/predefined-model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n    \"\"\"\n    Validate model credentials\n\n    :param model: model name\n    :param credentials: model credentials\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define PromptMessageRole Enum in Python\nDESCRIPTION: This code defines an Enum class `PromptMessageRole` to represent the different roles a message can have in a prompt, such as system, user, assistant, or tool.  This enum is used to categorize messages within a conversational context.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageRole(Enum):\n    \"\"\"\n    Enum class for prompt message.\n    \"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    TOOL = \"tool\"\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Strategy (YAML)\nDESCRIPTION: This YAML snippet defines the Agent strategy code in the `function_calling.yaml` file. It includes identity details such as name, author, and labels. It also defines parameters like `model`, `tools`, `query`, and `max_iterations` with their respective types, scopes, requirements, and labels. Additionally, it specifies the python source file that implements the strategy.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/agent.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: function_calling\n  author: Dify\n  label:\n    en_US: FunctionCalling\n    zh_Hans: FunctionCalling\n    pt_BR: FunctionCalling\ndescription:\n  en_US: Function Calling is a basic strategy for agent, model will use the tools provided to perform the task.\nparameters:\n  - name: model\n    type: model-selector\n    scope: tool-call&llm\n    required: true\n    label:\n      en_US: Model\n  - name: tools\n    type: array[tools]\n    required: true\n    label:\n      en_US: Tools list\n  - name: query\n    type: string\n    required: true\n    label:\n      en_US: Query\n  - name: max_iterations\n    type: number\n    required: false\n    default: 5\n    label:\n      en_US: Max Iterations\n    max: 50\n    min: 1\nextra:\n  python:\n    source: strategies/function_calling.py\n```\n\n----------------------------------------\n\nTITLE: Running DeepSeek R1 Model in Ollama (Bash)\nDESCRIPTION: This command instructs Ollama to download and run the DeepSeek R1 7B model. It's a crucial step to make the DeepSeek model available for integration with Dify. The command assumes Ollama is correctly installed and configured. This model will be used in the subsequent Dify setup.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nollama run deepseek-r1:7b\n```\n\n----------------------------------------\n\nTITLE: LLM Tool Invocation with Model Selector Python\nDESCRIPTION: This Python code shows how to invoke an LLM using the model selected through the UI. It retrieves the `model` parameter directly from the `tool_parameters` and passes it as the `model_config` to the `invoke` method. The rest of the code functions similarly to the previous LLM tool invocation, utilizing `SystemPromptMessage` and `UserPromptMessage` to format the prompt.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom typing import Any\n\nfrom dify_plugin import Tool\nfrom dify_plugin.entities.model.llm import LLMModelConfig\nfrom dify_plugin.entities.tool import ToolInvokeMessage\nfrom dify_plugin.entities.model.message import SystemPromptMessage, UserPromptMessage\n\nclass LLMTool(Tool):\n    def _invoke(self, tool_parameters: dict[str, Any]) -> Generator[ToolInvokeMessage]:\n        response = self.session.model.llm.invoke(\n            model_config=tool_parameters.get('model'),\n            prompt_messages=[\n                SystemPromptMessage(\n                    content='you are a helpful assistant'\n                ),\n                UserPromptMessage(\n                    content=tool_parameters.get('query')\n                )\n            ],\n            stream=True\n        )\n\n        for chunk in response:\n            if chunk.delta.message:\n                assert isinstance(chunk.delta.message.content, str)\n                yield self.create_text_message(text=chunk.delta.message.content)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry (Bash)\nDESCRIPTION: These commands use Poetry to set the Python environment to 3.12 and install the required dependencies for the Dify API service. Poetry manages project dependencies and ensures a consistent environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\npoetry env use 3.12\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Define Rerank Document Model (Python)\nDESCRIPTION: Defines a Pydantic model `RerankDocument` representing a document that was reranked. It includes attributes for the `index` of the document in the original list, the document `text`, and the reranking `score`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nclass RerankDocument(BaseModel):\n    \"\"\"\n    Model class for rerank document.\n    \"\"\"\n    index: int  # original index\n    text: str\n    score: float\n```\n\n----------------------------------------\n\nTITLE: LLM Result Class\nDESCRIPTION: Defines the `LLMResult` class, representing the result of a language model invocation. It contains the model used, prompt messages, the resulting message, usage information, and a system fingerprint.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResult(BaseModel):\n    \"\"\"\n    Model class for llm result.\n    \"\"\"\n    model: str  # 实际使用模型\n    prompt_messages: list[PromptMessage]  # prompt 消息列表\n    message: AssistantPromptMessage  # 回复消息\n    usage: LLMUsage  # 使用的 tokens 及费用信息\n    system_fingerprint: Optional[str] = None  # 请求指纹，可参考 OpenAI 该参数定义\n```\n\n----------------------------------------\n\nTITLE: Adding a Package plugin dependency to the Bundle\nDESCRIPTION: This command appends a Package plugin dependency to the Bundle project. The `--package_path` parameter specifies the path to the local plugin package file.  This command assumes that `dify-plugin` is in the current working directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/bundle.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndify-plugin bundle append package . --package_path=./openai.difypkg\n```\n\n----------------------------------------\n\nTITLE: Xinference Vendor YAML Configuration\nDESCRIPTION: Defines the vendor's metadata, supported model types, and credential schema for Xinference integration. This configuration is used by the Dify runtime to understand the vendor's capabilities and how to interact with it. The `configurate_methods` specifies if the vendor supports predefined or customizable models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/customizable-model.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprovider: xinference # Specify vendor identifier\nlabel: # Vendor display name, can be set in en_US (English) and zh_Hans (Simplified Chinese). If zh_Hans is not set, en_US will be used by default.\n  en_US: Xorbits Inference\nicon_small: # Small icon, refer to other vendors' icons, stored in the _assets directory under the corresponding vendor implementation directory. Language strategy is the same as label.\n  en_US: icon_s_en.svg\nicon_large: # Large icon\n  en_US: icon_l_en.svg\nhelp: # Help\n  title:\n    en_US: How to deploy Xinference\n    zh_Hans: 如何部署 Xinference\n  url:\n    en_US: https://github.com/xorbitsai/inference\nsupported_model_types: # Supported model types. Xinference supports LLM/Text Embedding/Rerank\n- llm\n- text-embedding\n- rerank\nconfigurate_methods: # Since Xinference is a locally deployed vendor and does not have predefined models, you need to deploy the required models according to Xinference's documentation. Therefore, only custom models are supported here.\n- customizable-model\nprovider_credential_schema:\n  credential_form_schemas:\n```\n\n----------------------------------------\n\nTITLE: Agent Strategy: Model Invocation with Tool Handling (Python)\nDESCRIPTION: This code defines an agent strategy (`BasicAgentAgentStrategy`) that invokes a language model, checks for tool calls in the LLM's response, extracts and executes those tools, and returns a formatted response.  It requires `dify_plugin` entities for agent, model, and tool interaction, as well as `pydantic` for data validation. The strategy uses a generator to yield `AgentInvokeMessage` instances, including text messages and tool invocation results.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom collections.abc import Generator\nfrom typing import Any, cast\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.model.llm import LLMModelConfig, LLMResult, LLMResultChunk\nfrom dify_plugin.entities.model.message import (\n    PromptMessageTool,\n    UserPromptMessage,\n)\nfrom dify_plugin.entities.tool import ToolInvokeMessage, ToolParameter, ToolProviderType\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n        chunks: Generator[LLMResultChunk, None, None] | LLMResult = (\n            self.session.model.llm.invoke(\n                model_config=LLMModelConfig(**params.model.model_dump(mode=\"json\")),\n                prompt_messages=[UserPromptMessage(content=params.query)],\n                tools=[\n                    self._convert_tool_to_prompt_message_tool(tool)\n                    for tool in params.tools\n                ],\n                stop=params.model.completion_params.get(\"stop\", [])\n                if params.model.completion_params\n                else [],\n                stream=True,\n            )\n        )\n        response = \"\"\n        tool_calls = []\n        tool_instances = (\n            {tool.identity.name: tool for tool in params.tools} if params.tools else {}\n        )\n\n        for chunk in chunks:\n            # ツール呼び出しがあるか確認\n            if self.check_tool_calls(chunk):\n                tool_calls = self.extract_tool_calls(chunk)\n                tool_call_names = \";\".join([tool_call[1] for tool_call in tool_calls])\n                try:\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls},\n                        ensure_ascii=False,\n                    )\n                except json.JSONDecodeError:\n                    # エンコードエラーを避けるため、asciiを保証\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls}\n                    )\n                print(tool_call_names, tool_call_inputs)\n            if chunk.delta.message and chunk.delta.message.content:\n                if isinstance(chunk.delta.message.content, list):\n                    for content in chunk.delta.message.content:\n                        response += content.data\n                        print(content.data, end=\"\", flush=True)\n                else:\n                    response += str(chunk.delta.message.content)\n                    print(str(chunk.delta.message.content), end=\"\", flush=True)\n\n            if chunk.delta.usage:\n                # モデルを使用する\n                usage = chunk.delta.usage\n\n        yield self.create_text_message(\n            text=f\"{response or json.dumps(tool_calls, ensure_ascii=False)}\\n\"\n        )\n        result = \"\"\n        for tool_call_id, tool_call_name, tool_call_args in tool_calls:\n            tool_instance = tool_instances[tool_call_name]\n            tool_invoke_responses = self.session.tool.invoke(\n                provider_type=ToolProviderType.BUILT_IN,\n                provider=tool_instance.identity.provider,\n                tool_name=tool_instance.identity.name,\n                parameters={**tool_instance.runtime_parameters, **tool_call_args},\n            )\n            if not tool_instance:\n                tool_invoke_responses = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": f\"there is not a tool named {tool_call_name}\",\n                }\n            else:\n                # ツールを呼び出す\n                tool_invoke_responses = self.session.tool.invoke(\n                    provider_type=ToolProviderType.BUILT_IN,\n                    provider=tool_instance.identity.provider,\n                    tool_name=tool_instance.identity.name,\n                    parameters={**tool_instance.runtime_parameters, **tool_call_args},\n                )\n                result = \"\"\n                for tool_invoke_response in tool_invoke_responses:\n                    if tool_invoke_response.type == ToolInvokeMessage.MessageType.TEXT:\n                        result += cast(\n                            ToolInvokeMessage.TextMessage, tool_invoke_response.message\n                        ).text\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.LINK\n                    ):\n                        result += (\n                            f\"result link: {cast(ToolInvokeMessage.TextMessage, tool_invoke_response.message).text}.\"\n                            + \" please tell user to check it.\"\n                        )\n                    elif tool_invoke_response.type in {\n                        ToolInvokeMessage.MessageType.IMAGE_LINK,\n                        ToolInvokeMessage.MessageType.IMAGE,\n                    }:\n                        result += (\n                            \"image has been created and sent to user already, \"\n                            + \"you do not need to create it, just tell the user to check it now.\"\n                        )\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.JSON\n                    ):\n                        text = json.dumps(\n                            cast(\n                                ToolInvokeMessage.JsonMessage,\n                                tool_invoke_response.message,\n                            ).json_object,\n                            ensure_ascii=False,\n                        )\n                        result += f\"tool response: {text}.\"\n                    else:\n                        result += f\"tool response: {tool_invoke_response.message!r}.\"\n\n                tool_response = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": result,\n                }\n        yield self.create_text_message(result)\n\n    def _convert_tool_to_prompt_message_tool(\n        self, tool: ToolEntity\n    ) -> PromptMessageTool:\n        \"\"\"\n        convert tool to prompt message tool\n        \"\"\"\n        message_tool = PromptMessageTool(\n            name=tool.identity.name,\n            description=tool.description.llm if tool.description else \"\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        )\n\n        parameters = tool.parameters\n        for parameter in parameters:\n            if parameter.form != ToolParameter.ToolParameterForm.LLM:\n                continue\n\n            parameter_type = parameter.type\n            if parameter.type in {\n                ToolParameter.ToolParameterType.FILE,\n                ToolParameter.ToolParameterType.FILES,\n            }:\n                continue\n            enum = []\n            if parameter.type == ToolParameter.ToolParameterType.SELECT:\n                enum = (\n                    [option.value for option in parameter.options]\n                    if parameter.options\n                    else []\n                )\n\n            message_tool.parameters[\"properties\"][parameter.name] = {\n                \"type\": parameter_type,\n                \"description\": parameter.llm_description or \"\",\n            }\n\n            if len(enum) > 0:\n                message_tool.parameters[\"properties\"][parameter.name][\"enum\"] = enum\n\n            if parameter.required:\n                message_tool.parameters[\"required\"].append(parameter.name)\n\n        return message_tool\n\n    def check_tool_calls(self, llm_result_chunk: LLMResultChunk) -> bool:\n        \"\"\"\n        Check if there is any tool call in llm result chunk\n        \"\"\"\n        return bool(llm_result_chunk.delta.message.tool_calls)\n\n    def extract_tool_calls(\n        self, llm_result_chunk: LLMResultChunk\n    ) -> list[tuple[str, str, dict[str, Any]]]:\n        \"\"\"\n        Extract tool calls from llm result chunk\n\n        Returns:\n            List[Tuple[str, str, Dict[str, Any]]]: [(tool_call_id, tool_call_name, tool_call_args)]\n        \"\"\"\n        tool_calls = []\n        for prompt_message in llm_result_chunk.delta.message.tool_calls:\n            args = {}\n            if prompt_message.function.arguments != \"\":\n                args = json.loads(prompt_message.function.arguments)\n\n            tool_calls.append(\n                (\n                    prompt_message.id,\n                    prompt_message.function.name,\n                    args,\n                )\n            )\n\n        return tool_calls\n```\n\n----------------------------------------\n\nTITLE: Invoking Text Embedding Model in Python\nDESCRIPTION: This method implements text embedding invocation. It takes model name, credentials, a list of texts, and an optional user identifier as input. It returns a TextEmbeddingResult object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              texts: list[str], user: Optional[str] = None) \\\n          -> TextEmbeddingResult:\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param texts: texts to embed\n      :param user: unique user id\n      :return: embeddings result\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Packaging a Dify Plugin Project\nDESCRIPTION: This command navigates to the parent directory of the plugin project and uses the `dify plugin package` command to package the plugin project into a `.difypkg` file. The resulting file can then be shared and installed in a Dify Workspace.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/package-plugin-file-and-publish.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd ../\ndify plugin package ./your_plugin_project\n```\n\n----------------------------------------\n\nTITLE: Invoke Custom Tool in Dify Plugin (Python)\nDESCRIPTION: Defines the interface for invoking a custom tool within a Dify plugin. The `invoke_api_tool` method allows plugins to use custom API based tools.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/tool.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef invoke_api_tool(\n    self, provider: str, tool_name: str, parameters: dict[str, Any]\n) -> Generator[ToolInvokeMessage, None, None]:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Dynamically Generate Model Parameter Schema (Python)\nDESCRIPTION: This Python code dynamically generates a model parameter schema based on the model being used. It creates ParameterRule objects for `temperature`, `top_p`, and `max_tokens`, and conditionally adds `top_k` if the model is 'A'. It returns an AIModelEntity with the defined parameter rules, enabling customized parameter configuration for each model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/customizable-model.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> AIModelEntity | None:\n        \"\"\"\n            used to define customizable model schema\n        \"\"\"\n        rules = [\n            ParameterRule(\n                name='temperature', type=ParameterType.FLOAT,\n                use_template='temperature',\n                label=I18nObject(\n                    zh_Hans='温度', en_US='Temperature'\n                )\n            ),\n            ParameterRule(\n                name='top_p', type=ParameterType.FLOAT,\n                use_template='top_p',\n                label=I18nObject(\n                    zh_Hans='Top P', en_US='Top P'\n                )\n            ),\n            ParameterRule(\n                name='max_tokens', type=ParameterType.INT,\n                use_template='max_tokens',\n                min=1,\n                default=512,\n                label=I18nObject(\n                    zh_Hans='最大生成长度', en_US='Max Tokens'\n                )\n            )\n        ]\n\n        # if model is A, add top_k to rules\n        if model == 'A':\n            rules.append(\n                ParameterRule(\n                    name='top_k', type=ParameterType.INT,\n                    use_template='top_k',\n                    min=1,\n                    default=50,\n                    label=I18nObject(\n                        zh_Hans='Top K', en_US='Top K'\n                    )\n                )\n            )\n\n        \"\"\"\n            some NOT IMPORTANT code here\n        \"\"\"\n\n        entity = AIModelEntity(\n            model=model,\n            label=I18nObject(\n                en_US=model\n            ),\n            fetch_from=FetchFrom.CUSTOMIZABLE_MODEL,\n            model_type=model_type,\n            model_properties={ \n                ModelPropertyKey.MODE:  ModelType.LLM,\n            },\n            parameter_rules=rules\n        )\n\n        return entity\n```\n\n----------------------------------------\n\nTITLE: Invoking Large Language Model (LLM) in Python\nDESCRIPTION: This Python code defines the `_invoke` method for invoking a large language model. It supports both streaming and synchronous responses. It takes parameters such as model name, credentials, prompt messages, model parameters, tools, stop words, and user identifier.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            prompt_messages: list[PromptMessage], model_parameters: dict,\n            tools: Optional[list[PromptMessageTool]] = None, stop: Optional[list[str]] = None,\n            stream: bool = True, user: Optional[str] = None) \\\n        -> Union[LLMResult, Generator]:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param model_parameters: model parameters\n    :param tools: tools for tool calling\n    :param stop: stop words\n    :param stream: is stream response\n    :param user: unique user id\n    :return: full response or stream response chunk generator result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Precompute Input Tokens - Python\nDESCRIPTION: This snippet demonstrates how to implement the `get_num_tokens` method, which calculates the number of tokens in the input prompt. If the model doesn't offer a built-in tokenization interface, it can return 0 or use `self._get_num_tokens_by_gpt2(text: str)` for an approximate calculation using GPT2's tokenizer. The method takes model details, credentials, prompt messages, and tools as input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/customizable-model.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                     tools: Optional[list[PromptMessageTool]] = None) -> int:\n      \"\"\"\n      Get number of tokens for given prompt messages\n\n      :param model: model name\n      :param credentials: model credentials\n      :param prompt_messages: prompt messages\n      :param tools: tools for tool calling\n      :return:\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking a Rerank Model (Python)\nDESCRIPTION: This code snippet demonstrates how to invoke a Rerank model in Dify. It inherits from the `__base.rerank_model.RerankModel` base class. It takes a query and a list of documents, and returns a `RerankResult` entity with the re-ranked documents.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              query: str, docs: list[str], score_threshold: Optional[float] = None, top_n: Optional[int] = None,\n              user: Optional[str] = None) \\\n          -> RerankResult:\n    \"\"\"\n    Invoke rerank model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param query: search query\n    :param docs: docs for reranking\n    :param score_threshold: score threshold\n    :param top_n: top n\n    :param user: unique user id\n    :return: rerank result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Docker Compose YAML Configuration\nDESCRIPTION: This YAML configuration defines the services for SearXNG, Redis, and Caddy (reverse proxy). It configures port mappings, volumes, and networking for each service to enable SearXNG to function correctly with Redis for caching and Caddy for handling HTTP/HTTPS traffic.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3'\n\nservices:\n  searxng:\n    image: searxng/searxng:latest\n    ports:\n      - \"8081:8080\"  # 将容器的 8080 端口映射到宿主机的 8081 端口\n    volumes:\n      - ./searxng:/etc/searxng  # 配置 SearXNG 配置文件的挂载\n    networks:\n      - searxng_network\n\n  redis:\n    image: valkey/valkey:8-alpine\n    ports:\n      - \"6379:6379\"  # Redis 服务映射端口\n    networks:\n      - searxng_network\n\n  caddy:\n    image: caddy:2-alpine\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    networks:\n      - searxng_network\n\nnetworks:\n  searxng_network:\n    driver: bridge\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Example (OpenAI)\nDESCRIPTION: Demonstrates the directory structure for a model provider (OpenAI) offering multiple model types (llm, text_embedding, moderation, speech2text, tts). Each model type has its own subdirectory under `/models`, promoting modularity and maintainability. Includes a `common_openai.py` for shared functionalities.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/predefined-model.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n├── models\n│   ├── common_openai.py\n│   ├── llm\n│   │   ├── _position.yaml\n│   │   ├── chatgpt-4o-latest.yaml\n│   │   ├── gpt-3.5-turbo.yaml\n│   │   ├── gpt-4-0125-preview.yaml\n│   │   ├── gpt-4-turbo.yaml\n│   │   ├── gpt-4o.yaml\n│   │   ├── llm.py\n│   │   ├── o1-preview.yaml\n│   │   └── text-davinci-003.yaml\n│   ├── moderation\n│   │   ├── moderation.py\n│   │   └── text-moderation-stable.yaml\n│   ├── speech2text\n│   │   ├── speech2text.py\n│   │   └── whisper-1.yaml\n│   ├── text_embedding\n│   │   ├── text-embedding-3-large.yaml\n│   │   └── text_embedding.py\n│   └── tts\n│       ├── tts-1-hd.yaml\n│       ├── tts-1.yaml\n│       └── tts.py\n```\n\n----------------------------------------\n\nTITLE: Parameter Validation with Zod (TypeScript)\nDESCRIPTION: This TypeScript snippet demonstrates how to use `zod` for parameter validation.  It defines a schema for validating the `point` and `params` parameters. The `point` can be either 'ping' or 'app.external_data_tool.query'. The params object include app_id, tool_variable, inputs, and query.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { zValidator } from \"@hono/zod-validator\";\n\nconst schema = z.object({\n  point: z.union([\n    z.literal(\"ping\"),\n    z.literal(\"app.external_data_tool.query\"),\n  ]), // Restricts 'point' to two specific values\n  params: z\n    .object({\n      app_id: z.string().optional(),\n      tool_variable: z.string().optional(),\n      inputs: z.record(z.any()).optional(),\n      query: z.any().optional(),  // string or null\n    })\n    .optional(),\n});\n```\n\n----------------------------------------\n\nTITLE: Handling Slack Events (endpoints/slack.py)\nDESCRIPTION: This Python code defines the logic for handling Slack events and interacting with the Dify App.  It receives Slack events, verifies the URL challenge, extracts the message from app mentions, invokes the Dify App using `self.session.app.chat.invoke`, and sends the response back to Slack.  It uses the `slack_sdk` library to interact with the Slack API and requires the Werkzeug Request and Response classes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport traceback\nfrom typing import Mapping\nfrom werkzeug import Request, Response\nfrom dify_plugin import Endpoint\nfrom slack_sdk import WebClient\nfrom slack_sdk.errors import SlackApiError\n\n\nclass SlackEndpoint(Endpoint):\n    def _invoke(self, r: Request, values: Mapping, settings: Mapping) -> Response:\n        \"\"\"\n        Invokes the endpoint with the given request.\n        \"\"\"\n        retry_num = r.headers.get(\"X-Slack-Retry-Num\")\n        if (not settings.get(\"allow_retry\") and (r.headers.get(\"X-Slack-Retry-Reason\") == \"http_timeout\" or ((retry_num is not None and int(retry_num) > 0)))): \n            return Response(status=200, response=\"ok\")\n        data = r.get_json()\n\n        # Slack URL検証チャレンジを処理する\n        if data.get(\"type\") == \"url_verification\":\n            return Response(\n                response=json.dumps({\"challenge\": data.get(\"challenge\")}),\n                status=200,\n                content_type=\"application/json\"\n            )\n        \n        if (data.get(\"type\") == \"event_callback\"):\n            event = data.get(\"event\")\n            if (event.get(\"type\") == \"app_mention\"):\n                message = event.get(\"text\", \"\")\n                if message.startswith(\"<@\"):\n                    message = message.split(\"> \", 1)[1] if \"> \" in message else message\n                    channel = event.get(\"channel\", \"\")\n                    blocks = event.get(\"blocks\", [])\n                    blocks[0][\"elements\"][0][\"elements\"] = blocks[0].get(\"elements\")[0].get(\"elements\")[1:]\n                    token = settings.get(\"bot_token\")\n                    client = WebClient(token=token)\n                    try: \n                        response = self.session.app.chat.invoke(\n                            app_id=settings[\"app\"][\"app_id\"],\n                            query=message,\n                            inputs={},\n                            response_mode=\"blocking\",\n                        )\n                        try:\n                            blocks[0][\"elements\"][0][\"elements\"][0][\"text\"] = response.get(\"answer\")\n                            result = client.chat_postMessage(\n                                channel=channel,\n                                text=response.get(\"answer\"),\n                                blocks=blocks\n                            )\n                            return Response(\n                                status=200,\n                                response=json.dumps(result),\n                                content_type=\"application/json\"\n                            )\n                        except SlackApiError as e:\n                            raise e\n                    except Exception as e:\n                        err = traceback.format_exc()\n                        return Response(\n                            status=200,\n                            response=\"Sorry, I'm having trouble processing your request. Please try again later.\" + str(err),\n                            content_type=\"text/plain\",\n                        )\n                else:\n                    return Response(status=200, response=\"ok\")\n            else:\n                return Response(status=200, response=\"ok\")\n        else:\n            return Response(status=200, response=\"ok\")\n```\n\n----------------------------------------\n\nTITLE: Cloning the Dify Repository using Git\nDESCRIPTION: This command clones the forked Dify repository from GitHub to your local machine. Replace <github_username> with your GitHub username.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/community/contribution.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone git@github.com:<github_username>/dify.git\n```\n\n----------------------------------------\n\nTITLE: Moderation Model Invocation Interface in Python\nDESCRIPTION: This code snippet defines the `_invoke` method for a moderation model. It takes text as input and returns a boolean indicating whether the text is safe (False) or not (True). Parameters include model name, credentials, the text to moderate, and optionally a user ID.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              text: str, user: Optional[str] = None) \\\n          -> bool:\n      \"\"\"\n      Invoke large language model\n  \n      :param model: model name\n      :param credentials: model credentials\n      :param text: text to moderate\n      :param user: unique user id\n      :return: false if text is safe, true otherwise\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Pre-calculating Tokens for Embedding in Python\nDESCRIPTION: This Python code snippet outlines the `get_num_tokens` method, which calculates the number of tokens for the given texts to embed. It takes model details, credentials, and a list of texts as input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, texts: list[str]) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param texts: texts to embed\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Moderation Invoke Endpoint (Python)\nDESCRIPTION: This code snippet presents the invoke method definition for content moderation. It takes a `ModerationModelConfig` and a text and returns a boolean value; true indicates the presence of sensitive content.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(self, model_config: ModerationModelConfig, text: str) -> bool:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Parsing Tool Invoke Response in Python\nDESCRIPTION: This function parses the response from a tool invocation, which is a generator of `AgentInvokeMessage` objects. It iterates through the responses and extracts the content based on the message type. It handles text, links, image links, images, and JSON responses, formatting the output accordingly.  Proper casting to the specific message types is shown, like `ToolInvokeMessage.TextMessage` and `ToolInvokeMessage.JsonMessage`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/agent.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom collections.abc import Generator\nfrom typing import cast\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.tool import ToolInvokeMessage\n\ndef parse_invoke_response(tool_invoke_responses: Generator[AgentInvokeMessage]) -> str:\n    result = \"\"\n    for response in tool_invoke_responses:\n        if response.type == ToolInvokeMessage.MessageType.TEXT:\n            result += cast(ToolInvokeMessage.TextMessage, response.message).text\n        elif response.type == ToolInvokeMessage.MessageType.LINK:\n            result += (\n                f\"result link: {cast(ToolInvokeMessage.TextMessage, response.message).text}.\"\n                + \" please tell user to check it.\"\n            )\n        elif response.type in {\n            ToolInvokeMessage.MessageType.IMAGE_LINK,\n            ToolInvokeMessage.MessageType.IMAGE,\n        }:\n            result += (\n                \"image has been created and sent to user already, \"\n                + \"you do not need to create it, just tell the user to check it now.\"\n            )\n        elif response.type == ToolInvokeMessage.MessageType.JSON:\n            text = json.dumps(cast(ToolInvokeMessage.JsonMessage, response.message).json_object, ensure_ascii=False)\n            result += f\"tool response: {text}.\"\n        else:\n            result += f\"tool response: {response.message!r}.\"\n    return result\n```\n\n----------------------------------------\n\nTITLE: Agent Strategy Model Invocation\nDESCRIPTION: This code snippet demonstrates how to implement an agent strategy plugin's model invocation, sending normalized requests to tools.  It defines the `BasicAgentAgentStrategy` class that extends `AgentStrategy`, including methods for invoking language models (`_invoke`), converting tools to prompt messages (`_convert_tool_to_prompt_message_tool`), and processing tool calls (`check_tool_calls`, `extract_tool_calls`). The `_invoke` method takes agent parameters, constructs prompt messages, invokes the LLM, and handles the responses, including any tool calls. The code utilizes `pydantic` for data validation and `dify_plugin` entities for structured data handling.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom collections.abc import Generator\nfrom typing import Any, cast\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.model.llm import LLMModelConfig, LLMResult, LLMResultChunk\nfrom dify_plugin.entities.model.message import (\n    PromptMessageTool,\n    UserPromptMessage,\n)\nfrom dify_plugin.entities.tool import ToolInvokeMessage, ToolParameter, ToolProviderType\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n        chunks: Generator[LLMResultChunk, None, None] | LLMResult = (\n            self.session.model.llm.invoke(\n                model_config=LLMModelConfig(**params.model.model_dump(mode=\"json\")),\n                prompt_messages=[UserPromptMessage(content=params.query)],\n                tools=[\n                    self._convert_tool_to_prompt_message_tool(tool)\n                    for tool in params.tools\n                ],\n                stop=params.model.completion_params.get(\"stop\", [])\n                if params.model.completion_params\n                else [],\n                stream=True,\n            )\n        )\n        response = \"\"\n        tool_calls = []\n        tool_instances = (\n            {tool.identity.name: tool for tool in params.tools} if params.tools else {}\n        )\n\n        for chunk in chunks:\n            # ツール呼び出しがあるか確認\n            if self.check_tool_calls(chunk):\n                tool_calls = self.extract_tool_calls(chunk)\n                tool_call_names = \";\".join([tool_call[1] for tool_call in tool_calls])\n                try:\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls},\n                        ensure_ascii=False,\n                    )\n                except json.JSONDecodeError:\n                    # エンコードエラーを避けるため、asciiを保証\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls}\n                    )\n                print(tool_call_names, tool_call_inputs)\n            if chunk.delta.message and chunk.delta.message.content:\n                if isinstance(chunk.delta.message.content, list):\n                    for content in chunk.delta.message.content:\n                        response += content.data\n                        print(content.data, end=\"\", flush=True)\n                else:\n                    response += str(chunk.delta.message.content)\n                    print(str(chunk.delta.message.content), end=\"\", flush=True)\n\n            if chunk.delta.usage:\n                # モデルを使用する\n                usage = chunk.delta.usage\n\n        yield self.create_text_message(\n            text=f\"{response or json.dumps(tool_calls, ensure_ascii=False)}\\n\"\n        )\n        result = \"\"\n        for tool_call_id, tool_call_name, tool_call_args in tool_calls:\n            tool_instance = tool_instances[tool_call_name]\n            tool_invoke_responses = self.session.tool.invoke(\n                provider_type=ToolProviderType.BUILT_IN,\n                provider=tool_instance.identity.provider,\n                tool_name=tool_instance.identity.name,\n                parameters={**tool_instance.runtime_parameters, **tool_call_args},\n            )\n            if not tool_instance:\n                tool_invoke_responses = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": f\"there is not a tool named {tool_call_name}\",\n                }\n            else:\n                # ツールを呼び出す\n                tool_invoke_responses = self.session.tool.invoke(\n                    provider_type=ToolProviderType.BUILT_IN,\n                    provider=tool_instance.identity.provider,\n                    tool_name=tool_instance.identity.name,\n                    parameters={**tool_instance.runtime_parameters, **tool_call_args},\n                )\n                result = \"\"\n                for tool_invoke_response in tool_invoke_responses:\n                    if tool_invoke_response.type == ToolInvokeMessage.MessageType.TEXT:\n                        result += cast(\n                            ToolInvokeMessage.TextMessage, tool_invoke_response.message\n                        ).text\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.LINK\n                    ):\n                        result += (\n                            f\"result link: {cast(ToolInvokeMessage.TextMessage, tool_invoke_response.message).text}.\"\n                            + \" please tell user to check it.\"\n                        )\n                    elif tool_invoke_response.type in {\n                        ToolInvokeMessage.MessageType.IMAGE_LINK,\n                        ToolInvokeMessage.MessageType.IMAGE,\n                    }:\n                        result += (\n                            \"image has been created and sent to user already, \"\n                            + \"you do not need to create it, just tell the user to check it now.\"\n                        )\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.JSON\n                    ):\n                        text = json.dumps(\n                            cast(\n                                ToolInvokeMessage.JsonMessage,\n                                tool_invoke_response.message,\n                            ).json_object,\n                            ensure_ascii=False,\n                        )\n                        result += f\"tool response: {text}.\"\n                    else:\n                        result += f\"tool response: {tool_invoke_response.message!r}.\"\n\n                tool_response = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": result,\n                }\n        yield self.create_text_message(result)\n\n    def _convert_tool_to_prompt_message_tool(\n        self, tool: ToolEntity\n    ) -> PromptMessageTool:\n        \"\"\"\n        convert tool to prompt message tool\n        \"\"\"\n        message_tool = PromptMessageTool(\n            name=tool.identity.name,\n            description=tool.description.llm if tool.description else \"\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        )\n\n        parameters = tool.parameters\n        for parameter in parameters:\n            if parameter.form != ToolParameter.ToolParameterForm.LLM:\n                continue\n\n            parameter_type = parameter.type\n            if parameter.type in {\n                ToolParameter.ToolParameterType.FILE,\n                ToolParameter.ToolParameterType.FILES,\n            }:\n                continue\n            enum = []\n            if parameter.type == ToolParameter.ToolParameterType.SELECT:\n                enum = (\n                    [option.value for option in parameter.options]\n                    if parameter.options\n                    else []\n                )\n\n            message_tool.parameters[\"properties\"][parameter.name] = {\n                \"type\": parameter_type,\n                \"description\": parameter.llm_description or \"\",\n            }\n\n            if len(enum) > 0:\n                message_tool.parameters[\"properties\"][parameter.name][\"enum\"] = enum\n\n            if parameter.required:\n                message_tool.parameters[\"required\"].append(parameter.name)\n\n        return message_tool\n\n    def check_tool_calls(self, llm_result_chunk: LLMResultChunk) -> bool:\n        \"\"\"\n        Check if there is any tool call in llm result chunk\n        \"\"\"\n        return bool(llm_result_chunk.delta.message.tool_calls)\n\n    def extract_tool_calls(\n        self, llm_result_chunk: LLMResultChunk\n    ) -> list[tuple[str, str, dict[str, Any]]]:\n        \"\"\"\n        Extract tool calls from llm result chunk\n\n        Returns:\n            List[Tuple[str, str, Dict[str, Any]]]: [(tool_call_id, tool_call_name, tool_call_args)]\n        \"\"\"\n        tool_calls = []\n        for prompt_message in llm_result_chunk.delta.message.tool_calls:\n            args = {}\n            if prompt_message.function.arguments != \"\":\n                args = json.loads(prompt_message.function.arguments)\n\n            tool_calls.append(\n                (\n                    prompt_message.id,\n                    prompt_message.function.name,\n                    args,\n                )\n            )\n\n        return tool_calls\n```\n\n----------------------------------------\n\nTITLE: Cloning SearXNG Docker Repository\nDESCRIPTION: This command clones the SearXNG Docker repository from GitHub to the Linux VM and navigates into the cloned directory. This provides the necessary files for setting up SearXNG using Docker.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/searxng/searxng-docker.git\ncd searxng-docker\n```\n\n----------------------------------------\n\nTITLE: Content Moderation Input Request Body Example\nDESCRIPTION: Illustrates the structure of the HTTP POST request body sent to the `app.moderation.input` extension point when reviewing user input content. It includes the extension point type, application ID, user-provided variable values, and the user's dialogue input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"point\": \"app.moderation.input\",\n    \"params\": {\n        \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n        \"inputs\": {\n            \"var_1\": \"I will kill you.\",\n            \"var_2\": \"I will fuck you.\"\n        },\n        \"query\": \"Happy everydays.\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Summary Invoke Endpoint Definition Python\nDESCRIPTION: This snippet defines the `invoke` endpoint for the summary functionality. It takes `text` to summarize and an `instruction` string for additional instructions or customization.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self, text: str, instruction: str,\n) -> str:\n```\n\n----------------------------------------\n\nTITLE: Validating Model Credentials in Python\nDESCRIPTION: This method validates the credentials for a given model. It takes the model name and credentials dictionary as input. If validation fails, it raises a CredentialsValidateFailedError. The credential information parameters are defined in the provider's YAML configuration file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n      \"\"\"\n      Validate model credentials\n\n      :param model: model name\n      :param credentials: model credentials\n      :return:\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implement Text-to-Speech Model Invocation in Python\nDESCRIPTION: This code snippet shows the implementation of the `_invoke` method for a text-to-speech model, inheriting from the `Text2SpeechModel` base class. It defines the parameters: `model` (model name), `credentials` (model credentials), `content_text` (text to convert), `streaming` (boolean to indicate streaming output), and `user` (unique user id). The method returns a translated audio file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, content_text: str, streaming: bool, user: Optional[str] = None):\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param content_text: text content to be translated\n    :param streaming: output is streaming\n    :param user: unique user id\n    :return: translated audio file\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining TextEmbeddingResult Data Model in Python\nDESCRIPTION: Defines a `TextEmbeddingResult` class that inherits from `BaseModel`, representing the result of a text embedding operation. It includes the model used, a list of embedding vectors (list of floats), and usage information via the `EmbeddingUsage` model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nclass TextEmbeddingResult(BaseModel):\n    \"\"\"\n    Model class for text embedding result.\n    \"\"\"\n    model: str  # Actual model used\n    embeddings: list[list[float]]  # List of embedding vectors, corresponding to the input texts list\n    usage: EmbeddingUsage  # Usage information\n```\n\n----------------------------------------\n\nTITLE: LLM Prompt for Structure Extraction in Dify\nDESCRIPTION: This prompt is designed for an LLM node in Dify to extract the structure of an article, detailing the content of each part and providing a logical analysis. It takes the document extractor's result as input and outputs a detailed analysis in Markdown format.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/workshop/intermediate/article-reader.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nRead the following article content and perform the task\n{{Result variable of the document extractor}}\n# Task\n\n- **Main Objective**: Thoroughly analyze the structure of the article.\n- **Objective**: Detail the content of each part of the article.\n- **Requirements**: Analyze as detailed as possible.\n- **Restrictions**: No specific format restrictions, but the analysis must be organized and logical.\n- **Expected Output**: A detailed analysis of the article structure, including the main content and role of each part.\n\n# Reasoning Order\n\n- **Reasoning Part**: By carefully reading the article, identify and analyze its structure.\n- **Conclusion Part**: Provide specific content and role for each part.\n\n# Output Format\n\n- **Analysis Format**: Each part should be listed in a headline format, followed by a detailed explanation of that part's content.\n- **Structure Form**: Markdown, to enhance readability.\n- **Specific Description**: The content and role of each part, including but not limited to the introduction, body, conclusion, citations, etc.\n```\n\n----------------------------------------\n\nTITLE: Model Credential Validation Method\nDESCRIPTION: Implements the validation of model credentials. This method is responsible for verifying the provided credentials against the specific model requirements, ensuring that the necessary authentication and authorization parameters are correctly configured.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/customizable-model.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n    \"\"\"\n    Validate model credentials\n\n    :param model: model name\n    :param credentials: model credentials\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for Text Embedding in Python\nDESCRIPTION: This method calculates the number of tokens for a given list of texts. It takes model name, credentials, and a list of texts as input. It returns the number of tokens as an integer.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, texts: list[str]) -> int:\n      \"\"\"\n      Get number of tokens for given prompt messages\n\n      :param model: model name\n      :param credentials: model credentials\n      :param texts: texts to embed\n      :return:\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating a New Plugin Project with Dify CLI\nDESCRIPTION: This command initializes a new Dify plugin project using the dify-plugin CLI tool.  It assumes the binary file is named `dify-plugin-darwin-arm64`. It creates the basic project structure and files needed for plugin development.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./dify-plugin-darwin-arm64 plugin init\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Remote Debugging\nDESCRIPTION: This snippet shows the contents of the `.env` file, which is used to configure the Dify plugin for remote debugging. It includes the installation method, remote server address, port, and debugging key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/debug-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: OpenAI Model Plugin Structure\nDESCRIPTION: This bash snippet demonstrates the file structure for OpenAI model plugins, featuring multiple model types. The structure outlines various categories, including LLM, moderation, speech2text, text_embedding, and tts, reflecting the diverse range of models offered by OpenAI.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n├── models\n│   ├── llm\n│   │   ├── chatgpt-4o-latest\n│   │   ├── gpt-3.5-turbo\n│   │   ├── gpt-4-0125-preview\n│   │   ├── gpt-4-turbo\n│   │   ├── gpt-4o\n│   │   ├── llm\n│   │   ├── o1-preview\n│   │   └── text-davinci-003\n│   ├── moderation\n│   │   ├── moderation\n│   │   └── text-moderation-stable\n│   ├── speech2text\n│   │   ├── speech2text\n│   │   └── whisper-1\n│   ├── text_embedding\n│   │   ├── text-embedding-3-large\n│   │   └── text_embedding\n│   └── tts\n│       ├── tts-1-hd\n│       ├── tts-1\n│       └── tts\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Base Class Definition\nDESCRIPTION: Defines a base class `PromptMessage` for all role message bodies. It includes the `role`, `content` (which can be a string or a list of `PromptMessageContent`), and an optional `name`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessage(ABC, BaseModel):\n    \"\"\"\n    Model class for prompt message.\n    \"\"\"\n    role: PromptMessageRole\n    content: Optional[str | list[PromptMessageContent]] = None  # Supports two types: string and content list. The content list is designed to meet the needs of multimodal inputs. For more details, see the PromptMessageContent explanation.\n    name: Optional[str] = None\n```\n\n----------------------------------------\n\nTITLE: Define Agent Parameters Class (Python)\nDESCRIPTION: This Python code defines the `BasicParams` class using Pydantic, which is used to validate the input parameters passed to the Agent plugin. It includes parameters like `maximum_iterations`, `model`, `tools`, and `query`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n```\n\n----------------------------------------\n\nTITLE: Updating Package List\nDESCRIPTION: This command updates the package list for the Ubuntu system, ensuring the latest package information is available before installing new software. It's a standard practice before installing any new package using apt.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\n```\n\n----------------------------------------\n\nTITLE: Question Classifier Node Interface\nDESCRIPTION: This snippet shows the interface for invoking the QuestionClassifier node.  The arguments are similar to ParameterExtractor, taking a list of ClassConfig objects, a ModelConfig, a query string and an instruction string.  The classification result is available in NodeResponse.outputs['class_name'].\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n        self,\n        classes: list[ClassConfig],\n        model: ModelConfig,\n        query: str,\n        instruction: str = \"\",\n    ) -> NodeResponse:\n        pass\n```\n\n----------------------------------------\n\nTITLE: LLM Invocation Signature in Python\nDESCRIPTION: This Python snippet defines the signature for invoking a large language model (LLM), supporting both streaming and synchronous responses. It includes parameters for model selection, credentials, prompt messages, model parameters, tools, stop words, and user context, demonstrating a comprehensive set of options for LLM interaction.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            prompt_messages: list[PromptMessage], model_parameters: dict,\n            tools: Optional[list[PromptMessageTool]] = None, stop: Optional[list[str]] = None,\n            stream: bool = True, user: Optional[str] = None) \\\n        -> Union[LLMResult, Generator]:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param model_parameters: model parameters\n    :param tools: tools for tool calling\n    :param stop: stop words\n    :param stream: is stream response\n    :param user: unique user id\n    :return: full response or stream response chunk generator result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Updating Document via Text using Dify API (curl)\nDESCRIPTION: This snippet shows how to update a document in a Dify knowledge base by providing new text content. It requires the dataset ID, document ID, and API key for authorization. The request includes the document name and updated text.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/update_by_text' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"name\": \"name\",\"text\": \"text\"}'\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Example (Anthropic LLM)\nDESCRIPTION: Illustrates the recommended directory structure for a model provider (Anthropic) that offers only LLM models.  It showcases the organization under the `/models` directory with YAML configuration files for different model versions and an `llm.py` file.  The structure facilitates easy maintenance and extension of the provider.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/predefined-model.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n├── models\n│   └── llm\n│       ├── _position.yaml\n│       ├── claude-2.1.yaml\n│       ├── claude-2.yaml\n│       ├── claude-3-5-sonnet-20240620.yaml\n│       ├── claude-3-haiku-20240307.yaml\n│       ├── claude-3-opus-20240229.yaml\n│       ├── claude-3-sonnet-20240229.yaml\n│       ├── claude-instant-1.2.yaml\n│       ├── claude-instant-1.yaml\n│       └── llm.py\n```\n\n----------------------------------------\n\nTITLE: Text Message Creation in Dify (Python)\nDESCRIPTION: This Python code snippet defines the interface for creating a text message within the Dify tool plugin framework. It takes a text string as input and returns a ToolInvokeMessage object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/tool.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef create_text_message(self, text: str) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Jinja2 Template Example in Dify\nDESCRIPTION: This Jinja2 template structures retrieved chunks and their metadata from a knowledge retrieval node into a formatted markdown. It iterates through a list of chunks, extracting the index, similarity score, title, and content for each. The content is formatted to replace newline characters with double newline characters for better readability in Markdown.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_71\n\nLANGUAGE: Jinja2\nCODE:\n```\n{% for item in chunks %}\n### Chunk {{ loop.index }}.\n### Similarity: {{ item.metadata.score | default('N/A') }}\n\n#### {{ item.title }}\n\n##### Content\n{{ item.content | replace('\\n', '\\n\\n') }}\n\n---\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Content Type Enum in Python\nDESCRIPTION: This code defines an Enum for Prompt Message Content Types, differentiating between 'TEXT' and 'IMAGE' content.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContentType(Enum):\n    \"\"\"\n    Enum class for prompt message content type.\n    \"\"\"\n    TEXT = 'text'\n    IMAGE = 'image'\n```\n\n----------------------------------------\n\nTITLE: Invoking Rerank Model in Python\nDESCRIPTION: This method invokes a rerank model. It takes the model name, credentials, search query, documents for reranking, score threshold, top n, and user ID as input. It returns a RerankResult entity containing the reranked results.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n                query: str, docs: list[str], score_threshold: Optional[float] = None, top_n: Optional[int] = None,\n                user: Optional[str] = None) \\\n            -> RerankResult:\n        \"\"\"\n        Invoke rerank model\n\n        :param model: model name\n        :param credentials: model credentials\n        :param query: search query\n        :param docs: docs for reranking\n        :param score_threshold: score threshold\n        :param top_n: top n\n        :param user: unique user id\n        :return: rerank result\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for LLM in Python\nDESCRIPTION: This method calculates the number of tokens for given prompt messages. It takes model name, credentials, prompt messages, and optional tools as input. It should return the number of tokens as an integer.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                     tools: Optional[list[PromptMessageTool]] = None) -> int:\n      \"\"\"\n      Get number of tokens for given prompt messages\n\n      :param model: model name\n      :param credentials: model credentials\n      :param prompt_messages: prompt messages\n      :param tools: tools for tool calling\n      :return:\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Minimal Provider Credentials Validation in Python\nDESCRIPTION: This Python code shows a minimal implementation of the `validate_provider_credentials` method for custom model providers. It simply passes without performing any validation. This is acceptable for custom model providers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass XinferenceProvider(Provider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: LLM Prompt for JSON Code Generation\nDESCRIPTION: This prompt instructs an LLM to generate either correct or incorrect JSON sample code based on user requirements. It specifies the role of the LLM as a teaching assistant. This prompt is used within the LLM node to generate JSON code that will be validated by subsequent code nodes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/error-handling/README.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nYou are a teaching assistant. According to the user's requirements, you only output a correct or incorrect sample code in json format.\n```\n\n----------------------------------------\n\nTITLE: Adding a Package plugin dependency to a Dify Bundle\nDESCRIPTION: This command adds a local plugin package dependency to the Dify Bundle project. The --package_path parameter specifies the path to the plugin package file (e.g., ./openai.difypkg). The dot (.) indicates the current directory as the location of the target bundle project.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/bundle.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndify-plugin bundle append package . --package_path=./openai.difypkg\n```\n\n----------------------------------------\n\nTITLE: Initializing Directory Structure for Custom Moderation\nDESCRIPTION: Shows the file structure required to add a new custom moderation type to Dify.  A new directory 'cloud_service' is created under 'api/core/moderation' with `__init__.py`, `cloud_service.py`, and `schema.json` files. This setup is crucial for defining the new moderation logic and UI components.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/code-based-extension/moderation.md#_snippet_0\n\nLANGUAGE: Plain\nCODE:\n```\n.\n└── api\n    └── core\n        └── moderation\n            └── cloud_service\n                ├── __init__.py\n                ├── cloud_service.py\n                └── schema.json\n```\n\n----------------------------------------\n\nTITLE: Speech-to-Text Invoke Endpoint Definition Python\nDESCRIPTION: This snippet defines the `invoke` endpoint for speech to text. It takes a `model_config` of type `Speech2TextModelConfig` and the audio file (`file`) as input.  The audio file is expected to be in mp3 format.  It returns the transcribed text.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self, model_config: Speech2TextModelConfig, file: IO[bytes]\n) -> str:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Build Frontend Docker Image from Source\nDESCRIPTION: This command builds the frontend Docker image from the source code located in the 'web' directory. It uses the Dockerfile in that directory and tags the image as 'dify-web'. This requires Docker to be installed and configured.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/start-the-frontend-docker-container.md#_snippet_1\n\nLANGUAGE: Dockerfile\nCODE:\n```\ncd web && docker build . -t dify-web\n```\n\n----------------------------------------\n\nTITLE: Configure Environment Variables (ENV)\nDESCRIPTION: Configures environment variables for the Dify web application.  This snippet shows example environment variables that need to be set in the .env.local file, including the deployment environment, edition, API prefixes, and Sentry configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_12\n\nLANGUAGE: env\nCODE:\n```\n# For production release, change this to PRODUCTION\nNEXT_PUBLIC_DEPLOY_ENV=DEVELOPMENT\n# The deployment edition, SELF_HOSTED or CLOUD\nNEXT_PUBLIC_EDITION=SELF_HOSTED\n# The base URL of console application, refers to the Console base URL of WEB service if console domain is\n# different from api or web app domain.\n# example: http://cloud.dify.ai/console/api\nNEXT_PUBLIC_API_PREFIX=http://localhost:5001/console/api\n# The URL for Web APP, refers to the Web App base URL of WEB service if web app domain is different from\n# console or api domain.\n# example: http://udify.app/api\nNEXT_PUBLIC_PUBLIC_API_PREFIX=http://localhost:5001/api\n\n# SENTRY\nNEXT_PUBLIC_SENTRY_DSN=\nNEXT_PUBLIC_SENTRY_ORG=\nNEXT_PUBLIC_SENTRY_PROJECT=\n```\n\n----------------------------------------\n\nTITLE: Finding Document Segments using Dify API (curl)\nDESCRIPTION: This snippet retrieves the segments associated with a specific document in a Dify knowledge base. It requires the dataset ID, document ID, and API key. The response includes an array of segment objects, each containing content, answer, keywords, and other relevant information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Agent Strategy Model Invocation\nDESCRIPTION: This code snippet demonstrates the implementation of the `BasicAgentAgentStrategy` class, an agent strategy that handles model invocation, tool calls, and response processing.  It defines a basic agent strategy, invokes LLM models, handles tool calls, and processes responses. It relies on several Dify plugin entities like `AgentInvokeMessage`, `LLMModelConfig`, `UserPromptMessage`, and `ToolEntity`. It showcases how to structure parameters, invoke tools, and manage responses from both the model and the tools.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom collections.abc import Generator\nfrom typing import Any, cast\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.model.llm import LLMModelConfig, LLMResult, LLMResultChunk\nfrom dify_plugin.entities.model.message import (\n    PromptMessageTool,\n    UserPromptMessage,\n)\nfrom dify_plugin.entities.tool import ToolInvokeMessage, ToolParameter, ToolProviderType\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n        chunks: Generator[LLMResultChunk, None, None] | LLMResult = (\n            self.session.model.llm.invoke(\n                model_config=LLMModelConfig(**params.model.model_dump(mode=\"json\")),\n                prompt_messages=[UserPromptMessage(content=params.query)],\n                tools=[\n                    self._convert_tool_to_prompt_message_tool(tool)\n                    for tool in params.tools\n                ],\n                stop=params.model.completion_params.get(\"stop\", [])\n                if params.model.completion_params\n                else [],\n                stream=True,\n            )\n        )\n        response = \"\"\n        tool_calls = []\n        tool_instances = (\n            {tool.identity.name: tool for tool in params.tools} if params.tools else {}\n        )\n\n        for chunk in chunks:\n            # check if there is any tool call\n            if self.check_tool_calls(chunk):\n                tool_calls = self.extract_tool_calls(chunk)\n                tool_call_names = \";\".join([tool_call[1] for tool_call in tool_calls])\n                try:\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls},\n                        ensure_ascii=False,\n                    )\n                except json.JSONDecodeError:\n                    # ensure ascii to avoid encoding error\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls}\n                    )\n                print(tool_call_names, tool_call_inputs)\n            if chunk.delta.message and chunk.delta.message.content:\n                if isinstance(chunk.delta.message.content, list):\n                    for content in chunk.delta.message.content:\n                        response += content.data\n                        print(content.data, end=\"\", flush=True)\n                else:\n                    response += str(chunk.delta.message.content)\n                    print(str(chunk.delta.message.content), end=\"\", flush=True)\n\n            if chunk.delta.usage:\n                # usage of the model\n                usage = chunk.delta.usage\n\n        yield self.create_text_message(\n            text=f\"{response or json.dumps(tool_calls, ensure_ascii=False)}\\n\"\n        )\n        result = \"\"\n        for tool_call_id, tool_call_name, tool_call_args in tool_calls:\n            tool_instance = tool_instances[tool_call_name]\n            tool_invoke_responses = self.session.tool.invoke(\n                provider_type=ToolProviderType.BUILT_IN,\n                provider=tool_instance.identity.provider,\n                tool_name=tool_instance.identity.name,\n                parameters={**tool_instance.runtime_parameters, **tool_call_args},\n            )\n            if not tool_instance:\n                tool_invoke_responses = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": f\"there is not a tool named {tool_call_name}\",\n                }\n            else:\n                # invoke tool\n                tool_invoke_responses = self.session.tool.invoke(\n                    provider_type=ToolProviderType.BUILT_IN,\n                    provider=tool_instance.identity.provider,\n                    tool_name=tool_instance.identity.name,\n                    parameters={**tool_instance.runtime_parameters, **tool_call_args},\n                )\n                result = \"\"\n                for tool_invoke_response in tool_invoke_responses:\n                    if tool_invoke_response.type == ToolInvokeMessage.MessageType.TEXT:\n                        result += cast(\n                            ToolInvokeMessage.TextMessage, tool_invoke_response.message\n                        ).text\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.LINK\n                    ):\n                        result += (\n                            f\"result link: {cast(ToolInvokeMessage.TextMessage, tool_invoke_response.message).text}.\"\n                            + \" please tell user to check it.\"\n                        )\n                    elif tool_invoke_response.type in {\n                        ToolInvokeMessage.MessageType.IMAGE_LINK,\n                        ToolInvokeMessage.MessageType.IMAGE,\n                    }:\n                        result += (\n                            \"image has been created and sent to user already, \"\n                            + \"you do not need to create it, just tell the user to check it now.\"\n                        )\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.JSON\n                    ):\n                        text = json.dumps(\n                            cast(\n                                ToolInvokeMessage.JsonMessage,\n                                tool_invoke_response.message,\n                            ).json_object,\n                            ensure_ascii=False,\n                        )\n                        result += f\"tool response: {text}.\"\n                    else:\n                        result += f\"tool response: {tool_invoke_response.message!r}.\"\n\n                tool_response = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": result,\n                }\n        yield self.create_text_message(result)\n\n    def _convert_tool_to_prompt_message_tool(\n        self, tool: ToolEntity\n    ) -> PromptMessageTool:\n        \"\"\"\n        convert tool to prompt message tool\n        \"\"\"\n        message_tool = PromptMessageTool(\n            name=tool.identity.name,\n            description=tool.description.llm if tool.description else \"\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        )\n\n        parameters = tool.parameters\n        for parameter in parameters:\n            if parameter.form != ToolParameter.ToolParameterForm.LLM:\n                continue\n\n            parameter_type = parameter.type\n            if parameter.type in {\n                ToolParameter.ToolParameterType.FILE,\n                ToolParameter.ToolParameterType.FILES,\n            }:\n                continue\n            enum = []\n            if parameter.type == ToolParameter.ToolParameterType.SELECT:\n                enum = (\n                    [option.value for option in parameter.options]\n                    if parameter.options\n                    else []\n                )\n\n            message_tool.parameters[\"properties\"][parameter.name] = {\n                \"type\": parameter_type,\n                \"description\": parameter.llm_description or \"\",\n            }\n\n            if len(enum) > 0:\n                message_tool.parameters[\"properties\"][parameter.name][\"enum\"] = enum\n\n            if parameter.required:\n                message_tool.parameters[\"required\"].append(parameter.name)\n\n        return message_tool\n\n    def check_tool_calls(self, llm_result_chunk: LLMResultChunk) -> bool:\n        \"\"\"\n        Check if there is any tool call in llm result chunk\n        \"\"\"\n        return bool(llm_result_chunk.delta.message.tool_calls)\n\n    def extract_tool_calls(\n        self, llm_result_chunk: LLMResultChunk\n    ) -> list[tuple[str, str, dict[str, Any]]]:\n        \"\"\"\n        Extract tool calls from llm result chunk\n\n        Returns:\n            List[Tuple[str, str, Dict[str, Any]]]: [(tool_call_id, tool_call_name, tool_call_args)]\n        \"\"\"\n        tool_calls = []\n        for prompt_message in llm_result_chunk.delta.message.tool_calls:\n            args = {}\n            if prompt_message.function.arguments != \"\":\n                args = json.loads(prompt_message.function.arguments)\n\n            tool_calls.append(\n                (\n                    prompt_message.id,\n                    prompt_message.function.name,\n                    args,\n                )\n            )\n\n        return tool_calls\n```\n\n----------------------------------------\n\nTITLE: Speech2text Model Invocation\nDESCRIPTION: Defines the interface for a speech-to-text model invocation. It takes an audio file and converts it to text. It requires the model name, credentials, and the audio file stream as input. An optional user identifier can also be provided.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, file: IO[bytes], user: Optional[str] = None) -> str:\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param file: audio file\n      :param user: unique user id\n      :return: text for given audio file\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Google Search Tool YAML\nDESCRIPTION: This YAML file defines the Google Search tool, including its name, author, labels, descriptions, and parameters (query and result_type). It specifies parameter types, whether they are required, options for select types, default values, descriptions, and form types (llm or form).  `llm` form type indicates the parameter is inferred by the Agent, and `form` means the user needs to input it.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/quick-tool-integration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nidentity: # 工具的基本信息\n  name: google_search # 工具名称，唯一，不允许和其他工具重名\n  author: Dify # 作者\n  label: # 标签，用于前端展示\n    en_US: GoogleSearch # 英文标签\n    zh_Hans: 谷歌搜索 # 中文标签\n    ja_JP: Google検索 # 日文标签\n    pt_BR: Pesquisa Google # 葡萄牙文标签\ndescription: # 描述，用于前端展示\n  human: # 用于前端展示的介绍，支持多语言\n    en_US: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.\n    zh_Hans: 一个用于执行 Google SERP 搜索并提取片段和网页的工具。输入应该是一个搜索查询。\n    ja_JP: Google SERP 検索を実行し、スニペットと Web ページを抽出するためのツール。入力は検索クエリである必要があります。\n    pt_BR: Uma ferramenta para realizar pesquisas no Google SERP e extrair snippets e páginas da web. A entrada deve ser uma consulta de pesquisa.\n  llm: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query. # 传递给 LLM 的介绍，为了使得LLM更好理解这个工具，我们建议在这里写上关于这个工具尽可能详细的信息，让 LLM 能够理解并使用这个工具\nparameters: # 参数列表\n  - name: query # 参数名称\n    type: string # 参数类型\n    required: true # 是否必填\n    label: # 参数标签\n      en_US: Query string # 英文标签\n      zh_Hans: 查询语句 # 中文标签\n      ja_JP: クエリステートメント # 日文标签\n      pt_BR: Declaração de consulta # 葡萄牙文标签\n    human_description: # 用于前端展示的介绍，支持多语言\n      en_US: used for searching\n      zh_Hans: 用于搜索网页内容\n      ja_JP: ネットの検索に使用する\n      pt_BR: usado para pesquisar\n    llm_description: key words for searching # 传递给LLM的介绍，同上，为了使得LLM更好理解这个参数，我们建议在这里写上关于这个参数尽可能详细的信息，让LLM能够理解这个参数\n    form: llm # 表单类型，llm表示这个参数需要由Agent自行推理出来，前端将不会展示这个参数\n  - name: result_type\n    type: select # 参数类型\n    required: true\n    options: # 下拉框选项\n      - value: text\n        label:\n          en_US: text\n          zh_Hans: 文本\n          ja_JP: テキスト\n          pt_BR: texto\n      - value: link\n        label:\n          en_US: link\n          zh_Hans: 链接\n          ja_JP: リンク\n          pt_BR: link\n    default: link\n    label:\n      en_US: Result type\n      zh_Hans: 结果类型\n      ja_JP: 結果タイプ\n      pt_BR: tipo de resultado\n    human_description:\n      en_US: used for selecting the result type, text or link\n      zh_Hans: 用于选择结果类型，使用文本还是链接进行展示\n      ja_JP: 結果の種類、テキスト、リンクを選択するために使用されます\n      pt_BR: usado para selecionar o tipo de resultado, texto ou link\n    form: form # 表单类型，form表示这个参数需要由用户在对话开始前在前端填写\n```\n\n----------------------------------------\n\nTITLE: Integrating History Messages into Model Invocation (Python)\nDESCRIPTION: This Python code demonstrates how to include history messages when invoking the LLM. It concatenates `params.model.history_prompt_messages` with the current user query within the `invoke` method of `self.session.model.llm` to create a comprehensive context for the LLM.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n\n        chunks: Generator[LLMResultChunk, None, None] | LLMResult = (\n            self.session.model.llm.invoke(\n                model_config=LLMModelConfig(**params.model.model_dump(mode=\"json\")),\n                # 履歴メッセージを追加\n                prompt_messages=params.model.history_prompt_messages\n                + [UserPromptMessage(content=params.query)],\n                tools=[\n                    self._convert_tool_to_prompt_message_tool(tool)\n                    for tool in params.tools\n                ],\n                stop=params.model.completion_params.get(\"stop\", [])\n                if params.model.completion_params\n                else [],\n                stream=True,\n            )\n        )\n        ...\n```\n\n----------------------------------------\n\nTITLE: Starting LocalAI with Docker Compose (Shell)\nDESCRIPTION: This snippet uses Docker Compose to start the LocalAI service. The `-d` flag runs the containers in detached mode, and the `--build` flag ensures that the images are built if they don't exist. The logs are then tailed using `docker logs -f langchain-chroma-api-1` to monitor the startup process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/localai.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# start with docker-compose\n$ docker-compose up -d --build\n\n# tail the logs & wait until the build completes\n$ docker logs -f langchain-chroma-api-1\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Example for AI Generation (Duplicate)\nDESCRIPTION: This code snippet represents a JSON schema generated by AI for user profiles. It includes fields for 'username' (string), 'age' (number), and 'interests' (array of strings). The 'required' array specifies that all three fields are mandatory. It is a duplicate from earlier in the document.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/llm.md#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"username\": {\n      \"type\": \"string\"\n    },\n    \"age\": {\n      \"type\": \"number\"\n    },\n    \"interests\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      }\n    }\n  },\n  \"required\": [\"username\", \"age\", \"interests\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing the Tool Session\nDESCRIPTION: This code snippet demonstrates accessing the `tool` session within a Dify plugin, which allows the plugin to request and utilize other installed tools, Workflow as Tool, and custom tools.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/tool.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.session.tool\n```\n\n----------------------------------------\n\nTITLE: Set Ollama Host Environment Variable on MacOS\nDESCRIPTION: This command sets the OLLAMA_HOST environment variable on macOS using launchctl. This is necessary when the Ollama service needs to be accessible from other machines or Docker containers. After setting the environment variable, the Ollama app needs to be restarted.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/ollama.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlaunchctl setenv OLLAMA_HOST \"0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Validating Model Credentials in Python\nDESCRIPTION: This snippet shows how to validate model credentials. It takes the model name and a dictionary of credentials as input. If the credentials are invalid, it should raise a CredentialsValidateFailedError.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n      \"\"\"\n      Validate model credentials\n\n      :param model: model name\n      :param credentials: model credentials\n      :return:\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: LiteLLM Configuration (litellm_config.yaml)\nDESCRIPTION: This YAML file configures LiteLLM with a list of models, their OpenAI-compatible names, and API details such as base URL and API key.  It specifies how LiteLLM should route requests to different underlying models like deepseek-chat and Azure OpenAI gpt-4 instances. The `model_name` is the identifier used within LiteLLM and Dify, while `litellm_params` contains the actual model details.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/litellm.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_list:\n  - model_name: deepseek-chat #调用 LiteLLM 的模型名词\n    litellm_params:\n      model: openai/deepseek-chat #`openai/` 前缀表示该模型与 openai 格式兼容\n      api_key: \n      api_base: https://api.deepseek.com/\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/chatgpt-v-2\n      api_base: https://openai-gpt-4-test-v-1.openai.azure.com/\n      api_version: \"2023-05-15\"\n      api_key: \n  - model_name: gpt-4\n    litellm_params:\n      model: azure/gpt-4\n      api_key: \n      api_base: https://openai-gpt-4-test-v-2.openai.azure.com/\n```\n\n----------------------------------------\n\nTITLE: Adding Docker Official Repository\nDESCRIPTION: This command adds the official Docker repository to the apt sources list. It creates a new file in the sources.list.d directory with the repository information, ensuring that apt can find and install Docker packages.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n```\n\n----------------------------------------\n\nTITLE: Define provider_credential_schema for Model Type\nDESCRIPTION: This YAML snippet defines the provider credential schema for the 'model_type' variable, allowing users to select the model type (text-generation, embeddings, or reranking). The `model_type` selection drives which backend model logic will be invoked.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprovider_credential_schema:\n  credential_form_schemas:\n  - variable: model_type\n    type: select\n    label:\n      en_US: Model type\n      zh_Hans: 模型类型\n    required: true\n    options:\n    - value: text-generation\n      label:\n        en_US: Language Model\n        zh_Hans: 语言模型\n    - value: embeddings\n      label:\n        en_US: Text Embedding\n    - value: reranking\n      label:\n        en_US: Rerank\n```\n\n----------------------------------------\n\nTITLE: Tool YAML Configuration (Google Search)\nDESCRIPTION: This YAML code defines the GoogleSearch tool configuration, including identity, description, parameters, and the path to the Python code file. The `parameters` section specifies the input parameters for the tool, such as the query string.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: google_search\n  author: Dify\n  label:\n    en_US: GoogleSearch\n    zh_Hans: 谷歌搜索\n    ja_JP: Google検索\n    pt_BR: Pesquisa Google\ndescription:\n  human:\n    en_US: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.\n    zh_Hans: 一个用于执行 Google SERP 搜索并提取片段和网页的工具。输入应该是一个搜索查询。\n    ja_JP: Google SERP 検索を実行し、スニペットと Web ページを抽出するためのツール。入力は検索クエリである必要があります。\n    pt_BR: Uma ferramenta para realizar pesquisas no Google SERP e extrair snippets e páginas da web. A entrada deve ser uma consulta de pesquisa.\n  llm: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.\nparameters:\n  - name: query\n    type: string\n    required: true\n    label:\n      en_US: Query string\n      zh_Hans: 查询字符串\n      ja_JP: クエリ文字列\n      pt_BR: Cadeia de consulta\n    human_description:\n      en_US: used for searching\n      zh_Hans: 用于搜索网页内容\n      ja_JP: ネットの検索に使用する\n      pt_BR: usado para pesquisar\n    llm_description: key words for searching\n    form: llm\nextra:\n  python:\n    source: tools/google_search.py\n```\n\n----------------------------------------\n\nTITLE: Creating Image Messages in Python\nDESCRIPTION: This Python code snippet shows the interface for creating image messages within the Dify tool plugin system.  It takes an image URL as input and returns a ToolInvokeMessage. Dify automatically downloads the image from the provided URL. No specific dependencies are required beyond the Dify plugin environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/tool.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef create_image_message(self, image: str) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Setting Ollama Host Environment Variable (macOS)\nDESCRIPTION: This command sets the OLLAMA_HOST environment variable on macOS to allow external access to the Ollama service. It uses launchctl to set the environment variable, and then the Ollama application needs to be restarted for the changes to take effect.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_79\n\nLANGUAGE: Shell\nCODE:\n```\nlaunchctl setenv OLLAMA_HOST \"0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Model Configuration (claude-3-5-sonnet-20240620.yaml)\nDESCRIPTION: An example of a YAML configuration file for the `claude-3-5-sonnet-20240620` model. It specifies the model name, label, model type (llm), supported features (agent-thought, vision, tool-call, etc.), model properties (mode, context_size), parameter rules (temperature, top_p, top_k, max_tokens, response_format), and pricing information (input, output, unit, currency).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/predefined-model.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodel: claude-3-5-sonnet-20240620\nlabel:\n  en_US: claude-3-5-sonnet-20240620\nmodel_type: llm\nfeatures:\n  - agent-thought\n  - vision\n  - tool-call\n  - stream-tool-call\n  - document\nmodel_properties:\n  mode: chat\n  context_size: 200000\nparameter_rules:\n  - name: temperature\n    use_template: temperature\n  - name: top_p\n    use_template: top_p\n  - name: top_k\n    label:\n      zh_Hans: \n      en_US: Top k\n    type: int\n    help:\n      zh_Hans: \n      en_US: Only sample from the top K options for each subsequent token.\n    required: false\n  - name: max_tokens\n    use_template: max_tokens\n    required: true\n    default: 8192\n    min: 1\n    max: 8192\n  - name: response_format\n    use_template: response_format\npricing:\n  input: '3.00'\n  output: '15.00'\n  unit: '0.000001'\n  currency: USD\n```\n\n----------------------------------------\n\nTITLE: Defining EmbeddingUsage Data Model in Python\nDESCRIPTION: Defines an `EmbeddingUsage` class that inherits from `ModelUsage`, representing the usage details of an embedding operation, including tokens used, total price, unit price, and latency. It utilizes `Decimal` for precise monetary values and includes a currency field.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nclass EmbeddingUsage(ModelUsage):\n    \"\"\"\n    Model class for embedding usage.\n    \"\"\"\n    tokens: int  # Number of tokens used\n    total_tokens: int  # Total number of tokens used\n    unit_price: Decimal  # Unit price\n    price_unit: Decimal  # Price unit, i.e., the unit price based on how many tokens\n    total_price: Decimal  # Total cost\n    currency: str  # Currency unit\n    latency: float  # Request latency (s)\n```\n\n----------------------------------------\n\nTITLE: Content Moderation Output Response Direct Output Example\nDESCRIPTION: Demonstrates the response format from the `app.moderation.output` extension point when the `action` is `direct_output`. It indicates that the LLM output violates moderation rules, providing a `preset_response` to be displayed to the user.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"direct_output\",\n    \"preset_response\": \"Your content violates our usage policy.\"\n}\n```\n\n----------------------------------------\n\nTITLE: WSL Proxy Configuration Script\nDESCRIPTION: This bash script configures proxy settings within WSL, dynamically obtaining the host IP and setting environment variables for HTTP and HTTPS proxies. It also updates Git configuration to use the proxy. This script is crucial for accessing external resources from WSL when behind a proxy server.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n#!/bin/sh\nhostip=$(cat /etc/resolv.conf | grep nameserver | awk '{ print $2 }')\nwslip=$(hostname -I | awk '{print $1}')\nport=7890\n \nPROXY_HTTP=\"http://${hostip}:${port}\"\n \nset_proxy(){\n  export http_proxy=\"${PROXY_HTTP}\"\n  export HTTP_PROXY=\"${PROXY_HTTP}\"\n \n  export https_proxy=\"${PROXY_HTTP}\"\n  export HTTPS_proxy=\"${PROXY_HTTP}\"\n \n  export ALL_PROXY=\"${PROXY_SOCKS5}\"\n  export all_proxy=${PROXY_SOCKS5}\n \n  git config --global http.https://github.com.proxy ${PROXY_HTTP}\n  git config --global https.https://github.com.proxy ${PROXY_HTTP}\n \n  echo \"Proxy has been opened.\"\n}\n \nunset_proxy(){\n  unset http_proxy\n  unset HTTP_PROXY\n  unset https_proxy\n  unset HTTPS_PROXY\n  unset ALL_PROXY\n  unset all_proxy\n  git config --global --unset http.https://github.com.proxy\n  git config --global --unset https.https://github.com.proxy\n \n  echo \"Proxy has been closed.\"\n}\n \ntest_setting(){\n  echo \"Host IP:\" ${hostip}\n  echo \"WSL IP:\" ${wslip}\n  echo \"Try to connect to Google...\"\n  resp=$(curl -I -s --connect-timeout 5 -m 5 -w \"%{http_code}\" -o /dev/null www.google.com)\n  if [ ${resp} = 200 ]; then\n    echo \"Proxy setup succeeded!\"\n  else\n    echo \"Proxy setup failed!\"\n  fi\n}\n \nif [ \"$1\" = \"set\" ]\nthen\n  set_proxy\n \nelif [ \"$1\" = \"unset\" ]\nthen\n  unset_proxy\n \nelif [ \"$1\" = \"test\" ]\nthen\n  test_setting\nelse\n  echo \"Unsupported arguments.\"\nfi\n```\n\n----------------------------------------\n\nTITLE: Tool Invocation Method Definition in Python\nDESCRIPTION: This code snippet defines the signature for the `invoke` method used to call tools within the agent strategy plugin. It specifies the parameters required for the tool invocation, including provider type, provider name, tool name, and input parameters. The method returns a generator yielding `ToolInvokeMessage` objects.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n def invoke(\n        self,\n        provider_type: ToolProviderType,\n        provider: str,\n        tool_name: str,\n        parameters: dict[str, Any],\n    ) -> Generator[ToolInvokeMessage, None, None]:...\n```\n\n----------------------------------------\n\nTITLE: Defining LLMResultChunkDelta Model in Python\nDESCRIPTION: Defines the `LLMResultChunkDelta` model, used in streaming LLM responses to represent incremental changes (delta). It includes the index, the assistant's message delta, usage information, and a finish reason (only in the last chunk).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunkDelta(BaseModel):\n    \"\"\"\n    Model class for llm result chunk delta.\n    \"\"\"\n    index: int\n    message: AssistantPromptMessage  # response message\n    usage: Optional[LLMUsage] = None  # usage info\n    finish_reason: Optional[str] = None  # finish reason, only the last one returns\n```\n\n----------------------------------------\n\nTITLE: Validate Provider Credentials Python\nDESCRIPTION: This Python code snippet demonstrates how to validate provider credentials within a ModelProvider class.  It inherits from a base class and implements the `validate_provider_credentials` method. It takes a dictionary of credentials as input and can raise a `CredentialsValidateFailedError` if validation fails. The credentials parameters are defined in the provider's YAML configuration file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Create Document from Text - Dify API (JSON Response)\nDESCRIPTION: This is the JSON response received after successfully creating a document from text via the Dify API. It contains details about the created document, such as its ID, position, data source type, name, creation timestamp, indexing status, and other relevant metadata.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"document\": {\n    \"id\": \"\",\n    \"position\": 1,\n    \"data_source_type\": \"upload_file\",\n    \"data_source_info\": {\n        \"upload_file_id\": \"\"\n    },\n    \"dataset_process_rule_id\": \"\",\n    \"name\": \"text.txt\",\n    \"created_from\": \"api\",\n    \"created_by\": \"\",\n    \"created_at\": 1695690280,\n    \"tokens\": 0,\n    \"indexing_status\": \"waiting\",\n    \"error\": null,\n    \"enabled\": true,\n    \"disabled_at\": null,\n    \"disabled_by\": null,\n    \"archived\": false,\n    \"display_status\": \"queuing\",\n    \"word_count\": 0,\n    \"hit_count\": 0,\n    \"doc_form\": \"text_model\"\n  },\n  \"batch\": \"\"\n}\n```\n\n----------------------------------------\n\nTITLE: TextEmbeddingResult Class Definition in Python\nDESCRIPTION: Defines the `TextEmbeddingResult` class as a Pydantic BaseModel representing the result of a text embedding operation. It includes the model name, a list of embedding vectors, and usage information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nclass TextEmbeddingResult(BaseModel):\n    \"\"\"\n    Model class for text embedding result.\n    \"\"\"\n    model: str  # 实际使用模型\n    embeddings: list[list[float]]  # embedding 向量列表，对应传入的 texts 列表\n    usage: EmbeddingUsage  # 使用信息\n```\n\n----------------------------------------\n\nTITLE: LLMResult Class Definition in Python\nDESCRIPTION: Defines the LLMResult class which encapsulates the result of a language model execution. It includes the model used, the list of prompt messages, the assistant's response message, the usage information (tokens and cost), and an optional system fingerprint.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResult(BaseModel):\n    \"\"\"\n    Model class for llm result.\n    \"\"\"\n    model: str  # 使用された実際のモデル\n    prompt_messages: list[PromptMessage]  # プロンプトメッセージのリスト\n    message: AssistantPromptMessage  # 返信メッセージ\n    usage: LLMUsage  # 使用したtokenとコスト情報\n    system_fingerprint: Optional[str] = None  # リクエスト指紋。OpenAIのこのパラメータの定義を参照。\n```\n\n----------------------------------------\n\nTITLE: Modifying Document Metadata (Assignment) with cURL\nDESCRIPTION: This cURL command modifies the metadata for a document in the Dify knowledge base by assigning values. It requires the dataset ID. The request body contains a list of document IDs and their corresponding metadata to be assigned. It returns a 200 success status if successful.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/metadata' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer {api_key}'\n--data '{\n    \"operation_data\":[\n        {\n            \"document_id\": \"3e928bc4-65ea-4201-87c8-cbcc5871f525\",\n            \"metadata_list\": [\n                    {\n                    \"id\": \"1887f5ec-966f-4c93-8c99-5ad386022f46\",\n                    \"value\": \"dify\",\n                    \"name\": \"test\"\n                }\n            ]\n        }\n    ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Add Tag for Plugin Versioning - Bash\nDESCRIPTION: This snippet adds a tag to the Git repository, indicating a specific version of the plugin. Replace `v0.0.1` with the desired version number.  This is recommended for future packaging and release management, making it easier to track and revert to specific plugin versions. After tagging locally, this tag is pushed to the remote repository.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit tag -a v0.0.1 -m \"Release version 0.0.1\"\ngit push origin v0.0.1\n```\n\n----------------------------------------\n\nTITLE: Copying Public Key to Directory (bash)\nDESCRIPTION: This command copies the public key file into the public keys directory created in the previous step. This makes the public key accessible to the plugin daemon for signature verification. Requires bash shell.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncp your_key_pair.public.pem docker/volumes/plugin_daemon/public_keys\n```\n\n----------------------------------------\n\nTITLE: Fetching URL Content in Dify (Python)\nDESCRIPTION: This snippet demonstrates how to fetch the content of a URL using Dify's built-in tool. It takes a URL and an optional user agent as input, and returns the crawled content as a string. If no user agent is provided, Dify uses a default user agent.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/advanced-tool-integration.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    def get_url(self, url: str, user_agent: str = None) -> str:\n        \"\"\"\n            get url\n        \"\"\" the crawled result\n```\n\n----------------------------------------\n\nTITLE: Update manifest.yaml configuration\nDESCRIPTION: This code snippet shows the required fields to configure in the `manifest.yaml` file for a Dify plugin.  It is crucial for plugin identification and versioning. The fields include version, author and name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/plugin-auto-publish-pr.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nversion: 0.0.x  # Version number\nauthor: your-github-username  # GitHub username/Author name\nname: your-plugin-name  # Plugin name\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Example for Enum Values\nDESCRIPTION: This code snippet defines a JSON schema for a string field with restricted enum values. It demonstrates how to limit possible values for a string field, such as allowing only 'red', 'green', or 'blue'. The 'type' is set to 'string', and the 'enum' array contains the allowed values.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/llm.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"string\",\n  \"enum\": [\"red\", \"green\", \"blue\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Get Knowledge Base List - Dify API (JSON Response)\nDESCRIPTION: This is the JSON response received when retrieving the knowledge base list from the Dify API. It contains an array of knowledge base objects, along with pagination information like `has_more`, `limit`, `total`, and `page`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [\n    {\n      \"id\": \"\",\n      \"name\": \"name\",\n      \"description\": \"desc\",\n      \"permission\": \"only_me\",\n      \"data_source_type\": \"upload_file\",\n      \"indexing_technique\": \"\",\n      \"app_count\": 2,\n      \"document_count\": 10,\n      \"word_count\": 1200,\n      \"created_by\": \"\",\n      \"created_at\": \"\",\n      \"updated_by\": \"\",\n      \"updated_at\": \"\"\n    },\n    ...\n  ],\n  \"has_more\": true,\n  \"limit\": 20,\n  \"total\": 50,\n  \"page\": 1\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Strategy (YAML)\nDESCRIPTION: Shows the structure of a `function_calling.yaml` file, which defines the agent strategy, including identity, parameters (model, tools, query, max_iterations), and extra configuration for Python source. This YAML configures the agent's behavior and interaction with tools and models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/agent.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: function_calling\n  author: Dify\n  label:\n    en_US: FunctionCalling\n    zh_Hans: FunctionCalling\n    pt_BR: FunctionCalling\ndescription:\n  en_US: Function Calling is a basic strategy for agent, model will use the tools provided to perform the task.\nparameters:\n  - name: model\n    type: model-selector\n    scope: tool-call&llm\n    required: true\n    label:\n      en_US: Model\n  - name: tools\n    type: array[tools]\n    required: true\n    label:\n      en_US: Tools list\n  - name: query\n    type: string\n    required: true\n    label:\n      en_US: Query\n  - name: max_iterations\n    type: number\n    required: false\n    default: 5\n    label:\n      en_US: Max Iterations\n    max: 50\n    min: 1\nextra:\n  python:\n    source: strategies/function_calling.py\n```\n\n----------------------------------------\n\nTITLE: Defining Front-end Component Schema\nDESCRIPTION: Defines the structure of the front-end component schema used for code extensions in Dify. This schema allows developers to specify input forms for custom types. It defines the 'label' for the component, 'form_schema' containing a list of form elements. Supported component 'type' includes 'select', 'text-input', and 'paragraph'. Each element specifies properties like 'label', 'variable', 'required', 'default', 'placeholder', 'options', and 'max_length'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/code-based-extension/README.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"label\": {\n        \"en-US\": \"Cloud Service\",\n        \"zh-Hans\": \"云服务\"\n    },\n    \"form_schema\": [\n        {\n            \"type\": \"select\",\n            \"label\": {\n                \"en-US\": \"Cloud Provider\",\n                \"zh-Hans\": \"云厂商\"\n            },\n            \"variable\": \"cloud_provider\",\n            \"required\": true,\n            \"options\": [\n                {\n                    \"label\": {\n                        \"en-US\": \"AWS\",\n                        \"zh-Hans\": \"亚马逊\"\n                    },\n                    \"value\": \"AWS\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Google Cloud\",\n                        \"zh-Hans\": \"谷歌云\"\n                    },\n                    \"value\": \"GoogleCloud\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Azure Cloud\",\n                        \"zh-Hans\": \"微软云\"\n                    },\n                    \"value\": \"Azure\"\n                }\n            ],\n            \"default\": \"GoogleCloud\",\n            \"placeholder\": \"\"\n        },\n        {\n            \"type\": \"text-input\",\n            \"label\": {\n                \"en-US\": \"API Endpoint\",\n                \"zh-Hans\": \"API Endpoint\"\n            },\n            \"variable\": \"api_endpoint\",\n            \"required\": true,\n            \"max_length\": 100,\n            \"default\": \"\",\n            \"placeholder\": \"https://api.example.com\"\n        },\n        {\n            \"type\": \"paragraph\",\n            \"label\": {\n                \"en-US\": \"API Key\",\n                \"zh-Hans\": \"API Key\"\n            },\n            \"variable\": \"api_keys\",\n            \"required\": true,\n            \"default\": \"\",\n            \"placeholder\": \"Paste your API key here\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Link Message in Dify (Python)\nDESCRIPTION: This code snippet shows how to create a link message in a Dify tool plugin. It requires a link (URL) as input and returns a `ToolInvokeMessage`. When the link is triggered, it directs the user to the specified URL.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/tool.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef create_link_message(self, link: str) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: LLM Node Prompt Example (JSON Code Generation) in Dify Workflow\nDESCRIPTION: This code snippet demonstrates an example prompt for an LLM node in a Dify workflow.  The LLM is instructed to act as a teaching assistant and output either correct or incorrect JSON sample code based on user requirements. This JSON code is then passed to another node for validation, and error handling is implemented if the JSON is invalid.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nYou are a teaching assistant. According to the user's requirements, you only output a correct or incorrect sample code in json format.\n```\n\n----------------------------------------\n\nTITLE: Starting Worker Service (Linux/MacOS)\nDESCRIPTION: Starts the worker service for consuming asynchronous queue tasks on Linux or MacOS. It uses Celery with gevent concurrency to process tasks related to dataset, generation, mail, and ops_trace queues.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run celery -A app.celery worker -P gevent -c 1 -Q dataset,generation,mail,ops_trace --loglevel INFO\n```\n\n----------------------------------------\n\nTITLE: External Data Tool Request Body Example\nDESCRIPTION: This JSON snippet shows an example request body for the `external_data_tool` extension. It includes parameters for the application ID, tool variable, inputs, and query.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/README.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"point\": \"app.external_data_tool.query\",\n    \"params\": {\n        \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n        \"tool_variable\": \"weather_retrieve\",\n        \"inputs\": {\n            \"location\": \"London\"\n        },\n        \"query\": \"How's the weather today?\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Validating Model Credentials in Python\nDESCRIPTION: This method validates the credentials for a specific model. It receives the model name and a dictionary containing the credentials. The credentials' format is defined in the provider's YAML configuration file under `provider_credential_schema` or `model_credential_schema`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n    \"\"\"\n    Validate model credentials\n\n    :param model: model name\n    :param credentials: model credentials\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Base Class\nDESCRIPTION: Defines the `PromptMessage` abstract base class, representing a general prompt message. It includes the `role`, `content` (either a string or a list of `PromptMessageContent` for multimodal support), and an optional `name`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessage(ABC, BaseModel):\n    \"\"\"\n    Model class for prompt message.\n    \"\"\"\n    role: PromptMessageRole  # 消息角色\n    content: Optional[str | list[PromptMessageContent]] = None  # 支持两种类型，字符串和内容列表，内容列表是为了满足多模态的需要，可详见 PromptMessageContent 说明。\n    name: Optional[str] = None  # 名称，可选。\n```\n\n----------------------------------------\n\nTITLE: Define PromptMessageContent Base Class in Python\nDESCRIPTION: This code defines the base class `PromptMessageContent` using Pydantic's `BaseModel`. It includes attributes for the content's `type` (from PromptMessageContentType) and `data` (the content itself as a string). This class serves as a base for specific content type implementations like text and image.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContent(BaseModel):\n    \"\"\"\n    Model class for prompt message content.\n    \"\"\"\n    type: PromptMessageContentType\n    data: str\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Content Base Class in Python\nDESCRIPTION: This code defines the base class for Prompt Message Content, used for declaring parameters. It includes the content type and the data itself.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContent(BaseModel):\n    \"\"\"\n    Model class for prompt message content.\n    \"\"\"\n    type: PromptMessageContentType\n    data: str\n```\n\n----------------------------------------\n\nTITLE: Installing FFmpeg on MacOS\nDESCRIPTION: Installs FFmpeg on macOS using Homebrew for enabling audio processing functionalities. Requires Homebrew to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nbrew install ffmpeg\n```\n\n----------------------------------------\n\nTITLE: Defining RerankDocument Data Model in Python\nDESCRIPTION: Defines a `RerankDocument` class that inherits from `BaseModel`, representing a reranked document.  It includes the original index, the text of the document, and its reranked score.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nclass RerankDocument(BaseModel):\n    \"\"\"\n    Model class for rerank document.\n    \"\"\"\n    index: int  # original index\n    text: str\n    score: float\n```\n\n----------------------------------------\n\nTITLE: Implement GoogleSearchTool in Python\nDESCRIPTION: This Python code implements the `GoogleSearchTool` class, which inherits from `BuiltinTool`. The `_invoke` method executes the tool's logic, taking `user_id` and `tool_Parameters` as input. It retrieves the query and result type from the parameters, uses the SerpAPI to perform the search, and returns either a text or link message based on the `result_type`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/quick-tool-integration.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom core.tools.tool.builtin_tool import BuiltinTool\nfrom core.tools.entities.tool_entities import ToolInvokeMessage\n\nfrom typing import Any, Dict, List, Union\n\nclass GoogleSearchTool(BuiltinTool):\n    def _invoke(self, \n                user_id: str,\n               tool_Parameters: Dict[str, Any], \n        ) -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:\n        \"\"\"\n            invoke tools\n        \"\"\"\n        query = tool_Parameters['query']\n        result_type = tool_Parameters['result_type']\n        api_key = self.runtime.credentials['serpapi_api_key']\n        # TODO: search with serpapi\n        result = SerpAPI(api_key).run(query, result_type=result_type)\n\n        if result_type == 'text':\n            return self.create_text_message(text=result)\n        return self.create_link_message(link=result)\n```\n\n----------------------------------------\n\nTITLE: Convert Tool to Prompt Message Tool\nDESCRIPTION: Converts a `ToolEntity` to a `PromptMessageTool` for integration with the LLM. This involves extracting relevant parameters and their descriptions from the tool's configuration and formatting them into a structure suitable for prompting the LLM.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n    def _convert_tool_to_prompt_message_tool(\n        self, tool: ToolEntity\n    ) -> PromptMessageTool:\n        \"\"\"\n        convert tool to prompt message tool\n        \"\"\"\n        message_tool = PromptMessageTool(\n            name=tool.identity.name,\n            description=tool.description.llm if tool.description else \"\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        )\n\n        parameters = tool.parameters\n        for parameter in parameters:\n            if parameter.form != ToolParameter.ToolParameterForm.LLM:\n                continue\n\n            parameter_type = parameter.type\n            if parameter.type in {\n                ToolParameter.ToolParameterType.FILE,\n                ToolParameter.ToolParameterType.FILES,\n            }:\n                continue\n            enum = []\n            if parameter.type == ToolParameter.ToolParameterType.SELECT:\n                enum = (\n                    [option.value for option in parameter.options]\n                    if parameter.options\n                    else []\n                )\n\n            message_tool.parameters[\"properties\"][parameter.name] = {\n                \"type\": parameter_type,\n                \"description\": parameter.llm_description or \"\",\n            }\n\n            if len(enum) > 0:\n                message_tool.parameters[\"properties\"][parameter.name][\"enum\"] = enum\n\n            if parameter.required:\n                message_tool.parameters[\"required\"].append(parameter.name)\n\n        return message_tool\n```\n\n----------------------------------------\n\nTITLE: Example Response - Update Document Segment - Dify API - JSON\nDESCRIPTION: This JSON represents the successful response after updating a segment of a document. It contains information about the updated segment, including its ID, content, keywords, and status.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [{\n    \"id\": \"\",\n    \"position\": 1,\n    \"document_id\": \"\",\n    \"content\": \"1\",\n    \"answer\": \"1\",\n    \"word_count\": 25,\n    \"tokens\": 0,\n    \"keywords\": [\n        \"a\"\n    ],\n    \"index_node_id\": \"\",\n    \"index_node_hash\": \"\",\n    \"hit_count\": 0,\n    \"enabled\": true,\n    \"disabled_at\": null,\n    \"disabled_by\": null,\n    \"status\": \"completed\",\n    \"created_by\": \"\",\n    \"created_at\": 1695312007,\n    \"indexing_at\": 1695312007,\n    \"completed_at\": 1695312007,\n    \"error\": null,\n    \"stopped_at\": null\n  }],\n  \"doc_form\": \"text_model\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running LiteLLM Proxy with Docker\nDESCRIPTION: This Docker command runs the LiteLLM proxy, mounting the configuration file (litellm_config.yaml) into the container. It exposes port 4000 for accessing the proxy and enables detailed debugging. The proxy acts as an intermediary between the application and multiple LLMs.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_78\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n    -v $(pwd)/litellm_config.yaml:/app/config.yaml \\\n    -p 4000:4000 \\\n    ghcr.io/berriai/litellm:main-latest \\\n    --config /app/config.yaml --detailed_debug\n```\n\n----------------------------------------\n\nTITLE: Variable Message Creation in Dify (Python)\nDESCRIPTION: This Python code snippet defines the interface for creating a variable message within the Dify tool plugin framework. It takes a variable name (string) and a variable value (Any) as input and returns a ToolInvokeMessage object. Later values override earlier ones.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/tool.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_variable_message(self, variable_name: str, variable_value: Any) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response - Connection Error\nDESCRIPTION: This JSON snippet displays an error message indicating a failure to establish a new connection to the OpenAI API. The error is likely due to a temporary failure in name resolution.  A common cause is having a proxy configured in the environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/llms-use-faq.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"error\": \"Error communicating with OpenAl: HTTPSConnectionPool(host='api.openai.com', port=443): Max retriesexceeded with url: /v1/chat/completions (Caused byNewConnectionError( <urllib3.connection.HTTPSConnection object at 0x7f0462ed7af0>; Failed toestablish a new connection: [Errno -3] Temporary failure in name resolution'))\"\n}\n```\n\n----------------------------------------\n\nTITLE: RerankDocument Class Definition in Python\nDESCRIPTION: Defines the `RerankDocument` class as a Pydantic BaseModel representing a single document in the result of a reranking operation. It includes the original index, the text content, and a score.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nclass RerankDocument(BaseModel):\n    \"\"\"\n    Model class for rerank document.\n    \"\"\"\n    index: int  # 原序号\n    text: str  # 分段文本内容\n    score: float  # 分数\n```\n\n----------------------------------------\n\nTITLE: List Datasets with Dify API\nDESCRIPTION: This snippet shows how to retrieve a list of knowledge bases (datasets) from the Dify API. It requires a valid API key. The request includes pagination parameters (`page` and `limit`) to control the number of results returned.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request GET 'https://api.dify.ai/v1/datasets?page=1&limit=20' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: CloudServiceModeration Class Implementation\nDESCRIPTION: This Python code defines the `CloudServiceModeration` class, which inherits from `Moderation` and implements custom content moderation logic.  It includes methods for validating configuration, moderating inputs, and moderating outputs.  The `name` class variable must be unique and match the directory and file name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/code-based-extension/moderation.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom core.moderation.base import Moderation, ModerationAction, ModerationInputsResult, ModerationOutputsResult\n\nclass CloudServiceModeration(Moderation):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"cloud_service\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user save the config.\n\n        Example:\n            .. code-block:: python\n                config = {\n                    \"cloud_provider\": \"GoogleCloud\",\n                    \"api_endpoint\": \"https://api.example.com\",\n                    \"api_keys\": \"123456\",\n                    \"inputs_config\": {\n                        \"enabled\": True,\n                        \"preset_response\": \"Your content violates our usage policy. Please revise and try again.\"\n                    },\n                    \"outputs_config\": {\n                        \"enabled\": True,\n                        \"preset_response\": \"Your content violates our usage policy. Please revise and try again.\"\n                    }\n                }\n\n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n\n        cls._validate_inputs_and_outputs_config(config, True)\n\n        if not config.get(\"cloud_provider\"):\n            raise ValueError(\"cloud_provider is required\")\n\n        if not config.get(\"api_endpoint\"):\n            raise ValueError(\"api_endpoint is required\")\n\n        if not config.get(\"api_keys\"):\n            raise ValueError(\"api_keys is required\")\n\n    def moderation_for_inputs(self, inputs: dict, query: str = \"\") -> ModerationInputsResult:\n        \"\"\"\n        Moderation for inputs.\n\n        :param inputs: user inputs\n        :param query: the query of chat app, there is empty if is completion app\n        :return: the moderation result\n        \"\"\"\n        flagged = False\n        preset_response = \"\"\n\n        if self.config['inputs_config']['enabled']:\n            preset_response = self.config['inputs_config']['preset_response']\n\n            if query:\n                inputs['query__'] = query\n            flagged = self._is_violated(inputs)\n\n        # return ModerationInputsResult(flagged=flagged, action=ModerationAction.overridden, inputs=inputs, query=query)\n        return ModerationInputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)\n\n    def moderation_for_outputs(self, text: str) -> ModerationOutputsResult:\n        \"\"\"\n        Moderation for outputs.\n\n        :param text: the text of LLM response\n        :return: the moderation result\n        \"\"\"\n        flagged = False\n        preset_response = \"\"\n\n        if self.config['outputs_config']['enabled']:\n            preset_response = self.config['outputs_config']['preset_response']\n\n            flagged = self._is_violated({'text': text})\n\n        # return ModerationOutputsResult(flagged=flagged, action=ModerationAction.overridden, text=text)\n        return ModerationOutputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)\n\n    def _is_violated(self, inputs: dict):\n        \"\"\"\n        The main logic of moderation.\n\n        :param inputs:\n        :return: the moderation result\n        \"\"\"\n        return False\n```\n\n----------------------------------------\n\nTITLE: LLM Request with Model Selector (Python)\nDESCRIPTION: This Python code shows how to make an LLM request using a model selected via the UI using the `model-selector` type parameter. The `model_config` is directly retrieved from the `tool_parameters` using `tool_parameters.get('model')`, allowing users to dynamically choose the model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom typing import Any\n\nfrom dify_plugin import Tool\nfrom dify_plugin.entities.model.llm import LLMModelConfig\nfrom dify_plugin.entities.tool import ToolInvokeMessage\nfrom dify_plugin.entities.model.message import SystemPromptMessage, UserPromptMessage\n\nclass LLMTool(Tool):\n    def _invoke(self, tool_parameters: dict[str, Any]) -> Generator[ToolInvokeMessage]:\n        response = self.session.model.llm.invoke(\n            model_config=tool_parameters.get('model'),\n            prompt_messages=[\n                SystemPromptMessage(\n                    content='you are a helpful assistant'\n                ),\n                UserPromptMessage(\n                    content=tool_parameters.get('query')\n                )\n            ],\n            stream=True\n        )\n\n        for chunk in response:\n            if chunk.delta.message:\n                assert isinstance(chunk.delta.message.content, str)\n                yield self.create_text_message(text=chunk.delta.message.content)\n```\n\n----------------------------------------\n\nTITLE: API Request Body Example\nDESCRIPTION: This JSON snippet shows the structure of the request body Dify sends to an API extension. It includes the extension point and parameters required for different modules.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/README.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"point\":  \"string\", //  扩展点，不同模块可能包含多个扩展点\n    \"params\": {\n        ...  // 各模块扩展点传入参数\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: SearXNG Docker Compose Configuration\nDESCRIPTION: This YAML configuration defines the services (SearXNG, Redis, and Caddy) required to run SearXNG using Docker Compose. It specifies the images to use, port mappings, volume mounts for configuration, and network settings.  Redis is used as a caching service, Caddy is used as a reverse proxy and SearXNG is the search service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/searxng.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '3'\n\nservices:\n  searxng:\n    image: searxng/searxng:latest\n    ports:\n      - \"8081:8080\"  # コンテナの8080ポートをホストマシンの8081ポートにマッピング\n    volumes:\n      - ./searxng:/etc/searxng  # SearXNG設定ファイルのマウント設定\n    networks:\n      - searxng_network\n\n  redis:\n    image: valkey/valkey:8-alpine\n    ports:\n      - \"6379:6379\"  # Redisサービスのマッピングポート\n    networks:\n      - searxng_network\n\n  caddy:\n    image: caddy:2-alpine\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    networks:\n      - searxng_network\n\nnetworks:\n  searxng_network:\n    driver: bridge\n```\n\n----------------------------------------\n\nTITLE: Initialize Agent Plugin Template (Bash)\nDESCRIPTION: This command initializes the Agent plugin development template using the Dify plugin scaffolding tool. It prompts the user to enter plugin name, author, description, language, and plugin type.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: Creating a Stream Variable Message in Dify (Python)\nDESCRIPTION: This code creates a stream variable message, enabling a 'typewriter' effect when displaying text.  The `create_stream_variable_message` function takes a variable name and a string value as input. Currently, only string types are supported.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/tool.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef create_stream_variable_message(\n        self, variable_name: str, variable_value: str\n    ) -> ToolInvokeMessage:\n```\n\n----------------------------------------\n\nTITLE: Initializing a new Dify plugin project (Bash)\nDESCRIPTION: This command initializes a new Dify plugin project using the Dify plugin scaffolding tool.  It assumes the binary file is either in the current directory or in the /usr/local/bin path. The plugin name, author, and description are configured interactively.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./dify-plugin-darwin-arm64 plugin init\n```\n\n----------------------------------------\n\nTITLE: Array to Text Conversion - Python Code Node\nDESCRIPTION: This Python code snippet demonstrates how to convert an array of strings to a single string with newline characters using a code node in Dify workflows. The `main` function takes a list of strings named `articleSections` as input and joins the elements with \"/n\" to create a single string assigned to the 'result' key in the returned dictionary. This is useful for converting an array output from the Iteration node to a text-based output.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_71\n\nLANGUAGE: python\nCODE:\n```\ndef main(articleSections: list):\n    data = articleSections\n    return {\n        \"result\": \"/n\".join(data)\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing API Logic in TypeScript\nDESCRIPTION: This TypeScript code snippet demonstrates how to implement the API extension's logic in `src/index.ts`. It fetches a Breaking Bad quote from an external API based on the `count` parameter provided in the request's inputs.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// ⬇️ ここにロジックを実装 ⬇️\n// point === \"app.external_data_tool.query\"\n// https://api.breakingbadquotes.xyz/v1/quotes\nconst count = params?.inputs?.count ?? 1;\nconst url = `https://api.breakingbadquotes.xyz/v1/quotes/${count}`;\nconst result = await fetch(url).then(res => res.text())\n// ⬆️ ここにロジックを実装 ⬆️\n```\n\n----------------------------------------\n\nTITLE: Define TextPromptMessageContent Class in Python\nDESCRIPTION: This code defines the `TextPromptMessageContent` class, which inherits from `PromptMessageContent`. It sets the `type` attribute to `PromptMessageContentType.TEXT`, indicating that this class represents text-based message content.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nclass TextPromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for text prompt message content.\n    \"\"\"\n    type: PromptMessageContentType = PromptMessageContentType.TEXT\n```\n\n----------------------------------------\n\nTITLE: Create Empty Dataset with Dify API\nDESCRIPTION: This snippet demonstrates how to create a new, empty knowledge base (dataset) in Dify using the API.  It requires a valid API key. The request includes the knowledge base's name and permission settings.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"name\": \"name\", \"permission\": \"only_me\"}'\n```\n\n----------------------------------------\n\nTITLE: Packaging Plugin Project (Bash)\nDESCRIPTION: This command packages the Dify plugin project into a `.difypkg` file. Replace `./google` with the actual path to the plugin project directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# 将 ./google 替换为插件项目的实际路径\n\ndify plugin package ./google \n```\n\n----------------------------------------\n\nTITLE: UserPromptMessage Class Definition in Python\nDESCRIPTION: Defines the `UserPromptMessage` class, inheriting from `PromptMessage`, representing a user's message in a prompt. It sets the `role` to `PromptMessageRole.USER`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass UserPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for user prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.USER\n```\n\n----------------------------------------\n\nTITLE: Bearer Authentication Logic (TypeScript)\nDESCRIPTION: This TypeScript snippet demonstrates the Bearer authentication logic using the `hono/bearer-auth` package.  The token is retrieved from the environment variable `c.env.TOKEN`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bearerAuth } from \"hono/bearer-auth\";\n\n(c, next) => {\n    const auth = bearerAuth({ token: c.env.TOKEN });\n    return auth(c, next);\n},\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Remote Debugging\nDESCRIPTION: This snippet shows the required environment variables in the `.env` file for configuring remote debugging of a Dify plugin. It includes the installation method, remote host address, port, and debugging key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: Initializing Plugin Project (Bash - Alternative)\nDESCRIPTION: This command initializes a new Dify plugin project using the plugin scaffolding tool. It assumes the `dify` binary has been renamed and placed in `/usr/local/bin`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: PromptMessageRole Enum Definition in Python\nDESCRIPTION: Defines the PromptMessageRole enumeration, specifying the possible roles for a prompt message: SYSTEM, USER, ASSISTANT, and TOOL. These roles help categorize the origin and purpose of each message within a conversation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageRole(Enum):\n    \"\"\"\n    Enum class for prompt message.\n    \"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    TOOL = \"tool\"\n```\n\n----------------------------------------\n\nTITLE: Copying Environment Configuration File\nDESCRIPTION: This command copies the example environment configuration file (.env.example) to a new file named .env. This .env file will be used to configure the Dify Docker containers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/docker-compose.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Invocation Error Mapping Table in Python\nDESCRIPTION: Defines the `_invoke_error_mapping` property, which maps model invocation errors to unified error types specified by the Runtime. This facilitates Dify to handle different errors differently.  The keys are the InvokeError types, and the values are lists of Exception types thrown by the model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/predefined-model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@property\n    def _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n        \"\"\"\n        Map model invoke error to unified error\n        The key is the error type thrown to the caller\n        The value is the error type thrown by the model,\n        which needs to be converted into a unified error type for the caller.\n\n        :return: Invoke error mapping\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking Custom Tool in Python\nDESCRIPTION: This snippet defines the endpoint for invoking custom tools within a Dify plugin. It takes a provider, tool name, and parameters as input and yields ToolInvokeMessage objects. `provider` is the tool's ID and `tool_name` is the `operation_id` in OpenAPI or the tool name automatically generated by Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/tool.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef invoke_api_tool(\n    self, provider: str, tool_name: str, parameters: dict[str, Any]\n) -> Generator[ToolInvokeMessage, None, None]:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Define JSON Schema for Math Reasoning\nDESCRIPTION: This JSON Schema defines a structured format for representing the steps and final answer in mathematical reasoning. It ensures that the LLM provides the solution in a structured way, containing an array of reasoning steps and a final answer. Each reasoning step consists of an explanation and the output of the step.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/how-to-use-json-schema-in-dify.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"math_reasoning\",\n    \"description\": \"Records steps and final answer for mathematical reasoning\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"steps\": {\n                \"type\": \"array\",\n                \"description\": \"Array of reasoning steps\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"explanation\": {\n                            \"type\": \"string\",\n                            \"description\": \"Explanation of the reasoning step\"\n                        },\n                        \"output\": {\n                            \"type\": \"string\",\n                            \"description\": \"Output of the reasoning step\"\n                        }\n                    },\n                    \"required\": [\"explanation\", \"output\"],\n                    \"additionalProperties\": false\n                }\n            },\n            \"final_answer\": {\n                \"type\": \"string\",\n                \"description\": \"The final answer to the mathematical problem\"\n            }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\"steps\", \"final_answer\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Completion Interface Invoke Method Definition (Python)\nDESCRIPTION: Defines the `invoke` method for the Completion Interface, allowing plugins to interact with Completion (text generation) applications. It outlines the parameters for `app_id`, `inputs`, `response_mode`, and `files`. The return type is either a generator of dictionaries for streaming or a single dictionary for blocking mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self,\n    app_id: str,\n    inputs: dict,\n    response_mode: Literal[\"streaming\", \"blocking\"],\n    files: list,\n) -> Generator[dict, None, None] | dict:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema Template\nDESCRIPTION: This JSON schema provides a generic template for defining JSON data structures, which includes a name, description, and a strict mode setting. The schema defines an object with various fields, each with a specified type (string, number, array, object) and description. It also defines constraints using the `required` property and ensures no additional properties are allowed by setting `additionalProperties` to false.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/extended-reading/how-to-use-json-schema-in-dify.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"template_schema\",\n    \"description\": \"A generic template for JSON Schema\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"field1\": {\n                \"type\": \"string\",\n                \"description\": \"Description of field1\"\n            },\n            \"field2\": {\n                \"type\": \"number\",\n                \"description\": \"Description of field2\"\n            },\n            \"field3\": {\n                \"type\": \"array\",\n                \"description\": \"Description of field3\",\n                \"items\": {\n                    \"type\": \"string\"\n                }\n            },\n            \"field4\": {\n                \"type\": \"object\",\n                \"description\": \"Description of field4\",\n                \"properties\": {\n                    \"subfield1\": {\n                        \"type\": \"string\",\n                        \"description\": \"Description of subfield1\"\n                    }\n                },\n                \"required\": [\"subfield1\"],\n                \"additionalProperties\": false\n            }\n        },\n        \"required\": [\"field1\", \"field2\", \"field3\", \"field4\"],\n        \"additionalProperties\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Upgrade Dify Version via Git\nDESCRIPTION: These commands switch to the specified Dify version using Git and then restart the Docker containers using Docker Compose.  This upgrades the Dify installation to the specified version.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/dify-premium.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout 1.0.0 # Switch to the 1.0.0 branch\ncd docker\ndocker compose -f docker-compose.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Specifying Tool Configuration File (YAML)\nDESCRIPTION: This YAML snippet specifies the location of the tool configuration file within the plugin project. The `google.yaml` file is expected to be an absolute path within the plugin project's file structure. It enables loading the specific tool definition.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nplugins:\n  tools:\n    - \"google.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Defining Frontend Component with JSON Schema\nDESCRIPTION: This JSON schema defines the structure and validation rules for the 'Cloud Service' moderation type's frontend configuration. It includes fields for selecting the cloud provider, specifying the API endpoint, and providing the API key. The schema also provides internationalized labels (en-US, zh-Hans) for each field.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/code-based-extension/moderation.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"label\": {\n        \"en-US\": \"Cloud Service\",\n        \"zh-Hans\": \"云服务\"\n    },\n    \"form_schema\": [\n        {\n            \"type\": \"select\",\n            \"label\": {\n                \"en-US\": \"Cloud Provider\",\n                \"zh-Hans\": \"云厂商\"\n            },\n            \"variable\": \"cloud_provider\",\n            \"required\": true,\n            \"options\": [\n                {\n                    \"label\": {\n                        \"en-US\": \"AWS\",\n                        \"zh-Hans\": \"亚马逊\"\n                    },\n                    \"value\": \"AWS\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Google Cloud\",\n                        \"zh-Hans\": \"谷歌云\"\n                    },\n                    \"value\": \"GoogleCloud\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Azure Cloud\",\n                        \"zh-Hans\": \"微软云\"\n                    },\n                    \"value\": \"Azure\"\n                }\n            ],\n            \"default\": \"GoogleCloud\",\n            \"placeholder\": \"\"\n        },\n        {\n            \"type\": \"text-input\",\n            \"label\": {\n                \"en-US\": \"API Endpoint\",\n                \"zh-Hans\": \"API Endpoint\"\n            },\n            \"variable\": \"api_endpoint\",\n            \"required\": true,\n            \"max_length\": 100,\n            \"default\": \"\",\n            \"placeholder\": \"https://api.example.com\"\n        },\n        {\n            \"type\": \"paragraph\",\n            \"label\": {\n                \"en-US\": \"API Key\",\n                \"zh-Hans\": \"API Key\"\n            },\n            \"variable\": \"api_keys\",\n            \"required\": true,\n            \"default\": \"\",\n            \"placeholder\": \"Paste your API key here\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Docker Host from Container\nDESCRIPTION: This shows the correct URL to use to access the Ollama service from within a Docker container. It replaces `localhost` with `host.docker.internal`, which allows the container to connect to the host machine's network.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_80\n\nLANGUAGE: Text\nCODE:\n```\nhttp://host.docker.internal:11434\n```\n\n----------------------------------------\n\nTITLE: Create Document by File with Dify API\nDESCRIPTION: This snippet shows how to create a new document in an existing Dify knowledge base by uploading a file.  It requires the `dataset_id` of the knowledge base and a valid API key. The request uses the `--form` option to send both metadata (indexing technique, processing rules) and the file itself.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/document/create-by-file' \\\n--header 'Authorization: Bearer {api_key}' \\\n--form 'data=\"{\"indexing_technique\":\"high_quality\",\"process_rule\":{\"rules\":{\"pre_processing_rules\":[{\"id\":\"remove_extra_spaces\",\"enabled\":true},{\"id\":\"remove_urls_emails\",\"enabled\":true}],\"segmentation\":{\"separator\":\"###\",\"max_tokens\":500}},\"mode\":\"custom\"}}\";type=text/plain' \\\n--form 'file=@\"/path/to/file\"'\n```\n\n----------------------------------------\n\nTITLE: Parameter Extractor Node Entry\nDESCRIPTION: This code snippet shows the entry point for accessing the Parameter Extractor node within a Dify plugin's session.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.session.workflow_node.parameter_extractor\n```\n\n----------------------------------------\n\nTITLE: Calling Chat Messages API with cURL\nDESCRIPTION: This cURL command demonstrates how to call the `/v1/chat-messages` endpoint to send chat messages to a Dify application. It sets the `Authorization` header with a Bearer token, specifies the `Content-Type` as `application/json`, and includes `inputs`, `query`, `response_mode`, `conversation_id`, and `user` parameters in the JSON payload. Replace `ENTER-YOUR-SECRET-KEY` with your actual API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-publishing/developing-with-apis.md#_snippet_2\n\nLANGUAGE: cURL\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/chat-messages' \\\n--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"inputs\": {},\n    \"query\": \"eh\",\n    \"response_mode\": \"streaming\",\n    \"conversation_id\": \"1c7e55fb-1ba2-4e10-81b5-30addcea2276\",\n    \"user\": \"abc-123\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Defining Model YAML Configuration\nDESCRIPTION: This YAML configuration defines the model 'claude-2.1' with its properties, features, and pricing.  It specifies the model type as 'llm' and includes parameters such as temperature, top_p, and max_tokens_to_sample. The configuration also defines pricing for input and output tokens.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/predefined-model.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodel: claude-2.1  # 模型标识\n# 模型展示名称，可设置 en_US 英文、zh_Hans 中文两种语言，zh_Hans 不设置将默认使用 en_US。\n# 也可不设置 label，则使用 model 标识内容。\nlabel:\n  en_US: claude-2.1\nmodel_type: llm  # 模型类型，claude-2.1 为 LLM\nfeatures:  # 支持功能，agent-thought 为支持 Agent 推理，vision 为支持图片理解\n- agent-thought\nmodel_properties:  # 模型属性\n  mode: chat  # LLM 模式，complete 文本补全模型，chat 对话模型\n  context_size: 200000  # 支持最大上下文大小\nparameter_rules:  # 模型调用参数规则，仅 LLM 需要提供\n- name: temperature  # 调用参数变量名\n  # 默认预置了 5 种变量内容配置模板，temperature/top_p/max_tokens/presence_penalty/frequency_penalty\n  # 可在 use_template 中直接设置模板变量名，将会使用 entities.defaults.PARAMETER_RULE_TEMPLATE 中的默认配置\n  # 若设置了额外的配置参数，将覆盖默认配置\n  use_template: temperature\n- name: top_p\n  use_template: top_p\n- name: top_k\n  label:  # 调用参数展示名称\n    zh_Hans: 取样数量\n    en_US: Top k\n  type: int  # 参数类型，支持 float/int/string/boolean\n  help:  # 帮助信息，描述参数作用\n    zh_Hans: 仅从每个后续标记的前 K 个选项中采样。\n    en_US: Only sample from the top K options for each subsequent token.\n  required: false  # 是否必填，可不设置\n- name: max_tokens_to_sample\n  use_template: max_tokens\n  default: 4096  # 参数默认值\n  min: 1  # 参数最小值，仅 float/int 可用\n  max: 4096  # 参数最大值，仅 float/int 可用\npricing:  # 价格信息\n  input: '8.00'  # 输入单价，即 Prompt 单价\n  output: '24.00'  # 输出单价，即返回内容单价\n  unit: '0.000001'  # 价格单位，即上述价格为每 100K 的单价\n  currency: USD  # 价格货币\n```\n\n----------------------------------------\n\nTITLE: Creating Agent Log Messages in Python\nDESCRIPTION: This code defines functions for creating and finishing log messages within the Dify Agent Plugin. The `create_log_message` function creates an `AgentLogMessage`, representing a node in the log tree, and the `finish_log_message` function updates the status of an existing log message.  The example shows how these can be used to mark a log message as \"start\" (thinking) and then \"Success\" once the action is complete.  It provides an example data structure.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/agent.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef create_log_message(\n    self,\n    label: str,\n    data: Mapping[str, Any],\n    status: AgentInvokeMessage.LogMessage.LogStatus = AgentInvokeMessage.LogMessage.LogStatus.SUCCESS,\n    parent: AgentInvokeMessage | None = None,\n) -> AgentInvokeMessage\n\ndef finish_log_message(\n    self,\n    log: AgentInvokeMessage,\n    status: AgentInvokeMessage.LogMessage.LogStatus = AgentInvokeMessage.LogMessage.LogStatus.SUCCESS,\n    error: Optional[str] = None,\n) -> AgentInvokeMessage\n```\n\n----------------------------------------\n\nTITLE: Update Document Segment - Dify API - Bash\nDESCRIPTION: This snippet demonstrates how to update a segment of a document within a Dify dataset using a POST request. It requires the dataset ID, document ID, and segment ID. The request includes headers for authorization (API key) and content type (application/json), along with the segment data in JSON format. The 'segment' parameter contains the content, answer, keywords and enabled status.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments/{segment_id}' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json'\\\n--data-raw '{\"segment\": {\"content\": \"1\",\"answer\": \"1\", \"keywords\": [\"a\"], \"enabled\": false}}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Settings Form (slack.yaml)\nDESCRIPTION: This YAML code defines the settings form for the Slack Bot plugin.  It includes fields for the Bot Token, whether to allow retries, and an App selector for choosing the Dify App to use. It's located in the `group` directory, and the filename corresponds to the basic information input during plugin creation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsettings:\n  - name: bot_token\n    type: secret-input\n    required: true\n    label:\n      en_US: Bot Token\n      zh_Hans: Bot Token\n      pt_BR: Token do Bot\n      ja_JP: Bot Token\n    placeholder:\n      en_US: Please input your Bot Token\n      zh_Hans: 请输入你的 Bot Token\n      pt_BR: Por favor, insira seu Token do Bot\n      ja_JP: ボットトークンを入力してください\n  - name: allow_retry\n    type: boolean\n    required: false\n    label:\n      en_US: Allow Retry\n      zh_Hans: 允许重试\n      pt_BR: Permitir Retentativas\n      ja_JP: 再試行を許可\n    default: false\n  - name: app\n    type: app-selector\n    required: true\n    label:\n      en_US: App\n      zh_Hans: 应用\n      pt_BR: App\n      ja_JP: アプリ\n    placeholder:\n      en_US: the app you want to use to answer Slack messages\n      zh_Hans: 你想要用来回答 Slack 消息的应用\n      pt_BR: o app que você deseja usar para responder mensagens do Slack\n      ja_JP: あなたが Slack メッセージに回答するために使用するアプリ\nendpoints:\n  - endpoints/slack.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating a Text Message in Dify (Python)\nDESCRIPTION: This code snippet illustrates how to create a text message within a Dify tool plugin. It accepts a text string as input and returns a `ToolInvokeMessage`.  The text is displayed to the user as is.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/tool.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef create_text_message(self, text: str) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: TextPromptMessageContent Class Definition in Python\nDESCRIPTION: Defines the `TextPromptMessageContent` class, which inherits from `PromptMessageContent`. It represents text-based content for a prompt message.  It sets the `type` to `PromptMessageContentType.TEXT` by default.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass TextPromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for text prompt message content.\n    \"\"\"\n    type: PromptMessageContentType = PromptMessageContentType.TEXT\n```\n\n----------------------------------------\n\nTITLE: LiteLLM Configuration YAML\nDESCRIPTION: This YAML file defines the models available through LiteLLM, including their Azure OpenAI endpoint details, API keys, and model names. It specifies the `model_name` and `litellm_params` which include the `model`, `api_base`, `api_version`, and `api_key` for each model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/litellm.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_list:\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/chatgpt-v-2\n      api_base: https://openai-gpt-4-test-v-1.openai.azure.com/\n      api_version: \"2023-05-15\"\n      api_key: \n  - model_name: gpt-4\n    litellm_params:\n      model: azure/gpt-4\n      api_key: \n      api_base: https://openai-gpt-4-test-v-2.openai.azure.com/\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/gpt-4\n      api_key: \n      api_base: https://openai-gpt-4-test-v-2.openai.azure.com/\n```\n\n----------------------------------------\n\nTITLE: Invoke Error Mapping\nDESCRIPTION: Defines the `_invoke_error_mapping` property to map model-specific exception types to unified `InvokeError` types. This allows the system to handle different error scenarios consistently (e.g., connection errors, rate limits, authorization failures). The returned dictionary maps `InvokeError` subclasses to lists of exception types thrown by the underlying model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/predefined-model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Image Prompt Message Content Class\nDESCRIPTION: Defines the `ImagePromptMessageContent` class, a subclass of `PromptMessageContent`, specifically for image-based content in prompts. It allows setting the detail level (LOW or HIGH) and takes image data as a URL or base64 encoded string.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass ImagePromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for image prompt message content.\n    \"\"\"\n    class DETAIL(Enum):\n        LOW = 'low'\n        HIGH = 'high'\n\n    type: PromptMessageContentType = PromptMessageContentType.IMAGE\n    detail: DETAIL = DETAIL.LOW  # 分辨率\n```\n\n----------------------------------------\n\nTITLE: Converting Array to Text using Django Template Node\nDESCRIPTION: This code snippet shows how to convert an array of strings into a single string, separated by newline characters, using a Django template node. It uses the `join` filter to concatenate the elements of the `articleSections` array with newline characters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/iteration.md#_snippet_1\n\nLANGUAGE: django\nCODE:\n```\n{{ articleSections | join(\"\\n\") }}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Remote Debugging\nDESCRIPTION: This snippet demonstrates how to set environment variables in a `.env` file for remote debugging a Dify plugin. It includes `INSTALL_METHOD`, `REMOTE_INSTALL_HOST`, `REMOTE_INSTALL_PORT`, and `REMOTE_INSTALL_KEY` parameters, which are essential for configuring the plugin's remote installation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: Anthropic Model Configuration YAML\nDESCRIPTION: This YAML configuration file defines the basic information, supported model types, configuration methods, and credential rules for the Anthropic model provider. It includes API key configuration, model definitions, and references to Python source files.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprovider: anthropic\nlabel:\n  en_US: Anthropic\ndescription:\n  en_US: Anthropic's powerful models, such as Claude 3.\n  zh_Hans: Anthropic 的强大模型，例如 Claude 3。\nicon_small:\n  en_US: icon_s_en.svg\nicon_large:\n  en_US: icon_l_en.svg\nbackground: \"#F0F0EB\"\nhelp:\n  title:\n    en_US: Get your API Key from Anthropic\n    zh_Hans: 从 Anthropic 获取 API Key\n  url:\n    en_US: https://console.anthropic.com/account/keys\nsupported_model_types:\n  - llm\nconfigurate_methods:\n  - predefined-model\nprovider_credential_schema:\n  credential_form_schemas:\n    - variable: anthropic_api_key\n      label:\n        en_US: API Key\n      type: secret-input\n      required: true\n      placeholder:\n        zh_Hans: 在此输入您的 API Key\n        en_US: Enter your API Key\n    - variable: anthropic_api_url\n      label:\n        en_US: API URL\n      type: text-input\n      required: false\n      placeholder:\n        zh_Hans: 在此输入您的 API URL\n        en_US: Enter your API URL\nmodels:\n  llm:\n    predefined:\n      - \"models/llm/*.yaml\"\n    position: \"models/llm/_position.yaml\"\nextra:\n  python:\n    provider_source: provider/anthropic.py\n    model_sources:\n      - \"models/llm/llm.py\"\n```\n\n----------------------------------------\n\nTITLE: Jinja2 Template for Chunk Formatting (Python)\nDESCRIPTION: This Jinja2 template structures retrieved chunks and their metadata from a knowledge retrieval node into a formatted markdown. It iterates through each chunk, displaying its index, similarity score, title, and content, separated by a horizontal rule.  It's useful for combining data from multiple sources into a specific structure required by subsequent nodes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_70\n\nLANGUAGE: jinja2\nCODE:\n```\n{% for item in chunks %}\n### Chunk {{ loop.index }}.\n### Similarity: {{ item.metadata.score | default('N/A') }}\n\n#### {{ item.title }}\n\n##### Content\n{{ item.content | replace('\\n', '\\n\\n') }}\n\n---\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Role Enum Definition\nDESCRIPTION: Defines an enumeration for prompt message roles, including system, user, assistant, and tool. This enum is used to specify the role of a message within a prompt.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageRole(Enum):\n    \"\"\"\n    Enum class for prompt message.\n    \"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    TOOL = \"tool\"\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Credentials Validation in XinferenceProvider (Python)\nDESCRIPTION: This example demonstrates a minimal implementation of the `validate_provider_credentials` method for a custom model provider, specifically for Xinference. It simply passes without performing any actual validation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass XinferenceProvider(Provider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Chat Interface Invoke Specification Python\nDESCRIPTION: Defines the invoke method for chat-type applications within Dify, specifying parameters for app_id, inputs, response_mode (streaming or blocking), conversation_id, and files. It returns a generator of dictionaries for streaming mode or a single dictionary for blocking mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self,\n    app_id: str,\n    inputs: dict,\n    response_mode: Literal[\"streaming\", \"blocking\"],\n    conversation_id: str,\n    files: list,\n) -> Generator[dict, None, None] | dict:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Setting up CUDA environment variables\nDESCRIPTION: This snippet modifies the `~/.bashrc` file to include CUDA-related paths in the environment variables. This is essential for accessing CUDA libraries and executables.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64\nexport PATH=$PATH:/usr/local/cuda-12.2/lib64\n```\n\n----------------------------------------\n\nTITLE: Getting Customizable Model Schema (Python)\nDESCRIPTION: This method retrieves the customizable model schema for a given model.  It is intended for use when a provider supports adding custom LLMs, allowing users to access model rules.  By default, it returns None if no custom schema is available.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> Optional[AIModelEntity]:\n    \"\"\"\n    Get customizable model schema\n\n    :param model: model name\n    :param credentials: model credentials\n    :return: model schema\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Base Class Definition\nDESCRIPTION: This code snippet defines the abstract base class for all prompt messages. It includes attributes for 'role', 'content' (which can be a string or a list of PromptMessageContent for multimodal prompts), and an optional 'name'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessage(ABC, BaseModel):\n    \"\"\"\n    Model class for prompt message.\n    \"\"\"\n    role: PromptMessageRole  # メッセージの役割\n    content: Optional[str | list[PromptMessageContent]] = None  # 文字列またはコンテンツリストのいずれかを指定できます。コンテンツリストはマルチモーダルに対応するためのもので、詳細は PromptMessageContent の説明を参照してください。\n    name: Optional[str] = None  # 名前（オプション）\n```\n\n----------------------------------------\n\nTITLE: Define Basic Parameters Model\nDESCRIPTION: Defines a Pydantic model `BasicParams` to structure the input parameters required for the agent strategy. This model includes parameters for maximum iterations, the LLM model configuration, a list of available tools, and the user query.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n```\n\n----------------------------------------\n\nTITLE: Upgrading Dify on EC2\nDESCRIPTION: These commands are used to upgrade the Dify instance running on an EC2 instance. It clones the latest Dify repository, moves the Docker configuration files, removes the temporary directory, and restarts the Docker containers with the updated configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/dify-premium.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git /tmp/dify\nmv -f /tmp/dify/docker/* /dify/\nrm -rf /tmp/dify\ndocker-compose down\ndocker-compose pull\ndocker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Invoking Parameter Extractor for Name Extraction\nDESCRIPTION: This code demonstrates how to invoke the ParameterExtractor node to extract a person's name from a given text. It sets up the necessary `ParameterConfig` and `ModelConfig` and then calls the `invoke` method. The extracted name is then yielded as a text message.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom dify_plugin.entities.tool import ToolInvokeMessage\nfrom dify_plugin import Tool\nfrom dify_plugin.entities.workflow_node import ModelConfig, ParameterConfig\n\nclass ParameterExtractorTool(Tool):\n    def _invoke(\n        self, tool_parameters: dict\n    ) -> Generator[ToolInvokeMessage, None, None]:\n        response = self.session.workflow_node.parameter_extractor.invoke(\n            parameters=[\n                ParameterConfig(\n                    name=\"name\",\n                    description=\"name of the person\",\n                    required=True,\n                    type=\"string\",\n                )\n            ],\n            model=ModelConfig(\n                provider=\"langgenius/openai/openai\",\n                name=\"gpt-4o-mini\",\n                completion_params={},\n            ),\n            query=\"My name is John Doe\",\n            instruction=\"Extract the name of the person\",\n        )\n        yield self.create_text_message(response.outputs[\"name\"])\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Anthropic LLM Models\nDESCRIPTION: This bash snippet illustrates the directory structure for Anthropic LLM models, showing the location of configuration files (YAML) and the main model code (llm.py). It demonstrates the organization of model definitions within the llm subdirectory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n├── models\n│   └── llm\n│       ├── _position.yaml\n│       ├── claude-2.1.yaml\n│       ├── claude-2.yaml\n│       ├── claude-3-5-sonnet-20240620.yaml\n│       ├── claude-3-haiku-20240307.yaml\n│       ├── claude-3-opus-20240229.yaml\n│       ├── claude-3-sonnet-20240229.yaml\n│       ├── claude-instant-1.2.yaml\n│       ├── claude-instant-1.yaml\n│       └── llm.py\n```\n\n----------------------------------------\n\nTITLE: Getting Storage Value by Key in Dify Plugin\nDESCRIPTION: This snippet defines the endpoint for retrieving a value from the plugin's persistent storage using a given key. It returns the value as bytes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/persistent-storage.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get(self, key: str) -> bytes:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Implementing Plugin Functionality (Python)\nDESCRIPTION: This Python code implements the plugin's functionality, which involves returning a cat-themed ASCII art animation. It uses Flask to create an endpoint that serves an HTML page with JavaScript to animate the ASCII art.  Dependencies include werkzeug, flask, and dify-plugin.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Mapping\nfrom werkzeug import Request, Response\nfrom flask import Flask, render_template_string\nfrom dify_plugin import Endpoint\n\napp = Flask(__name__)\n\nclass NekoEndpoint(Endpoint):\n    def _invoke(self, r: Request, values: Mapping, settings: Mapping) -> Response:\n        ascii_art = '''\n⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛⬛️⬜️⬜️⬜️⬜️⬜⬜️⬜️️\n🟥🟥⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️🟥🟥🟥🟥🟥🟥🟥🟥⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬛🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧⬛️⬜️⬜️⬜️⬜️⬜⬜️️\n🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥⬛️🥧🥧🥧💟💟💟💟💟💟💟💟💟💟💟💟💟🥧🥧🥧⬛️⬜️⬜️⬜️⬜⬜️️\n🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥⬛️🥧🥧💟💟💟💟💟💟🍓💟💟🍓💟💟💟💟💟🥧🥧⬛️⬜️⬜️⬜️⬜️⬜️️\n🟧🟧🟥🟥🟥🟥🟥🟥🟥🟥🟧🟧🟧🟧🟧🟧🟧🟧🟥🟥🟥🟥🟥🟥🟥⬛🥧💟💟🍓💟💟💟💟💟💟💟💟💟💟💟💟💟💟🥧⬛️⬜️⬜️⬜️⬜⬜️️\n🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧⬛️🥧💟💟💟💟💟💟💟💟💟💟⬛️⬛️💟💟🍓💟💟🥧⬛️⬜️⬛️️⬛️️⬜⬜️️\n🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧⬛️🥧💟💟💟💟💟💟💟💟💟⬛️🌫🌫⬛💟💟💟💟🥧⬛️⬛️🌫🌫⬛⬜️️\n🟨🟨🟧🟧🟧🟧🟧🟧🟧🟧🟨🟨🟨🟨🟨🟨🟨🟨🟧⬛️⬛️⬛️⬛️🟧🟧⬛️🥧💟💟💟💟💟💟🍓💟💟⬛️🌫🌫🌫⬛💟💟💟🥧⬛️🌫🌫🌫⬛⬜️️\n🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨⬛️🌫🌫⬛️⬛️🟧⬛️🥧💟💟💟💟💟💟💟💟💟⬛️🌫🌫🌫🌫⬛️⬛️⬛️⬛️🌫🌫🌫🌫⬛⬜️️\n🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨⬛️⬛️🌫🌫⬛️⬛️⬛️🥧💟💟💟🍓💟💟💟💟💟⬛️🌫🌫🌫🌫🌫🌫🌫🌫🌫🌫🌫🌫⬛⬜️️\n🟩🟩🟨🟨🟨🟨🟨🟨🟨🟨🟩🟩🟩🟩🟩🟩🟩🟩🟨🟨⬛⬛️🌫🌫⬛️⬛️🥧💟💟💟💟💟💟💟🍓⬛️🌫🌫🌫⬜️⬛️🌫🌫🌫🌫🌫⬜️⬛️🌫🌫⬛️\n🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩⬛️⬛️🌫🌫⬛️🥧💟🍓💟💟💟💟💟💟⬛️🌫🌫🌫⬜️⬛️🌫🌫🌫⬛️🌫⬛️⬛️🌫🌫⬛️\n️🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩⬛️⬛️⬛️⬛️🥧💟💟💟💟💟💟💟💟⬛️🌫🌫🌫⬛️⬛️🌫🌫🌫⬛️🌫⬛️⬛️🌫🌫⬛️\n🟦🟦🟩🟩🟩🟩🟩🟩🟩🟩🟦🟦🟦🟦🟦🟦🟦🟦🟩🟩🟩🟩🟩🟩⬛️⬛️🥧💟💟💟💟💟🍓💟💟⬛🌫🟥🟥🌫🌫🌫🌫🌫🌫🌫🌫🌫🟥🟥⬛️\n🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦⬛️🥧🥧💟🍓💟💟💟💟💟⬛️🌫🟥🟥🌫⬛️🌫🌫⬛️🌫🌫⬛️🌫🟥🟥⬛️\n🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦⬛️🥧🥧🥧💟💟💟💟💟💟💟⬛️🌫🌫🌫⬛️⬛️⬛️⬛️⬛️⬛️⬛️🌫🌫⬛️⬜️\n🟪🟪🟦🟦🟦🟦🟦🟦🟦🟦🟪🟪🟪🟪🟪🟪🟪🟪🟦🟦🟦🟦🟦🟦⬛️⬛️⬛️🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧⬛️🌫🌫🌫🌫🌫🌫🌫🌫🌫🌫⬛️⬜️⬜️\n🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪⬛️🌫🌫🌫⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬜️⬜️⬜️\n🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪⬛️🌫🌫⬛️⬛️⬜️⬛️🌫🌫⬛️⬜️⬜️⬜️⬜️⬜️⬛️🌫🌫⬛️⬜️⬛️🌫🌫⬛️⬜️⬜️⬜️⬜️\n⬜️⬜️🟪🟪🟪🟪🟪🟪🟪🟪⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️🟪🟪🟪🟪🟪⬛️⬛️⬛️⬛⬜️⬜️⬛️⬛️⬛️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬛️⬛️⬛️⬜️⬜️⬛️⬛️⬜️⬜️⬜️⬜️⬜️️\n        '''\n        ascii_art_lines = ascii_art.strip().split('\\n')\n        with app.app_context():\n            return Response(render_template_string('''\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <style>\n            body {\n                background-color: black;\n                color: white;\n                overflow: hidden;\n                margin: 0;\n                padding: 0;\n            }\n            #ascii-art {\n                font-family: monospace;\n                white-space: pre;\n                position: absolute;\n                top: 50%;\n                transform: translateY(-50%);\n                display: inline-block;\n                font-size: 16px;\n                line-height: 1;\n            }\n        </style>\n    </head>\n    <body>\n        <div id=\"ascii-art\"></div>\n        <script>\n            var asciiArtLines = {{ ascii_art_lines | tojson }};\n            var asciiArtDiv = document.getElementById(\"ascii-art\");\n            var index = 0;\n            function displayNextLine() {\n                if (index < asciiArtLines.length) {\n                    var line = asciiArtLines[index];\n                    var lineElement = document.createElement(\"div\");\n                    lineElement.innerHTML = line;\n                    asciiArtDiv.appendChild(lineElement);\n                    index++;\n                    setTimeout(displayNextLine, 100);\n                } else {\n                    animateCat();\n                }\n            }\n            function animateCat() {\n                var pos = 0;\n                var screenWidth = window.innerWidth;\n                var catWidth = asciiArtDiv.offsetWidth;\n                function move() {\n                    asciiArtDiv.style.left = pos + \"px\";\n                    pos += 2;\n                    if (pos > screenWidth) {\n                        pos = -catWidth;\n                    }\n                    requestAnimationFrame(move);\n                }\n                move();\n            }\n            displayNextLine();\n        </script>\n    </body>\n    </html>\n        ''', ascii_art_lines=ascii_art_lines), status=200, content_type=\"text/html\")\n```\n\n----------------------------------------\n\nTITLE: Invoke Error Mapping Property\nDESCRIPTION: This python code defines a property that maps model invocation errors to unified error types. This allows Dify to handle different errors appropriately based on the type of exception raised during model invocation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Python script to merge markdown files\nDESCRIPTION: This Python script concatenates all .md files in a given directory into a single output file. It requires the input directory path as a command-line argument and creates a combined Markdown file named 'output.md' in the same directory.  This is used to prepare training data for the Dify knowledge base.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/train-a-qa-chatbot-that-belongs-to-you.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport sys\n\ndef merge_markdown_files(directory):\n    \"\"\"Merges all markdown files in a directory into a single output file.\"\"\"\n    output_file = os.path.join(directory, \"output.md\")\n    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n        for filename in os.listdir(directory):\n            if filename.endswith(\".md\"):\n                filepath = os.path.join(directory, filename)\n                with open(filepath, \"r\", encoding=\"utf-8\") as infile:\n                    outfile.write(infile.read())\n                    outfile.write(\"\\n\")  # Add a newline between files\n    print(f\"Merged markdown files into: {output_file}\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python merge_md.py <directory>\")\n        sys.exit(1)\n    directory = sys.argv[1]\n    merge_markdown_files(directory)\n```\n\n----------------------------------------\n\nTITLE: Concatenating Data in Dify using Python\nDESCRIPTION: This Python code snippet demonstrates how to concatenate two lists of data, `knowledge1` and `knowledge2`, within a Dify workflow. The `main` function takes two lists as input and returns a dictionary with the combined list under the key `result`. This is useful for merging data from multiple sources.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_68\n\nLANGUAGE: Python\nCODE:\n```\ndef main(knowledge1: list, knowledge2: list) -> list:\n    return {\n        # Note to declare 'result' in the output variables\n        'result': knowledge1 + knowledge2\n    }\n```\n\n----------------------------------------\n\nTITLE: Cloudflare Worker Content Moderation Implementation (TypeScript)\nDESCRIPTION: This TypeScript code snippet demonstrates a Cloudflare Worker implementation for content moderation. It uses the Hono framework and zod for validation and defines keyword-based filtering logic for both user input and LLM output.  It requires the `@hono/zod-validator`, `hono`, `zod`, `@anatine/zod-openapi` dependencies and a `TOKEN` environment variable for Bearer authentication.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/moderation.md#_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { Hono } from \"hono\";\nimport { bearerAuth } from \"hono/bearer-auth\";\nimport { z } from \"zod\";\nimport { zValidator } from \"@hono/zod-validator\";\nimport { generateSchema } from '@anatine/zod-openapi';\n\ntype Bindings = {\n  TOKEN: string;\n};\n\nconst app = new Hono<{ Bindings: Bindings }>();\n\n// API 格式校验 ⬇️\nconst schema = z.object({\n  point: z.union([\n    z.literal(\"ping\"),\n    z.literal(\"app.external_data_tool.query\"),\n    z.literal(\"app.moderation.input\"),\n    z.literal(\"app.moderation.output\"),\n  ]), // Restricts 'point' to two specific values\n  params: z\n    .object({\n      app_id: z.string().optional(),\n      tool_variable: z.string().optional(),\n      inputs: z.record(z.any()).optional(),\n      query: z.any(),\n      text: z.any()\n    })\n    .optional(),\n});\n\n\n// Generate OpenAPI schema\napp.get(\"/\", (c) => {\n  return c.json(generateSchema(schema));\n});\n\napp.post(\n  \"/\",\n  (c, next) => {\n    const auth = bearerAuth({ token: c.env.TOKEN });\n    return auth(c, next);\n  },\n  zValidator(\"json\", schema),\n  async (c) => {\n    const { point, params } = c.req.valid(\"json\");\n    if (point === \"ping\") {\n      return c.json({\n        result: \"pong\",\n      });\n    }\n    // ⬇️ impliment your logic here ⬇️\n    // point === \"app.external_data_tool.query\"\n    else if (point === \"app.moderation.input\"){\n    // 输入检查 ⬇️\n    const inputkeywords = [\"输入过滤测试1\", \"输入过滤测试2\", \"输入过滤测试3\"];\n\n    if (inputkeywords.some(keyword => params.query.includes(keyword)))\n      {\n      return c.json({\n        \"flagged\": true,\n        \"action\": \"direct_output\",\n        \"preset_response\": \"输入存在违法内容，请换个问题再试！\"\n      });\n    } else {\n      return c.json({\n        \"flagged\": false,\n        \"action\": \"direct_output\",\n        \"preset_response\": \"输入无异常\"\n      });\n    }\n    // 输入检查完毕 \n    }\n    \n    else {\n      // 输出检查 ⬇️\n      const outputkeywords = [\"输出过滤测试1\", \"输出过滤测试2\", \"输出过滤测试3\"]; \n\n  if (outputkeywords.some(keyword => params.text.includes(keyword)))\n    {\n      return c.json({\n        \"flagged\": true,\n        \"action\": \"direct_output\",\n        \"preset_response\": \"输出存在敏感内容，已被系统过滤，请换个问题再问！\"\n      });\n    }\n  \n  else {\n    return c.json({\n      \"flagged\": false,\n      \"action\": \"direct_output\",\n      \"preset_response\": \"输出无异常\"\n    });\n  };\n    }\n    // 输出检查完毕 \n  }\n);\n\nexport default app;\n\n```\n\n----------------------------------------\n\nTITLE: Dify SearXNG Base URL Configuration\nDESCRIPTION: This text provides the format for the Base URL required to connect the Dify platform with the self-hosted SearXNG instance running on a Linux VM.  It specifies the VM's IP address and the port 8081.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nhttp://<your-linux-vm-ip>:8081\n```\n\n----------------------------------------\n\nTITLE: Moderation Output Request Body JSON\nDESCRIPTION: Defines the structure for the request body of the `app.moderation.output` extension point. It includes the extension point name, application ID, and the LLM's text output.  The LLM output is checked for policy compliance. Depends on app_id for context.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation-extension.md#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"point\": \"app.moderation.output\",\n    \"params\": {\n        \"app_id\": string,\n        \"text\": string\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Define ToolPromptMessage Model\nDESCRIPTION: Defines the `ToolPromptMessage` class, which represents tool messages, used for conveying the results of a tool execution to the model. It inherits from `PromptMessage` and includes the `tool_call_id` to identify the tool invocation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nclass ToolPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for tool prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.TOOL\n    tool_call_id: str  # Tool invocation ID. If OpenAI tool call is not supported, the name of the tool can also be inputted.\n```\n\n----------------------------------------\n\nTITLE: Calling the AI Application via API using curl\nDESCRIPTION: This snippet demonstrates how to call the AI application created in Dify via its API using a curl command. It requires an API key and optionally a conversation ID to maintain context across multiple requests.  The `user` field helps track individual user interactions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/build-an-notion-ai-assistant.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/chat-messages' \\\n--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"inputs\": {},\n    \"query\": \"eh\",\n    \"response_mode\": \"streaming\",\n    \"conversation_id\": \"\",\n    \"user\": \"abc-123\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Rerank Invocation Interface in Python\nDESCRIPTION: Defines the interface for invoking a rerank model. Parameters include model name, credentials, query, list of documents, score threshold, top n, and user identifier. Returns a RerankResult entity.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            query: str, docs: list[str], score_threshold: Optional[float] = None, top_n: Optional[int] = None,\n            user: Optional[str] = None) \\\n        -> RerankResult:\n    \"\"\"\n    Invoke rerank model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param query: search query\n    :param docs: docs for reranking\n    :param score_threshold: score threshold\n    :param top_n: top n\n    :param user: unique user id\n    :return: rerank result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Model Credential Validation - Python\nDESCRIPTION: This function validates model credentials scoped to a single model. It ensures that the provided credentials are valid for the specific model being used.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n    \"\"\"\n    Validate model credentials.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: OpenAI Model Credential Schema YAML Configuration\nDESCRIPTION: This YAML configuration demonstrates the `model_credential_schema` for OpenAI, which provides fine-tuned models. It defines the structure for specifying model names and their associated credentials, including API keys, organization IDs, and API base URLs. This schema allows users to configure credentials for each fine-tuned model individually.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/new-provider.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nmodel_credential_schema:\n  model: # 微調整モデルの名称\n    label:\n      en_US: Model Name\n      zh_Hans: 模型名称\n    placeholder:\n      en_US: Enter your model name\n      zh_Hans: 输入模型名称\n  credential_form_schemas:\n  - variable: openai_api_key\n    label:\n      en_US: API Key\n    type: secret-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入你的 API Key\n      en_US: Enter your API Key\n  - variable: openai_organization\n    label:\n        zh_Hans: 组织 ID\n        en_US: Organization\n    type: text-input\n    required: false\n    placeholder:\n      zh_Hans: 在此输入你的组织 ID\n      en_US: Enter your Organization ID\n  - variable: openai_api_base\n    label:\n      zh_Hans: API Base\n      en_US: API Base\n    type: text-input\n    required: false\n    placeholder:\n      zh_Hans: 在此输入你的 API Base\n      en_US: Enter your API Base\n```\n\n----------------------------------------\n\nTITLE: Invoking Workflow as Tool\nDESCRIPTION: This code snippet defines the endpoint for invoking a Workflow as Tool within a plugin.  The `provider` represents the workflow's ID, `tool_name` is the name defined during workflow creation, and `parameters` are the input values for the workflow.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/tool.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef invoke_workflow_tool(\n    self, provider: str, tool_name: str, parameters: dict[str, Any]\n) -> Generator[ToolInvokeMessage, None, None]:\n    pass\n```\n\n----------------------------------------\n\nTITLE: ImagePromptMessageContent Class Definition in Python\nDESCRIPTION: Defines the `ImagePromptMessageContent` class, which inherits from `PromptMessageContent`. It represents image-based content for a prompt message. It includes an Enum for DETAIL (image resolution) and sets the default content `type` to `PromptMessageContentType.IMAGE` and `detail` to `DETAIL.LOW`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass ImagePromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for image prompt message content.\n    \"\"\"\n    class DETAIL(Enum):\n        LOW = 'low'\n        HIGH = 'high'\n\n    type: PromptMessageContentType = PromptMessageContentType.IMAGE\n    detail: DETAIL = DETAIL.LOW  # 分辨率\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Example for Importing\nDESCRIPTION: This code snippet shows a JSON schema example that can be imported to create a schema with comment and rating fields. The example includes a 'comment' field of type string and a 'rating' field of type number. This snippet illustrates how to quickly define a simple schema structure.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/llm.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n \"comment\": \"This is great!\",\n \"rating\": 5\n}\n```\n\n----------------------------------------\n\nTITLE: Rerank Model Invocation Interface in Python\nDESCRIPTION: This code snippet defines the `_invoke` method for a rerank model. It takes a query and a list of documents as input, reranks them based on the query, and returns a rerank result. Parameters include model name, credentials, query, documents, score threshold, number of top segments to return, and user ID.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              query: str, docs: list[str], score_threshold: Optional[float] = None, top_n: Optional[int] = None,\n              user: Optional[str] = None) \\\n          -> RerankResult:\n      \"\"\"\n      Invoke rerank model\n  \n      :param model: model name\n      :param credentials: model credentials\n      :param query: search query\n      :param docs: docs for reranking\n      :param score_threshold: score threshold\n      :param top_n: top n\n      :param user: unique user id\n      :return: rerank result\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: LLMResultChunk Class Definition in Python\nDESCRIPTION: Defines the `LLMResultChunk` class as a Pydantic BaseModel representing a chunk of a streamed language model result.  It contains the model name, prompt messages, a system fingerprint, and the delta (incremental change).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunk(BaseModel):\n    \"\"\"\n    Model class for llm result chunk.\n    \"\"\"\n    model: str  # 实际使用模型\n    prompt_messages: list[PromptMessage]  # prompt 消息列表\n    system_fingerprint: Optional[str] = None  # 请求指纹，可参考 OpenAI 该参数定义\n    delta: LLMResultChunkDelta  # 每个迭代存在变化的内容\n```\n\n----------------------------------------\n\nTITLE: Plugin Debugging: .env file\nDESCRIPTION: This is an example of the `.env` file used for debugging the Dify plugin. It sets the environment variables required to connect to the remote debugging server.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: UI Generation JSON Schema Example\nDESCRIPTION: This JSON Schema defines a structure for dynamically generating UI components, enabling recursive composition. It includes fields for component type, label, nested children, and arbitrary attributes.  The `$ref` keyword enables the recursive definition.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/how-to-use-json-schema-in-dify.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n        \"name\": \"ui\",\n        \"description\": \"Dynamically generated UI\",\n        \"strict\": true,\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"type\": {\n                    \"type\": \"string\",\n                    \"description\": \"The type of the UI component\",\n                    \"enum\": [\"div\", \"button\", \"header\", \"section\", \"field\", \"form\"]\n                },\n                \"label\": {\n                    \"type\": \"string\",\n                    \"description\": \"The label of the UI component, used for buttons or form fields\"\n                },\n                \"children\": {\n                    \"type\": \"array\",\n                    \"description\": \"Nested UI components\",\n                    \"items\": {\n                        \"$ref\": \"#\"\n                    }\n                },\n                \"attributes\": {\n                    \"type\": \"array\",\n                    \"description\": \"Arbitrary attributes for the UI component, suitable for any element\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\n                                \"type\": \"string\",\n                                \"description\": \"The name of the attribute, for example onClick or className\"\n                            },\n                            \"value\": {\n                                \"type\": \"string\",\n                                \"description\": \"The value of the attribute\"\n                            }\n                        },\n                      \"additionalProperties\": false,\n                      \"required\": [\"name\", \"value\"]\n                    }\n                }\n            },\n            \"required\": [\"type\", \"label\", \"children\", \"attributes\"],\n            \"additionalProperties\": false\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Define LLMResultChunkDelta Class in Python\nDESCRIPTION: This code defines the `LLMResultChunkDelta` class, which inherits from `BaseModel`.  It is used for streaming returns and contains the `delta` entity. It includes attributes for `index`, `message` (an AssistantPromptMessage), `usage` (optional LLMUsage), and `finish_reason` (optional finish reason).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunkDelta(BaseModel):\n    \"\"\"\n    Model class for llm result chunk delta.\n    \"\"\"\n    index: int\n    message: AssistantPromptMessage  # response message\n    usage: Optional[LLMUsage] = None  # usage info\n    finish_reason: Optional[str] = None  # finish reason, only the last one returns\n```\n\n----------------------------------------\n\nTITLE: Celery Broker URL (Redis Direct)\nDESCRIPTION: Specifies the URL for the Celery broker when using direct Redis connection mode.  It consists of the protocol (redis), credentials (username and password), host, port, and database number. Replace placeholders with your actual Redis credentials and connection details.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/environments.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nredis://<redis_username>:<redis_password>@<redis_host>:<redis_port>/<redis_database>\n```\n\n----------------------------------------\n\nTITLE: Configuring the .env file (Shell)\nDESCRIPTION: This snippet renames the `.env.example` file to `.env`, which is necessary for configuring the LocalAI environment variables. The user should then edit the `.env` file to configure settings like THREADS (which should not exceed the number of CPU cores).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/localai.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ mv .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Converting Tool to Prompt Message Tool Python\nDESCRIPTION: This Python code defines the `_convert_tool_to_prompt_message_tool` method. Its purpose is to transform a `ToolEntity` into a `PromptMessageTool`. It configures the tool's parameters based on the `ToolEntity`'s definition, taking into account the parameter types, descriptions, and whether they are required for the prompt message.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n    def _convert_tool_to_prompt_message_tool(\n        self, tool: ToolEntity\n    ) -> PromptMessageTool:\n        \"\"\"\n        convert tool to prompt message tool\n        \"\"\"\n        message_tool = PromptMessageTool(\n            name=tool.identity.name,\n            description=tool.description.llm if tool.description else \"\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n            },\n        )\n\n        parameters = tool.parameters\n        for parameter in parameters:\n            if parameter.form != ToolParameter.ToolParameterForm.LLM:\n                continue\n\n            parameter_type = parameter.type\n            if parameter.type in {\n                ToolParameter.ToolParameterType.FILE,\n                ToolParameter.ToolParameterType.FILES,\n            }:\n                continue\n            enum = []\n            if parameter.type == ToolParameter.ToolParameterType.SELECT:\n                enum = (\n                    [option.value for option in parameter.options]\n                    if parameter.options\n                    else []\n                )\n\n            message_tool.parameters[\"properties\"][parameter.name] = {\n                \"type\": parameter_type,\n                \"description\": parameter.llm_description or \"\",\n            }\n\n            if len(enum) > 0:\n                message_tool.parameters[\"properties\"][parameter.name][\"enum\"] = enum\n\n            if parameter.required:\n                message_tool.parameters[\"required\"].append(parameter.name)\n\n        return message_tool\n\n```\n\n----------------------------------------\n\nTITLE: LLM Input Tokens Calculation Interface in Python\nDESCRIPTION: Defines the interface for getting the number of tokens for given prompt messages. Parameters include model name, credentials, prompt messages, and tools. If the model does not provide a pre-calculated tokens interface, the implementation can directly return 0.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                   tools: Optional[list[PromptMessageTool]] = None) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param tools: tools for tool calling\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Extract Tool Calls from LLM Result Chunk in Python\nDESCRIPTION: The `extract_tool_calls` function parses an `LLMResultChunk` to extract details about tool calls. It iterates through each `prompt_message` in the `tool_calls` list, extracts the arguments from the function call (parsing them as JSON), and assembles a list of tuples containing the tool call ID, name, and arguments. This list is then returned for further processing.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n    def extract_tool_calls(\n        self, llm_result_chunk: LLMResultChunk\n    ) -> list[tuple[str, str, dict[str, Any]]]:\n        \"\"\"\n        Extract tool calls from llm result chunk\n\n        Returns:\n            List[Tuple[str, str, Dict[str, Any]]]: [(tool_call_id, tool_call_name, tool_call_args)]\n        \"\"\"\n        tool_calls = []\n        for prompt_message in llm_result_chunk.delta.message.tool_calls:\n            args = {}\n            if prompt_message.function.arguments != \"\":\n                args = json.loads(prompt_message.function.arguments)\n\n            tool_calls.append(\n```\n\n----------------------------------------\n\nTITLE: Parameter Extractor Node Interface Definition\nDESCRIPTION: This snippet defines the interface for the ParameterExtractor node's invoke method. It specifies the required parameters (parameters, model, query, instruction) and the return type (NodeResponse). The parameters argument is a list of ParameterConfig objects, model conforms to LLMModelConfig, query is the source text, and instruction provides additional guidance to the LLM.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n        self,\n        parameters: list[ParameterConfig],\n        model: ModelConfig,\n        query: str,\n        instruction: str = \"\",\n    ) -> NodeResponse:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Restarting Dify After Environment Variable Changes\nDESCRIPTION: These commands restart the Dify application after modifying the environment variables in the `.env` file, ensuring that the changes take effect.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: .env File Configuration for Plugin Debugging\nDESCRIPTION: This bash code shows the configuration for the .env file, which is used for setting up the plugin debugging environment. It defines variables for install method, remote install host, remote install port, and remote install key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: General Mode Chunk Identifier Configuration with Regex - Dify\nDESCRIPTION: This snippet describes how to configure the chunk identifier using regular expressions in Dify's General Mode. The default value is `\\n`, which splits text by paragraphs. Users can customize this with regex to split based on other delimiters like sentences.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/create-knowledge-and-upload-documents/chunking-and-cleaning-text.md#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Anthropic Provider Class\nDESCRIPTION: This Python code defines the `AnthropicProvider` class, which inherits from the `ModelProvider` base class. It includes a `validate_provider_credentials` method to validate the provider's credentials by calling the model instance's validate credentials method.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom dify_plugin.entities.model import ModelType\nfrom dify_plugin.errors.model import CredentialsValidateFailedError\nfrom dify_plugin import ModelProvider\n\nlogger = logging.getLogger(__name__)\n\n\nclass AnthropicProvider(ModelProvider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        \"\"\"\n        Validate provider credentials\n\n        if validate failed, raise exception\n\n        :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n        \"\"\"\n        try:\n            model_instance = self.get_model_instance(ModelType.LLM)\n            model_instance.validate_credentials(model=\"claude-3-opus-20240229\", credentials=credentials)\n        except CredentialsValidateFailedError as ex:\n            raise ex\n        except Exception as ex:\n            logger.exception(f\"{self.get_provider_schema().provider} credentials validate failed\")\n            raise ex\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Cloud Service Moderation\nDESCRIPTION: Defines the directory structure required for a custom 'Cloud Service' moderation type within the Dify project.  This structure organizes the necessary files for the moderation logic, schema definition, and initialization.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/code-based-extension/moderation.md#_snippet_0\n\nLANGUAGE: Plain\nCODE:\n```\n.\n└── api\n    └── core\n        └── moderation\n            └── cloud_service\n                ├── __init__.py\n                ├── cloud_service.py\n                └── schema.json\n```\n\n----------------------------------------\n\nTITLE: Model Credentials Validation Function\nDESCRIPTION: This Python function validates the credentials for a given model. It takes the model name and credentials as input and performs the necessary checks to ensure that the credentials are valid. This function is crucial for ensuring secure access to the models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/customizable-model.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n        \"\"\"\n        Validate model credentials\n\n        :param model: model name\n        :param credentials: model credentials\n        :return:\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Hierarchical Log Creation\nDESCRIPTION: This Python code snippet shows how to create hierarchical logs by setting the `parent` parameter when creating a log message. This allows for a more structured and organized log output, making it easier to track the execution flow.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfunction_call_round_log = self.create_log_message(\n    label=\"Function Call Round1 \",\n    data={},\n    metadata={},\n)\nyield function_call_round_log\n\nmodel_log = self.create_log_message(\n    label=f\"{params.model.model} Thought\",\n    data={},\n    metadata={\"start_at\": model_started_at, \"provider\": params.model.provider},\n    status=ToolInvokeMessage.LogMessage.LogStatus.START,\n    # 親ログを追加\n    parent=function_call_round_log,\n)\nyield model_log\n```\n\n----------------------------------------\n\nTITLE: Initialize Local Git Repository for Plugin - Bash\nDESCRIPTION: This snippet initializes a Git repository in the plugin's project directory, adds all files to the staging area, and creates an initial commit with a descriptive message. This is a prerequisite to pushing the plugin to a remote repository.  The user must have Git installed locally.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit init\ngit add .\ngit commit -m \"Initial commit: Add plugin files\"\n```\n\n----------------------------------------\n\nTITLE: Text Completion Conversational App Template\nDESCRIPTION: This is the prompt template for building conversational applications using text completion models in Dify.  It incorporates context, pre-prompt, conversation history, and a query variable to generate the response.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nUse the following context as your learned knowledge, inside <context></context> XML tags.\n\n<context>\n{{#context#}}\n</context>\n\nWhen answering the user:\n- If you don't know, just say that you don't know.\n- If you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n\n{{pre_prompt}}\n\nHere are the chat histories between human and assistant, inside <histories></histories> XML tags.\n\n<histories>\n{{#histories#}}\n</histories>\n\nHuman: {{#query#}}\n\nAssistant: \n```\n\n----------------------------------------\n\nTITLE: LLMResultChunkDelta Class Definition in Python\nDESCRIPTION: Defines the `LLMResultChunkDelta` class as a Pydantic BaseModel representing the incremental changes (delta) in a streamed language model result. It contains the index, the updated message, usage information (only in the last chunk), and the finish reason (only in the last chunk).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunkDelta(BaseModel):\n    \"\"\"\n    Model class for llm result chunk delta.\n    \"\"\"\n    index: int  # 序号\n    message: AssistantPromptMessage  # 回复消息\n    usage: Optional[LLMUsage] = None  # 使用的 tokens 及费用信息，仅最后一条返回\n    finish_reason: Optional[str] = None  # 结束原因，仅最后一条返回\n```\n\n----------------------------------------\n\nTITLE: Celery Broker URL (Redis Direct Connection)\nDESCRIPTION: This code snippet shows the format of the Celery Broker URL when using a direct Redis connection. It includes the Redis username, password, host, port, and database number. Ensure that you replace the placeholders with your actual Redis credentials.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/environments.md#_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nredis://<redis_username>:<redis_password>@<redis_host>:<redis_port>/<redis_database>\n```\n\n----------------------------------------\n\nTITLE: Moderation Model Invocation Interface\nDESCRIPTION: Defines the `_invoke` method for moderating text content. It takes a model name, credentials, the text to moderate, and an optional user identifier. It returns a boolean indicating whether the text is safe (False) or not (True).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              text: str, user: Optional[str] = None) \\\n          -> bool:\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param text: text to moderate\n      :param user: unique user id\n      :return: false if text is safe, true otherwise\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Example - Visual Editor\nDESCRIPTION: This example demonstrates a basic JSON schema structure with string and number types, useful for simple data definition within the LLM node's visual editor. It defines 'comment' as a string and 'rating' as a number, making them required fields.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/llm.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n \"comment\": \"This is great!\",\n \"rating\": 5\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking Text Embedding Model in Python\nDESCRIPTION: This method invokes a text embedding model. It takes the model name, credentials, texts to embed, and user ID as input. It returns a TextEmbeddingResult entity containing the embeddings.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n                texts: list[str], user: Optional[str] = None) \\\n            -> TextEmbeddingResult:\n        \"\"\"\n        Invoke large language model\n\n        :param model: model name\n        :param credentials: model credentials\n        :param texts: texts to embed\n        :param user: unique user id\n        :return: embeddings result\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Validating Model Credentials in Python\nDESCRIPTION: This code snippet demonstrates how to validate model credentials.  It takes the model name and credentials as input and raises a CredentialsValidateFailedError if validation fails. This function needs to be implemented for all models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n      \"\"\"\n      Validate model credentials\n\n      :param model: model name\n      :param credentials: model credentials\n      :return:\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for Ollama Host (macOS)\nDESCRIPTION: This command sets the OLLAMA_HOST environment variable to 0.0.0.0 using launchctl on macOS. This allows Ollama to be accessible from other containers or devices on the network by binding it to all available network interfaces.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nlaunchctl setenv OLLAMA_HOST \"0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Customizing Dify Chatbot Button Style with containerProps (Inline)\nDESCRIPTION: This JavaScript snippet demonstrates how to customize the Dify chatbot bubble button's appearance using the `containerProps` option with inline styles.  It shows how to modify the background color, width, height, and border radius.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-publishing/embedding-in-websites.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.difyChatbotConfig = {\n    // ... 其他配置\n    containerProps: {\n        style: {\n            backgroundColor: '#ABCDEF',\n            width: '60px',\n            height: '60px',\n            borderRadius: '30px',\n        },\n        // 对于较小的样式覆盖，也可以使用字符串作为 `style` 属性的值：\n        // style: 'background-color: #ABCDEF; width: 60px;',\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Defining TextEmbeddingResult Model (Python)\nDESCRIPTION: Defines the `TextEmbeddingResult` model, representing the result of a text embedding operation. It contains the `model` used, the `embeddings` (a list of embedding vectors), and `usage` information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nclass TextEmbeddingResult(BaseModel):\n    \"\"\"\n    Model class for text embedding result.\n    \"\"\"\n    model: str  # Actual model used\n    embeddings: list[list[float]]  # List of embedding vectors, corresponding to the input texts list\n    usage: EmbeddingUsage  # Usage information\n```\n\n----------------------------------------\n\nTITLE: Stop Services and Backup Data\nDESCRIPTION: This snippet stops the Docker Compose services and then creates a compressed archive of the volumes directory. This ensures a backup of the application's data before proceeding with the upgrade.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/migration/migrate-to-v1.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\ntar -cvf volumes-$(date +%s).tgz volumes\n```\n\n----------------------------------------\n\nTITLE: Upgrade Dify Version via Git and Docker Compose\nDESCRIPTION: This snippet fetches the latest changes from the origin, checks out the `1.0.0` branch, navigates to the `docker` directory, modifies the `.env` file, and then brings up the Docker Compose services in detached mode. This effectively updates the Dify version.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/migration/migrate-to-v1.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit fetch origin\ngit checkout 1.0.0 # 切换至 1.0.0 分支\ncd docker\nnano .env # 修改环境配置文件同步 .env.example 文件\ndocker compose -f docker-compose.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: EmbeddingUsage Class Definition in Python\nDESCRIPTION: Defines the `EmbeddingUsage` class as a Pydantic BaseModel representing usage information for an embedding model, inheriting from `ModelUsage`. It includes the number of tokens used, pricing, and latency.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nclass EmbeddingUsage(ModelUsage):\n    \"\"\"\n    Model class for embedding usage.\n    \"\"\"\n    tokens: int  # 使用 token 数\n    total_tokens: int  # 总使用 token 数\n    unit_price: Decimal  # 单价\n    price_unit: Decimal  # 价格单位，即单价基于多少 tokens\n    total_price: Decimal  # 总费用\n    currency: str  # 货币单位\n    latency: float  # 请求耗时(s)\n```\n\n----------------------------------------\n\nTITLE: Text Embedding Request Entry Point (Python)\nDESCRIPTION: This code defines the entry point for requesting text embeddings using the Dify plugin. It uses the session's model to access the text embedding functionality.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.text_embedding\n```\n\n----------------------------------------\n\nTITLE: Validating Model Credentials in Python\nDESCRIPTION: This function validates the credentials associated with a specific model. It takes the model name and a dictionary of credentials as input and should raise a CredentialsValidateFailedError if the credentials are invalid. The credentials' parameters are defined either in the `provider_credential_schema` or `model_credential_schema` of the supplier's YAML configuration file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n    \"\"\"\n    Validate model credentials\n\n    :param model: model name\n    :param credentials: model credentials\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Modifying Dify Chatbot Button Background Color with CSS\nDESCRIPTION: This CSS snippet demonstrates how to override the default background color of the Dify chatbot bubble button using the `--dify-chatbot-bubble-button-bg-color` CSS variable.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-publishing/embedding-in-websites.md#_snippet_2\n\nLANGUAGE: css\nCODE:\n```\n#dify-chatbot-bubble-button {\n    --dify-chatbot-bubble-button-bg-color: #ABCDEF;\n}\n```\n\n----------------------------------------\n\nTITLE: Schema Definition Example in JSON\nDESCRIPTION: This JSON schema defines the structure for a cloud service configuration form.  It includes internationalized labels, form schema elements for selecting a cloud provider (select), entering an API endpoint (text-input), and providing an API key (paragraph).  The schema specifies types, labels, variable names, required fields, default values, placeholders, options for select components, and maximum lengths.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/code-based-extension/README.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"label\": {\n        \"en-US\": \"Cloud Service\",\n        \"zh-Hans\": \"云服务\"\n    },\n    \"form_schema\": [\n        {\n            \"type\": \"select\",\n            \"label\": {\n                \"en-US\": \"Cloud Provider\",\n                \"zh-Hans\": \"云厂商\"\n            },\n            \"variable\": \"cloud_provider\",\n            \"required\": true,\n            \"options\": [\n                {\n                    \"label\": {\n                        \"en-US\": \"AWS\",\n                        \"zh-Hans\": \"亚马逊\"\n                    },\n                    \"value\": \"AWS\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Google Cloud\",\n                        \"zh-Hans\": \"谷歌云\"\n                    },\n                    \"value\": \"GoogleCloud\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Azure Cloud\",\n                        \"zh-Hans\": \"微软云\"\n                    },\n                    \"value\": \"Azure\"\n                }\n            ],\n            \"default\": \"GoogleCloud\",\n            \"placeholder\": \"\"\n        },\n        {\n            \"type\": \"text-input\",\n            \"label\": {\n                \"en-US\": \"API Endpoint\",\n                \"zh-Hans\": \"API Endpoint\"\n            },\n            \"variable\": \"api_endpoint\",\n            \"required\": true,\n            \"max_length\": 100,\n            \"default\": \"\",\n            \"placeholder\": \"https://api.example.com\"\n        },\n        {\n            \"type\": \"paragraph\",\n            \"label\": {\n                \"en-US\": \"API Key\",\n                \"zh-Hans\": \"API Key\"\n            },\n            \"variable\": \"api_keys\",\n            \"required\": true,\n            \"default\": \"\",\n            \"placeholder\": \"Paste your API key here\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Schema for UI Generation\nDESCRIPTION: This JSON schema defines a structure for dynamically generating UI components. It uses recursion to allow nested UI elements and includes properties for type, label, children, and attributes. The schema specifies the allowed types using the `enum` property and includes attributes as an array of name-value pairs, restricting additional properties and requiring certain fields.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/extended-reading/how-to-use-json-schema-in-dify.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n        \"name\": \"ui\",\n        \"description\": \"Dynamically generated UI\",\n        \"strict\": true,\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"type\": {\n                    \"type\": \"string\",\n                    \"description\": \"The type of the UI component\",\n                    \"enum\": [\"div\", \"button\", \"header\", \"section\", \"field\", \"form\"]\n                },\n                \"label\": {\n                    \"type\": \"string\",\n                    \"description\": \"The label of the UI component, used for buttons or form fields\"\n                },\n                \"children\": {\n                    \"type\": \"array\",\n                    \"description\": \"Nested UI components\",\n                    \"items\": {\n                        \"$ref\": \"#\"\n                    }\n                },\n                \"attributes\": {\n                    \"type\": \"array\",\n                    \"description\": \"Arbitrary attributes for the UI component, suitable for any element\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\n                                \"type\": \"string\",\n                                \"description\": \"The name of the attribute, for example onClick or className\"\n                            },\n                            \"value\": {\n                                \"type\": \"string\",\n                                \"description\": \"The value of the attribute\"\n                            }\n                        },\n                      \"additionalProperties\": false,\n                      \"required\": [\"name\", \"value\"]\n                    }\n                }\n            },\n            \"required\": [\"type\", \"label\", \"children\", \"attributes\"],\n            \"additionalProperties\": false\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Updating Document Segment with cURL\nDESCRIPTION: This cURL command updates a specific segment of a document in the Dify knowledge base. It requires the dataset ID, document ID, and segment ID.  The request body contains the updated segment content, answer, keywords and enabled status. It returns the updated segment data.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments/{segment_id}' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json'\\\n--data-raw '{\"segment\": {\"content\": \"1\",\"answer\": \"1\", \"keywords\": [\"a\"], \"enabled\": false}}'\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Base Class in Python\nDESCRIPTION: This code defines the base class for all Role message bodies, used for parameter declaration. It includes the role, content (string or content list), and name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessage(ABC, BaseModel):\n    \"\"\"\n    Model class for prompt message.\n    \"\"\"\n    role: PromptMessageRole\n    content: Optional[str | list[PromptMessageContent]] = None  # Supports two types: string and content list. The content list is designed to meet the needs of multimodal inputs. For more details, see the PromptMessageContent explanation.\n    name: Optional[str] = None\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables in .env File\nDESCRIPTION: This code snippet shows the structure of the .env file, which stores sensitive information like Twilio Account SID, Auth Token, Dify URL, and API Key.  These variables are used to authenticate with Twilio and Dify services.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-whatsapp.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nTWILIO_NUMBER=+14155238886\nTWILIO_ACCOUNT_SID=<在(4)獲取的Twilio Account SID>\nTWILIO_AUTH_TOKEN=<在(4)獲取的Twilio Auth Token>\nDIFY_URL=<在(3)獲取的Dify API服务器地址>\nDIFY_API_KEY=<在(3)獲取的Dify API密钥>\n```\n\n----------------------------------------\n\nTITLE: Example VECTOR_STORE Configuration in docker-compose.yaml\nDESCRIPTION: Illustrates how to set the vector store type to 'weaviate' in the `docker-compose.yaml` file. This configuration must be applied to both API and worker services to ensure consistency. This is necessary when migrating to another vector store.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n# The type of vector store to use. Supported values are `weaviate`, `qdrant`, `milvus`, `analyticdb`.\nVECTOR_STORE: weaviate\n```\n\n----------------------------------------\n\nTITLE: Reloading Systemd Daemon and Restarting Ollama (Linux)\nDESCRIPTION: These commands reload the systemd daemon to apply changes made to the Ollama service file and then restart the Ollama service. This ensures that the new configuration, including the OLLAMA_HOST environment variable, is active.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsystemctl daemon-reload\nsystemctl restart ollama\n```\n\n----------------------------------------\n\nTITLE: Setting Remote Debugging Environment\nDESCRIPTION: This code snippet shows how to configure the environment variables for remote debugging of a Dify plugin. It sets the installation method to 'remote', specifies the remote host and port, and includes a remote installation key. These settings allow the plugin to be debugged remotely using Dify's remote debugging features.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: Model Credential Validation - Python\nDESCRIPTION: This code shows the `validate_credentials` method, used for validating individual model credentials. It takes the model name and credentials as input and should raise an exception if the credentials are invalid.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/customizable-model.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n        \"\"\"\n        Validate model credentials\n\n        :param model: model name\n        :param credentials: model credentials\n        :return:\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Format Knowledge Retrieval Output to Markdown using Jinja2\nDESCRIPTION: This snippet demonstrates how to format the output from a knowledge retrieval node into a structured Markdown format using Jinja2. It iterates through a list of chunks, displaying the index, similarity score, title, and content for each chunk. It uses a raw tag to prevent Jinja2 from interpreting the Markdown syntax and replaces newline characters with double newlines for proper Markdown rendering.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/template.md#_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{% raw %}\n{% for item in chunks %}\n### Chunk {{ loop.index }}. \n### Similarity: {{ item.metadata.score | default('N/A') }}\n\n#### {{ item.title }}\n\n##### Content\n{{ item.content | replace('\\n', '\\n\\n') }}\n\n---\n{% endfor %}\n{% endraw %}\n```\n\n----------------------------------------\n\nTITLE: Creating Empty Knowledge Base using Dify API (curl)\nDESCRIPTION: This snippet shows how to create an empty knowledge base (dataset) using the Dify API. It requires an API key for authorization and the dataset name and permission settings.  It's primarily intended for creating an empty container for subsequent data uploads.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"name\": \"name\", \"permission\": \"only_me\"}'\n```\n\n----------------------------------------\n\nTITLE: Implementing Weather Search Logic Python\nDESCRIPTION: This Python code defines the `WeatherSearch` class, which extends `ExternalDataTool`.  It includes a `validate_config` method for validating the configuration schema defined in `schema.json`, and a `query` method to implement the weather search logic based on user inputs and configuration. The `name` class variable uniquely identifies the tool and must match the directory and file name.  The example implementation returns a hardcoded weather string based on selected temperature unit and city input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/code-based-extension/external-data-tool.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nfrom core.external_data_tool.base import ExternalDataTool\n\n\nclass WeatherSearch(ExternalDataTool):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"weather_search\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user save the config.\n\n        Example:\n            .. code-block:: python\n                config = {\n                    \"temperature_unit\": \"centigrade\"\n                }\n\n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n\n        if not config.get('temperature_unit'):\n            raise ValueError('temperature unit is required')\n\n    def query(self, inputs: dict, query: Optional[str] = None) -> str:\n        \"\"\"\n        Query the external data tool.\n\n        :param inputs: user inputs\n        :param query: the query of chat app\n        :return: the tool query result\n        \"\"\"\n        city = inputs.get('city')\n        temperature_unit = self.config.get('temperature_unit')\n\n        if temperature_unit == 'fahrenheit':\n            return f'Weather in {city} is 32°F'\n        else:\n            return f'Weather in {city} is 0°C'\n```\n\n----------------------------------------\n\nTITLE: Define Google Tool Provider Credentials in YAML\nDESCRIPTION: This YAML configuration defines the Google tool provider credentials, specifically a SerpApi API key. It specifies the type of credential input (`secret-input`), whether it is required, labels and placeholders for multiple languages, a help text, and a URL for obtaining the API key. The `secret-input` type ensures the input is hidden on the frontend and encrypted on the backend.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/quick-tool-integration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: Dify\n  name: google\n  label:\n    en_US: Google\n    zh_Hans: Google\n    ja_JP: Google\n    pt_BR: Google\n  description:\n    en_US: Google\n    zh_Hans: Google\n    ja_JP: Google\n    pt_BR: Google\n  icon: icon.svg\ncredentials_for_provider: # 認証情報フィールド\n  serpapi_api_key: # 認証情報フィールド名\n    type: secret-input # 認証情報フィールドタイプ\n    required: true # 必須かどうか\n    label: # 認証情報フィールドラベル\n      en_US: SerpApi API key # 英語ラベル\n      zh_Hans: SerpApi API key # 中国語ラベル\n      ja_JP: SerpApi API key # 日本語ラベル\n      pt_BR: chave de API SerpApi # プルトガル語ラベル\n    placeholder: # 認証情報フィールドプレースホルダー\n      en_US: Please input your SerpApi API key # 英語プレースホルダー\n      zh_Hans: 请输入你的 SerpApi API key # 中国語プレースホルダー\n      ja_JP: SerpApi API keyを入力してください # 日本語プレースホルダー\n      pt_BR: Por favor, insira sua chave de API SerpApi # プルトガル語プレースホルダー\n    help: # 認証情報フィールドヘルプテキスト\n      en_US: Get your SerpApi API key from SerpApi # 英語ヘルプテキスト\n      zh_Hans: 从 SerpApi 获取你的 SerpApi API key # 中国語ヘルプテキスト\n      ja_JP: SerpApiからSerpApi APIキーを取得する # 日本語ヘルプテキスト\n      pt_BR: Obtenha sua chave de API SerpApi da SerpApi # プルトガル語ラベル\n    url: https://serpapi.com/manage-api-key # 認証情報フィールドヘルプリンク\n```\n\n----------------------------------------\n\nTITLE: Defining Frontend Component Schema\nDESCRIPTION: This JSON schema defines the structure for frontend components in Dify's code extensions. It allows specifying custom type names, form contents (select, text-input, paragraph), labels, variables, requirements, default values, placeholders, options for dropdowns, and maximum length for text inputs. It uses \"en-US\" and \"zh-Hans\" for multilingual support.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/code-based-extension/README.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"label\": {\n        \"en-US\": \"Cloud Service\",\n        \"zh-Hans\": \"云服务\"\n    },\n    \"form_schema\": [\n        {\n            \"type\": \"select\",\n            \"label\": {\n                \"en-US\": \"Cloud Provider\",\n                \"zh-Hans\": \"云厂商\"\n            },\n            \"variable\": \"cloud_provider\",\n            \"required\": true,\n            \"options\": [\n                {\n                    \"label\": {\n                        \"en-US\": \"AWS\",\n                        \"zh-Hans\": \"亚马逊\"\n                    },\n                    \"value\": \"AWS\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Google Cloud\",\n                        \"zh-Hans\": \"谷歌云\"\n                    },\n                    \"value\": \"GoogleCloud\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Azure Cloud\",\n                        \"zh-Hans\": \"微软云\"\n                    },\n                    \"value\": \"Azure\"\n                }\n            ],\n            \"default\": \"GoogleCloud\",\n            \"placeholder\": \"\"\n        },\n        {\n            \"type\": \"text-input\",\n            \"label\": {\n                \"en-US\": \"API Endpoint\",\n                \"zh-Hans\": \"API Endpoint\"\n            },\n            \"variable\": \"api_endpoint\",\n            \"required\": true,\n            \"max_length\": 100,\n            \"default\": \"\",\n            \"placeholder\": \"https://api.example.com\"\n        },\n        {\n            \"type\": \"paragraph\",\n            \"label\": {\n                \"en-US\": \"API Key\",\n                \"zh-Hans\": \"API Key\"\n            },\n            \"variable\": \"api_keys\",\n            \"required\": true,\n            \"default\": \"\",\n            \"placeholder\": \"Paste your API key here\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking Text Embedding Model in Dify (Python)\nDESCRIPTION: This method invokes a text embedding model to generate embeddings for the given input texts.  It takes a list of strings as input and returns a `TextEmbeddingResult` object containing the generated embeddings. The model and credentials parameters are used for authentication and model selection.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            texts: list[str], user: Optional[str] = None) \\\n        -> TextEmbeddingResult:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param texts: texts to embed\n    :param user: unique user id\n    :return: embeddings result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking Text Embedding Model in Python\nDESCRIPTION: This code snippet shows how to invoke a text embedding model. It takes model name, credentials, a list of texts, and an optional user identifier as input. The method returns a TextEmbeddingResult entity.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              texts: list[str], user: Optional[str] = None) \\\n          -> TextEmbeddingResult:\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param texts: texts to embed\n      :param user: unique user id\n      :return: embeddings result\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Xinference Vendor YAML\nDESCRIPTION: This YAML snippet defines the basic structure of the Xinference vendor, specifying its identifier, display name, icons, help information, supported model types (LLM, text embedding, and rerank), and the customization method. It also initializes the provider credential schema.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/customizable-model.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprovider: xinference # Specify vendor identifier\nlabel: # Vendor display name, can be set in en_US (English) and zh_Hans (Simplified Chinese). If zh_Hans is not set, en_US will be used by default.\n  en_US: Xorbits Inference\nicon_small: # Small icon, refer to other vendors' icons, stored in the _assets directory under the corresponding vendor implementation directory. Language strategy is the same as label.\n  en_US: icon_s_en.svg\nicon_large: # Large icon\n  en_US: icon_l_en.svg\nhelp: # Help\n  title:\n    en_US: How to deploy Xinference\n    zh_Hans: 如何部署 Xinference\n  url:\n    en_US: https://github.com/xorbitsai/inference\nsupported_model_types: # Supported model types. Xinference supports LLM/Text Embedding/Rerank\n- llm\n- text-embedding\n- rerank\nconfigurate_methods: # Since Xinference is a locally deployed vendor and does not have predefined models, you need to deploy the required models according to Xinference's documentation. Therefore, only custom models are supported here.\n- customizable-model\nprovider_credential_schema:\n  credential_form_schemas:\n```\n\n----------------------------------------\n\nTITLE: Rerank Model Invocation\nDESCRIPTION: Defines the interface for a rerank model invocation. It takes a query and a list of documents, reranks the documents based on the query, and returns a rerank result. Optional parameters include score threshold, top_n, and user identifier.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              query: str, docs: list[str], score_threshold: Optional[float] = None, top_n: Optional[int] = None,\n              user: Optional[str] = None) \\\n          -> RerankResult:\n      \"\"\"\n      Invoke rerank model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param query: search query\n      :param docs: docs for reranking\n      :param score_threshold: score threshold\n      :param top_n: top n\n      :param user: unique user id\n      :return: rerank result\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Packaging a Dify Plugin\nDESCRIPTION: This command packages a Dify plugin project into a `.difypkg` file. It requires the Dify plugin development tools to be installed and configured, and that the manifest.yaml and provider YAML files have a matching GitHub ID in the 'author' field.  The plugin project should also have passed remote connection testing prior to packaging. The command must be run from the parent directory of the plugin project.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/package-plugin-file-and-publish.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin package ./your_plugin_project\n```\n\n----------------------------------------\n\nTITLE: Sending Message to Dify and Handling Response\nDESCRIPTION: This Python code sends a message to the Dify API and handles the streaming response. It constructs a JSON payload with the user's message, conversation ID, and other parameters. It also updates the conversation ID dictionary with the new conversation ID returned by Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-whatsapp.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n    url = dify_url\n    headers = {  \n        'Content-Type': 'application/json',  \n        'Authorization': f\"Bearer {dify_api_key}\",  \n    }  \n    data = {  \n        'inputs': {},  \n        'query': Body,  \n        'response_mode': 'streaming',  \n        'conversation_id': conversation_ids.get(whatsapp_number, ''),  \n        'user': whatsapp_number,  \n    }  \n    response = requests.post(url, headers=headers, data=json.dumps(data), stream=True)  \n    answer = []  \n    for line in response.iter_lines():  \n        if line:  \n            decoded_line = line.decode('utf-8')  \n            if decoded_line.startswith('data: '):\n                decoded_line = decoded_line[6:]\n            try:  \n                json_line = json.loads(decoded_line) \n                if \"conversation_id\" in json_line:\n                    conversation_ids[whatsapp_number] = json_line[\"conversation_id\"]\n                if json_line[\"event\"] == \"agent_thought\":  \n                    answer.append(json_line[\"thought\"])\n            except json.JSONDecodeError: \n                print(json_line)  \n                continue  \n\n    merged_answer = ''.join(answer)\n```\n\n----------------------------------------\n\nTITLE: Packaging the Plugin (Bash)\nDESCRIPTION: This command packages the plugin into a `.difypkg` file.  Replace ./slack_bot with your actual plugin project path.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Replace ./slack_bot with your actual plugin project path.\n\ndify plugin package ./slack_bot\n```\n\n----------------------------------------\n\nTITLE: Invoke Built-in Tool in Dify Plugin (Python)\nDESCRIPTION: Defines the interface for invoking a built-in tool within a Dify plugin. The `invoke_builtin_tool` method allows plugins to call other installed tools within the same workspace.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/tool.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef invoke_builtin_tool(\n    self, provider: str, tool_name: str, parameters: dict[str, Any]\n) -> Generator[ToolInvokeMessage, None, None]:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Defining Google Tool Provider YAML\nDESCRIPTION: This YAML file defines the Google tool provider's information, including author, name, labels, descriptions, and icon. The icon file should be placed in the `_assets` folder within the provider's directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/quick-tool-integration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nidentity: # 工具供应商的基本信息\n  author: Dify # 作者\n  name: google # 名称，唯一，不允许和其他供应商重名\n  label: # 标签，用于前端展示\n    en_US: Google # 英文标签\n    zh_Hans: Google # 中文标签\n    ja_JP: : Google # 日文标签\n    pt_BR: : : Google # 葡萄牙文标签\n  description: # 描述，用于前端展示\n    en_US: Google # 英文描述\n    zh_Hans: Google # 中文描述\n    ja_JP: : Google # 日文描述\n    pt_BR: : Google # 葡萄牙文描述\n  icon: icon.svg # 图标，需要放置在当前模块的_assets文件夹下\n```\n\n----------------------------------------\n\nTITLE: Overriding Button Background Color with CSS\nDESCRIPTION: This CSS code snippet shows how to override the default background color of the Dify Chatbot Bubble Button to `#ABCDEF` using a CSS variable. It targets the button's ID and sets the `--dify-chatbot-bubble-button-bg-color` variable to the desired color.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-publishing/embedding-in-websites.md#_snippet_2\n\nLANGUAGE: css\nCODE:\n```\n#dify-chatbot-bubble-button {\n    --dify-chatbot-bubble-button-bg-color: #ABCDEF;\n}\n```\n\n----------------------------------------\n\nTITLE: Moderation Input Request Example JSON\nDESCRIPTION: Illustrates an example of the request body for the `app.moderation.input` extension point. It shows the format for passing the application ID, input variables, and the user query within the request. Input variables include potentially offensive phrases. It depends on the moderation service to flag sensitive or inappropriate content.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation-extension.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"point\": \"app.moderation.input\",\n    \"params\": {\n        \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n        \"inputs\": {\n            \"var_1\": \"I will kill you.\",\n            \"var_2\": \"I will fuck you.\"\n        },\n        \"query\": \"Happy everydays.\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Upgrading Dify using Git and Docker Compose\nDESCRIPTION: These commands upgrade the Dify deployment by pulling the latest code from the main branch, pulling the latest Docker images, and restarting the containers. It assumes the user is already in the dify/docker directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/docker-compose.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd dify/docker\ndocker compose down\ngit pull origin main\ndocker compose pull\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Workflow Interface Invocation\nDESCRIPTION: Specifies the entry point to access the Workflow interface: `self.session.app.workflow`.  This allows a plugin to invoke a workflow application in Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    self.session.app.workflow\n```\n\n----------------------------------------\n\nTITLE: Installing Frontend Dependencies with PNPM\nDESCRIPTION: Installs the frontend dependencies using PNPM, a fast and disk space efficient package manager. This ensures that all required packages for the web service are installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_13\n\nLANGUAGE: Bash\nCODE:\n```\nnpm i -g pnpm\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Setting Storage Key-Value Pair in Dify Plugin\nDESCRIPTION: This snippet defines the endpoint for setting a key-value pair in the plugin's persistent storage. It accepts a key (string) and a value (bytes) for storing data. Bytes are used so you can actually store file data.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/persistent-storage.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef set(self, key: str, val: bytes) -> None:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Delete Document Segment with Dify API\nDESCRIPTION: This snippet demonstrates how to delete a specific segment from a document within a Dify knowledge base using the API. It requires the `dataset_id`, `document_id`, and `segment_id`, along with a valid API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments/{segment_id}' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Role Enum Definition\nDESCRIPTION: Defines an enumeration `PromptMessageRole` representing the different roles a message can have: system, user, assistant, or tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageRole(Enum):\n    \"\"\"\n    Enum class for prompt message.\n    \"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    TOOL = \"tool\"\n```\n\n----------------------------------------\n\nTITLE: Implementation Class Template (Python)\nDESCRIPTION: This code shows the template for custom Moderation class implementation. Developers need to implement their own business logic here. The template includes the necessary imports, class definition, and placeholder methods for validation, input moderation, and output moderation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/code-based-extension/moderation.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom core.moderation.base import Moderation, ModerationAction, ModerationInputsResult, ModerationOutputsResult\n\nclass CloudServiceModeration(Moderation):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"cloud_service\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user saves the config.\n        \n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n        cls._validate_inputs_and_outputs_config(config, True)\n        \n        # implement your own logic here\n\n    def moderation_for_inputs(self, inputs: dict, query: str = \"\") -> ModerationInputsResult:\n        \"\"\"\n        Moderation for inputs.\n\n        :param inputs: user inputs\n        :param query: the query of chat app, there is empty if is completion app\n        :return: the moderation result\n        \"\"\"\n        flagged = False\n        preset_response = \"\"\n        \n        # implement your own logic here\n        \n        # return ModerationInputsResult(flagged=flagged, action=ModerationAction.overridden, inputs=inputs, query=query)\n        return ModerationInputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)\n\n    def moderation_for_outputs(self, text: str) -> ModerationOutputsResult:\n        \"\"\"\n        Moderation for outputs.\n\n        :param text: the text of LLM response\n        :return: the moderation result\n        \"\"\"\n        flagged = False\n        preset_response = \"\"\n        \n        # implement your own logic here\n\n        # return ModerationOutputsResult(flagged=flagged, action=ModerationAction.overridden, text=text)\n        return ModerationOutputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)\n```\n\n----------------------------------------\n\nTITLE: Run Frontend Docker Image from Local Build\nDESCRIPTION: This command runs the 'dify-web' Docker image, which is assumed to be built locally. It maps port 3000 to port 3000 and sets environment variables CONSOLE_URL and APP_URL to point to the backend service running on localhost port 5001. This configuration allows the user to access the application locally.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/start-the-frontend-docker-container.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run -it -p 3000:3000 -e CONSOLE_URL=http://127.0.0.1:5001 -e APP_URL=http://127.0.0.1:5001 dify-web\n```\n\n----------------------------------------\n\nTITLE: Setting Ollama Host Environment Variable on Linux\nDESCRIPTION: These commands configure the `OLLAMA_HOST` environment variable within the `ollama.service` systemd service. This allows Ollama to be accessible from other machines on the network. The systemd daemon needs to be reloaded and the Ollama service restarted for the changes to take effect.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/ollama.md#_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nsystemctl daemon-reload\nsystemctl restart ollama\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Plugin Parameters (YAML)\nDESCRIPTION: This YAML code defines the parameters required for the Agent plugin in `strategies/basic_agent.yaml`. It includes parameters for selecting the LLM model (`model`), defining the list of available tools (`tools`), setting the prompt or input content (`query`), and limiting the maximum number of iterations (`maximum_iterations`). These parameters are used to determine the core functionality of the plugin.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: basic_agent # agent_strategyの名前\n  author: novice # agent_strategyの作者\n  label:\n    en_US: BasicAgent # agent_strategyの英語ラベル\ndescription:\n  en_US: BasicAgent # agent_strategyの英語説明\nparameters:\n  - name: model # modelパラメータの名前\n    type: model-selector # model-type\n    scope: tool-call&llm # パラメータのスコープ\n    required: true\n    label:\n      en_US: Model\n      zh_Hans: 模型\n      pt_BR: Model\n  - name: tools # toolsパラメータの名前\n    type: array[tools] # toolパラメータの型\n    required: true\n    label:\n      en_US: Tools list\n      zh_Hans: 工具列表\n      pt_BR: Tools list\n  - name: query # queryパラメータの名前\n    type: string # queryパラメータの型\n    required: true\n    label:\n      en_US: Query\n      zh_Hans: 查询\n      pt_BR: Query\n  - name: maximum_iterations\n    type: number\n    required: false\n    default: 5\n    label:\n      en_US: Maxium Iterations\n      zh_Hans: 最大迭代次数\n      pt_BR: Maxium Iterations\n    max: 50 # maxとminの値を設定すると、パラメータ表示がスライダーになります\n    min: 1\nextra:\n  python:\n    source: strategies/basic_agent.py\n```\n\n----------------------------------------\n\nTITLE: JSON Validation Code - Python\nDESCRIPTION: This Python code snippet validates a JSON string by attempting to parse it using `json.loads()`. The `main` function takes a JSON string (`json_str`) as input, attempts to parse it into a Python object (`obj`), and returns the parsed object in a dictionary under the key `result`. If the JSON string is invalid, the `json.loads()` function will raise an exception, which is not explicitly handled in this snippet. In the context of the documentation, this node is used to check for errors and trigger exception handling.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_73\n\nLANGUAGE: Python\nCODE:\n```\ndef main(json_str: str) -> dict:\n    obj = json.loads(json_str)\n    return {'result': obj}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx Ports in Docker Compose\nDESCRIPTION: These environment variables configure the ports exposed by the Nginx service within the Dify Docker Compose setup. `EXPOSE_NGINX_PORT` sets the HTTP port, and `EXPOSE_NGINX_SSL_PORT` sets the HTTPS port.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/faq.md#_snippet_3\n\nLANGUAGE: docker\nCODE:\n```\nEXPOSE_NGINX_PORT=80\nEXPOSE_NGINX_SSL_PORT=443\n```\n\n----------------------------------------\n\nTITLE: Pre-calculating Input Tokens in Python\nDESCRIPTION: This Python code snippet outlines the `get_num_tokens` method, which calculates the number of tokens for given prompt messages.  If the model doesn't provide a pre-calculated tokens interface, the method can directly return 0. It takes model details, credentials, prompt messages, and tools as input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                     tools: Optional[list[PromptMessageTool]] = None) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param tools: tools for tool calling\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Mapping Model Invocation Errors in Python\nDESCRIPTION: This property defines a mapping between specific model invocation errors (Exceptions) and the unified InvokeError types used by the Dify runtime. This mapping facilitates consistent error handling across different models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Running SearXNG Docker Container\nDESCRIPTION: This command starts the SearXNG Docker container, mapping port 8081 on the host to port 8080 in the container and mounting a volume for configuration. The -d flag runs the container in detached mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd dify\ndocker run --rm -d -p 8081:8080 -v \"${PWD}/api/core/tools/provider/builtin/searxng/docker:/etc/searxng\" searxng/searxng\n```\n\n----------------------------------------\n\nTITLE: Anthropic Provider YAML Configuration\nDESCRIPTION: This YAML snippet defines the configuration for the Anthropic provider in Dify, including its identifier, display labels, icons, supported model types, configuration methods, and credential schema.  It specifies the API key as a required secret input and allows for an optional API URL. It illustrates the structure required for provider configurations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/new-provider.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nprovider: anthropic  # 供应商标识\nlabel:  # 供应商展示名称，可设置 en_US 英文、zh_Hans 中文两种语言，zh_Hans 不设置将默认使用 en_US。\n  en_US: Anthropic\nicon_small:  # 供应商小图标，存储在对应供应商实现目录下的 _assets 目录，中英文策略同 label\n  en_US: icon_s_en.png\nicon_large:  # 供应商大图标，存储在对应供应商实现目录下的 _assets 目录，中英文策略同 label\n  en_US: icon_l_en.png\nsupported_model_types:  # 支持的模型类型，Anthropic 仅支持 LLM\n- llm\nconfigurate_methods:  # 支持的配置方式，Anthropic 仅支持预定义模型\n- predefined-model\nprovider_credential_schema:  # 供应商凭据规则，由于 Anthropic 仅支持预定义模型，则需要定义统一供应商凭据规则\n  credential_form_schemas:  # 凭据表单项列表\n  - variable: anthropic_api_key  # 凭据参数变量名\n    label:  # 展示名称\n      en_US: API Key\n    type: secret-input  # 表单类型，此处 secret-input 代表加密信息输入框，编辑时只展示屏蔽后的信息。\n    required: true  # 是否必填\n    placeholder:  # PlaceHolder 信息\n      zh_Hans: 在此输入你的 API Key\n      en_US: Enter your API Key\n  - variable: anthropic_api_url\n    label:\n      en_US: API URL\n    type: text-input  # 表单类型，此处 text-input 代表文本输入框\n    required: false\n    placeholder:\n      zh_Hans: 在此输入你的 API URL\n      en_US: Enter your API URL\n```\n\n----------------------------------------\n\nTITLE: Handling Slack Events (Python)\nDESCRIPTION: This Python code defines an endpoint that handles Slack events. It verifies the Slack URL verification challenge, processes app mentions, and sends messages to a Dify app for processing. It uses the `slack_sdk` library to interact with the Slack API and calls the Dify app's `chat.invoke` method.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport traceback\nfrom typing import Mapping\nfrom werkzeug import Request, Response\nfrom dify_plugin import Endpoint\nfrom slack_sdk import WebClient\nfrom slack_sdk.errors import SlackApiError\n\n\nclass SlackEndpoint(Endpoint):\n    def _invoke(self, r: Request, values: Mapping, settings: Mapping) -> Response:\n        \"\"\"\n        Invokes the endpoint with the given request.\n        \"\"\"\n        retry_num = r.headers.get(\"X-Slack-Retry-Num\")\n        if (not settings.get(\"allow_retry\") and (r.headers.get(\"X-Slack-Retry-Reason\") == \"http_timeout\" or ((retry_num is not None and int(retry_num) > 0)))):# noqa: E501\n            return Response(status=200, response=\"ok\")\n        data = r.get_json()\n\n        # Handle Slack URL verification challenge\n        if data.get(\"type\") == \"url_verification\":\n            return Response(\n                response=json.dumps({\"challenge\": data.get(\"challenge\")}),\n                status=200,\n                content_type=\"application/json\"\n            )\n        \n        if (data.get(\"type\") == \"event_callback\"):\n            event = data.get(\"event\")\n            if (event.get(\"type\") == \"app_mention\"):\n                message = event.get(\"text\", \"\")\n                if message.startswith(\"<@\"):\n                    message = message.split(\"> \", 1)[1] if \"> \" in message else message\n                    channel = event.get(\"channel\", \"\")\n                    blocks = event.get(\"blocks\", [])\n                    blocks[0][\"elements\"][0][\"elements\"] = blocks[0].get(\"elements\")[0].get(\"elements\")[1:]\n                    token = settings.get(\"bot_token\")\n                    client = WebClient(token=token)\n                    try: \n                        response = self.session.app.chat.invoke(\n                            app_id=settings[\"app\"][\"app_id\"],\n                            query=message,\n                            inputs={},\n                            response_mode=\"blocking\",\n                        )\n                        try:\n                            blocks[0][\"elements\"][0][\"elements\"][0][\"text\"] = response.get(\"answer\")\n                            result = client.chat_postMessage(\n                                channel=channel,\n                                text=response.get(\"answer\"),\n                                blocks=blocks\n                            )\n                            return Response(\n                                status=200,\n                                response=json.dumps(result),\n                                content_type=\"application/json\"\n                            )\n                        except SlackApiError as e:\n                            raise e\n                    except Exception as e:\n                        err = traceback.format_exc()\n                        return Response(\n                            status=200,\n                            response=\"Sorry, I'm having trouble processing your request. Please try again later.\" + str(err),\n                            content_type=\"text/plain\",\n                        )\n                else:\n                    return Response(status=200, response=\"ok\")\n            else:\n                return Response(status=200, response=\"ok\")\n        else:\n            return Response(status=200, response=\"ok\")\n```\n\n----------------------------------------\n\nTITLE: Defining Frontend Component Schema (JSON)\nDESCRIPTION: Defines the structure of the frontend component using a JSON schema. This schema specifies the labels, form elements (select, text-input, paragraph), variables, and validation rules for the custom moderation settings in the Dify UI. The 'label' provides translations for different languages, and 'form_schema' dictates how the configuration form looks and behaves.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/code-based-extension/moderation.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"label\": {\n        \"en-US\": \"Cloud Service\",\n        \"zh-Hans\": \"クラウドサービス\"\n    },\n    \"form_schema\": [\n        {\n            \"type\": \"select\",\n            \"label\": {\n                \"en-US\": \"Cloud Provider\",\n                \"zh-Hans\": \"クラウドプロバイダー\"\n            },\n            \"variable\": \"cloud_provider\",\n            \"required\": true,\n            \"options\": [\n                {\n                    \"label\": {\n                        \"en-US\": \"AWS\",\n                        \"zh-Hans\": \"AWS\"\n                    },\n                    \"value\": \"AWS\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Google Cloud\",\n                        \"zh-Hans\": \"GoogleCloud\"\n                    },\n                    \"value\": \"GoogleCloud\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Azure Cloud\",\n                        \"zh-Hans\": \"Azure\"\n                    },\n                    \"value\": \"Azure\"\n                }\n            ],\n            \"default\": \"GoogleCloud\",\n            \"placeholder\": \"\"\n        },\n        {\n            \"type\": \"text-input\",\n            \"label\": {\n                \"en-US\": \"API Endpoint\",\n                \"zh-Hans\": \"APIエンドポイント\"\n            },\n            \"variable\": \"api_endpoint\",\n            \"required\": true,\n            \"max_length\": 100,\n            \"default\": \"\",\n            \"placeholder\": \"https://api.example.com\"\n        },\n        {\n            \"type\": \"paragraph\",\n            \"label\": {\n                \"en-US\": \"API Key\",\n                \"zh-Hans\": \"APIキー\"\n            },\n            \"variable\": \"api_keys\",\n            \"required\": true,\n            \"default\": \"\",\n            \"placeholder\": \"ここにAPIキーを貼り付けてください\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Modifying API Extension Logic (TypeScript)\nDESCRIPTION: This TypeScript snippet shows an example of fetching data from a third-party API (Breaking Bad Quotes). It retrieves a specified number of quotes based on the `count` parameter and returns the result.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// ⬇️ implement your logic here ⬇️\n// point === \"app.external_data_tool.query\"\n// https://api.breakingbadquotes.xyz/v1/quotes\nconst count = params?.inputs?.count ?? 1;\nconst url = `https://api.breakingbadquotes.xyz/v1/quotes/${count}`;\nconst result = await fetch(url).then(res => res.text())\n// ⬆️ implement your logic here ⬆️\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository (Bash)\nDESCRIPTION: This command clones the Dify repository from GitHub, providing the source code necessary for setting up the self-hosted environment. It fetches the latest version of the code, including all necessary scripts and configuration files.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\n----------------------------------------\n\nTITLE: Defining RerankResult Data Model in Python\nDESCRIPTION: Defines a `RerankResult` class that inherits from `BaseModel`, representing the result of a reranking operation. It includes the model used and a list of `RerankDocument` objects.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nclass RerankResult(BaseModel):\n    \"\"\"\n    Model class for rerank result.\n    \"\"\"\n    model: str  # Actual model used\n    docs: list[RerankDocument]  # Reranked document list\t\n```\n\n----------------------------------------\n\nTITLE: Bypassing Plugin Signature Verification in Dify\nDESCRIPTION: This snippet demonstrates how to disable plugin signature verification in Dify by setting the `FORCE_VERIFYING_SIGNATURE` environment variable to `false` in the `/docker/.env` file. It then restarts the Dify service using Docker Compose to apply the changes. Use with caution and only in test environments for unknown plugins.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/faq.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ndocker compose down\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Assistant Prompt Template for Chat Models (Conversational/Text Generation Apps)\nDESCRIPTION: This template is designed for the assistant's response in chat models and text generation apps. It provides a placeholder for the model's generated output. Currently, it's an empty string, implying the model is expected to generate content from scratch.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/prompt-engineering/prompt-template.md#_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n\"\"\n```\n\n----------------------------------------\n\nTITLE: Authorization Header Example\nDESCRIPTION: This snippet shows the required format for the Authorization header.  It includes the 'Bearer' scheme followed by the API key.  The API key is used to authenticate requests to the external knowledge base.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/external-knowledge-api-documentation.md#_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nAuthorization: Bearer {API_KEY}\n```\n\n----------------------------------------\n\nTITLE: Define Text Embedding Result Model (Python)\nDESCRIPTION: Defines a Pydantic model `TextEmbeddingResult` representing the result of a text embedding operation. It includes attributes for the `model` used, the list of `embeddings` (each embedding is a list of floats), and the `usage` information (as an `EmbeddingUsage` object).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nclass TextEmbeddingResult(BaseModel):\n    \"\"\"\n    Model class for text embedding result.\n    \"\"\"\n    model: str  # Actual model used\n    embeddings: list[list[float]]  # List of embedding vectors, corresponding to the input texts list\n    usage: EmbeddingUsage  # Usage information\n```\n\n----------------------------------------\n\nTITLE: Running Plugin in Debug Mode\nDESCRIPTION: This snippet shows the command to run the Dify plugin in debug mode. This command starts the plugin and connects to the remote debugging server.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/debug-plugin.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npython -m main\n```\n\n----------------------------------------\n\nTITLE: Install Plugins\nDESCRIPTION: This snippet demonstrates how to install the extracted plugins using `poetry` within the `docker-api` container. It assumes that the network can access `https://marketplace.dify.ai`. `a3cb19c2****` is a placeholder for the actual container ID. The `--workers` argument controls parallel downloads.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/dify-premium.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask install-plugins --workers=2\n```\n\n----------------------------------------\n\nTITLE: Calculating Variance in Dify Workflow (Python)\nDESCRIPTION: This Python snippet shows how to calculate the variance of an array of numbers within a Dify workflow. It takes a list `x` as input and returns the calculated variance as the 'result' output variable. The snippet uses a list comprehension for conciseness.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/code.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef main(x: list) -> float:\n    return {\n        # Note to declare 'result' in the output variables\n        'result': sum([(i - sum(x) / len(x)) ** 2 for i in x]) / len(x)\n    }\n```\n\n----------------------------------------\n\nTITLE: Tool Prompt Message Class\nDESCRIPTION: Defines the `ToolPromptMessage` class, representing a message from a tool. It includes the `tool_call_id` and uses the `content` field for the tool execution result.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nclass ToolPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for tool prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.TOOL\n    tool_call_id: str  # 工具调用 ID，若不支持 OpenAI tool call，也可传入工具名称\n```\n\n----------------------------------------\n\nTITLE: Connect Local Repository to GitHub - Bash\nDESCRIPTION: This snippet connects the local Git repository to a remote GitHub repository. Replace `<your-username>` with the user's GitHub username and `<repository-name>` with the name of the GitHub repository. This allows pushing local changes to the remote repository.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add origin https://github.com/<your-username>/<repository-name>.git\n```\n\n----------------------------------------\n\nTITLE: Run plugin using Python\nDESCRIPTION: This command is used to start the plugin by executing the `main.py` file within the project using the Python interpreter. It initiates the plugin process and makes it available for testing and debugging within the Dify environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/debug-plugin.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython -m main\n```\n\n----------------------------------------\n\nTITLE: Accessing the Administrator Initialization Page\nDESCRIPTION: This command provides the URL to access the Dify administrator initialization page, where the admin account can be set up. The URL varies depending on whether it's a local or server environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/docker-compose.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Local environment\nhttp://localhost/install\n\n# Server environment\nhttp://your_server_ip/install\n```\n\n----------------------------------------\n\nTITLE: Get Document Embedding Status - Dify API (JSON Response)\nDESCRIPTION: This is the JSON response received when retrieving the document embedding status from the Dify API. It contains an array of objects, each representing a document's indexing status, including information about processing timestamps, completion status, errors, and segment progress.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\":[{\n    \"id\": \"\",\n    \"indexing_status\": \"indexing\",\n    \"processing_started_at\": 1681623462.0,\n    \"parsing_completed_at\": 1681623462.0,\n    \"cleaning_completed_at\": 1681623462.0,\n    \"splitting_completed_at\": 1681623462.0,\n    \"completed_at\": null,\n    \"paused_at\": null,\n    \"error\": null,\n    \"stopped_at\": null,\n    \"completed_segments\": 24,\n    \"total_segments\": 100\n  }]\n}\n```\n\n----------------------------------------\n\nTITLE: Get Parameters and Execute Logic (Python)\nDESCRIPTION: This Python code shows how to retrieve and utilize the parameters that were provided by the user. An instance of the `BasicParams` class is created using the input `parameters` dictionary. It then proceeds to perform specific business logic using these parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack on Linux/MacOS\nDESCRIPTION: This command installs GPUStack as a service on systemd or launchd based systems. It downloads and executes an installation script from the specified URL.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/gpustack.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | sh -s -\n```\n\n----------------------------------------\n\nTITLE: Frontend Schema for Weather Search Tool (JSON)\nDESCRIPTION: Defines the frontend schema for the 'Weather Search' external data tool using a JSON structure. It specifies the label, form schema (containing a select field for temperature unit with Fahrenheit and Centigrade options), variable name, requirement status, default value, and placeholder text.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/external-data-tool.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"label\": {\n        \"en-US\": \"Weather Search\",\n        \"zh-Hans\": \"天气查询\"\n    },\n    \"form_schema\": [\n        {\n            \"type\": \"select\",\n            \"label\": {\n                \"en-US\": \"Temperature Unit\",\n                \"zh-Hans\": \"温度单位\"\n            },\n            \"variable\": \"temperature_unit\",\n            \"required\": true,\n            \"options\": [\n                {\n                    \"label\": {\n                        \"en-US\": \"Fahrenheit\",\n                        \"zh-Hans\": \"华氏度\"\n                    },\n                    \"value\": \"fahrenheit\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Centigrade\",\n                        \"zh-Hans\": \"摄氏度\"\n                    },\n                    \"value\": \"centigrade\"\n                }\n            ],\n            \"default\": \"centigrade\",\n            \"placeholder\": \"Please select temperature unit\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Array to Text Conversion with Code Node in Python\nDESCRIPTION: This Python code snippet demonstrates how to convert an array of strings (articleSections) into a single string, joining the array elements with newline characters. This addresses the limitation that the iteration node outputs in array format, which is often not directly suitable for final output.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/iteration.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef main(articleSections: list):\n    data = articleSections\n    return {\n        \"result\": \"/n\".join(data)\n    }\n```\n\n----------------------------------------\n\nTITLE: Creating Streaming Variable Messages in Python\nDESCRIPTION: This Python code snippet demonstrates creating streaming variable messages for a typewriter effect in Dify. It takes a variable name (string) and a variable value (string) as input, returning a ToolInvokeMessage. Currently, only string data is supported for streaming variables.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/tool.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef create_stream_variable_message(\n    self, variable_name: str, variable_value: str\n) -> ToolInvokeMessage:\n```\n\n----------------------------------------\n\nTITLE: Configuring Workflow Application for WeChat\nDESCRIPTION: This code snippet demonstrates the configuration needed in the `config.json` file to integrate a Dify workflow application with WeChat.  It involves specifying the API base, API key, application type as 'workflow', and the channel type.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n  {\n    \"dify_api_base\": \"https://api.dify.ai/v1\",\n    \"dify_api_key\": \"app-xxx\",\n    \"dify_app_type\": \"workflow\",\n    \"channel_type\": \"wx\",\n    \"model\": \"dify\",\n    \"single_chat_prefix\": [\"\"],\n    \"single_chat_reply_prefix\": \"\",\n    \"group_chat_prefix\": [\"@bot\"],\n    \"group_name_white_list\": [\"ALL_GROUP\"]\n }\n\n```\n\n----------------------------------------\n\nTITLE: Anthropic Model Configuration (YAML)\nDESCRIPTION: This YAML file defines the configuration for the Anthropic model provider. It specifies the provider name, labels, descriptions, icons, supported model types, configuration methods, credential schema, and model definitions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprovider: anthropic\nlabel:\n  en_US: Anthropic\ndescription:\n  en_US: Anthropic's powerful models, such as Claude 3.\n  zh_Hans: Anthropicの強力なモデル（例：Claude 3）。\nicon_small:\n  en_US: icon_s_en.svg\nicon_large:\n  en_US: icon_l_en.svg\nbackground: \"#F0F0EB\"\nhelp:\n  title:\n    en_US: Get your API Key from Anthropic\n    zh_Hans: AnthropicからAPIキーを取得\n  url:\n    en_US: https://console.anthropic.com/account/keys\nsupported_model_types:\n  - llm\nconfigurate_methods:\n  - predefined-model\nprovider_credential_schema:\n  credential_form_schemas:\n    - variable: anthropic_api_key\n      label:\n        en_US: API Key\n      type: secret-input\n      required: true\n      placeholder:\n        zh_Hans: APIキーを入力してください\n        en_US: Enter your API Key\n    - variable: anthropic_api_url\n      label:\n        en_US: API URL\n      type: text-input\n      required: false\n      placeholder:\n        zh_Hans: API URLを入力してください\n        en_US: Enter your API URL\nmodels:\n  llm:\n    predefined:\n      - \"models/llm/*.yaml\"\n    position: \"models/llm/_position.yaml\"\nextra:\n  python:\n    provider_source: provider/anthropic.py\n    model_sources:\n      - \"models/llm/llm.py\"\n```\n\n----------------------------------------\n\nTITLE: Define JSON Schema for UI Generation\nDESCRIPTION: This JSON Schema defines a structure for dynamically generating UIs with nested components. It allows for recursive UI structures using the $ref keyword. It specifies the types, labels, children, and attributes that can be associated with UI components, enabling the generation of complex user interfaces.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/how-to-use-json-schema-in-dify.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n        \"name\": \"ui\",\n        \"description\": \"Dynamically generated UI\",\n        \"strict\": true,\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"type\": {\n                    \"type\": \"string\",\n                    \"description\": \"The type of the UI component\",\n                    \"enum\": [\"div\", \"button\", \"header\", \"section\", \"field\", \"form\"]\n                },\n                \"label\": {\n                    \"type\": \"string\",\n                    \"description\": \"The label of the UI component, used for buttons or form fields\"\n                },\n                \"children\": {\n                    \"type\": \"array\",\n                    \"description\": \"Nested UI components\",\n                    \"items\": {\n                        \"$ref\": \"#\"\n                    }\n                },\n                \"attributes\": {\n                    \"type\": \"array\",\n                    \"description\": \"Arbitrary attributes for the UI component, suitable for any element\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\n                                \"type\": \"string\",\n                                \"description\": \"The name of the attribute, for example onClick or className\"\n                            },\n                            \"value\": {\n                                \"type\": \"string\",\n                                \"description\": \"The value of the attribute\"\n                            }\n                        },\n                      \"additionalProperties\": false,\n                      \"required\": [\"name\", \"value\"]\n                    }\n                }\n            },\n            \"required\": [\"type\", \"label\", \"children\", \"attributes\"],\n            \"additionalProperties\": false\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Checking for Tool Calls in LLM Result Python\nDESCRIPTION: This Python code defines the `check_tool_calls` method. It checks if an LLM result chunk contains any tool calls by inspecting the `llm_result_chunk.delta.message.tool_calls` attribute.  It returns a boolean value indicating the presence of tool calls.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n    def check_tool_calls(self, llm_result_chunk: LLMResultChunk) -> bool:\n        \"\"\"\n        Check if there is any tool call in llm result chunk\n        \"\"\"\n        return bool(llm_result_chunk.delta.message.tool_calls)\n\n```\n\n----------------------------------------\n\nTITLE: Calculating Variance with Python\nDESCRIPTION: This Python code snippet calculates the variance of a list of numbers. It takes a list `x` as input, calculates the mean, and then computes the sum of squared differences from the mean, divided by the length of the list. The result is returned in a dictionary under the key 'result'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/code.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef main(x: list) -> float:\n    return {\n        # 出力変数にresultを宣言することに注意\n        'result': sum([(i - sum(x) / len(x)) ** 2 for i in x]) / len(x)\n    }\n```\n\n----------------------------------------\n\nTITLE: JSON Validation Code (Code Node)\nDESCRIPTION: This code snippet validates a JSON string in a code node. It attempts to parse the JSON string and returns a dictionary containing the parsed object. An error in the JSON format results in the execution of a predefined alternative branch.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef main(json_str: str) -> dict:\n    obj = json.loads(json_str)\n    return {'result': obj}\n```\n\n----------------------------------------\n\nTITLE: Installing Xinference with pip\nDESCRIPTION: This snippet installs Xinference, a distributed inference framework, using pip. It installs the base dependencies and optionally the ggml and PyTorch dependencies for specific model types. Xinference provides the ability to deploy and manage LLMs for inference.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\n$ pip install \"xinference\"\n$ pip install \"xinference[ggml]\"\n$ pip install \"xinference[pytorch]\"\n$ pip install \"xinference[all]\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Cloud Service Moderation Class in Python\nDESCRIPTION: This Python code defines the `CloudServiceModeration` class, which extends the `Moderation` base class. It includes methods for validating the configuration (`validate_config`), moderating inputs (`moderation_for_inputs`), moderating outputs (`moderation_for_outputs`), and implementing the custom moderation logic (`_is_violated`). The class adheres to the Dify custom moderation extension architecture.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/code-based-extension/moderation.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom core.moderation.base import Moderation, ModerationAction, ModerationInputsResult, ModerationOutputsResult\n\nclass CloudServiceModeration(Moderation):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"cloud_service\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user saves the config.\n\n        Example:\n            .. code-block:: python\n                config = {\n                    \"cloud_provider\": \"GoogleCloud\",\n                    \"api_endpoint\": \"https://api.example.com\",\n                    \"api_keys\": \"123456\",\n                    \"inputs_config\": {\n                        \"enabled\": True,\n                        \"preset_response\": \"Your content violates our usage policy. Please revise and try again.\"\n                    },\n                    \"outputs_config\": {\n                        \"enabled\": True,\n                        \"preset_response\": \"Your content violates our usage policy. Please revise and try again.\"\n                    }\n                }\n\n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n\n        cls._validate_inputs_and_outputs_config(config, True)\n\n        if not config.get(\"cloud_provider\"):\n            raise ValueError(\"cloud_provider is required\")\n\n        if not config.get(\"api_endpoint\"):\n            raise ValueError(\"api_endpoint is required\")\n\n        if not config.get(\"api_keys\"):\n            raise ValueError(\"api_keys is required\")\n\n    def moderation_for_inputs(self, inputs: dict, query: str = \"\") -> ModerationInputsResult:\n        \"\"\"\n        Moderation for inputs.\n\n        :param inputs: user inputs\n        :param query: the query of chat app, there is empty if is completion app\n        :return: the moderation result\n        \"\"\"\n        flagged = False\n        preset_response = \"\"\n\n        if self.config['inputs_config']['enabled']:\n            preset_response = self.config['inputs_config']['preset_response']\n\n            if query:\n                inputs['query__'] = query\n            flagged = self._is_violated(inputs)\n\n        # return ModerationInputsResult(flagged=flagged, action=ModerationAction.overridden, inputs=inputs, query=query)\n        return ModerationInputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)\n\n    def moderation_for_outputs(self, text: str) -> ModerationOutputsResult:\n        \"\"\"\n        Moderation for outputs.\n\n        :param text: the text of LLM response\n        :return: the moderation result\n        \"\"\"\n        flagged = False\n        preset_response = \"\"\n\n        if self.config['outputs_config']['enabled']:\n            preset_response = self.config['outputs_config']['preset_response']\n\n            flagged = self._is_violated({'text': text})\n\n        # return ModerationOutputsResult(flagged=flagged, action=ModerationAction.overridden, text=text)\n        return ModerationOutputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)\n\n    def _is_violated(self, inputs: dict):\n        \"\"\"\n        The main logic of moderation.\n\n        :param inputs:\n        :return: the moderation result\n        \"\"\"\n        return False\n```\n\n----------------------------------------\n\nTITLE: CloudServiceModeration Class Template (Python)\nDESCRIPTION: Provides a template for the `CloudServiceModeration` class, illustrating the structure and methods needed for custom moderation.  Includes `validate_config` for schema validation and `moderation_for_inputs` and `moderation_for_outputs` for implementing input and output moderation logic, respectively. The template shows where custom logic should be added.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/code-based-extension/moderation.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom core.moderation.base import Moderation, ModerationAction, ModerationInputsResult, ModerationOutputsResult\n\nclass CloudServiceModeration(Moderation):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"cloud_service\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user saves the config.\n        \n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n        cls._validate_inputs_and_outputs_config(config, True)\n        \n        # implement your own logic here\n\n    def moderation_for_inputs(self, inputs: dict, query: str = \"\") -> ModerationInputsResult:\n        \"\"\"\n        Moderation for inputs.\n\n        :param inputs: user inputs\n        :param query: the query of chat app, there is empty if is completion app\n        :return: the moderation result\n        \"\"\"\n        flagged = False\n        preset_response = \"\"\n        \n        # implement your own logic here\n        \n        # return ModerationInputsResult(flagged=flagged, action=ModerationAction.overridden, inputs=inputs, query=query)\n        return ModerationInputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)\n\n    def moderation_for_outputs(self, text: str) -> ModerationOutputsResult:\n        \"\"\"\n        Moderation for outputs.\n\n        :param text: the text of LLM response\n        :return: the moderation result\n        \"\"\"\n        flagged = False\n        preset_response = \"\"\n        \n        # implement your own logic here\n\n        # return ModerationOutputsResult(flagged=flagged, action=ModerationAction.overridden, text=text)\n        return ModerationOutputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dify Chatbot Bubble Button\nDESCRIPTION: This JavaScript code snippet demonstrates how to configure the Dify Chatbot Bubble Button using the `window.difyChatbotConfig` object. It includes options for setting the token, development mode, base URL, container properties, draggable behavior, drag axis, and input values. The token is mandatory, while the others are optional configurations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-publishing/embedding-in-websites.md#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.difyChatbotConfig = {\n    // 必須：Dify によって自動的に生成されます\n    token: 'YOUR_TOKEN',\n    // オプション：デフォルトは false です\n    isDev: false,\n    // オプション：isDev が true の場合、デフォルトは '[https://dev.udify.app](https://dev.udify.app)'、それ以外の場合は '[https://udify.app](https://udify.app)' です\n    baseUrl: 'YOUR_BASE_URL',\n    // オプション：`id` 以外の有効な HTMLElement 属性（例：`style`、`className` など）を受け入れます\n    containerProps: {},\n    // オプション：ボタンのドラッグを許可するかどうか、デフォルトは `false` です\n    draggable: false,\n    // オプション：ボタンのドラッグを許可する軸、デフォルトは 'both'、'x'、'y'、'both' のいずれかを指定できます\n    dragAxis: 'both',\n    // オプション:dify チャットボットに設定されている入力オブジェクト\n    inputs: {\n        // key は変数名です\n        // 例:\n        // name: \"NAME\"\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: LLM Prompt for Structure Extraction in Dify (Japanese)\nDESCRIPTION: This prompt is designed for the structure extraction LLM node in Dify. It instructs the LLM to comprehensively analyze the structure of an article, detailing the content of each section. The prompt provides guidelines on the expected output format, including markdown formatting and specific instructions for analyzing different parts of the article such as introduction, methodology, results, and discussion.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/workshop/intermediate/article-reader.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n以下の記事を読み、タスクを実行してください\n{{文書抽出器の結果の変数}}\n\n#### タスク\n\n- **主要目標**：記事の構造を包括的に解析すること。\n- **目標**：各部分の内容を詳細に説明すること。\n- **要求**：可能な限り詳細に分析すること。\n- **制限**：特定の形式の制限はないが、解析の整理と論理性を維持する必要があります。\n- **期待される出力**：各部分の主要な内容と役割を含む、記事の構造の詳細な解析。\n\n# 推論の手順\n\n- **推論部分**：記事を注意深く読み、その構造を識別し解析します。\n- **結論部分**：各部分の具体的な内容と役割を提供します。\n\n# 出力形式\n\n- **解析形式**：各部分は見出し形式でリストアップされ、その後に詳細な説明が続きます。\n- **構造形式**：Markdownを使用し、可読性を向上させます。\n- **具体的な説明**：導入、本文、結論、引用など、各部分の内容と役割について。\n\n# サンプル出力\n\n## サンプル記事の解析\n\n### 導入\n- **内容**：研究の背景、目的、重要性を紹介します。\n- **役割**：読者の注意を引き、記事のコンテキストを提供します。\n\n### 方法\n- **内容**：研究の具体的な方法や手順、実験設計、データ収集、分析技術を説明します。\n- **役割**：読者に研究の科学性と再現性を理解させます。\n\n### 結果\n- **内容**：研究の主な発見とデータを示します。\n- **役割**：研究結論の根拠を提供します。\n\n### 議論\n- **内容**：結果の意義を説明し、他の研究と比較し、改善の可能性を提案します。\n- **役割**：読者に結果の広範な影響と将来の研究の可能性を理解させます。\n\n### 結論\n- **内容**：研究の主な発見と貢献を要約します。\n- **役割**：記事の中核情報を強調し、明確な結論を提供します。\n\n### 引用\n- **内容**：記事で引用されているすべての文献をリストアップします。\n- **役割**：さらなる読書のリソースを提供し、学術的誠実性を確保します。\n\n# 備考\n\n- **境界条件**：記事の構造が典型的でない場合（たとえば、特定の部分が欠落している場合や余分な部分がある場合など）、解析でこれらの特殊な状況を明示する必要があります。\n- **重要な考慮事項**：解析時には記事の論理性と整合性に注意し、各部分の内容が記事全体の目標と一致していることを確認してください。\n```\n\n----------------------------------------\n\nTITLE: Defining Plugin Endpoint Path (YAML)\nDESCRIPTION: This YAML code defines the endpoint for the plugin, specifying the path, method, and the Python file that implements the functionality.  The path is set to `/neko`, the method is `GET`, and the implementation code is in `endpoints/test_plugin.py`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npath: \"/neko\"\nmethod: \"GET\"\nextra:\n  python:\n    source: \"endpoints/test_plugin.py\"\n```\n\n----------------------------------------\n\nTITLE: Restart Dify Services (Bash)\nDESCRIPTION: These commands restart the Dify services using Docker Compose. First, it stops the running containers, and then it starts them again in detached mode. This is necessary to apply the environment variable changes made in the Docker Compose override file, enabling the third-party signature verification feature.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ndocker compose down\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: ToolPromptMessage Class Definition in Python\nDESCRIPTION: Defines the `ToolPromptMessage` class, inheriting from `PromptMessage`, representing a message from a tool. It sets the `role` to `PromptMessageRole.TOOL` and includes the `tool_call_id`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nclass ToolPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for tool prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.TOOL\n    tool_call_id: str  # 工具调用 ID，若不支持 OpenAI tool call，也可传入工具名称\n```\n\n----------------------------------------\n\nTITLE: Agent Plugin Template Initialization Prompts (Bash)\nDESCRIPTION: This code shows the prompts and options presented during the Agent plugin template initialization. It requests the plugin name, author, and description, and lets the user select the programming language (Python) and plugin type (agent-strategy). It also allows configuration of permissions like backwards invocation, tool usage, and model invocation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n➜ ./dify-plugin-darwin-arm64 plugin init                                                                                                                                 ─╯\nEdit profile of the plugin\nPlugin name (press Enter to next step): # プラグイン名を入力\nAuthor (press Enter to next step): # プラグイン作者を入力\nDescription (press Enter to next step): # プラグインの説明を入力\n---\nSelect the language you want to use for plugin development, and press Enter to continue,\nBTW, you need Python 3.12+ to develop the Plugin if you choose Python.\n-> python # Python環境を選択\n  go (not supported yet)\n---\nBased on the ability you want to extend, we have divided the Plugin into four types: Tool, Model, Extension, and Agent Strategy.\n\n- Tool: It's a tool provider, but not only limited to tools, you can implement an endpoint there, for example, you need both Sending Message and Receiving Message if you are\n- Model: Just a model provider, extending others is not allowed.\n- Extension: Other times, you may only need a simple http service to extend the functionalities, Extension is the right choice for you.\n- Agent Strategy: Implement your own logics here, just by focusing on Agent itself\n\nWhat's more, we have provided the template for you, you can choose one of them below:\n  tool\n-> agent-strategy # Agent戦略テンプレートを選択\n  llm\n  text-embedding\n---\nConfigure the permissions of the plugin, use up and down to navigate, tab to select, after selection, press enter to finish\nBackwards Invocation:\nTools:\n    Enabled: [✔]  You can invoke tools inside Dify if it's enabled # デフォルトで有効\nModels:\n    Enabled: [✔]  You can invoke models inside Dify if it's enabled # デフォルトで有効\n    LLM: [✔]  You can invoke LLM models inside Dify if it's enabled # デフォルトで有効\n  → Text Embedding: [✘]  You can invoke text embedding models inside Dify if it's enabled\n    Rerank: [✘]  You can invoke rerank models inside Dify if it's enabled\n    TTS: [✘]  You can invoke TTS models inside Dify if it's enabled\n    Speech2Text: [✘]  You can invoke speech2text models inside Dify if it's enabled\n    Moderation: [✘]  You can invoke moderation models inside Dify if it's enabled\nApps:\n    Enabled: [✘]  Ability to invoke apps like BasicChat/ChatFlow/Agent/Workflow etc.\nResources:\nStorage:\n    Enabled: [✘]  Persistence storage for the plugin\n    Size: N/A  The maximum size of the storage\nEndpoints:\n    Enabled: [✘]  Ability to register endpoints\n```\n\n----------------------------------------\n\nTITLE: Convert Array to Text with Code Node - Python\nDESCRIPTION: This Python code snippet demonstrates how to convert an array (list) of strings into a single string with newline characters using the `join` method. The input is a list called `articleSections`. The output is a dictionary containing a key `result` with the joined string as its value.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_73\n\nLANGUAGE: python\nCODE:\n```\ndef main(articleSections: list):\n    data = articleSections\n    return {\n        \"result\": \"/n\".join(data)\n    }\n```\n\n----------------------------------------\n\nTITLE: Importing Application DSL file via URL\nDESCRIPTION: Demonstrates the URL format required to import an application's configuration using a Dify DSL (Domain Specific Language) file. This approach allows applications to be created based on configurations hosted online.  The URL should point to a valid YML file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-orchestrate/creating-an-application.md#_snippet_0\n\nLANGUAGE: url\nCODE:\n```\nhttps://example.com/your_dsl.yml\n```\n\n----------------------------------------\n\nTITLE: Get Dataset Metadata List - Dify API - Bash\nDESCRIPTION: This snippet retrieves the list of metadata fields for a Dify dataset using a GET request. It requires the dataset ID. The request includes a header for authorization (API key).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Get Number of Tokens Method\nDESCRIPTION: Implements the method for calculating the number of tokens for given prompt messages. If the model does not provide a native token counting interface, the method can either return 0 directly or use the `_get_num_tokens_by_gpt2` method as an alternative.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/customizable-model.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                     tools: Optional[list[PromptMessageTool]] = None) -> int:\n  \"\"\"\n  Get number of tokens for given prompt messages\n\n  :param model: model name\n  :param credentials: model credentials\n  :param prompt_messages: prompt messages\n  :param tools: tools for tool calling\n  :return:\n  \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implement Plugin Functionality (Python)\nDESCRIPTION: This Python code defines the functionality of the plugin, which serves an animated ASCII art of a rainbow cat.  It uses Flask to create a web server and the dify_plugin library to define an endpoint. It requires the werkzeug, flask, and dify-plugin libraries.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Mapping\nfrom werkzeug import Request, Response\nfrom flask import Flask, render_template_string\nfrom dify_plugin import Endpoint\n\napp = Flask(__name__)\n\nclass NekoEndpoint(Endpoint):\n    def _invoke(self, r: Request, values: Mapping, settings: Mapping) -> Response:\n        ascii_art = '''\n⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛⬛️⬜️⬜️⬜️⬜️⬜⬜️⬜️️\n🟥🟥⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️🟥🟥🟥🟥🟥🟥🟥🟥⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬛🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧⬛️⬜️⬜️⬜️⬜️⬜⬜️️\n🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥⬛️🥧🥧🥧💟💟💟💟💟💟💟💟💟💟💟💟💟🥧🥧🥧⬛️⬜️⬜️⬜️⬜⬜️️\n🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥🟥⬛️🥧🥧💟💟💟💟💟💟🍓💟💟🍓💟💟💟💟💟🥧🥧⬛️⬜️⬜️⬜️⬜️⬜️️\n🟧🟧🟥🟥🟥🟥🟥🟥🟥🟥🟧🟧🟧🟧🟧🟧🟧🟧🟥🟥🟥🟥🟥🟥🟥⬛🥧💟💟🍓💟💟💟💟💟💟💟💟💟💟💟💟💟💟🥧⬛️⬜️⬜️⬜️⬜⬜️️\n🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧⬛️🥧💟💟💟💟💟💟💟💟💟💟⬛️⬛️💟💟🍓💟💟🥧⬛️⬜️⬛️️⬛️️⬜⬜️️\n🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧🟧⬛️🥧💟💟💟💟💟💟💟💟💟⬛️🌫🌫⬛💟💟💟💟🥧⬛️⬛️🌫🌫⬛⬜️️\n🟨🟨🟧🟧🟧🟧🟧🟧🟧🟧🟨🟨🟨🟨🟨🟨🟨🟨🟧⬛️⬛️⬛️⬛️🟧🟧⬛️🥧💟💟💟💟💟💟🍓💟💟⬛️🌫🌫🌫⬛💟💟💟🥧⬛️🌫🌫🌫⬛⬜️️\n🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨⬛️🌫🌫⬛️⬛️🟧⬛️🥧💟💟💟💟💟💟💟💟💟⬛️🌫🌫🌫🌫⬛️⬛️⬛️⬛️🌫🌫🌫🌫⬛⬜️️\n🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨🟨⬛️⬛️🌫🌫⬛️⬛️⬛️🥧💟💟💟🍓💟💟💟💟💟⬛️🌫🌫🌫🌫🌫🌫🌫🌫🌫🌫🌫🌫⬛⬜️️\n🟩🟩🟨🟨🟨🟨🟨🟨🟨🟨🟩🟩🟩🟩🟩🟩🟩🟩🟨🟨⬛⬛️🌫🌫⬛️⬛️🥧💟💟💟💟💟💟💟🍓⬛️🌫🌫🌫⬜️⬛️🌫🌫🌫🌫🌫⬜️⬛️🌫🌫⬛️\n🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩⬛️⬛️🌫🌫⬛️🥧💟🍓💟💟💟💟💟💟⬛️🌫🌫🌫⬜️⬛️🌫🌫🌫🌫🌫⬜️⬛️🌫🌫⬛️\n️🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩⬛️⬛️⬛️⬛️🥧💟💟💟💟💟💟💟💟⬛️🌫🌫🌫⬛️⬛️🌫🌫🌫⬛️🌫⬛️⬛️🌫🌫⬛️\n🟦🟦🟩🟩🟩🟩🟩🟩🟩🟩🟦🟦🟦🟦🟦🟦🟦🟦🟩🟩🟩🟩🟩🟩⬛️⬛️🥧💟💟💟💟💟🍓💟💟⬛🌫🟥🟥🌫🌫🌫🌫🌫🌫🌫🌫🌫🟥🟥⬛️\n🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦⬛️🥧🥧💟🍓💟💟💟💟💟⬛️🌫🟥🟥🌫⬛️🌫🌫⬛️🌫🌫⬛️🌫🟥🟥⬛️\n🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦⬛️🥧🥧🥧💟💟💟💟💟💟💟⬛️🌫🌫🌫⬛️⬛️⬛️⬛️⬛️⬛️⬛️🌫🌫⬛️⬜️\n🟪🟪🟦🟦🟦🟦🟦🟦🟦🟦🟪🟪🟪🟪🟪🟪🟪🟪🟦🟦🟦🟦🟦🟦⬛️⬛️⬛️🥧🥧🥧🥧🥧🥧🥧🥧🥧🥧⬛️🌫🌫🌫🌫🌫🌫🌫🌫🌫🌫⬛️⬜️⬜️\n🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪⬛️🌫🌫🌫⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬜️⬜️⬜️\n🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪⬛️🌫🌫⬛️⬛️⬜️⬛️🌫🌫⬛️⬜️⬜️⬜️⬜️⬜️⬛️🌫🌫⬛️⬜️⬛️🌫🌫⬛️⬜️⬜️⬜️⬜️\n⬜️⬜️🟪🟪🟪🟪🟪🟪🟪🟪⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬜️🟪🟪🟪🟪🟪⬛️⬛️⬛️⬛⬜️⬜️⬛️⬛️⬛️⬜️⬜️⬜️⬜️⬜️⬜️⬜️⬛️⬛️⬛️⬜️⬜️⬛️⬛️⬜️⬜️⬜️⬜️⬜️️\n        '''\n        ascii_art_lines = ascii_art.strip().split('\\n')\n        with app.app_context():\n            return Response(render_template_string('''\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <style>\n            body {\n                background-color: black;\n                color: white;\n                overflow: hidden;\n                margin: 0;\n                padding: 0;\n            }\n            #ascii-art {\n                font-family: monospace;\n                white-space: pre;\n                position: absolute;\n                top: 50%;\n                transform: translateY(-50%);\n                display: inline-block;\n                font-size: 16px;\n                line-height: 1;\n            }\n        </style>\n    </head>\n    <body>\n        <div id=\"ascii-art\"></div>\n        <script>\n            var asciiArtLines = {{ ascii_art_lines | tojson }};\n            var asciiArtDiv = document.getElementById(\"ascii-art\");\n            var index = 0;\n            function displayNextLine() {\n                if (index < asciiArtLines.length) {\n                    var line = asciiArtLines[index];\n                    var lineElement = document.createElement(\"div\");\n                    lineElement.innerHTML = line;\n                    asciiArtDiv.appendChild(lineElement);\n                    index++;\n                    setTimeout(displayNextLine, 100);\n                } else {\n                    animateCat();\n                }\n            }\n            function animateCat() {\n                var pos = 0;\n                var screenWidth = window.innerWidth;\n                var catWidth = asciiArtDiv.offsetWidth;\n                function move() {\n                    asciiArtDiv.style.left = pos + \"px\";\n                    pos += 2;\n                    if (pos > screenWidth) {\n                        pos = -catWidth;\n                    }\n                    requestAnimationFrame(move);\n                }\n                move();\n            }\n            displayNextLine();\n        </script>\n    </body>\n    </html>\n        ''', ascii_art_lines=ascii_art_lines), status=200, content_type=\"text/html\")\n```\n\n----------------------------------------\n\nTITLE: Creating an Image Message in Dify (Python)\nDESCRIPTION: This code snippet demonstrates the interface for creating an image message in a Dify tool plugin. It takes an image URL as input and returns a `ToolInvokeMessage`. The `create_image_message` function will automatically download the image and return it to the user.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/tool.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef create_image_message(self, image: str) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Initializing Conversation IDs Dictionary\nDESCRIPTION: This Python code initializes a dictionary to store conversation IDs for each WhatsApp number. This allows the chatbot to maintain context across multiple messages from the same user.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-whatsapp.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconversation_ids = {}\n```\n\n----------------------------------------\n\nTITLE: Uploading Local Files to Cloud Storage (Flask Command)\nDESCRIPTION: Uploads local files to cloud storage.  Requires cloud storage configuration (e.g., aliyun-oss) to be set up.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nflask upload-local-files-to-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Example for Adding Fields\nDESCRIPTION: This JSON schema example demonstrates how to add individual fields, in this case, 'username', to a schema. It defines the 'name', 'type', 'description' and 'required' properties of the field. This example shows the fundamental structure of defining a single field within a larger JSON schema.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/llm.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"name\": \"username\",\n  \"type\": \"string\",\n  \"description\": \"user's name\",\n  \"required\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Extract plugins from previous version\nDESCRIPTION: This command extracts tools and model providers from the previous Dify version and converts them into plugins. It runs within the `docker-api` container using Poetry to manage dependencies. The `workers` parameter controls the number of parallel processes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/migration/migrate-to-v1.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask extract-plugins --workers=20\n```\n\n----------------------------------------\n\nTITLE: Create Document from File - Dify API (JSON Response)\nDESCRIPTION: This is the JSON response received after successfully creating a document from a file via the Dify API. It includes details about the newly created document, such as its ID, position, data source type, name, creation timestamp, indexing status, and other relevant metadata. A batch ID is also returned for tracking the processing of the document.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"document\": {\n    \"id\": \"\",\n    \"position\": 1,\n    \"data_source_type\": \"upload_file\",\n    \"data_source_info\": {\n      \"upload_file_id\": \"\"\n    },\n    \"dataset_process_rule_id\": \"\",\n    \"name\": \"Dify.txt\",\n    \"created_from\": \"api\",\n    \"created_by\": \"\",\n    \"created_at\": 1695308667,\n    \"tokens\": 0,\n    \"indexing_status\": \"waiting\",\n    \"error\": null,\n    \"enabled\": true,\n    \"disabled_at\": null,\n    \"disabled_by\": null,\n    \"archived\": false,\n    \"display_status\": \"queuing\",\n    \"word_count\": 0,\n    \"hit_count\": 0,\n    \"doc_form\": \"text_model\"\n  },\n  \"batch\": \"\"\n}\n\n```\n\n----------------------------------------\n\nTITLE: Customizing Dify Chatbot Button with CSS Variables\nDESCRIPTION: This CSS snippet demonstrates how to customize the appearance of the Dify chatbot bubble button using CSS variables.  It shows how to modify properties such as position, background color, size, border radius, box shadow, and hover transformation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-publishing/embedding-in-websites.md#_snippet_1\n\nLANGUAGE: css\nCODE:\n```\n/* 按钮距离底部的距离，默认为 `1rem` */\n--dify-chatbot-bubble-button-bottom\n\n/* 按钮距离右侧的距离，默认为 `1rem` */\n--dify-chatbot-bubble-button-right\n\n/* 按钮距离左侧的距离，默认为 `unset` */\n--dify-chatbot-bubble-button-left\n\n/* 按钮距离顶部的距离，默认为 `unset` */\n--dify-chatbot-bubble-button-top\n\n/* 按钮背景颜色，默认为 `#155EEF` */\n--dify-chatbot-bubble-button-bg-color\n\n/* 按钮宽度，默认为 `50px` */\n--dify-chatbot-bubble-button-width\n\n/* 按钮高度，默认为 `50px` */\n--dify-chatbot-bubble-button-height\n\n/* 按钮边框半径，默认为 `25px` */\n--dify-chatbot-bubble-button-border-radius\n\n/* 按钮盒阴影，默认为 `rgba(0, 0, 0, 0.2) 0px 4px 8px 0px)` */\n--dify-chatbot-bubble-button-box-shadow\n\n/* 按钮悬停变换，默认为 `scale(1.1)` */\n--dify-chatbot-bubble-button-hover-transform\n```\n\n----------------------------------------\n\nTITLE: User Prompt Message Class\nDESCRIPTION: Defines a class for a user prompt message, which represents a user's message. It inherits from PromptMessage and sets the role to USER.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nclass UserPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for user prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.USER\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Weather Search Tool\nDESCRIPTION: Defines the directory structure required to add a custom 'Weather Search' external data tool within the Dify project. It shows the location of the __init__.py, weather_search.py, and schema.json files within the api/core/external_data_tool/weather_search directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/external-data-tool.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n.\\n└── api\\n    └── core\\n        └── external_data_tool\\n            └── weather_search\\n                ├── __init__.py\n                ├── weather_search.py\n                └── schema.json\n```\n\n----------------------------------------\n\nTITLE: PromptMessageContent BaseModel Definition in Python\nDESCRIPTION: Defines the base class `PromptMessageContent` as a Pydantic BaseModel representing the content of a prompt message. It includes the content type and the actual data.  This class is intended to be inherited.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContent(BaseModel):\n    \"\"\"\n    Model class for prompt message content.\n    \"\"\"\n    type: PromptMessageContentType\n    data: str  # 内容数据\n```\n\n----------------------------------------\n\nTITLE: Get Document Indexing Status with Dify API\nDESCRIPTION: This snippet shows how to retrieve the indexing status of a document in a Dify knowledge base via the API. It requires the `dataset_id` and `batch` ID associated with the indexing process, along with a valid API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{batch}/indexing-status' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Install system dependencies on CentOS\nDESCRIPTION: Installs required system packages such as pkgconfig, gcc, libseccomp-devel, git, and wget on CentOS-based Linux distributions. These dependencies are necessary for compiling and building DifySandbox.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/backend/sandbox/README.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nsudo yum install pkgconfig gcc libseccomp-devel git wget\n```\n\n----------------------------------------\n\nTITLE: Metadata List of the Dataset via Dify API\nDESCRIPTION: Retrieves a list of metadata fields defined for a specific dataset within the Dify knowledge base. Requires the dataset ID as a path parameter and a valid API key in the Authorization header. The response includes metadata properties like ID, type, name, and usage count.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Passing inputs in Dify Chatbot Configuration\nDESCRIPTION: This JavaScript code snippet demonstrates how to pass inputs to the Dify Chatbot using the `inputs` configuration option. It shows a simple example of setting a value for the 'name' input. The value provided for inputs will be processed (compressed and encoded) before being appended to the URL.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-publishing/embedding-in-websites.md#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.difyChatbotConfig = {\n    // Other configuration settings...\n    inputs: {\n        name: 'apple',\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Speech2text Model Invocation Interface\nDESCRIPTION: Defines the `_invoke` method for converting speech to text. It takes a model name, credentials, an audio file, and an optional user identifier. It returns the text converted from the audio file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, file: IO[bytes], user: Optional[str] = None) -> str:\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param file: audio file\n      :param user: unique user id\n      :return: text for given audio file\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Agent Application for Work WeChat\nDESCRIPTION: This code snippet shows the `config.json` configuration for integrating a Dify agent application with Work WeChat (WeCom). It sets the API base URL, API key, application type to 'agent', channel type to 'wework', and other relevant parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dify_api_base\": \"https://api.dify.ai/v1\",\n  \"dify_api_key\": \"app-xxx\",\n  \"dify_app_type\": \"agent\",\n  \"channel_type\": \"wework\",\n  \"model\": \"dify\",\n  \"single_chat_prefix\": [\"\"],\n  \"single_chat_reply_prefix\": \"\",\n  \"group_chat_prefix\": [\"@bot\"],\n  \"group_name_white_list\": [\"ALL_GROUP\"]\n}\n\n```\n\n----------------------------------------\n\nTITLE: Request Syntax for External Knowledge Base API\nDESCRIPTION: Demonstrates the structure of the POST request to the external knowledge base retrieval endpoint. It specifies the Content-Type as 'application/json', includes an Authorization header with the API key, and provides a JSON body containing the knowledge_id, user's query, and retrieval settings (top_k and score_threshold).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_24\n\nLANGUAGE: json\nCODE:\n```\nPOST <your-endpoint>/retrieval HTTP/1.1\n-- header\nContent-Type: application/json\nAuthorization: Bearer your-api-key\n-- data\n{\n    \"knowledge_id\": \"your-knowledge-id\",\n    \"query\": \"your question\",\n    \"retrieval_setting\":{\n        \"top_k\": 2,\n        \"score_threshold\": 0.5\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Delete Dataset with Dify API\nDESCRIPTION: This snippet demonstrates how to delete a knowledge base (dataset) using the Dify API. It requires the `dataset_id` of the knowledge base to be deleted and a valid API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Strategy in function_calling.yaml (YAML)\nDESCRIPTION: This snippet defines an Agent strategy in `function_calling.yaml` file. It specifies the identity, description, parameters (model, tools, query, max_iterations), and the Python source file containing the implementation. The parameters are configured with types, scopes, and labels for internationalization, and specifies whether they are required or optional, and sets default and maximum values.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/agent.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nidentity:\n  name: function_calling\n  author: Dify\n  label:\n    en_US: FunctionCalling\n    zh_Hans: FunctionCalling\n    pt_BR: FunctionCalling\ndescription:\n  en_US: Function Calling is a basic strategy for agent, model will use the tools provided to perform the task.\n  zh_Hans: Function Calling 是一个基本的 Agent 策略，模型将使用提供的工具来执行任务。\n  pt_BR: Function Calling is a basic strategy for agent, model will use the tools provided to perform the task.\nparameters:\n  - name: model\n    type: model-selector\n    scope: tool-call&llm\n    required: true\n    label:\n      en_US: Model\n      zh_Hans: 模型\n      pt_BR: Model\n  - name: tools\n    type: array[tools]\n    required: true\n    label:\n      en_US: Tools list\n      zh_Hans: 工具列表\n      pt_BR: Tools list\n  - name: query\n    type: string\n    required: true\n    label:\n      en_US: Query\n      zh_Hans: 用户提问\n      pt_BR: Query\n  - name: max_iterations\n    type: number\n    required: false\n    default: 5\n    label:\n      en_US: Max Iterations\n      zh_Hans: 最大迭代次数\n      pt_BR: Max Iterations\n    max: 50\n    min: 1\nextra:\n  python:\n    source: strategies/function_calling.py\n```\n\n----------------------------------------\n\nTITLE: Modify Knowledge Base Metadata Field - Dify API - Bash\nDESCRIPTION: This snippet illustrates how to modify an existing metadata field in a Dify knowledge base using a PATCH request. It requires the dataset ID and metadata ID. The request includes headers for authorization (API key) and content type (application/json), along with the updated metadata definition (name) in JSON format.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request PATCH 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata/{metadata_id}' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer {api_key}' \\\n--data '{\n    \"name\":\"test\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Appending a Marketplace Plugin Dependency\nDESCRIPTION: This command appends a plugin dependency from the Dify Marketplace to the bundle project. The `--marketplace_pattern` argument specifies the plugin's identifier and version in the format `organization_name/plugin_name:version_number`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/bundle.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndify-plugin bundle append marketplace . --marketplace_pattern=langgenius/openai:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Define Generic JSON Schema Template\nDESCRIPTION: Defines a generic JSON Schema template with string, number, array, and object types.  It includes example fields with descriptions, required fields, and additionalProperties set to false. This template provides a basic structure for defining schemas with different data types and constraints.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/how-to-use-json-schema-in-dify.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"template_schema\",\n    \"description\": \"A generic template for JSON Schema\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"field1\": {\n                \"type\": \"string\",\n                \"description\": \"Description of field1\"\n            },\n            \"field2\": {\n                \"type\": \"number\",\n                \"description\": \"Description of field2\"\n            },\n            \"field3\": {\n                \"type\": \"array\",\n                \"description\": \"Description of field3\",\n                \"items\": {\n                    \"type\": \"string\"\n                }\n            },\n            \"field4\": {\n                \"type\": \"object\",\n                \"description\": \"Description of field4\",\n                \"properties\": {\n                    \"subfield1\": {\n                        \"type\": \"string\",\n                        \"description\": \"Description of subfield1\"\n                    }\n                },\n                \"required\": [\"subfield1\"],\n                \"additionalProperties\": false\n            }\n        },\n        \"required\": [\"field1\", \"field2\", \"field3\", \"field4\"],\n        \"additionalProperties\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Endpoint Group Configuration in YAML\nDESCRIPTION: Defines the settings and endpoints for an endpoint group using YAML. It specifies API key settings with internationalized labels and placeholders, and lists the YAML files defining individual endpoints.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/endpoint.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsettings:\n  api_key:\n    type: secret-input\n    required: true\n    label:\n      en_US: API key\n      zh_Hans: API key\n      ja_Jp: API key\n      pt_BR: API key\n    placeholder:\n      en_US: Please input your API key\n      zh_Hans: 请输入你的 API key\n      ja_Jp: あなたの API key を入れてください\n      pt_BR: Por favor, insira sua chave API\nendpoints:\n  - endpoints/duck.yaml\n  - endpoints/neko.yaml\n```\n\n----------------------------------------\n\nTITLE: Calculating Variance (Python)\nDESCRIPTION: This Python code snippet calculates the variance of a list of numbers. It takes a list `x` as input and calculates the mean and then the sum of squared differences from the mean, dividing by the number of elements. It returns a dictionary containing the 'result', which is the calculated variance. The snippet assumes that the input `x` is a list of numerical values.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/code.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef main(x: list) -> float:\n    return {\n        # 注意在输出变量中声明result\n        'result' : sum([(i - sum(x) / len(x)) ** 2 for i in x]) / len(x)\n    }\n```\n\n----------------------------------------\n\nTITLE: Initializing Dify Plugin Project (CLI)\nDESCRIPTION: This command initializes a new Dify plugin project using the CLI tool. It assumes the binary is either in the current directory or has been renamed to `dify` and added to the system's PATH. It is a prerequisite for creating a new plugin.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./dify-plugin-darwin-arm64 plugin init\n```\n\n----------------------------------------\n\nTITLE: Deleting a Key from Storage (Python)\nDESCRIPTION: This snippet demonstrates the `delete` method, which removes a key-value pair from the persistent storage. It takes a key (string) as input and deletes the corresponding entry.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/persistent-storage.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef delete(self, key: str) -> None:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Defining EmbeddingUsage Model in Python\nDESCRIPTION: Defines the `EmbeddingUsage` model, providing details about the usage of an embedding operation. It includes token counts, pricing information (unit price, price unit, total cost), currency, and request latency. Requires Decimal and ModelUsage.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nclass EmbeddingUsage(ModelUsage):\n    \"\"\"\n    Model class for embedding usage.\n    \"\"\"\n    tokens: int  # Number of tokens used\n    total_tokens: int  # Total number of tokens used\n    unit_price: Decimal  # Unit price\n    price_unit: Decimal  # Price unit, i.e., the unit price based on how many tokens\n    total_price: Decimal  # Total cost\n    currency: str  # Currency unit\n    latency: float  # Request latency (s)\n```\n\n----------------------------------------\n\nTITLE: Moderation Output Response - Direct Output JSON\nDESCRIPTION: Illustrates an example API response for the `app.moderation.output` extension point where the action is `direct_output`. This indicates that the LLM output was flagged as inappropriate and a preset response should be directly returned to the user. Requires action to inform the user of a policy violation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation-extension.md#_snippet_8\n\nLANGUAGE: JSON\nCODE:\n```\n{\n      \"flagged\": true,\n      \"action\": \"direct_output\",\n      \"preset_response\": \"Your content violates our usage policy.\"\n  }\n```\n\n----------------------------------------\n\nTITLE: Completion Interface Invocation\nDESCRIPTION: Specifies the entry point to access the Completion interface: `self.session.app.completion`.  This allows a plugin to invoke a completion application in Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    self.session.app.completion\n```\n\n----------------------------------------\n\nTITLE: Configure Endpoint Method in YAML\nDESCRIPTION: This YAML configuration file specifies the endpoint for handling Slack messages. It sets the HTTP method to POST to properly receive and process incoming messages from Slack's webhook.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npath: \"/\"\nmethod: \"POST\"\nextra:\n  python:\n    source: \"endpoints/slack.py\"\n```\n\n----------------------------------------\n\nTITLE: Start Web Service (NPM, Yarn, PNPM)\nDESCRIPTION: Starts the web service using npm, yarn, or pnpm. Serves the built web application, making it accessible to users.  Each option represents an alternative package manager.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_49\n\nLANGUAGE: JavaScript\nCODE:\n```\nnpm run start\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\nyarn start\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\npnpm start\n```\n\n----------------------------------------\n\nTITLE: DallE3 Image Generation and Variable Pool Storage (Python)\nDESCRIPTION: This code snippet demonstrates how to use the DallE3 tool to generate an image based on a text prompt and save the generated image to the variable pool. It uses the OpenAI API for image generation and saves the image data (in b64_json format) to the variable pool using `self.create_blob_message` with the key `self.VARIABLE_KEY.IMAGE.value`. The `b64decode` function is used to decode the image data.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/advanced-tool-integration.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, List, Union\nfrom core.tools.entities.tool_entities import ToolInvokeMessage\nfrom core.tools.tool.builtin_tool import BuiltinTool\n\nfrom base64 import b64decode\n\nfrom openai import OpenAI\n\nclass DallE3Tool(BuiltinTool):\n    def _invoke(self, \n                user_id: str, \n               tool_Parameters: Dict[str, Any], \n        ) -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:\n        \"\"\"\n            invoke tools\n        \"\"\"\n        client = OpenAI(\n            api_key=self.runtime.credentials['openai_api_key'],\n        )\n\n        # prompt\n        prompt = tool_Parameters.get('prompt', '')\n        if not prompt:\n            return self.create_text_message('Please input prompt')\n\n        # call openapi dalle3\n        response = client.images.generate(\n            prompt=prompt, model='dall-e-3',\n            size='1024x1024', n=1, style='vivid', quality='standard',\n            response_format='b64_json'\n        )\n\n        result = []\n        for image in response.data:\n            # Save all images to the variable pool through the save_as parameter. The variable name is self.VARIABLE_KEY.IMAGE.value. If new images are generated later, they will overwrite the previous images.\n            result.append(self.create_blob_message(blob=b64decode(image.b64_json), \n                                                   meta={ 'mime_type': 'image/png' },\n                                                    save_as=self.VARIABLE_KEY.IMAGE.value))\n\n        return result\n```\n\n----------------------------------------\n\nTITLE: Example Request Body for Weather Retrieval\nDESCRIPTION: This is an example JSON payload demonstrating a request to retrieve weather information for a specific location. The 'app_id' identifies the application, 'tool_variable' specifies the 'weather_retrieve' tool, 'inputs' contains the 'location' variable set to 'London', and 'query' holds the user's question about the weather.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/external-data-tool.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"point\": \"app.external_data_tool.query\",\n    \"params\": {\n        \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n        \"tool_variable\": \"weather_retrieve\",\n        \"inputs\": {\n            \"location\": \"London\"\n        },\n        \"query\": \"How's the weather today?\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: コンテンツを要約 (Python)\nDESCRIPTION: ユーザーIDと要約するテキストを渡すと、Difyが現在のワークスペースのデフォルトモデルを使ってテキストを要約するツールです。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/advanced-tool-integration.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef summary(self, user_id: str, content: str) -> str:\n    \"\"\"\n        コンテンツを要約\n\n        :param user_id: ユーザーID\n        :param content: コンテンツ\n        :return: 要約\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Example Manifest File (YAML)\nDESCRIPTION: This is a sample manifest.yaml file that demonstrates the structure and required fields for defining a Dify plugin.  It shows how to specify plugin version, type, author, resources, permissions, and other metadata. The code needs a yaml parser to load the manifest file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/manifest.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 0.0.1\n\n```\n\nLANGUAGE: yaml\nCODE:\n```\ntype: \"plugin\"\n\n```\n\nLANGUAGE: yaml\nCODE:\n```\nauthor: \"Yeuoly\"\n\n```\n\nLANGUAGE: yaml\nCODE:\n```\nname: \"neko\"\n\n```\n\nLANGUAGE: yaml\nCODE:\n```\nlabel:\n  en_US: \"Neko\"\n\n```\n\nLANGUAGE: yaml\nCODE:\n```\ncreated_at: \"2024-07-12T08:03:44.658609186Z\"\n\n```\n\nLANGUAGE: yaml\nCODE:\n```\nicon: \"icon.svg\"\n\n```\n\nLANGUAGE: yaml\nCODE:\n```\nresource:\n  memory: 1048576\n  permission:\n    tool:\n      enabled: true\n    model:\n      enabled: true\n      llm: true\n    endpoint:\n      enabled: true\n    app:\n      enabled: true\n    storage: \n      enabled: true\n      size: 1048576\n\n```\n\nLANGUAGE: yaml\nCODE:\n```\nplugins:\n  endpoints:\n    - \"provider/neko.yaml\"\n\n```\n\nLANGUAGE: yaml\nCODE:\n```\nmeta:\n  version: 0.0.1\n  arch:\n    - \"amd64\"\n    - \"arm64\"\n  runner:\n    language: \"python\"\n    version: \"3.11\"\n    entrypoint: \"main\"\n\n```\n\nLANGUAGE: yaml\nCODE:\n```\nprivacy: \"./privacy.md\"\n```\n\n----------------------------------------\n\nTITLE: Flask API for Bedrock Retrieval\nDESCRIPTION: This Flask RESTful API endpoint handles retrieval requests from Dify and interacts with the AWS Bedrock Knowledge Base. It parses the request, performs authorization, and calls the `ExternalDatasetService` to retrieve information.  It requires Flask and Flask-RESTful libraries and expects a valid 'Bearer <api-key>' authorization header.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/how-to-connect-aws-bedrock.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flask import request\nfrom flask_restful import Resource, reqparse\n\nfrom bedrock.knowledge_service import ExternalDatasetService\n\n\nclass BedrockRetrievalApi(Resource):\n    # url : <your-endpoint>/retrieval\n    def post(self):\n        parser = reqparse.RequestParser()\n        parser.add_argument(\"retrieval_setting\", nullable=False, required=True, type=dict, location=\"json\")\n        parser.add_argument(\"query\", nullable=False, required=True, type=str,)\n        parser.add_argument(\"knowledge_id\", nullable=False, required=True, type=str)\n        args = parser.parse_args()\n\n        # Authorization check\n        auth_header = request.headers.get(\"Authorization\")\n        if \" \" not in auth_header:\n            return {\n                \"error_code\": 1001,\n                \"error_msg\": \"Invalid Authorization header format. Expected 'Bearer <api-key>' format.\"\n            }, 403\n        auth_scheme, auth_token = auth_header.split(None, 1)\n        auth_scheme = auth_scheme.lower()\n        if auth_scheme != \"bearer\":\n            return {\n                \"error_code\": 1001,\n                \"error_msg\": \"Invalid Authorization header format. Expected 'Bearer <api-key>' format.\"\n            }, 403\n        if auth_token:\n            # process your authorization logic here\n            pass\n\n        # Call the knowledge retrieval service\n        result = ExternalDatasetService.knowledge_retrieval(\n            args[\"retrieval_setting\"], args[\"query\"], args[\"knowledge_id\"]\n        )\n        return result, 200\n```\n\n----------------------------------------\n\nTITLE: app.moderation.input API Response Example (JSON)\nDESCRIPTION: This JSON payload showcases example responses from the `app.moderation.input` API, where the action is either `direct_output` or `overridden`. It includes a flagged status, action type, preset response (when applicable), modified input variables (when applicable), and the overridden query (when applicable).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/moderation.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"direct_output\",\n    \"preset_response\": \"Your content violates our usage policy.\"\n}\n```\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"overridden\",\n    \"inputs\": {\n        \"var_1\": \"I will *** you.\",\n        \"var_2\": \"I will *** you.\"\n    },\n    \"query\": \"Happy everydays.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Model Prompt Template for Conversational Apps\nDESCRIPTION: This prompt template is designed for Complete models when building conversational applications. It incorporates context, pre-prompt, conversation history, and the current query. It also uses XML tags to structure context and histories, as well as placeholders for dynamic content.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/prompt-engineering/prompt-template.md#_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nUse the following context as your learned knowledge, inside <context></context> XML tags.\n\n<context>\n{{#context#}}\n</context>\n\nWhen answer to user:\n- If you don't know, just say that you don't know.\n- If you don't know when you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n\n{{pre_prompt}}\n\nHere is the chat histories between human and assistant, inside <histories></histories> XML tags.\n\n<histories>\n{{#histories#}}\n</histories>\n\n\nHuman: {{#query#}}\n\nAssistant: \n```\n\n----------------------------------------\n\nTITLE: Updating Dify (Git Pull, Down, Pull, Up)\nDESCRIPTION: This sequence of commands updates Dify to the latest version. It pulls the latest changes from the `main` branch, stops the existing containers, pulls the latest Docker images, and restarts the containers in detached mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd dify/docker\ngit pull origin main\ndocker compose down\ndocker compose pull\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Restart Dify with Docker Compose\nDESCRIPTION: These commands are used to restart the Dify application after modifying the environment variables.  It stops all Docker containers defined in the docker-compose files and then starts them again in detached mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/dify-premium.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose down\ndocker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Accessing Plugin Storage\nDESCRIPTION: This snippet shows how to access the storage instance within a Dify plugin. This instance is then used to perform set, get, and delete operations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/persistent-storage.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.session.storage\n```\n\n----------------------------------------\n\nTITLE: Config.json Example (Workflow)\nDESCRIPTION: An example of the config.json file that needs to be created in the root directory of the project for the workflow chatbot. It includes configuration options such as the Dify API base URL, API key, application type (chatbot), and other WeChat-related settings.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dify_api_base\": \"https://api.dify.ai/v1\",\n  \"dify_api_key\": \"app-xxx\",\n  \"dify_app_type\": \"chatbot\",\n  \"channel_type\": \"wx\",\n  \"model\": \"dify\",\n  \"single_chat_prefix\": [\"\"],\n  \"single_chat_reply_prefix\": \"\",\n  \"group_chat_prefix\": [\"@bot\"],\n  \"group_name_white_list\": [\"ALL_GROUP\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Sandbox Service with Docker Compose (Bash)\nDESCRIPTION: This Bash snippet demonstrates how to start the sandbox service using Docker Compose.  This is required for local Dify deployments to ensure code execution security.  The `docker-compose.middleware.yaml` file is used to define the service configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/code.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\ndocker-compose -f docker-compose.middleware.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Reload Systemd Daemon and Restart Ollama\nDESCRIPTION: These commands reload the systemd daemon and restart the Ollama service after modifying the service configuration file. This is necessary to apply the changes, such as setting the OLLAMA_HOST environment variable, and ensure that the Ollama service is running with the new configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/ollama.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsystemctl daemon-reload\nsystemctl restart ollama\n```\n\n----------------------------------------\n\nTITLE: Anthropic Provider YAML Configuration\nDESCRIPTION: This YAML configuration defines the basic information for the Anthropic provider, including its identifier, display name, supported model types, configuration methods, and credential rules. The `credential_form_schemas` section specifies the variables, labels, types, requirements, and placeholders for the credentials.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/new-provider.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nprovider: anthropic  # プロバイダーの識別子\nlabel:  # プロバイダーの表示名、en_US英語、zh_Hans中国語の二言語を設定できます。zh_Hansが設定されていない場合、en_USがデフォルトで使用されます。\n  en_US: Anthropic\nicon_small:  # プロバイダーの小アイコン、対応するプロバイダーの実装ディレクトリ内の_assetsディレクトリに保存されます。labelと同じく二言語の設定が可能です。\n  en_US: icon_s_en.png\nicon_large:  # プロバイダーの大アイコン、対応するプロバイダーの実装ディレクトリ内の_assetsディレクトリに保存されます。labelと同じく二言語の設定が可能です。\n  en_US: icon_l_en.png\nsupported_model_types:  # 対応するモデルタイプ、AnthropicはLLMのみ対応\n- llm\nconfigurate_methods:  # 対応する設定方法、Anthropicは事前定義モデルのみ対応\n- predefined-model\nprovider_credential_schema:  # プロバイダーのクレデンシャルルール、Anthropicは事前定義モデルのみ対応するため、統一されたプロバイダーのクレデンシャルルールを定義する必要があります\n  credential_form_schemas:  # クレデンシャルフォーム項目リスト\n  - variable: anthropic_api_key  # クレデンシャルパラメーターの変数名\n    label:  # 表示名\n      en_US: API Key\n    type: secret-input  # フォームタイプ、ここではsecret-inputは暗号化された情報入力フィールドを意味し、編集時にはマスクされた情報のみが表示されます。\n    required: true  # 必須かどうか\n    placeholder:  # プレースホルダー情報\n      zh_Hans: 在此输入你的 API Key\n      en_US: Enter your API Key\n  - variable: anthropic_api_url\n    label:\n      en_US: API URL\n    type: text-input  # フォームタイプ、ここではtext-inputはテキスト入力フィールドを意味します\n    required: false\n    placeholder:\n      zh_Hans: 在此输入你的 API URL\n      en_US: Enter your API URL\n```\n\n----------------------------------------\n\nTITLE: Tool Provider YAML with Credentials\nDESCRIPTION: This YAML defines a tool provider that requires credentials, specifically a SerpApi API key.  It demonstrates how to configure the `credentials_for_provider` section, including specifying the credential type (`secret-input`, `text-input`, or `select`), whether it's required, labels, placeholders, help text, and a help URL.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/quick-tool-integration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: Dify\n  name: google\n  label:\n    en_US: Google\n    zh_Hans: Google\n    ja_JP: Google\n    pt_BR: Google\n  description:\n    en_US: Google\n    zh_Hans: Google\n    ja_JP: Google\n    pt_BR: Google\n  icon: icon.svg\ncredentials_for_provider: # Credential field\n  serpapi_api_key: # Credential field name\n    type: secret-input # Credential field type\n    required: true # Required or not\n    label: # Credential field label\n      en_US: SerpApi API key # English label\n      zh_Hans: SerpApi API key # Chinese label\n      ja_JP: SerpApi API key # Japanese label\n      pt_BR: chave de API SerpApi # Portuguese label\n    placeholder: # Credential field placeholder\n      en_US: Please input your SerpApi API key # English placeholder\n      zh_Hans: 请输入你的 SerpApi API key # Chinese placeholder\n      ja_JP: SerpApi API keyを入力してください # Japanese placeholder\n      pt_BR: Por favor, insira sua chave de API SerpApi # Portuguese placeholder\n    help: # Credential field help text\n      en_US: Get your SerpApi API key from SerpApi # English help text\n      zh_Hans: 从 SerpApi 获取你的 SerpApi API key # Chinese help text\n      ja_JP: SerpApiからSerpApi APIキーを取得する # Japanese help text\n      pt_BR: Obtenha sua chave de API SerpApi da SerpApi # Portuguese help text\n    url: https://serpapi.com/manage-api-key # Credential field help link\n```\n\n----------------------------------------\n\nTITLE: Get Customizable Model Schema Method\nDESCRIPTION: Dynamically generates the model parameter schema based on the model being used. It defines rules for parameters such as temperature, top_p, and max_tokens, and can conditionally add parameters like top_k based on the specific model's capabilities. This method ensures that the model parameters are correctly configured for each model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/customizable-model.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> AIModelEntity | None:\n    \"\"\"\n        used to define customizable model schema\n    \"\"\"\n    rules = [\n        ParameterRule(\n            name='temperature', type=ParameterType.FLOAT,\n            use_template='temperature',\n            label=I18nObject(\n                zh_Hans='温度', en_US='Temperature'\n            )\n        ),\n        ParameterRule(\n            name='top_p', type=ParameterType.FLOAT,\n            use_template='top_p',\n            label=I18nObject(\n                zh_Hans='Top P', en_US='Top P'\n            )\n        ),\n        ParameterRule(\n            name='max_tokens', type=ParameterType.INT,\n            use_template='max_tokens',\n            min=1,\n            default=512,\n            label=I18nObject(\n                zh_Hans='最大生成长度', en_US='Max Tokens'\n            )\n        )\n    ]\n\n    # if model is A, add top_k to rules\n    if model == 'A':\n        rules.append(\n            ParameterRule(\n                name='top_k', type=ParameterType.INT,\n                use_template='top_k',\n                min=1,\n                default=50,\n                label=I18nObject(\n                    zh_Hans='Top K', en_US='Top K'\n                )\n            )\n        )\n\n    \"\"\"\n        some NOT IMPORTANT code here\n    \"\"\"\n\n    entity = AIModelEntity(\n        model=model,\n        label=I18nObject(\n            en_US=model\n        ),\n        fetch_from=FetchFrom.CUSTOMIZABLE_MODEL,\n        model_type=model_type,\n        model_properties={ \n            ModelPropertyKey.MODE:  ModelType.LLM,\n        },\n        parameter_rules=rules\n    )\n\n    return entity\n```\n\n----------------------------------------\n\nTITLE: Debug History Messages (Python)\nDESCRIPTION: This Python snippet demonstrates how to access and print the history messages within the AgentStrategy's _invoke method. This can be used to debug and inspect the content of the historical conversation that the model is using for context.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n        print(f\"history_messages: {params.model.history_prompt_messages}\")\n        ...\n```\n\n----------------------------------------\n\nTITLE: Moderation Model Invocation Interface in Python\nDESCRIPTION: This snippet defines the `_invoke` method for a moderation model. It accepts a model name, credentials, the text to moderate, and an optional user ID. It returns a boolean indicating whether the text is safe.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              text: str, user: Optional[str] = None) \\\n          -> bool:\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param text: text to moderate\n      :param user: unique user id\n      :return: false if text is safe, true otherwise\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository\nDESCRIPTION: Clones the Dify repository from GitHub. This command downloads the entire source code to your local machine, allowing you to modify and run the application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\n----------------------------------------\n\nTITLE: Adding Agent Strategies to Manifest (YAML)\nDESCRIPTION: Demonstrates how to add an agent strategy to the `manifest.yaml` file by including the `plugins.agent_strategies` field and defining the agent provider. This configuration links the plugin to the specified agent strategy YAML file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/agent.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 0.0.2\ntype: plugin\nauthor: \"langgenius\"\nname: \"agent\"\nplugins:\n  agent_strategies:\n    - \"provider/agent.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Add QingLong Subscription\nDESCRIPTION: This command adds a subscription to the QingLong Panel, pulling the Dify Schedule repository. It specifies the repository URL and filters for specific files or directories.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/use-cases/dify-schedule.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nql repo https://github.com/leochen-g/dify-schedule.git \"ql_\" \"utils\" \"sdk\"\n```\n\n----------------------------------------\n\nTITLE: Starting OpenLLM with Docker\nDESCRIPTION: This command starts an OpenLLM server using Docker, deploying a specified language model.  It maps port 3333 on the host machine to port 3000 on the container. The model facebook/opt-1.3b is used as an example and the backend is set to PyTorch (pt).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/openllm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -it -p 3333:3000 ghcr.io/bentoml/openllm start facebook/opt-1.3b --backend pt\n```\n\n----------------------------------------\n\nTITLE: JSON Validation with Python in Dify Code Node\nDESCRIPTION: This Python code snippet is used in a Dify Code node to validate a JSON string. It attempts to parse the input JSON string using `json.loads()` and returns a dictionary containing the parsed JSON object. This is used for implementing error handling where invalid JSON triggers a fail branch.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\ndef main(json_str: str) -> dict:\n    obj = json.loads(json_str)\n    return {'result': obj}\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Compose\nDESCRIPTION: These commands install Docker Compose on a Linux system.  It downloads the Docker Compose binary from the GitHub releases page, makes it executable, and then verifies the installation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/searxng.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo curl -L \"https://github.com/docker/compose/releases/download/2.32.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\n```\n\n----------------------------------------\n\nTITLE: Completion Interface Specification\nDESCRIPTION: Defines the signature of the `invoke` method for Completion interface calls. It outlines the expected input parameters: `app_id` (application identifier), `inputs` (input data dictionary), `response_mode` (specifies streaming or blocking response), and `files` (list of file objects). It returns a generator for streaming (`Generator[dict, None, None]`) or a dictionary (`dict`).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n    def invoke(\n        self,\n        app_id: str,\n        inputs: dict,\n        response_mode: Literal[\"streaming\", \"blocking\"],\n        files: list,\n    ) -> Generator[dict, None, None] | dict:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Cloning the Dify repository\nDESCRIPTION: This command clones the Dify.AI repository from GitHub to the local machine. This is the first step in deploying Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\n----------------------------------------\n\nTITLE: Resetting Encryption Key Pair (Source Code)\nDESCRIPTION: Resets the encryption key pair when running Dify from source code. Executes a flask command from the api directory to regenerate the keys.  Assumes the user is already in the `api` directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nflask reset-encrypt-key-pair\n```\n\n----------------------------------------\n\nTITLE: Convert object to string using JSON in Python\nDESCRIPTION: This Python code snippet takes a list containing a dictionary as input, converts the dictionary to a JSON string, and wraps the JSON string in `<answer>` tags. It handles potential exceptions during the JSON conversion process, returning an error message within `<answer>` tags if an error occurs. The function expects a list where the first element is a dictionary.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/variable-assigner.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\ndef main(arg1: list) -> str:\n    try:\n        # Assume arg1[0] is the dictionary we need to process\n        context = arg1[0] if arg1 else {}\n        \n        # Construct the memory object\n        memory = {\"memory\": context}\n        \n        # Convert the object to a JSON string\n        json_str = json.dumps(memory, ensure_ascii=False, indent=2)\n        \n        # Wrap the JSON string in <answer> tags\n        result = f\"<answer>{json_str}</answer>\"\n        \n        return {\n            \"result\": result\n        }\n    except Exception as e:\n        return {\n            \"result\": f\"<answer>Error: {str(e)}</answer>\"\n        }\n```\n\n----------------------------------------\n\nTITLE: Text to Speech Model Invocation Python\nDESCRIPTION: This Python snippet defines the `_invoke` method for a text-to-speech model. It takes a model name, credentials, text content, a streaming flag, and an optional user ID as input. It returns the translated audio file or stream.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, content_text: str, streaming: bool, user: Optional[str] = None):\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param content_text: text content to be translated\n    :param streaming: output is streaming\n    :param user: unique user id\n    :return: translated audio file\n    \"\"\"        \n```\n\n----------------------------------------\n\nTITLE: Moderation Output Response - Overridden JSON\nDESCRIPTION: Illustrates an example API response for the `app.moderation.output` extension point where the action is `overridden`. This means the flagged content in the LLM output has been replaced. Provides the replacement text.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation-extension.md#_snippet_9\n\nLANGUAGE: JSON\nCODE:\n```\n{\n      \"flagged\": true,\n      \"action\": \"overridden\",\n      \"text\": \"I will *** you.\"\n  }\n```\n\n----------------------------------------\n\nTITLE: Reranking Model Invocation Interface in Python\nDESCRIPTION: This snippet defines the `_invoke` method for a reranking model. It takes a model name, credentials, query, list of documents, optional score threshold, top N, and user ID as input, and returns a `RerankResult` object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              query: str, docs: list[str], score_threshold: Optional[float] = None, top_n: Optional[int] = None,\n              user: Optional[str] = None) \\\n          -> RerankResult:\n      \"\"\"\n      Invoke rerank model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param query: search query\n      :param docs: docs for reranking\n      :param score_threshold: score threshold\n      :param top_n: top n\n      :param user: unique user id\n      :return: rerank result\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Agent Plugin Initialization Prompts (Bash)\nDESCRIPTION: This displays the prompts and options when initializing a Dify Agent plugin, including plugin name, author, description, language selection (Python), template selection (Agent Strategy), and permission configuration.  The selected options are python and agent-strategy.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n➜ ./dify-plugin-darwin-arm64 plugin init                                                                                                                                 ─╯\nEdit profile of the plugin\nPlugin name (press Enter to next step): # 填写插件的名称\nAuthor (press Enter to next step): # 填写插件作者\nDescription (press Enter to next step): # 填写插件的描述\n---\nSelect the language you want to use for plugin development, and press Enter to continue,\nBTW, you need Python 3.12+ to develop the Plugin if you choose Python.\n-> python # 选择 Python 环境\n  go (not supported yet)\n---\nBased on the ability you want to extend, we have divided the Plugin into four types: Tool, Model, Extension, and Agent Strategy.\n\n- Tool: It's a tool provider, but not only limited to tools, you can implement an endpoint there, for example, you need both Sending Message and Receiving Message if you are\n- Model: Just a model provider, extending others is not allowed.\n- Extension: Other times, you may only need a simple http service to extend the functionalities, Extension is the right choice for you.\n- Agent Strategy: Implement your own logics here, just by focusing on Agent itself\n\nWhat's more, we have provided the template for you, you can choose one of them below:\n  tool\n-> agent-strategy # 选择 Agent 策略模板\n  llm\n  text-embedding\n---\nConfigure the permissions of the plugin, use up and down to navigate, tab to select, after selection, press enter to finish\nBackwards Invocation:\nTools:\n    Enabled: [✔]  You can invoke tools inside Dify if it's enabled # 默认开启\nModels:\n    Enabled: [✔]  You can invoke models inside Dify if it's enabled # 默认开启\n    LLM: [✔]  You can invoke LLM models inside Dify if it's enabled # 默认开启\n  → Text Embedding: [✘]  You can invoke text embedding models inside Dify if it's enabled\n    Rerank: [✘]  You can invoke rerank models inside Dify if it's enabled\n    TTS: [✘]  You can invoke TTS models inside Dify if it's enabled\n    Speech2Text: [✘]  You can invoke speech2text models inside Dify if it's enabled\n    Moderation: [✘]  You can invoke moderation models inside Dify if it's enabled\nApps:\n    Enabled: [✘]  Ability to invoke apps like BasicChat/ChatFlow/Agent/Workflow etc.\nResources:\nStorage:\n    Enabled: [✘]  Persistence storage for the plugin\n    Size: N/A  The maximum size of the storage\nEndpoints:\n    Enabled: [✘]  Ability to register endpoints\n```\n\n----------------------------------------\n\nTITLE: Setting Stop Sequences for LLM Text Generation\nDESCRIPTION: This example demonstrates how to set stop sequences in Dify to control LLM text generation and prevent unnecessary content from being generated. The stop sequence \"Human1:\" is used to signal the LLM to stop generating text before reaching the next turn in a few-shot prompt example.  This relies on the LLM and is specific to the Dify platform.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/prompt-engineering/prompt-engineering-1/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nHuman1: What color is the sky?\nAssistant1: The sky is blue.\nHuman1: What color is fire?\nAssistant1: Fire is red.\nHuman1: What color is soil?\nAssistant1: \n```\n\n----------------------------------------\n\nTITLE: Invoking Custom Tools\nDESCRIPTION: This code snippet shows the endpoint for invoking a custom tool via its OpenAPI specification.  The `provider` specifies the tool's ID, `tool_name` is the OpenAPI `operation_id` (or a Dify-generated name if it doesn't exist), and `parameters` are the input parameters for the API call.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/tool.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef invoke_api_tool(\n    self, provider: str, tool_name: str, parameters: dict[str, Any]\n) -> Generator[ToolInvokeMessage, None, None]:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for Text Embedding in Python\nDESCRIPTION: This code snippet gets the number of tokens for given texts intended for embedding. It takes model name, credentials, and a list of texts as input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, texts: list[str]) -> int:\n      \"\"\"\n      Get number of tokens for given prompt messages\n\n      :param model: model name\n      :param credentials: model credentials\n      :param texts: texts to embed\n      :return:\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Endpoint Definition (endpoints/slack.yaml)\nDESCRIPTION: This YAML code defines the endpoint configuration, specifying the request path and method.  The `method` is set to `POST`, indicating that the endpoint will handle POST requests. This file configures the connection between Dify and the plugin endpoint.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npath: \"/\"\nmethod: \"POST\"\nextra:\n  python:\n    source: \"endpoints/slack.py\"\n```\n\n----------------------------------------\n\nTITLE: List Documents in Dataset with Dify API\nDESCRIPTION: This snippet shows how to retrieve a list of documents within a specific Dify knowledge base using the API. It requires the `dataset_id` of the knowledge base and a valid API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Starting the Worker Service (Bash/Windows)\nDESCRIPTION: This command starts the Celery worker service on Windows, which consumes asynchronous tasks from the queue. It uses the solo pool and disables gossip and mingle features.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run celery -A app.celery worker -P solo --without-gossip --without-mingle -Q dataset,generation,mail,ops_trace --loglevel INFO\n```\n\n----------------------------------------\n\nTITLE: LLMResultChunkDelta Class Definition in Python\nDESCRIPTION: Defines the LLMResultChunkDelta class which represents the delta (change) in a streamed LLM result chunk. It contains the index, the assistant's message (delta), optional usage information, and the finish reason (only in the last chunk).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunkDelta(BaseModel):\n    \"\"\"\n    Model class for llm result chunk delta.\n    \"\"\"\n    index: int  # インデックス\n    message: AssistantPromptMessage  # 返信メッセージ\n    usage: Optional[LLMUsage] = None  # 使用したトークンとコスト情報（最後の1つのみ）\n    finish_reason: Optional[str] = None  # 終了理由（最後の1つのみ）\n```\n\n----------------------------------------\n\nTITLE: Invoke Dify AI Application via API using curl\nDESCRIPTION: This code snippet demonstrates how to invoke a Dify AI application using the curl command. It sends a POST request to the Dify API endpoint with the user's query and other necessary parameters such as the API key, conversation ID, and user identifier. Remember to replace `ENTER-YOUR-SECRET-KEY` and `conversation_id` with your actual API key and the appropriate conversation ID. The `conversation_id` can be left empty for the first request, and then updated with the ID returned in the response for subsequent requests to continue the conversation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/use-cases/build-an-notion-ai-assistant.md#_snippet_0\n\nLANGUAGE: curl\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/chat-messages' \\\n--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\\n    \"inputs\": {},\\n    \"query\": \"eh\",\\n    \"response_mode\": \"streaming\",\\n    \"conversation_id\": \"\",\\n    \"user\": \"abc-123\"\\n}'\n```\n\n----------------------------------------\n\nTITLE: Define Authorization Header for Retrieval API\nDESCRIPTION: Specifies the structure of the Authorization HTTP Header, using the 'Bearer' scheme with an API Key. This API Key is used for permission verification. The actual authentication logic must be implemented in the retrieval API by the developer.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_23\n\nLANGUAGE: none\nCODE:\n```\nAuthorization: Bearer {API_KEY}\n```\n\n----------------------------------------\n\nTITLE: Applying CSS Classes with containerProps\nDESCRIPTION: This JavaScript code snippet demonstrates how to apply CSS classes to the Dify Chatbot Bubble Button using the `containerProps` option. The `className` attribute is used to add custom CSS classes to the button, allowing for more complex styling.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-publishing/embedding-in-websites.md#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.difyChatbotConfig = {\n    // ... 他の設定\n    containerProps: {\n        className: 'dify-chatbot-bubble-button-custom my-custom-class',\n    },\n};\n```\n\n----------------------------------------\n\nTITLE: Defining Model Type Selection in YAML\nDESCRIPTION: This YAML code defines the `model_type` selection field within the `provider_credential_schema` for Xinference. It uses a `select` type to allow users to choose between 'text-generation', 'embeddings', and 'reranking' model types.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprovider_credential_schema:\n  credential_form_schemas:\n  - variable: model_type\n    type: select\n    label:\n      en_US: Model type\n      zh_Hans: 模型类型\n    required: true\n    options:\n<strong>    - value: text-generation\n</strong>      label:\n        en_US: Language Model\n        zh_Hans: 语言模型\n<strong>    - value: embeddings\n</strong>      label:\n        en_US: Text Embedding\n<strong>    - value: reranking\n</strong>      label:\n        en_US: Rerank\n```\n\n----------------------------------------\n\nTITLE: Xinference Provider Class (Python)\nDESCRIPTION: This Python code defines the XinferenceProvider class, which inherits from the Provider base class. It includes an empty validate_provider_credentials method, serving as a placeholder for custom model providers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass XinferenceProvider(Provider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: LLM Prompt for Structure Extraction\nDESCRIPTION: This snippet defines the prompt for the Structure Extraction LLM node. The prompt instructs the LLM to analyze the structure of an article, identify key sections, and provide detailed explanations of each part using Markdown format.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/workshop/intermediate/article-reader.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n阅读以下文章内容并执行任务\n{{文档提取器结果的变量}}\n# 任务\n\n- **主要目标**：全面解析文章的结构。\n- **目标**：详细说明文章每个部分的内容。\n- **要求**：尽可能详细地分析。\n- **限制**：无特别的格式限制，但需要保持解析的条理性和逻辑性。\n- **预期输出**：文章结构的详细解析，包括每个部分的主要内容和作用。\n\n# 推理顺序\n\n- **推理部分**：通过仔细阅读文章，识别和解析其结构。\n- **结论部分**：提供每个部分的具体内容和作用。\n\n# 输出格式\n\n- **解析格式**：每个部分应以标题形式列出，后跟对该部分内容的详细说明。\n- **结构形式**：Markdown，以增强可读性。\n- **具体说明**：每个部分的内容和作用，包括但不限于引言、正文、结论、引用等。\n\n# 示例输出\n\n## 示例文章解析\n\n### 引言\n- **内容**：介绍研究的背景、目的和重要性。\n- **作用**：吸引读者的注意力，为文章内容提供上下文。\n\n### 方法\n- **内容**：描述研究的具体方法和步骤，包括实验设计、数据收集和分析技术。\n- **作用**：使读者了解研究的科学性和可重复性。\n\n### 结果\n- **内容**：展示研究的主要发现和数据。\n- **作用**：提供研究结论的证据基础。\n\n### 讨论\n- **内容**：解释结果的意义，对比其他研究，提出可能的改进方向。\n- **作用**：帮助读者理解结果的广泛影响和未来研究的潜力。\n\n### 结论\n- **内容**：总结研究的主要发现和贡献。\n- **作用**：强化文章的核心信息，提供明确的结论。\n\n### 引用\n- **内容**：列出文章中引用的所有文献。\n- **作用**：提供进一步阅读的资源，确保学术诚信。\n\n# 备注\n\n- **边缘情况**：如果文章结构不典型（例如，缺少某些部分或有额外的部分），应在解析中明确指出这些特殊情况。\n- **重要考虑事项**：解析时应关注文章的逻辑性和连贯性，确保每个部分的内容与文章的整体目标一致。\n```\n\n----------------------------------------\n\nTITLE: Implement Agent Strategy Logic\nDESCRIPTION: Implements the core logic of the agent strategy within the `BasicAgentAgentStrategy` class. This involves invoking the LLM, checking for tool calls, extracting tool call details, and generating appropriate log messages and responses.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport time\nfrom collections.abc import Generator\nfrom typing import Any, cast\n\nfrom dify_plugin.entities.agent import AgentInvokeMessage\nfrom dify_plugin.entities.model.llm import LLMModelConfig, LLMResult, LLMResultChunk\nfrom dify_plugin.entities.model.message import (\n    PromptMessageTool,\n    UserPromptMessage,\n)\nfrom dify_plugin.entities.tool import ToolInvokeMessage, ToolParameter, ToolProviderType\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\nfrom pydantic import BaseModel\n\nclass BasicParams(BaseModel):\n    maximum_iterations: int\n    model: AgentModelConfig\n    tools: list[ToolEntity]\n    query: str\n\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n        function_call_round_log = self.create_log_message(\n            label=\"Function Call Round1 \",\n            data={},\n            metadata={},\n        )\n        yield function_call_round_log\n        model_started_at = time.perf_counter()\n        model_log = self.create_log_message(\n            label=f\"{params.model.model} Thought\",\n            data={},\n            metadata={\"start_at\": model_started_at, \"provider\": params.model.provider},\n            status=ToolInvokeMessage.LogMessage.LogStatus.START,\n            parent=function_call_round_log,\n        )\n        yield model_log\n        chunks: Generator[LLMResultChunk, None, None] | LLMResult = (\n            self.session.model.llm.invoke(\n                model_config=LLMModelConfig(**params.model.model_dump(mode=\"json\")),\n                prompt_messages=[UserPromptMessage(content=params.query)],\n                tools=[\n                    self._convert_tool_to_prompt_message_tool(tool)\n                    for tool in params.tools\n                ],\n                stop=params.model.completion_params.get(\"stop\", [])\n                if params.model.completion_params\n                else [],\n                stream=True,\n            )\n        )\n        response = \"\"\n        tool_calls = []\n        tool_instances = (\n            {tool.identity.name: tool for tool in params.tools} if params.tools else {}\n        )\n        tool_call_names = \"\"\n        tool_call_inputs = \"\"\n        for chunk in chunks:\n            # ツール呼び出しがあるか確認\n            if self.check_tool_calls(chunk):\n                tool_calls = self.extract_tool_calls(chunk)\n                tool_call_names = \";\".join([tool_call[1] for tool_call in tool_calls])\n                try:\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls},\n                        ensure_ascii=False,\n                    )\n                except json.JSONDecodeError:\n                    # エンコードエラーを避けるため、asciiを保証\n                    tool_call_inputs = json.dumps(\n                        {tool_call[1]: tool_call[2] for tool_call in tool_calls}\n                    )\n                print(tool_call_names, tool_call_inputs)\n            if chunk.delta.message and chunk.delta.message.content:\n                if isinstance(chunk.delta.message.content, list):\n                    for content in chunk.delta.message.content:\n                        response += content.data\n                        print(content.data, end=\"\", flush=True)\n                else:\n                    response += str(chunk.delta.message.content)\n                    print(str(chunk.delta.message.content), end=\"\", flush=True)\n\n            if chunk.delta.usage:\n                # モデルを使用する\n                usage = chunk.delta.usage\n\n        yield self.finish_log_message(\n            log=model_log,\n            data={\n                \"output\": response,\n                \"tool_name\": tool_call_names,\n                \"tool_input\": tool_call_inputs,\n            },\n            metadata={\n                \"started_at\": model_started_at,\n                \"finished_at\": time.perf_counter(),\n                \"elapsed_time\": time.perf_counter() - model_started_at,\n                \"provider\": params.model.provider,\n            },\n        )\n        yield self.create_text_message(\n            text=f\"{response or json.dumps(tool_calls, ensure_ascii=False)}\\n\"\n        )\n        result = \"\"\n        for tool_call_id, tool_call_name, tool_call_args in tool_calls:\n            tool_instance = tool_instances[tool_call_name]\n            tool_invoke_responses = self.session.tool.invoke(\n                provider_type=ToolProviderType.BUILT_IN,\n                provider=tool_instance.identity.provider,\n                tool_name=tool_instance.identity.name,\n                parameters={**tool_instance.runtime_parameters, **tool_call_args},\n            )\n            if not tool_instance:\n                tool_invoke_responses = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": f\"there is not a tool named {tool_call_name}\",\n                }\n            else:\n                # ツールを呼び出す\n                tool_invoke_responses = self.session.tool.invoke(\n                    provider_type=ToolProviderType.BUILT_IN,\n                    provider=tool_instance.identity.provider,\n                    tool_name=tool_instance.identity.name,\n                    parameters={**tool_instance.runtime_parameters, **tool_call_args},\n                )\n                result = \"\"\n                for tool_invoke_response in tool_invoke_responses:\n                    if tool_invoke_response.type == ToolInvokeMessage.MessageType.TEXT:\n                        result += cast(\n                            ToolInvokeMessage.TextMessage, tool_invoke_response.message\n                        ).text\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.LINK\n                    ):\n                        result += (\n                            f\"result link: {cast(ToolInvokeMessage.TextMessage, tool_invoke_response.message).text}.\"\n                            + \" please tell user to check it.\"\n                        )\n                    elif tool_invoke_response.type in {\n                        ToolInvokeMessage.MessageType.IMAGE_LINK,\n                        ToolInvokeMessage.MessageType.IMAGE,\n                    }:\n                        result += (\n                            \"image has been created and sent to user already, \"\n                            + \"you do not need to create it, just tell the user to check it now.\"\n                        )\n                    elif (\n                        tool_invoke_response.type == ToolInvokeMessage.MessageType.JSON\n                    ):\n                        text = json.dumps(\n                            cast(\n                                ToolInvokeMessage.JsonMessage,\n                                tool_invoke_response.message,\n                            ).json_object,\n                            ensure_ascii=False,\n                        )\n                        result += f\"tool response: {text}.\"\n                    else:\n                        result += f\"tool response: {tool_invoke_response.message!r}.\"\n\n                tool_response = {\n                    \"tool_call_id\": tool_call_id,\n                    \"tool_call_name\": tool_call_name,\n                    \"tool_response\": result,\n                }\n        yield self.create_text_message(result)\n```\n\n----------------------------------------\n\nTITLE: Define LLM Result Chunk Delta Model (Python)\nDESCRIPTION: Defines a Pydantic model `LLMResultChunkDelta` representing the delta of a chunk in a streaming LLM result.  It includes attributes for the `index`, the `message` (as an `AssistantPromptMessage`), optional `usage` information (as an `LLMUsage` object), and an optional `finish_reason`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunkDelta(BaseModel):\n    \"\"\"\n    Model class for llm result chunk delta.\n    \"\"\"\n    index: int\n    message: AssistantPromptMessage  # response message\n    usage: Optional[LLMUsage] = None  # usage info\n    finish_reason: Optional[str] = None  # finish reason, only the last one returns\n```\n\n----------------------------------------\n\nTITLE: UI Generator Prompt Example\nDESCRIPTION: This prompt instructs the LLM to act as a UI generator, converting user input into a UI structure.  This relies on the UI generation JSON schema to define the structure of the output.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/how-to-use-json-schema-in-dify.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nYou are a UI generator AI. Convert the user input into a UI.\n```\n\n----------------------------------------\n\nTITLE: Initializing a new Dify plugin project (Bash - Alternate)\nDESCRIPTION: This command initializes a new Dify plugin project using the Dify plugin scaffolding tool. It is an alternate command assuming that the binary file has been renamed to `dify` and copied to `/usr/local/bin` path.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: Fetching Custom Model Schema in Python\nDESCRIPTION: This method gets the customizable model schema. It takes the model name and credentials as input. When the provider supports adding custom LLMs, this method can be implemented to allow custom models to fetch model schema. The default return is null.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> Optional[AIModelEntity]:\n      \"\"\"\n      Get customizable model schema\n\n      :param model: model name\n      :param credentials: model credentials\n      :return: model schema\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: FastAPI Implementation\nDESCRIPTION: This Python code provides a FastAPI implementation to handle Dify API requests. It defines the endpoint `/api/dify/receive`, validates the API key, processes the 'ping' point for verification, and handles the 'app.external_data_tool.query' point to retrieve weather information based on location. It uses pydantic for data validation and FastAPI's dependency injection for handling headers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI, Body, HTTPException, Header\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass InputData(BaseModel):\n    point: str\n    params: dict = {}\n\n\n@app.post(\"/api/dify/receive\")\nasync def dify_receive(data: InputData = Body(...), authorization: str = Header(None)):\n    \"\"\"\n    DifyからのAPIクエリデータを受信します。\n    \"\"\"\n    expected_api_key = \"123456\"  # TODO このAPIのAPIキー\n    auth_scheme, _, api_key = authorization.partition(' ')\n\n    if auth_scheme.lower() != \"bearer\" or api_key != expected_api_key:\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n\n    point = data.point\n\n    # デバッグ用\n    print(f\"point: {point}\")\n\n    if point == \"ping\":\n        return {\n            \"result\": \"pong\"\n        }\n    if point == \"app.external_data_tool.query\":\n        return handle_app_external_data_tool_query(params=data.params)\n    # elif point == \"{point name}\":\n        # TODO その他のポイントの実装\n\n    raise HTTPException(status_code=400, detail=\"Not implemented\")\n\n\ndef handle_app_external_data_tool_query(params: dict):\n    app_id = params.get(\"app_id\")\n    tool_variable = params.get(\"tool_variable\")\n    inputs = params.get(\"inputs\")\n    query = params.get(\"query\")\n\n    # デバッグ用\n    print(f\"app_id: {app_id}\")\n    print(f\"tool_variable: {tool_variable}\")\n    print(f\"inputs: {inputs}\")\n    print(f\"query: {query}\")\n\n    # TODO 外部データツールクエリの実装\n    # 返り値は\"result\"キーを持つ辞書でなければならず、その値はクエリの結果でなければならない\n    if inputs.get(\"location\") == \"London\":\n        return {\n            \"result\": \"City: London\\nTemperature: 10°C\\nRealFeel®: 8°C\\nAir Quality: Poor\\nWind Direction: ENE\\nWind \"\n                          \"Speed: 8 km/h\\nWind Gusts: 14 km/h\\nPrecipitation: Light rain\"\n        }\n    else:\n        return {\"result\": \"Unknown city\"}\n\n```\n\n----------------------------------------\n\nTITLE: Xinference Provider YAML Configuration\nDESCRIPTION: Defines the provider settings for Xinference in a YAML file, specifying supported model types (LLM, Text Embedding, Rerank), display labels, icons, help documentation, and configurable methods.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprovider: xinference # サプライヤIDを決定\nlabel: # サプライヤの表示名。en_US（英語）とzh_Hans（中国語）の2つの言語を設定できます。zh_Hansを設定しない場合、デフォルトでen_USが使用されます。\n  en_US: Xorbits Inference\nicon_small: # 小さいアイコン。他のサプライヤのアイコンを参考に、対応するサプライヤ実装ディレクトリの_assetsディレクトリに保存してください。言語設定はlabelと同様です。\n  en_US: icon_s_en.svg\nicon_large: # 大きいアイコン\n  en_US: icon_l_en.svg\nhelp: # ヘルプ\n  title:\n    en_US: How to deploy Xinference\n    zh_Hans: 如何部署 Xinference (Xinferenceのデプロイ方法)\n  url:\n    en_US: https://github.com/xorbitsai/inference\n<strong>supported_model_types: # サポートされているモデルタイプ。XinferenceはLLM/Text Embedding/Rerankをサポートしています。\n</strong><strong>- llm\n</strong><strong>- text-embedding\n</strong><strong>- rerank\n</strong>configurate_methods: # Xinferenceはローカルにデプロイするサプライヤであり、事前定義されたモデルはありません。使用するモデルはXinferenceのドキュメントに従ってデプロイする必要があるため、ここではカスタムモデルを使用します。\n- customizable-model\nprovider_credential_schema:\n  credential_form_schemas:\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Template Example\nDESCRIPTION: This JSON Schema provides a general template with various field types (string, number, array, object) and constraints (required, additionalProperties) to guide the LLM's output structure. The schema includes definitions for `field1` (string), `field2` (number), `field3` (array of strings), and `field4` (object with `subfield1` string).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/how-to-use-json-schema-in-dify.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"template_schema\",\n    \"description\": \"JSON Schemaの汎用テンプレート\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"field1\": {\n                \"type\": \"string\",\n                \"description\": \"field1の説明\"\n            },\n            \"field2\": {\n                \"type\": \"number\",\n                \"description\": \"field2の説明\"\n            },\n            \"field3\": {\n                \"type\": \"array\",\n                \"description\": \"field3の説明\",\n                \"items\": {\n                    \"type\": \"string\"\n                }\n            },\n            \"field4\": {\n                \"type\": \"object\",\n                \"description\": \"field4の説明\",\n                \"properties\": {\n                    \"subfield1\": {\n                        \"type\": \"string\",\n                        \"description\": \"subfield1の説明\"\n                    }\n                },\n                \"required\": [\"subfield1\"],\n                \"additionalProperties\": false\n            }\n        },\n        \"required\": [\"field1\", \"field2\", \"field3\", \"field4\"],\n        \"additionalProperties\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining RerankResult Model (Python)\nDESCRIPTION: Defines the `RerankResult` model, representing the result of a reranking operation. It includes the `model` used and a list of `docs` which are instances of the `RerankDocument` model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nclass RerankResult(BaseModel):\n    \"\"\"\n    Model class for rerank result.\n    \"\"\"\n    model: str  # Actual model used\n    docs: list[RerankDocument]  # Reranked document list\n```\n\n----------------------------------------\n\nTITLE: Defining Model Type Credential Schema\nDESCRIPTION: Specifies the 'model_type' as a selectable credential for the Xinference vendor. It supports 'text-generation', 'embeddings', and 'reranking' options. This allows users to define the model type when configuring the vendor.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/customizable-model.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprovider_credential_schema:\n  credential_form_schemas:\n  - variable: model_type\n    type: select\n    label:\n      en_US: Model type\n      zh_Hans: 模型类型\n    required: true\n    options:\n    - value: text-generation\n      label:\n        en_US: Language Model\n        zh_Hans: 言語モデル\n    - value: embeddings\n      label:\n        en_US: Text Embedding\n    - value: reranking\n      label:\n        en_US: Rerank\n```\n\n----------------------------------------\n\nTITLE: Initialize Dify Plugin Project (CLI - Alternate)\nDESCRIPTION: This command initializes a new Dify plugin project using the Dify plugin scaffolding tool.  It assumes the binary has been renamed to `dify` and copied to `/usr/local/bin`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: Xinference Provider Class (Customizable Model)\nDESCRIPTION: This Python code defines the `XinferenceProvider` class, which inherits from the `Provider` base class. It provides a placeholder implementation for providers that offer customizable models. The `validate_provider_credentials` method is intentionally left empty because the actual validation is handled on a per-model basis, not at the provider level. This avoids instantiating abstract class.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/new-provider.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass XinferenceProvider(Provider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Generating Key Pair for Signing and Verification (bash)\nDESCRIPTION: This command generates a new key pair (private and public keys) for adding and verifying a plugin's signature. The private key is used to sign the plugin, and the public key is used to verify the plugin's signature. It takes the desired key pair filename as input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndify signature generate -f your_key_pair\n```\n\n----------------------------------------\n\nTITLE: Testing SearXNG Service with Curl\nDESCRIPTION: This curl command tests if the SearXNG service is running correctly. It sends a search query for \"apple\" and requests the results in JSON format. Requires curl to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/searxng.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"http://<your-linux-vm-ip>:8081/search?q=apple&format=json&categories=general\"\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for LLM Prompt (Python)\nDESCRIPTION: This method calculates the number of tokens in the given prompt messages for a large language model. It utilizes a tokenizer appropriate for the specified model. If the model does not provide a tokenizer, it falls back to the `_get_num_tokens_by_gpt2` method in the `AIModel` base class.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                   tools: Optional[list[PromptMessageTool]] = None) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param tools: tools for tool calling\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define Authentication Header\nDESCRIPTION: This snippet demonstrates the authentication header format required to authorize API requests for connecting to an external knowledge base. It uses a Bearer token.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_24\n\nLANGUAGE: HTTP\nCODE:\n```\nAuthorization: Bearer {API_KEY}\n```\n\n----------------------------------------\n\nTITLE: Text Embedding Input Tokens Calculation Interface in Python\nDESCRIPTION: Defines the interface for getting the number of tokens for given input texts. Parameters include model name, credentials, and the list of texts to embed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, texts: list[str]) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param texts: texts to embed\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Importing DSL File via URL\nDESCRIPTION: Illustrates how to import a DSL file via a URL within the Dify environment. This method allows for the quick deployment of pre-configured applications from online sources.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-orchestrate/creating-an-application.md#_snippet_0\n\nLANGUAGE: url\nCODE:\n```\nhttps://example.com/your_dsl.yml\n```\n\n----------------------------------------\n\nTITLE: Connect to Remote Repository\nDESCRIPTION: Connects the local Git repository to the remote GitHub repository using the `git remote add` command. This allows the local repository to push changes to the specified GitHub repository. Replace `<your-username>` and `<repository-name>` with the actual GitHub username and repository name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add origin https://github.com/<your-username>/<repository-name>.git\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Containers with Compose V2\nDESCRIPTION: This command starts the Dify Docker containers using Docker Compose V2. The '-d' flag runs the containers in detached mode (in the background).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/docker-compose.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Creating Document via File using Dify API (curl)\nDESCRIPTION: This snippet demonstrates how to create a document in a Dify knowledge base by uploading a file. It uses the `create_by_file` endpoint and requires the dataset ID and API key. The request includes form data for the file and processing rules.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/document/create_by_file' \\\n--header 'Authorization: Bearer {api_key}' \\\n--form 'data=\"{\"indexing_technique\":\"high_quality\",\"process_rule\":{\"rules\":{\"pre_processing_rules\":[{\"id\":\"remove_extra_spaces\",\"enabled\":true},{\"id\":\"remove_urls_emails\",\"enabled\":true}],\"segmentation\":{\"separator\":\"###\",\"max_tokens\":500}},\"mode\":\"custom\"}}\";type=text/plain' \\\n--form 'file=@\"/path/to/file\"'\n```\n\n----------------------------------------\n\nTITLE: .env File Configuration for Remote Debugging\nDESCRIPTION: This bash snippet shows the `.env` file configuration for remote debugging a plugin, including the installation method, remote host, port, and key. This configuration enables developers to test and debug their plugins remotely within the Dify environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: Packaging Dify Plugin\nDESCRIPTION: This snippet shows the command-line tool used for packaging a Dify plugin after it has been developed and debugged.  The command `dify plugin package ./basic_agent/` packages the plugin located in the `./basic_agent/` directory, creating a `google.difypkg` file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin package ./basic_agent/\n```\n\n----------------------------------------\n\nTITLE: Reload Systemd Daemon and Restart Ollama Service\nDESCRIPTION: These commands reload the systemd daemon and restart the Ollama service after the service file has been edited. This ensures that the new environment variables are applied to the Ollama service. This is a crucial step after configuring environment variables for the Ollama service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/ollama.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsystemctl daemon-reload\nsystemctl restart ollama\n```\n\n----------------------------------------\n\nTITLE: Handling Stream and Sync Responses in Python\nDESCRIPTION: This Python snippet shows how to handle both streaming and synchronous responses from an LLM. It uses separate handler functions for each type of response. The `_invoke` function dispatches to the appropriate handler based on the `stream` flag, enhancing code clarity and adaptability.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, stream: bool, **kwargs) -> Union[LLMResult, Generator]:\n    \"\"\"根据返回类型调用对应的处理函数。\"\"\"\n    if stream:\n        return self._handle_stream_response(**kwargs)\n    return self._handle_sync_response(**kwargs)\n\ndef _handle_stream_response(self, **kwargs) -> Generator:\n    \"\"\"处理流式返回逻辑。\"\"\"\n    for chunk in response:  # 假设 response 是流式数据迭代器\n        yield chunk\n\ndef _handle_sync_response(self, **kwargs) -> LLMResult:\n    \"\"\"处理同步返回逻辑。\"\"\"\n    return LLMResult(**response)  # 假设 response 是完整的响应字典\n```\n\n----------------------------------------\n\nTITLE: Start LiteLLM Proxy with Docker\nDESCRIPTION: This command starts the LiteLLM Proxy server using Docker. It mounts the `litellm_config.yaml` file to `/app/config.yaml` inside the container, exposes port 4000, uses the latest LiteLLM image, and specifies the configuration file path with detailed debug logging enabled.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/litellm.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker run \\\n    -v $(pwd)/litellm_config.yaml:/app/config.yaml \\\n    -p 4000:4000 \\\n    ghcr.io/berriai/litellm:main-latest \\\n    --config /app/config.yaml --detailed_debug\n```\n\n----------------------------------------\n\nTITLE: Initialize Git Repository\nDESCRIPTION: Initializes a local Git repository in the plugin project folder, adds all files to the staging area, and creates an initial commit.  This is the first step to track changes and push the plugin code to GitHub. It requires Git to be installed locally.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit init\ngit add .\ngit commit -m \"Initial commit: Add plugin files\"\n```\n\n----------------------------------------\n\nTITLE: Concatenating Data with Python\nDESCRIPTION: This Python code snippet concatenates two lists, `knowledge1` and `knowledge2`, into a single list. It defines a `main` function that takes two lists as input and returns a dictionary containing the concatenated list under the key 'result'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\ndef main(knowledge1: list, knowledge2: list) -> list:\n    return {\n        # Note to declare 'result' in the output variables\n        'result': knowledge1 + knowledge2\n    }\n```\n\n----------------------------------------\n\nTITLE: Accessing Storage in Dify Plugin (Entry Point)\nDESCRIPTION: This snippet shows how to access the storage functionality within a Dify plugin session. It's used as the entry point to the storage functionalities.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/persistent-storage.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.session.storage\n```\n\n----------------------------------------\n\nTITLE: Authorization Header Example\nDESCRIPTION: This snippet shows the correct format for the Authorization header, which is required for authenticating requests to the Retrieval API. It uses a Bearer token with an API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/external-knowledge-api-documentation.md#_snippet_0\n\nLANGUAGE: HTTP\nCODE:\n```\nAuthorization: Bearer {API_KEY}\n```\n\n----------------------------------------\n\nTITLE: Ngrok Setup Commands (Shell)\nDESCRIPTION: Commands to configure and run Ngrok, allowing access to a local API service from the public internet for debugging.  `yourToken` should be replaced by your actual Ngrok token.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/README.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\n$ unzip /path/to/ngrok.zip\n$ ./ngrok config add-authtoken 你的Token\n```\n\nLANGUAGE: Shell\nCODE:\n```\n$ ./ngrok http [port number]\n```\n\n----------------------------------------\n\nTITLE: Chatflow App: Image File Processing\nDESCRIPTION: Describes the configuration for processing uploaded image files in a Chatflow app. It involves enabling the file upload feature, selecting 'image' as the file type, adding an LLM node, enabling the VISION feature and selecting the `sys.files` variable.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/additional-features.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n1. Enable Features and select 'image' as the file type.\n2. Add an LLM node, enable the VISION feature and select the `sys.files` variable.\n3. Add a 'Answer' node and fill in the output variable of the LLM node.\n```\n\n----------------------------------------\n\nTITLE: Request Syntax Example\nDESCRIPTION: Demonstrates the complete request structure in JSON format, including the required parameters for `knowledge_id`, `query`, and `retrieval_setting`. The example includes placeholders for actual values.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_23\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST <your-endpoint>/retrieval HTTP/1.1\n-- header\nContent-Type: application/json\nAuthorization: Bearer your-api-key\n-- data\n{\n    \"knowledge_id\": \"your-knowledge-id\",\n    \"query\": \"your question\",\n    \"retrieval_setting\":{\n        \"top_k\": 2,\n        \"score_threshold\": 0.5\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Python 3.12 with pyenv (Bash)\nDESCRIPTION: These commands use pyenv to install Python 3.12 and set it as the global Python version for the environment. This ensures that the API and worker services have the correct Python version for proper execution.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\npyenv install 3.12\n```\n\nLANGUAGE: Bash\nCODE:\n```\npyenv global 3.12\n```\n\n----------------------------------------\n\nTITLE: Provider Credential Validation in Python\nDESCRIPTION: Defines the interface for validating provider credentials. It expects a dictionary of credentials as input and raises an exception if validation fails. The credentials form is defined in `provider_credential_schema`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: WeatherSearch Implementation Template (Python)\nDESCRIPTION: Provides a template for implementing a custom external data tool in Python. It includes the basic class structure, the validate_config method for schema validation (to be implemented by the developer), and the query method for data retrieval (also to be implemented). The class variable `name` must match the directory and file name for the tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/external-data-tool.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nfrom core.external_data_tool.base import ExternalDataTool\n\n\nclass WeatherSearch(ExternalDataTool):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"weather_search\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user save the config.\n\n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n\n        # implement your own logic here\n\n    def query(self, inputs: dict, query: Optional[str] = None) -> str:\n        \"\"\"\n        Query the external data tool.\n\n        :param inputs: user inputs\n        :param query: the query of chat app\n        :return: the tool query result\n        \"\"\"\n       \n        # implement your own logic here\n        return \"your own data.\"\n```\n\n----------------------------------------\n\nTITLE: Invoking Speech to Text Model in Python\nDESCRIPTION: This function invokes a speech-to-text model to transcribe an audio file into text. The function receives the file content as a byte stream, the model to use and credentials.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            file: IO[bytes], user: Optional[str] = None) \\\n        -> str:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param file: audio file\n    :param user: unique user id\n    :return: text for given audio file\n    \"\"\"        \n```\n\n----------------------------------------\n\nTITLE: Invocation Error Mapping Table - Python\nDESCRIPTION: This snippet shows the `_invoke_error_mapping` property, which is a dictionary that maps model invocation errors to Runtime-specified `InvokeError` types. This mapping allows Dify to handle different errors consistently. The keys of the dictionary are the `InvokeError` types, and the values are lists of exception types thrown by the model that should be converted to the corresponding `InvokeError`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/customizable-model.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@property\n    def _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n        \"\"\"\n        Map model invoke error to unified error\n        The key is the error type thrown to the caller\n        The value is the error type thrown by the model,\n        which needs to be converted into a unified error type for the caller.\n\n        :return: Invoke error mapping\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Text Message in Dify (Python)\nDESCRIPTION: This code snippet illustrates the creation of a text message using the `Tool` class in Dify. It takes a text string as input and returns a `ToolInvokeMessage` object, allowing for text-based communication with the LLM and users. The `save_as` parameter's function is not specified.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/advanced-tool-integration.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    def create_text_message(self, text: str, save_as: str = '') -> ToolInvokeMessage:\n        \"\"\"\n            create a text message\n\n            :param text: the text of the message\n            :return: the text message\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Example JSON Error Response\nDESCRIPTION: This JSON object represents an example error response from the Dify API. It includes a code, a message, and a status. The code provides a machine-readable identifier for the error, while the message offers a human-readable explanation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n  {\n    \"code\": \"no_file_uploaded\",\n    \"message\": \"Please upload your file.\",\n    \"status\": 400\n  }\n```\n\n----------------------------------------\n\nTITLE: Enable/Disable Built-in Fields via Dify API\nDESCRIPTION: Enables or disables built-in metadata fields in a dataset within the Dify knowledge base. Requires the dataset ID and an action (presumably 'enable' or 'disable') as path parameters, along with a valid API key in the Authorization header.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata/built-in/{action}' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Packaging a Dify Plugin\nDESCRIPTION: This bash command packages a Dify plugin using the `dify plugin package` command. Replace `./google` with the actual path to the plugin project.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# Replace ./google with your actual plugin project path.\n\ndify plugin package ./google\n```\n\n----------------------------------------\n\nTITLE: Upgrade Dify Version to 1.0.0\nDESCRIPTION: This snippet details the commands to upgrade to version 1.0.0 of Dify using Git and Docker Compose. It checks out the specified tag and then starts the application with Docker Compose.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/dify-premium.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout 1.0.0 # 切换至 1.0.0 分支\ncd docker\ndocker compose -f docker-compose.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Web Interface with Domain - Bash\nDESCRIPTION: This snippet shows how to access the main Dify web interface when a domain name has been configured. Replace `yourdomain` with the actual domain name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/bt-panel.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# If you have set domain\nhttp://yourdomain/\n```\n\n----------------------------------------\n\nTITLE: Creating a Python 3.11 environment using Conda\nDESCRIPTION: This snippet creates a Python 3.11 environment using Conda. It first creates the environment and then activates it. This is a crucial first step when setting up a development environment to ensure dependencies are managed correctly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name python-3-11 python=3.11\nconda activate python-3-11\n```\n\n----------------------------------------\n\nTITLE: Get WSL IP Address\nDESCRIPTION: This command retrieves the IP address assigned to the WSL (Windows Subsystem for Linux) environment.  This is necessary for configuring the Xinference server URL within Dify when running Dify within WSL, ensuring Dify can correctly communicate with Xinference. The output is the IP address.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_13\n\nLANGUAGE: Bash\nCODE:\n```\nhostname -I\n```\n\n----------------------------------------\n\nTITLE: Cloning the Dify Documentation Repository with Git\nDESCRIPTION: This snippet demonstrates how to clone the Dify documentation repository from GitHub to your local machine.  It requires Git to be installed. Replace <your-github-account> with your GitHub username.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/community/docs-contribution.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/<your-github-account>/dify-docs.git\n```\n\n----------------------------------------\n\nTITLE: Executing Database Migrations\nDESCRIPTION: Applies database migrations to update the database schema to the latest version. This ensures that the database structure is compatible with the application code.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run flask db upgrade\n```\n\n----------------------------------------\n\nTITLE: LLM Prompt for Question Generation\nDESCRIPTION: This snippet provides the prompt used for the Question Generation LLM node. It instructs the LLM to read the summarized article structure and formulate insightful questions for each section, encouraging deeper thought during the reading process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/workshop/intermediate/article-reader.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n阅读以下文章内容并执行任务\n{{结构提取的输出}}\n# 任务\n\n- **主要目标**：全面阅读上文，针对文章各个部分提出尽可能多的问题。\n- **要求**：问题要有意义和价值，值得思考。\n- **限制**：无特定限制。\n- **预期输出**：一系列针对文章各个部分的问题，每个问题都应有深度和思考价值。\n\n# 推理顺序\n\n- **推理部分**：全面阅读文章，分析文章各个部分的内容，思考每个部分可能引发的深层次问题。\n- **结论部分**：提出有意义和有价值的问题，确保问题能够引发深入的思考。\n\n# 输出格式\n\n- **格式**：每个问题单独成行，编号列出。\n- **内容**：针对文章的各个部分（如引言、背景、方法、结果、讨论、结论等）提出问题。\n- **数量**：尽可能多，但每个问题都应有意义和价值。\n\n# 备注\n\n- **边缘情况**：如果文章的某些部分内容较少，可以适当调整问题的数量和深度，但每个问题都应有思考价值。\n- **重要考虑事项**：确保问题能够引导读者深入理解文章内容，而不仅仅是表面的疑问。\n```\n\n----------------------------------------\n\nTITLE: Google Icon SVG Definition\nDESCRIPTION: This SVG code defines the icon for the Google tool provider. It should be placed in the `_assets` folder within the provider's module directory. The SVG is used to visually represent the tool provider in the user interface.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/quick-tool-integration.md#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"25\" viewBox=\"0 0 24 25\" fill=\"none\">\n  <path d=\"M22.501 12.7332C22.501 11.8699 22.4296 11.2399 22.2748 10.5865H12.2153V14.4832H18.12C18.001 15.4515 17.3582 16.9099 15.9296 17.8898L15.9096 18.0203L19.0902 20.435L19.3106 20.4565C21.3343 18.6249 22.501 15.9298 22.501 12.7332Z\" fill=\"#4285F4\"/>\n  <path d=\"M12.214 23C15.1068 23 17.5353 22.0666 19.3092 20.4567L15.9282 17.8899C15.0235 18.5083 13.8092 18.9399 12.214 18.9399C9.38069 18.9399 6.97596 17.1083 6.11874 14.5766L5.99309 14.5871L2.68583 17.0954L2.64258 17.2132C4.40446 20.6433 8.0235 23 12.214 23Z\" fill=\"#34A853\"/>\n  <path d=\"M6.12046 14.5766C5.89428 13.9233 5.76337 13.2233 5.76337 12.5C5.76337 11.7766 5.89428 11.0766 6.10856 10.4233L6.10257 10.2841L2.75386 7.7355L2.64429 7.78658C1.91814 9.20993 1.50146 10.8083 1.50146 12.5C1.50146 14.1916 1.91814 15.7899 2.64429 17.2132L6.12046 14.5766Z\" fill=\"#FBBC05\"/>\n  <path d=\"M12.2141 6.05997C14.2259 6.05997 15.583 6.91163 16.3569 7.62335L19.3807 4.73C17.5236 3.03834 15.1069 2 12.2141 2C8.02353 2 4.40447 4.35665 2.64258 7.78662L6.10686 10.4233C6.97598 7.89166 9.38073 6.05997 12.2141 6.05997Z\" fill=\"#EB4335\"/>\n</svg>\n```\n\n----------------------------------------\n\nTITLE: Define Agent Parameters (YAML)\nDESCRIPTION: This YAML configuration defines the parameters required for the Agent plugin, including model, tools, query, and maximum_iterations. These parameters determine the plugin's core functionality, such as calling LLM models and using tools.  The 'extra' field specifies the python source file for this strategy.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: basic_agent # the name of the agent_strategy\n  author: novice # the author of the agent_strategy\n  label:\n    en_US: BasicAgent # the engilish label of the agent_strategy\ndescription:\n  en_US: BasicAgent # the english description of the agent_strategy\nparameters:\n  - name: model # the name of the model parameter\n    type: model-selector # model-type\n    scope: tool-call&llm # the scope of the parameter\n    required: true\n    label:\n      en_US: Model\n      zh_Hans: 模型\n      pt_BR: Model\n  - name: tools # the name of the tools parameter\n    type: array[tools] # the type of tool parameter\n    required: true\n    label:\n      en_US: Tools list\n      zh_Hans: 工具列表\n      pt_BR: Tools list\n  - name: query # the name of the query parameter\n    type: string # the type of query parameter\n    required: true\n    label:\n      en_US: Query\n      zh_Hans: 查询\n      pt_BR: Query\n  - name: maximum_iterations\n    type: number\n    required: false\n    default: 5\n    label:\n      en_US: Maxium Iterations\n      zh_Hans: 最大迭代次数\n      pt_BR: Maxium Iterations\n    max: 50 # if you set the max and min value, the display of the parameter will be a slider\n    min: 1\nextra:\n  python:\n    source: strategies/basic_agent.py\n```\n\n----------------------------------------\n\nTITLE: JSON Schema for UI Generation\nDESCRIPTION: This JSON Schema is designed for generating dynamic UIs. It uses recursion to define nested UI components. It includes properties for 'type' (UI component type), 'label', 'children' (nested components, using $ref to recursively reference the same schema), and 'attributes' (arbitrary UI component attributes).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/how-to-use-json-schema-in-dify.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"ui\",\n    \"description\": \"動的に生成されたUI\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"type\": {\n                \"type\": \"string\",\n                \"description\": \"UIコンポーネントのタイプ\",\n                \"enum\": [\"div\", \"button\", \"header\", \"section\", \"field\", \"form\"]\n            },\n            \"label\": {\n                \"type\": \"string\",\n                \"description\": \"UIコンポーネントのラベル、ボタンやフォームフィールドに使用\"\n            },\n            \"children\": {\n                \"type\": \"array\",\n                \"description\": \"入れ子のUIコンポーネント\",\n                \"items\": {\n                    \"$ref\": \"#\"\n                }\n            },\n            \"attributes\": {\n                \"type\": \"array\",\n                \"description\": \"UIコンポーネントのための任意の属性、任意の要素に適しています\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"name\": {\n                            \"type\": \"string\",\n                            \"description\": \"属性の名前、例えばonClickやclassName\"\n                        },\n                        \"value\": {\n                            \"type\": \"string\",\n                            \"description\": \"属性の値\"\n                        }\n                    },\n                    \"additionalProperties\": false,\n                    \"required\": [\"name\", \"value\"]\n                }\n            }\n        },\n        \"required\": [\"type\", \"label\", \"children\", \"attributes\"],\n        \"additionalProperties\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Dify Project Code Structure\nDESCRIPTION: Illustrates the directory structure of the Dify project, showcasing key components like the server entry point, shared libraries, HTTP request handlers, middleware, and core logic for code execution. It highlights the separation of concerns and the location of critical configuration files.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/backend/sandbox/contribution.md#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n[cmd/]\n├── server                // サーバー起動のエントリーポイント\n├── lib                   // 共有ライブラリのエントリーポイント\n└── test                  // 一般的なテストスクリプト\n[build/]                  // 各種アーキテクチャとプラットフォーム用のビルドスクリプト\n[internal/]               // 内部パッケージ\n├── controller            // HTTPリクエストハンドラ\n├── middleware            // リクエスト処理用ミドルウェア\n├── server                // サーバーのセットアップと設定\n├── service               // コントローラー用のサービス提供\n├── static                // 設定ファイル\n│   ├── nodejs_syscall    // Node.jsシステムコールのホワイトリスト\n│   └── python_syscall    // Pythonシステムコールのホワイトリスト\n├── types                 // エンティティ\n├── core                  // 分離と実行のためのコアロジック\n│   ├── lib               // 共有ライブラリ\n│   ├── runner            // コード実行\n│   │   ├── nodejs        // Node.jsランナー\n|   |   └── python        // Pythonランナー\n└── tests                 // CI/CD用のテスト\n```\n\n----------------------------------------\n\nTITLE: Handling Stream and Sync Responses in LLM\nDESCRIPTION: This python code demonstrates how to handle both stream and synchronous responses when invoking a language model. It includes methods for selecting the appropriate handler based on the `stream` parameter and implementing separate functions for stream and sync response logic.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, stream: bool, **kwargs) -> Union[LLMResult, Generator]:\n    \"\"\"戻り値の型に応じて、対応する処理関数を呼び出します。\"\"\"\n    if stream:\n        return self._handle_stream_response(**kwargs)\n    return self._handle_sync_response(**kwargs)\n\ndef _handle_stream_response(self, **kwargs) -> Generator:\n    \"\"\"ストリームリターンのロジックを処理します。\"\"\"\n    for chunk in response:  # responseがストリームデータのイテレーターであると仮定します\n        yield chunk\n\ndef _handle_sync_response(self, **kwargs) -> LLMResult:\n    \"\"\"同期リターンのロジックを処理します。\"\"\"\n    return LLMResult(**response)  # responseが完全な応答の辞書であると仮定します\n```\n\n----------------------------------------\n\nTITLE: Rerank Document Class\nDESCRIPTION: Defines the `RerankDocument` class, representing a single document in a reranking result. It contains the original index, the text content, and the score.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nclass RerankDocument(BaseModel):\n    \"\"\"\n    Model class for rerank document.\n    \"\"\"\n    index: int  # 原序号\n    text: str  # 分段文本内容\n    score: float  # 分数\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for Text Embedding in Python\nDESCRIPTION: This method gets the number of tokens for given texts for text embedding. It takes the model name, credentials, and texts as input.  It returns the number of tokens.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, texts: list[str]) -> int:\n      \"\"\"\n      Get number of tokens for given prompt messages\n\n      :param model: model name\n      :param credentials: model credentials\n      :param texts: texts to embed\n      :return:\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Start Middleware Services\nDESCRIPTION: Starts the middleware services (PostgreSQL, Redis, Weaviate) using Docker Compose. These services are essential for Dify's functionality. Requires Docker and Docker Compose to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_35\n\nLANGUAGE: Bash\nCODE:\n```\ncd docker\ncp middleware.env.example middleware.env\ndocker compose -f docker-compose.middleware.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Configuring .env file for Dify Plugin Remote Debugging\nDESCRIPTION: This snippet shows how to configure the `.env` file with the remote server address and debug key to enable remote debugging of a Dify plugin. The `INSTALL_METHOD` is set to `remote`, along with the `REMOTE_INSTALL_HOST`, `REMOTE_INSTALL_PORT`, and `REMOTE_INSTALL_KEY`. The port is typically 5003.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/debug-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: Run Llama3.2 with Ollama\nDESCRIPTION: This command runs the Llama3.2 model using the Ollama CLI. It assumes Ollama is installed and configured. The command starts an API service on the local port 11434, making the model accessible for integration with platforms like Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/ollama.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama run llama3.2\n```\n\n----------------------------------------\n\nTITLE: Chat Model Dialog Template ASSISTANT Prompt - Dify\nDESCRIPTION: This is the ASSISTANT prompt within the chat model dialog template for Dify. Currently, it is an empty string, indicating that the LLM will generate the assistant's response based on the SYSTEM and USER prompts.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n\"\" \n```\n\n----------------------------------------\n\nTITLE: Escape String to Object - Python\nDESCRIPTION: This Python function `main` takes a JSON string as input and parses it to extract specific data. It extracts the 'memory' object from the input, retrieves 'facts', 'preferences', and 'memories' from it, and returns a dictionary containing these extracted values under the key 'mem'. The function includes error handling for invalid JSON strings and other exceptions, returning an error message in case of failure.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/variable-assigner.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\ndef main(arg1: str) -> object:\n    try:\n        # Parse the input JSON string\n        input_data = json.loads(arg1)\n        \n        # Extract the memory object\n        memory = input_data.get(\"memory\", {})\n        \n        # Construct the return object\n        result = {\n            \"facts\": memory.get(\"facts\", []),\n            \"preferences\": memory.get(\"preferences\", []),\n            \"memories\": memory.get(\"memories\", [])\n        }\n        \n        return {\n            \"mem\": result\n        }\n    except json.JSONDecodeError:\n        return {\n            \"result\": \"Error: Invalid JSON string\"\n        }\n    except Exception as e:\n        return {\n            \"result\": f\"Error: {str(e)}\"\n        }\n```\n\n----------------------------------------\n\nTITLE: Cloning and Configuring the Example Repository (Bash)\nDESCRIPTION: These commands clone the example GitHub repository for a Dify API extension and copies the example wrangler configuration file for modification.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/crazywoola/dify-extension-workers.git\ncp wrangler.toml.example wrangler.toml\n```\n\n----------------------------------------\n\nTITLE: Define LLM Usage Model (Python)\nDESCRIPTION: Defines a Pydantic model `LLMUsage` representing the usage information for a language model (LLM) call. It extends the `ModelUsage` model and includes attributes for token counts, unit prices, price units, costs, currency, and latency.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nclass LLMUsage(ModelUsage):\n    \"\"\"\n    Model class for LLM usage.\n    \"\"\"\n    prompt_tokens: int  # Tokens used for prompt\n    prompt_unit_price: Decimal  # Unit price for prompt\n    prompt_price_unit: Decimal  # Price unit for prompt, i.e., the unit price based on how many tokens\n    prompt_price: Decimal  # Cost for prompt\n    completion_tokens: int  # Tokens used for response\n    completion_unit_price: Decimal  # Unit price for response\n    completion_price_unit: Decimal  # Price unit for response, i.e., the unit price based on how many tokens\n    completion_price: Decimal  # Cost for response\n    total_tokens: int  # Total number of tokens used\n    total_price: Decimal  # Total cost\n    currency: str  # Currency unit\n    latency: float  # Request latency (s)\n```\n\n----------------------------------------\n\nTITLE: Configuration for Remote Plugin Installation\nDESCRIPTION: This snippet shows the necessary configuration for remote plugin installation. It includes setting the installation method to 'remote' and specifying the remote server address and debugging key. The parameters `REMOTE_INSTALL_HOST`, `REMOTE_INSTALL_PORT`, and `REMOTE_INSTALL_KEY` must be configured with values obtained from the Dify plugin management console.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: SystemPromptMessage Class Definition in Python\nDESCRIPTION: Defines the `SystemPromptMessage` class, inheriting from `PromptMessage`, representing a system message used to provide instructions to the model. It sets the `role` to `PromptMessageRole.SYSTEM`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nclass SystemPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for system prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.SYSTEM\n```\n\n----------------------------------------\n\nTITLE: Starting Xinference locally\nDESCRIPTION: This command starts the Xinference server locally, making it accessible for model deployment and inference. The output provides the endpoint for accessing the Xinference service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ xinference-local\n2023-08-20 19:21:05,265 xinference   10148 INFO     Xinference successfully started. Endpoint: http://127.0.0.1:9997\n2023-08-20 19:21:05,266 xinference.core.supervisor 10148 INFO     Worker 127.0.0.1:37822 has been added successfully\n2023-08-20 19:21:05,267 xinference.deploy.worker 10148 INFO     Xinference worker successfully started.\n```\n\n----------------------------------------\n\nTITLE: オブジェクトをJSON文字列に変換 (Python)\nDESCRIPTION: 与えられたオブジェクト（辞書）をJSON文字列に変換します。入力としてリストを受け取り、その最初の要素が処理対象の辞書であると想定しています。変換されたJSON文字列は、`<answer>`タグで囲んで返します。変換時に`ensure_ascii=False`を指定することで、日本語などの非ASCII文字が正しくエンコードされるようにします。エラーが発生した場合は、エラーメッセージを`<answer>`タグで囲んで返します。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/variable-assigner.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\ndef main(arg1: list) -> str:\n    try:\n        # Assume arg1[0] is the dictionary we need to process\n        context = arg1[0] if arg1 else {}\n        \n        # Construct the memory object\n        memory = {\"memory\": context}\n        \n        # Convert the object to a JSON string\n        json_str = json.dumps(memory, ensure_ascii=False, indent=2)\n        \n        # Wrap the JSON string in <answer> tags\n        result = f\"<answer>{json_str}</answer>\"\n        \n        return {\n            \"result\": result\n        }\n    except Exception as e:\n        return {\n            \"result\": f\"<answer>Error: {str(e)}</answer>\"\n        }\n\n```\n\n----------------------------------------\n\nTITLE: TTS Request Entry Point (Python)\nDESCRIPTION: This code shows the entry point for requesting text-to-speech (TTS) conversion. It uses the session model's `tts` property.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.tts\n```\n\n----------------------------------------\n\nTITLE: Generate Key Pair for Plugin Signing (Bash)\nDESCRIPTION: This command generates a new key pair (private and public keys) used for signing and verifying Dify plugins. The `-f` option specifies the base name for the generated key files. The private key is used for signing, and the public key is used for verification. Ensure to keep the private key secure.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndify signature generate -f your_key_pair\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Containers with Compose V1\nDESCRIPTION: This command starts the Dify Docker containers using Docker Compose V1. The '-d' flag runs the containers in detached mode (in the background).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/docker-compose.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: PromptMessageTool Class Definition in Python\nDESCRIPTION: Defines the `PromptMessageTool` class as a Pydantic BaseModel representing a tool that can be used in prompt engineering. It includes the name, description, and parameters of the tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageTool(BaseModel):\n    \"\"\"\n    Model class for prompt message tool.\n    \"\"\"\n    name: str  # 工具名称\n    description: str  # 工具描述\n    parameters: dict  # 工具参数 dict\n```\n\n----------------------------------------\n\nTITLE: Squid Proxy Configuration for SSRF Protection\nDESCRIPTION: This snippet demonstrates how to customize the Squid proxy configuration file (`squid.conf`) to restrict access to specific IP addresses within a local network, enhancing security and preventing potential SSRF attacks. It configures ACL rules to deny access to a restricted IP while allowing access from the rest of the local network.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nacl restricted_ip dst 192.168.101.19\nacl localnet src 192.168.101.0/24\n\nhttp_access deny restricted_ip\nhttp_access allow localnet\nhttp_access deny all\n```\n\n----------------------------------------\n\nTITLE: LLM Prompt for JSON Code Generation\nDESCRIPTION: This LLM prompt instructs the language model to output a correct or incorrect JSON code sample based on user requirements, acting as a teaching assistant. The language model should only output the JSON code and nothing else.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_73\n\nLANGUAGE: text\nCODE:\n```\nYou are a teaching assistant. According to the user's requirements, you only output a correct or incorrect sample code in json format.\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Anthropic LLM Models\nDESCRIPTION: This bash code snippet illustrates the directory structure for Anthropic LLM models within a Dify plugin. It shows the location of YAML files for different Claude models and the llm.py file for model logic implementation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n├── models\n│   └── llm\n│       ├── _position.yaml\n│       ├── claude-2.1.yaml\n│       ├── claude-2.yaml\n│       ├── claude-3-5-sonnet-20240620.yaml\n│       ├── claude-3-haiku-20240307.yaml\n│       ├── claude-3-opus-20240229.yaml\n│       ├── claude-3-sonnet-20240229.yaml\n│       ├── claude-instant-1.2.yaml\n│       ├── claude-instant-1.yaml\n│       └── llm.py\n```\n\n----------------------------------------\n\nTITLE: Installing Xinference via PyPI\nDESCRIPTION: This command installs Xinference with all optional dependencies using pip. This is the first step to deploying Xinference locally.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/xinference.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install \"xinference[all]\"\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response: Internal Server Error\nDESCRIPTION: This snippet shows an error response indicating an internal server error when using the OpenAI API. It suggests that either the server is overloaded or there is an error in the application.  The user should try again later.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/llms-use-faq.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"The server encountered an internal error and was unable to complete your request。Either the server is overloaded or there is an error in the application\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Sending message to Dify and processing the response\nDESCRIPTION: This Python code sends a message to the Dify API and processes the streaming response. It constructs a JSON payload with the user's query and conversation ID and makes a POST request to the Dify API. The response is streamed, and the code extracts the agent's thoughts to construct the answer. It also updates the conversation ID for the user.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/dify-on-whatsapp.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n    url = dify_url\n    headers = {  \n        'Content-Type': 'application/json',  \n        'Authorization': f\"Bearer {dify_api_key}\",  \n    }  \n    data = {  \n        'inputs': {},  \n        'query': Body,  \n        'response_mode': 'streaming',  \n        'conversation_id': conversation_ids.get(whatsapp_number, ''),  \n        'user': whatsapp_number,  \n    }  \n    response = requests.post(url, headers=headers, data=json.dumps(data), stream=True)  \n    answer = []  \n    for line in response.iter_lines():  \n        if line:  \n            decoded_line = line.decode('utf-8')  \n            if decoded_line.startswith('data: '):\n                decoded_line = decoded_line[6:]\n            try:  \n                json_line = json.loads(decoded_line) \n                if \"conversation_id\" in json_line:\n                    conversation_ids[whatsapp_number] = json_line[\"conversation_id\"]\n                if json_line[\"event\"] == \"agent_thought\":  \n                    answer.append(json_line[\"thought\"])\n            except json.JSONDecodeError: \n                print(json_line)  \n                continue  \n\n    merged_answer = ''.join(answer)  \n```\n\n----------------------------------------\n\nTITLE: Creating an Image Message in Dify (Python)\nDESCRIPTION: This snippet demonstrates how to create an image message within the Dify tool framework. It takes an image URL as input and returns a ToolInvokeMessage object. Dify automatically downloads the image and returns it to the user.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/advanced-tool-integration.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    def create_image_message(self, image: str, save_as: str = '') -> ToolInvokeMessage:\n        \"\"\"\n            create an image message\n\n            :param image: the url of the image\n            :return: the image message\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating JSON Messages in Python\nDESCRIPTION: This Python code snippet presents the interface for creating JSON messages within the Dify tool plugin. It takes a Python dictionary, which will be treated as JSON, as input and returns a ToolInvokeMessage.  These are useful for data interchange between nodes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/tool.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef create_json_message(self, json: dict) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Upgrade Dify Version to 1.0.0 - Bash\nDESCRIPTION: These commands fetch the latest changes from the remote repository, switch to the 1.0.0 branch, navigate to the 'docker' directory, and then start the Docker containers using Docker Compose.  It assumes the user has Git installed and configured.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/migration/migrate-to-v1.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit fetch origin\ngit checkout 1.0.0 # Switch to the 1.0.0 branch\ncd docker\nnano .env # Modify the environment configuration file to synchronizing .env.example file\ndocker compose -f docker-compose.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Install Xinference via PyPI\nDESCRIPTION: This command installs Xinference with all dependencies using pip. It is the first step in setting up Xinference for local deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/xinference.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install \"xinference[all]\"\n```\n\n----------------------------------------\n\nTITLE: Getting a Storage Key-Value Pair\nDESCRIPTION: This snippet demonstrates the `get` method for retrieving a value associated with a key. The method returns a byte string if the key exists, otherwise likely returns None or raises an exception (though not explicitly stated here).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/persistent-storage.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get(self, key: str) -> bytes:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Deleting Document using Dify API (curl)\nDESCRIPTION: This snippet shows how to delete a specific document from a Dify knowledge base. It requires the dataset ID, document ID, and API key.  A successful operation returns a JSON response indicating 'success'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Tool Prompt Message Class in Python\nDESCRIPTION: This code defines the Tool Prompt Message class, inheriting from PromptMessage. It represents a tool message, used for conveying the results of a tool execution to the model. It specifies the role as TOOL and includes the tool_call_id.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nclass ToolPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for tool prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.TOOL\n    tool_call_id: str  # Tool invocation ID. If OpenAI tool call is not supported, the name of the tool can also be inputted.\n```\n\n----------------------------------------\n\nTITLE: Starting Middleware Services with Docker Compose\nDESCRIPTION: Starts middleware services such as PostgreSQL, Redis, and Weaviate using Docker Compose. These services are required for Dify to function correctly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_18\n\nLANGUAGE: Bash\nCODE:\n```\ncd docker\ndocker compose -f docker-compose.middleware.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Defining enrolled WhatsApp numbers\nDESCRIPTION: This Python code defines a list of WhatsApp numbers that are allowed to use the service.  This list is used to check if an incoming message is from an enrolled number. This provides a basic form of access control.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/dify-on-whatsapp.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nenrolled_numbers = ['+14155238886']\n```\n\n----------------------------------------\n\nTITLE: MCP SSE Plugin Configuration JSON\nDESCRIPTION: Configuration JSON for the MCP SSE plugin in Dify, specifying the Zapier MCP Server URL, headers, timeout, and SSE read timeout. This allows the Dify Agent to communicate with the Zapier MCP service for automated tasks.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/best-practice/how-to-use-mcp-zapier.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"server_name\": {\n    \"url\": \"https://actions.zapier.com/mcp/*******/sse\",\n    \"headers\": {},\n    \"timeout\": 5,\n    \"sse_read_timeout\": 300\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Vectorizer.AI Tool Integration with Dify (Python)\nDESCRIPTION: This snippet demonstrates integrating the Vectorizer.AI tool for converting images to vector graphics within Dify. It retrieves an image from Dify's variable pool (assumes an image is already stored there using the `self.VARIABLE_KEY.IMAGE` key), converts it to an SVG, and returns the SVG as a blob message. It also implements dynamic parameter generation and tool availability checks based on the contents of the variable pool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/advanced-tool-integration.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom core.tools.tool.builtin_tool import BuiltinTool\nfrom core.tools.entities.tool_entities import ToolInvokeMessage, ToolParameter\nfrom core.tools.errors import ToolProviderCredentialValidationError\n\nfrom typing import Any, Dict, List, Union\nfrom httpx import post\nfrom base64 import b64decode\n\nclass VectorizerTool(BuiltinTool):\n    def _invoke(self, user_id: str, tool_Parameters: Dict[str, Any]) \\\n        -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:\n        \"\"\"\n            invoke tools\n        \"\"\"\n        api_key_name = self.runtime.credentials.get('api_key_name', None)\n        api_key_value = self.runtime.credentials.get('api_key_value', None)\n\n        if not api_key_name or not api_key_value:\n            raise ToolProviderCredentialValidationError('Please input api key name and value')\n\n        # 获取 image_id，image_id 的定义可以在 get_runtime_parameters 中找到\n        image_id = tool_Parameters.get('image_id', '')\n        if not image_id:\n            return self.create_text_message('Please input image id')\n\n        # 从变量池中获取到之前 DallE 生成的图片\n        image_binary = self.get_variable_file(self.VARIABLE_KEY.IMAGE)\n        if not image_binary:\n            return self.create_text_message('Image not found, please request user to generate image firstly.')\n\n        # 生成矢量图\n        response = post(\n            'https://vectorizer.ai/api/v1/vectorize',\n            files={ 'image': image_binary },\n            data={ 'mode': 'test' },\n            auth=(api_key_name, api_key_value), \n            timeout=30\n        )\n\n        if response.status_code != 200:\n            raise Exception(response.text)\n        \n        return [\n            self.create_text_message('the vectorized svg is saved as an image.'),\n            self.create_blob_message(blob=response.content,\n                                    meta={'mime_type': 'image/svg+xml'})\n        ]\n    \n    def get_runtime_parameters(self) -> List[ToolParameter]:\n        \"\"\"\n        override the runtime parameters\n        \"\"\"\n        # 这里，我们重写了工具参数列表，定义了 image_id，并设置了它的选项列表为当前变量池中的所有图片，这里的配置与 yaml 中的配置是一致的\n        return [\n            ToolParameter.get_simple_instance(\n                name='image_id',\n                llm_description=f'the image id that you want to vectorize, \\\n                    and the image id should be specified in \\\n                        {[i.name for i in self.list_default_image_variables()]}',\n                type=ToolParameter.ToolParameterType.SELECT,\n                required=True,\n                options=[i.name for i in self.list_default_image_variables()]\n            )\n        ]\n    \n    def is_tool_available(self) -> bool:\n        # 只有当变量池中有图片时，LLM 才需要使用这个工具\n        return len(self.list_default_image_variables()) > 0\n```\n\n----------------------------------------\n\nTITLE: Fetching Custom Model Schema in Python\nDESCRIPTION: This code snippet fetches the customizable model schema. It takes model name and credentials as input and returns the model schema. This method can be implemented to allow custom models to fetch model schema when the provider supports adding custom LLMs.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> Optional[AIModelEntity]:\n      \"\"\"\n      Get customizable model schema\n\n      :param model: model name\n      :param credentials: model credentials\n      :return: model schema\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Example of Customized Pre-Prompt for iPhone Support in Dify\nDESCRIPTION: This snippet provides an example of a customized pre-prompt for creating an iPhone support chatbot in Dify's expert mode. It includes instructions for the LLM to provide iPhone consultation services, list detailed parameters in a Markdown table, and handle cases where information is not available. It demonstrates how to insert this pre-prompt into the initial prompt template.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/prompt-engineering/prompt-engineering-1/README.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nWhen answer to user:\n- If you don't know, just say that you don't know.\n- If you don't know when you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n\n你是 Apple 公司的一位客服助手，你可以为用户提供 iPhone 的咨询服务。\n当你回答时需要列出 iPhone 详细参数，你必须一定要把这些信息输出为竖向 MARKDOWN 表格，若列表过多则进行转置。\n你被允许长时间思考从而生成更合理的输出。\n注意：你目前掌握的只是一部分 iPhone 型号，而不是全部。\n```\n\n----------------------------------------\n\nTITLE: Define Embedding Usage Model (Python)\nDESCRIPTION: Defines a Pydantic model `EmbeddingUsage` representing the usage information for a text embedding operation. It extends the `ModelUsage` model and includes attributes for token counts, total tokens, unit price, price unit, total price, currency, and latency.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nclass EmbeddingUsage(ModelUsage):\n    \"\"\"\n    Model class for embedding usage.\n    \"\"\"\n    tokens: int  # Number of tokens used\n    total_tokens: int  # Total number of tokens used\n    unit_price: Decimal  # Unit price\n    price_unit: Decimal  # Price unit, i.e., the unit price based on how many tokens\n    total_price: Decimal  # Total cost\n    currency: str  # Currency unit\n    latency: float  # Request latency (s)\n```\n\n----------------------------------------\n\nTITLE: System Prompt Message Class\nDESCRIPTION: Defines the `SystemPromptMessage` class, representing a system message. It sets the role to SYSTEM, typically used for setting the initial instructions for the model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nclass SystemPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for system prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.SYSTEM\n```\n\n----------------------------------------\n\nTITLE: Cloning and Installing Dify (Bash)\nDESCRIPTION: This set of commands clones the Dify GitHub repository, navigates to the Docker directory, copies the example environment file, and starts the Dify containers using Docker Compose. It sets up the Dify Community Edition, requiring Docker and Docker Compose to be pre-installed. This is the foundation for building AI applications within a private environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\ncd dify/docker\ncp .env.example .env\ndocker compose up -d  # Use `docker-compose up -d` if running Docker Compose V1\n```\n\n----------------------------------------\n\nTITLE: Image Prompt Message Content Definition\nDESCRIPTION: Defines a class `ImagePromptMessageContent` that inherits from `PromptMessageContent` and specifies the `type` as image. It also includes a `detail` field for specifying the image resolution (low or high).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nclass ImagePromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for image prompt message content.\n    \"\"\"\n    class DETAIL(Enum):\n        LOW = 'low'\n        HIGH = 'high'\n\n    type: PromptMessageContentType = PromptMessageContentType.IMAGE\n    detail: DETAIL = DETAIL.LOW  # Resolution\n```\n\n----------------------------------------\n\nTITLE: Validating Provider Credentials Python\nDESCRIPTION: This code defines a function to validate provider credentials. It inherits from a base class and raises an exception if validation fails, using credential information defined in the provider's YAML configuration file. The parameters of credential information are defined by the `provider_credential_schema` in the provider's YAML configuration file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Create Retrieval API Endpoint with Flask\nDESCRIPTION: This code snippet defines a Flask RESTful API endpoint for handling retrieval requests. It parses the request, performs authorization checks, and calls the knowledge retrieval service to fetch relevant information from the external knowledge base. It requires the Flask and Flask-RESTful libraries.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/how-to-connect-aws-bedrock.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flask import request\nfrom flask_restful import Resource, reqparse\n\nfrom bedrock.knowledge_service import ExternalDatasetService\n\n\nclass BedrockRetrievalApi(Resource):\n    # url : <your-endpoint>/retrieval\n    def post(self):\n        parser = reqparse.RequestParser()\n        parser.add_argument(\"retrieval_setting\", nullable=False, required=True, type=dict, location=\"json\")\n        parser.add_argument(\"query\", nullable=False, required=True, type=str,)\n        parser.add_argument(\"knowledge_id\", nullable=False, required=True, type=str)\n        args = parser.parse_args()\n\n        # Authorization check\n        auth_header = request.headers.get(\"Authorization\")\n        if \" \" not in auth_header:\n            return {\n                \"error_code\": 1001,\n                \"error_msg\": \"Invalid Authorization header format. Expected 'Bearer <api-key>' format.\"\n            }, 403\n        auth_scheme, auth_token = auth_header.split(None, 1)\n        auth_scheme = auth_scheme.lower()\n        if auth_scheme != \"bearer\":\n            return {\n                \"error_code\": 1001,\n                \"error_msg\": \"Invalid Authorization header format. Expected 'Bearer <api-key>' format.\"\n            }, 403\n        if auth_token:\n            # process your authorization logic here\n            pass\n\n        # Call the knowledge retrieval service\n        result = ExternalDatasetService.knowledge_retrieval(\n            args[\"retrieval_setting\"], args[\"query\"], args[\"knowledge_id\"]\n        )\n        return result, 200\n```\n\n----------------------------------------\n\nTITLE: Listing deployed models with Xinference CLI\nDESCRIPTION: This command lists the models that have been deployed through Xinference.  It provides information such as the model's UID, type, name, format, size, and quantization.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\n$ xinference list\n```\n\n----------------------------------------\n\nTITLE: Generating Random Secret Key\nDESCRIPTION: This command generates a random secret key using `openssl` and replaces the value of `SECRET_KEY` in the `.env` file. This enhances the security of the application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\nawk -v key=\"$(openssl rand -base64 42)\" '/^SECRET_KEY=/ {sub(/=.*/, \"=\" key)} 1' .env > temp_env && mv temp_env .env\n```\n\n----------------------------------------\n\nTITLE: Moderation Input Response - Direct Output JSON\nDESCRIPTION: Illustrates an example API response for the `app.moderation.input` extension point where the action is `direct_output`. This indicates that the content was flagged as inappropriate and a preset response should be directly returned to the user. The `preset_response` is a string explaining the policy violation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation-extension.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"direct_output\",\n    \"preset_response\": \"Your content violates our usage policy.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Generating a Secret Key using OpenSSL\nDESCRIPTION: This command generates a cryptographically secure random key, encoded in Base64, suitable for use as a SECRET_KEY. It utilizes OpenSSL to create 42 random bytes and encodes them. This variable is used to securely sign session cookies and encrypt sensitive information in the database.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/environments.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nopenssl rand -base64 42\n```\n\n----------------------------------------\n\nTITLE: Customizing Chatbot Position with iFrame\nDESCRIPTION: This snippet illustrates how to customize the chatbot's position by modifying the `style` attribute in the iFrame code. It demonstrates fixing the chatbot to the bottom right corner of the webpage using CSS properties like `position`, `bottom`, and `right`. These properties are added directly to the `style` attribute.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/use-cases/how-to-integrate-dify-chatbot-to-your-wix-website.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n<iframe src=\"https://udify.app/chatbot/ez1pf83HVV3JgWO4\" style=\"width: 100%; height: 100%; min-height: 700px; position: fixed; bottom: 20px; right: 20px;\" frameborder=\"0\" allow=\"microphone\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: Vector Database Migration via Flask Command\nDESCRIPTION: This command is used to migrate data from one vector database to another.  It uses the flask command-line interface to trigger the migration process, assuming the destination database is already configured.  The command requires the Flask application context to be initialized with the correct vector database settings in the .env file or docker-compose.yaml file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nflask vdb-migrate # or docker exec -it docker-api-1 flask vdb-migrate\n```\n\n----------------------------------------\n\nTITLE: Generating Secret Key (Bash)\nDESCRIPTION: Generates a random secret key and replaces the SECRET_KEY value in the .env file. This provides security for the application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\nawk -v key=\"$(openssl rand -base64 42)\" '/^SECRET_KEY=/ {sub(/=.*/, \"=\" key)} 1' .env > temp_env && mv temp_env .env\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Engine\nDESCRIPTION: This command installs the Docker Engine, Docker CLI, and Containerd.io. These are the core components required to run Docker containers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nsudo apt install docker-ce docker-ce-cli containerd.io\n```\n\n----------------------------------------\n\nTITLE: Convert User ID to Twitter URL with Python\nDESCRIPTION: This Python code snippet takes a Twitter user ID as input and constructs a complete Twitter profile URL using the Crawlbase URL format. It prepares the URL for use in subsequent HTTP requests to scrape the user's profile data.  The input parameter is 'id' (string), and the output is a dictionary containing the constructed 'url' (string).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/workshop/intermediate/twitter-chatflow.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef main(id: str) -> dict:\n    return {\n        \"url\": \"https%3A%2F%2Ftwitter.com%2F\"+id,\n    }\n```\n\n----------------------------------------\n\nTITLE: Getting Document Indexing Status using Dify API (curl)\nDESCRIPTION: This snippet retrieves the indexing status of a document batch in a Dify knowledge base. It requires the dataset ID, batch ID, and API key. The response provides details about the indexing progress, including completed segments and any errors.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{batch}/indexing-status' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Defining a Numeric Array\nDESCRIPTION: This code snippet demonstrates how to define a numeric array variable. The array contains a list of numbers enclosed in square brackets, with each number separated by a comma. This type of array can be used for storing and processing numerical data.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/what-is-array-variable.md#_snippet_0\n\nLANGUAGE: N/A\nCODE:\n```\n[0,1,2,3,4,5]\n```\n\n----------------------------------------\n\nTITLE: Copying Environment Configuration File\nDESCRIPTION: This command copies the example environment configuration file (`.env.example`) to a new file named `.env`. This allows for customization of environment variables for the Dify application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Xinference Provider YAML Configuration\nDESCRIPTION: This YAML code defines the Xinference provider, specifying its ID, display name, icons, help links, supported model types (LLM, text embedding, rerank), configuration methods (customizable-model), and credential schema. It sets the foundation for integrating Xinference models into the Dify platform.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/customizable-model.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprovider: xinference #确定供应商标识\nlabel: # 供应商展示名称，可设置 en_US 英文、zh_Hans 中文两种语言，zh_Hans 不设置将默认使用 en_US。\n  en_US: Xorbits Inference\nicon_small: # 小图标，可以参考其他供应商的图标，存储在对应供应商实现目录下的 _assets 目录，中英文策略同 label\n  en_US: icon_s_en.svg\nicon_large: # 大图标\n  en_US: icon_l_en.svg\nhelp: # 帮助\n  title:\n    en_US: How to deploy Xinference\n    zh_Hans: 如何部署 Xinference\n  url:\n    en_US: https://github.com/xorbitsai/inference\nsupported_model_types: # 支持的模型类型，Xinference同时支持LLM/Text Embedding/Rerank\n- llm\n- text-embedding\n- rerank\nconfigurate_methods: # 因为Xinference为本地部署的供应商，并且没有预定义模型，需要用什么模型需要根据Xinference的文档自己部署，所以这里只支持自定义模型\n- customizable-model\nprovider_credential_schema:\n  credential_form_schemas:\n```\n\n----------------------------------------\n\nTITLE: Implementing Google Provider Class with Credential Validation in Python\nDESCRIPTION: This code snippet defines a GoogleProvider class that extends BuiltinToolProviderController. It implements the _validate_credentials method to verify the provided credentials by instantiating a GoogleSearchTool, forking the tool runtime with the credentials, and invoking it with test parameters. If validation fails, a ToolProviderCredentialValidationError exception is raised.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/quick-tool-integration.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom core.tools.entities.tool_entities import ToolInvokeMessage, ToolProviderType\nfrom core.tools.tool.tool import Tool\nfrom core.tools.provider.builtin_tool_provider import BuiltinToolProviderController\nfrom core.tools.errors import ToolProviderCredentialValidationError\n\nfrom core.tools.provider.builtin.google.tools.google_search import GoogleSearchTool\n\nfrom typing import Any, Dict\n\nclass GoogleProvider(BuiltinToolProviderController):\n    def _validate_credentials(self, credentials: Dict[str, Any]) -> None:\n        try:\n            # 1. この場所でGoogleSearchTool()をインスタンス化し、GoogleSearchToolのyaml設定を自動的に読み込みますが、この時点では認証情報が内部にありません。\n            # 2. その後、fork_tool_runtimeメソッドを使用して、現在の認証情報をGoogleSearchToolに渡します。\n            # 3. 最後にinvokeします。パラメーターはGoogleSearchToolのyamlに記載されたパラメーター規則に従って渡します。\n            GoogleSearchTool().fork_tool_runtime(\n                meta={\n                    \"credentials\": credentials,\n                }\n            ).invoke(\n                user_id='',\n                tool_parameters={\n                    \"query\": \"test\",\n                    \"result_type\": \"link\"\n                },\n            )\n        except Exception as e:\n            raise ToolProviderCredentialValidationError(str(e))\n```\n\n----------------------------------------\n\nTITLE: CloudServiceModeration Class Template\nDESCRIPTION: Provides a Python code template for the `CloudServiceModeration` class.  It highlights the methods to implement for custom content moderation, including `validate_config`, `moderation_for_inputs`, and `moderation_for_outputs`.  The template provides comments indicating where to implement custom logic.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/code-based-extension/moderation.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom core.moderation.base import Moderation, ModerationAction, ModerationInputsResult, ModerationOutputsResult\n\nclass CloudServiceModeration(Moderation):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"cloud_service\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user save the config.\n        \n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n        cls._validate_inputs_and_outputs_config(config, True)\n        \n        # implement your own logic here\n\n    def moderation_for_inputs(self, inputs: dict, query: str = \"\") -> ModerationInputsResult:\n        \"\"\"\n        Moderation for inputs.\n\n        :param inputs: user inputs\n        :param query: the query of chat app, there is empty if is completion app\n        :return: the moderation result\n        \"\"\"\n        flagged = False\n        preset_response = \"\"\n        \n        # implement your own logic here\n        \n        # return ModerationInputsResult(flagged=flagged, action=ModerationAction.overridden, inputs=inputs, query=query)\n        return ModerationInputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)\n\n    def moderation_for_outputs(self, text: str) -> ModerationOutputsResult:\n        \"\"\"\n        Moderation for outputs.\n\n        :param text: the text of LLM response\n        :return: the moderation result\n        \"\"\"\n        flagged = False\n        preset_response = \"\"\n        \n        # implement your own logic here\n\n        # return ModerationOutputsResult(flagged=flagged, action=ModerationAction.overridden, text=text)\n        return ModerationOutputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)\n```\n\n----------------------------------------\n\nTITLE: Listing Documents in Knowledge Base using Dify API (curl)\nDESCRIPTION: This snippet shows how to list all documents within a given knowledge base in Dify. It requires the dataset ID and API key.  The response contains a paginated list of document objects with their associated metadata.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Embedding Usage Class\nDESCRIPTION: Defines the `EmbeddingUsage` class, extending `ModelUsage`, representing the usage information for an embedding operation. It includes token counts, prices, and latency.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nclass EmbeddingUsage(ModelUsage):\n    \"\"\"\n    Model class for embedding usage.\n    \"\"\"\n    tokens: int  # 使用 token 数\n    total_tokens: int  # 总使用 token 数\n    unit_price: Decimal  # 单价\n    price_unit: Decimal  # 价格单位，即单价基于多少 tokens\n    total_price: Decimal  # 总费用\n    currency: str  # 货币单位\n    latency: float  # 请求耗时(s)\n```\n\n----------------------------------------\n\nTITLE: Starting LocalAI with Docker Compose (Shell)\nDESCRIPTION: This code snippet provides a shell command to start LocalAI using Docker Compose. The command builds the Docker images and starts the LocalAI containers in detached mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n# start with docker-compose\n$ docker-compose up -d --build\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Example with User Data\nDESCRIPTION: This JSON schema example defines the structure for user data, including username, age, and interests.  It specifies the data type for each field, requiring username and age. Used for LLM output structuring.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/llm.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"username\": {\n      \"type\": \"string\"\n    },\n    \"age\": {\n      \"type\": \"number\"\n    },\n    \"interests\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      }\n    }\n  },\n  \"required\": [\n    \"username\",\n    \"age\",\n    \"interests\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Plugin with Python\nDESCRIPTION: This command is used to start the Dify plugin within its directory using the Python interpreter.  It executes the 'main.py' file as a module, initiating the plugin's execution and enabling it to connect to Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\npython -m main\n```\n\n----------------------------------------\n\nTITLE: Running Llava Model with Ollama\nDESCRIPTION: This command launches the Llava model using Ollama. It assumes Ollama is installed and running. After successful launch, Ollama starts an API service on local port 11434, which can be accessed at `http://localhost:11434`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_80\n\nLANGUAGE: Shell\nCODE:\n```\nollama run llava\n```\n\n----------------------------------------\n\nTITLE: Pre-prompt example for iPhone consultation - Markdown\nDESCRIPTION: This is an example of a pre-prompt that can be inserted into the built-in prompt to guide the LLM to act as a customer service assistant for Apple Inc., providing consultation services for iPhones. The LLM should list detailed iPhone parameters in a Markdown table.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/prompt-engineering/prompt-engineering-1/README.md#_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nWhen answering the user:\n- If you don't know, just say that you don't know.\n- If you don't know or are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n\nYou are a customer service assistant for Apple Inc., and you can provide consultation services for iPhones.\nWhen you answer, you need to list detailed iPhone parameters, and you must output this information as a vertical MARKDOWN table. If the list is too long, transpose it.\nYou are allowed to think for a long time to generate a more reasonable output.\nNote: You currently only have information on some iPhone models, not all of them.\n```\n\n----------------------------------------\n\nTITLE: Downloading LLM and Embedding Models (Bash)\nDESCRIPTION: This snippet downloads two pre-trained models: `all-MiniLM-L6-v2` for embeddings and `ggml-gpt4all-j` for LLM. These models are downloaded from Hugging Face and gpt4all.io and saved to the 'models' directory. These are used as the default LLM and embedding models for quick local deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/localai.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O models/bert\n$ wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for Text Embedding in Python\nDESCRIPTION: This function retrieves the number of tokens for embedding model, and allows vendors to monitor and detect fraud behaviour.  The function returns number of tokens.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, texts: list[str]) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param texts: texts to embed\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define Model Type Credential Schema (YAML)\nDESCRIPTION: This YAML snippet defines a credential schema for specifying the model type, including options for text generation, embeddings, and reranking. The `model_type` variable uses a `select` type, allowing the user to choose from predefined options, providing crucial information for accessing Xinference models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/customizable-model.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprovider_credential_schema:\n  credential_form_schemas:\n  - variable: model_type\n    type: select\n    label:\n      en_US: Model type\n      zh_Hans: 模型类型\n    required: true\n    options:\n    - value: text-generation\n      label:\n        en_US: Language Model\n        zh_Hans: 语言模型\n    - value: embeddings\n      label:\n        en_US: Text Embedding\n    - value: reranking\n      label:\n        en_US: Rerank\n```\n\n----------------------------------------\n\nTITLE: Ping Check Request Body (JSON)\nDESCRIPTION: Shows the request body for the 'ping' endpoint, used for API availability checks. When Dify sends this request, the API should respond with `result=pong`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/README.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"point\": \"ping\"\n}\n```\n\n----------------------------------------\n\nTITLE: Package Dify Plugin (Bash)\nDESCRIPTION: プラグインプロジェクトをパッケージ化します。`manifest.yaml` ファイルの `author` フィールドが GitHub ID と一致していることを確認してください。パッケージ化の前にリモート接続テストを完了させてください。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/package-plugin-file-and-publish.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin package ./your_plugin_project\n```\n\n----------------------------------------\n\nTITLE: Switching to Python 3.12 (Bash)\nDESCRIPTION: Switches the global Python environment to version 3.12 using pyenv. This ensures that the correct Python version is used for Dify's API service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\npyenv global 3.12\n```\n\n----------------------------------------\n\nTITLE: Customizing the Pre-prompt for English Responses\nDESCRIPTION: This snippet illustrates how to modify the initial prompt template to force LLM responses to be in English.  By adjusting the 'answer according to the language' instruction, the system ensures consistent output language regardless of the user's input language. It also covers handling unknown information and avoiding mentioning context source.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/prompt-engineering/prompt-engineering-expert-mode.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nWhen answer to user:\n- If you don't know, just say that you don't know.\n- If you don't know when you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language English.\n```\n\n----------------------------------------\n\nTITLE: Example Request Syntax\nDESCRIPTION: This is an example of the request syntax required to connect to an external knowledge base with the Dify Platform. This includes the header and data in JSON format.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_25\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST <your-endpoint>/retrieval HTTP/1.1\n-- header\nContent-Type: application/json\nAuthorization: Bearer your-api-key\n-- data\n{\n    \"knowledge_id\": \"your-knowledge-id\",\n    \"query\": \"your question\",\n    \"retrieval_setting\":{\n        \"top_k\": 2,\n        \"score_threshold\": 0.5\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Schema - Complete Structure\nDESCRIPTION: This comprehensive JSON schema structure example includes multiple properties with specific types, constraints, and descriptions, suitable for defining complex data structures for structured output from LLMs. It covers strings, numbers, arrays, object and requirements.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/llm.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"comment\": {\n      \"type\": \"string\"\n    },\n    \"rating\": {\n      \"type\": \"number\"\n    }\n  },\n  \"required\": [\n    \"comment\",\n    \"rating\"\n  ],\n  \"additionalProperties\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Install FFmpeg on Ubuntu\nDESCRIPTION: Installs FFmpeg on an Ubuntu system. FFmpeg is required for OpenAI TTS to work properly due to audio stream segmentation.  These commands update the package list and then install the ffmpeg package.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install ffmpeg\n```\n\n----------------------------------------\n\nTITLE: Simplified Provider Credential Validation (Python)\nDESCRIPTION: This code snippet shows a simplified implementation of provider credential validation for custom model suppliers. It inherits from the `Provider` class and provides an empty implementation for the `validate_provider_credentials` method.  This is suitable for cases where detailed validation is not required.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass XinferenceProvider(Provider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Copy Public Key to Directory (Bash)\nDESCRIPTION: This command copies the public key file to the directory created in the previous step. This ensures that the `plugin_daemon` container can access the public key for signature verification during plugin installation. The location `/docker/volumes/plugin_daemon/public_keys` is mounted to `/app/storage` inside the container.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncp your_key_pair.public.pem docker/volumes/plugin_daemon/public_keys\n```\n\n----------------------------------------\n\nTITLE: Start LocalAI with Docker Compose\nDESCRIPTION: Uses docker-compose to build and start the LocalAI service in detached mode. It then tails the logs to monitor the build process and ensure the service starts correctly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/localai.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# docker-composeを使用して起動\n$ docker-compose up -d --build\n\n# ログを追跡し、ビルドが完了するまで待つ\n$ docker logs -f langchain-chroma-api-1\n7:16AM INF Starting LocalAI using 4 threads, with models path: /models\n7:16AM INF LocalAI version: v1.24.1 (9cc8d9086580bd2a96f5c96a6b873242879c70bc)\n\n ┌───────────────────────────────────────────────────┐ \n │                   Fiber v2.48.0                   │ \n │               http://127.0.0.1:8080               │ \n │       (bound on host 0.0.0.0 and port 8080)       │ \n │                                                   │ \n │ Handlers ............ 55  Processes ........... 1 │ \n │ Prefork ....... Disabled  PID ................ 14 │ \n └───────────────────────────────────────────────────┘\n```\n\n----------------------------------------\n\nTITLE: Define Plugin Endpoint (YAML)\nDESCRIPTION: This YAML configuration defines a plugin endpoint at the path `/neko` with a GET method. It specifies that the endpoint's logic is implemented in the `endpoints/test_plugin.py` file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npath: \"/neko\"\nmethod: \"GET\"\nextra:\n  python:\n    source: \"endpoints/test_plugin.py\"\n```\n\n----------------------------------------\n\nTITLE: Assistant Prompt Message Class in Python\nDESCRIPTION: This code defines the Assistant Prompt Message class, inheriting from PromptMessage. It represents a message returned by the model and can include tool calls. It specifies the role as ASSISTANT and includes a list of ToolCall objects.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass AssistantPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for assistant prompt message.\n    \"\"\"\n    class ToolCall(BaseModel):\n        \"\"\"\n        Model class for assistant prompt message tool call.\n        \"\"\"\n        class ToolCallFunction(BaseModel):\n            \"\"\"\n            Model class for assistant prompt message tool call function.\n            \"\"\"\n            name: str  # tool name\n            arguments: str  # tool arguments\n\n        id: str  # Tool ID, effective only in OpenAI tool calls. It's the unique ID for tool invocation and the same tool can be called multiple times.\n        type: str  # default: function\n        function: ToolCallFunction  # tool call information\n\n    role: PromptMessageRole = PromptMessageRole.ASSISTANT\n    tool_calls: list[ToolCall] = []  # The result of tool invocation in response from the model (returned only when tools are input and the model deems it necessary to invoke a tool).\n```\n\n----------------------------------------\n\nTITLE: Edit Ollama Systemd Service (Linux) (INI)\nDESCRIPTION: This configuration snippet shows how to set the OLLAMA_HOST environment variable in a Linux systemd service file. This allows Docker containers to connect to the Ollama service when it is running as a systemd service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_4\n\nLANGUAGE: ini\nCODE:\n```\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Adding a GitHub plugin dependency to the Bundle\nDESCRIPTION: This command appends a GitHub plugin dependency to the Bundle project. The `--repo_pattern` parameter specifies the plugin's location on GitHub, using the format `organization_name/repository_name:release_tag/asset_filename`.  This command assumes that `dify-plugin` is in the current working directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/bundle.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndify-plugin bundle append github . --repo_pattern=langgenius/openai:0.0.1/openai.difypkg\n```\n\n----------------------------------------\n\nTITLE: LLM Prompt for Question Generation in Dify (Japanese)\nDESCRIPTION: This prompt is designed for the question generation LLM node in Dify. It instructs the LLM to read the summarized content from the structure extraction node and generate meaningful questions for each section of the article. The prompt emphasizes the importance of asking questions that encourage deep thinking and are not superficial.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/workshop/intermediate/article-reader.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n以下の記事を読み、タスクを実行してください\n{{構造抽出の出力}}\n\n# タスク\n\n- **主要目標**：上記の内容を包括的に読み、各部分に可能な限り多くの問題を提起すること。\n- **要求**：意義深く、価値のある問題を提起すること。\n- **制限**：特定の制限はありません。\n- **期待される出力**：各部分に関連する一連の問題、それぞれが深い考察と価値を持つ必要があります。\n\n# 推論手順\n\n- **推論部分**：記事全体を網羅的に読み、各部分の内容を分析し、各部分が引き起こす潜在的な問題を考えます。\n- **結論部分**：意義深く、価値のある問題を提起し、深い考察を促します。\n\n# 出力形式\n\n- **形式**：それぞれの問題を個別の行にし、番号でリストアップします。\n- **内容**：導入、背景、方法、結果、議論、結論など、記事の各部分に関連する問題を提起します。\n- **数量**：可能な限り多く、ただし各問題は意義深く、価値のあるものである必要があります。\n\n# 備考\n\n- **境界条件**：記事の特定の部分の内容が少ない場合、問題の数量や深さを適宜調整することができますが、各問題は考察に値するものである必要があります。\n- **重要な考慮事項**：問題が読者を記事の内容を深く理解し、表面的な疑問にとどまらず、深い考察を促すことを確実にしてください。\n```\n\n----------------------------------------\n\nTITLE: Identify Docker API Container ID\nDESCRIPTION: This command is used to find the container ID of the docker-api container.  It provides necessary information for accessing the container to extract and install plugins.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/dify-premium.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker ps\n```\n\n----------------------------------------\n\nTITLE: Defining Front-End Component Schema for Weather Search JSON\nDESCRIPTION: This JSON snippet defines the schema for the front-end component of the 'Weather Search' external data tool. It specifies a 'select' input field for choosing the temperature unit (Fahrenheit or Centigrade). The schema includes labels for both English (en-US) and Chinese (zh-Hans), options for the select field, a default value, and a placeholder. The 'variable' property defines the name of the variable to be used in the configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/code-based-extension/external-data-tool.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"label\": {\n        \"en-US\": \"Weather Search\",\n        \"zh-Hans\": \"天气查询\"\n    },\n    \"form_schema\": [\n        {\n            \"type\": \"select\",\n            \"label\": {\n                \"en-US\": \"Temperature Unit\",\n                \"zh-Hans\": \"温度单位\"\n            },\n            \"variable\": \"temperature_unit\",\n            \"required\": true,\n            \"options\": [\n                {\n                    \"label\": {\n                        \"en-US\": \"Fahrenheit\",\n                        \"zh-Hans\": \"华氏度\"\n                    },\n                    \"value\": \"fahrenheit\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Centigrade\",\n                        \"zh-Hans\": \"摄氏度\"\n                    },\n                    \"value\": \"centigrade\"\n                }\n            ],\n            \"default\": \"centigrade\",\n            \"placeholder\": \"Please select temperature unit\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Tool Provider YAML Configuration\nDESCRIPTION: This YAML code defines the tool provider configuration, including basic information such as author, name, labels, description, icon, and tags.  It also specifies the credentials required for the SerpApi API and the path to the tool's YAML file and Python code file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: Dify\n  name: google\n  label:\n    en_US: Google\n    zh_Hans: Google\n    ja_JP: Google\n    pt_BR: Google\n  description:\n    en_US: Google\n    zh_Hans: GoogleSearch\n    ja_JP: Google\n    pt_BR: Google\n  icon: icon.svg\n  tags:\n    - search\ncredentials_for_provider: # credentials_for_provider フィールドを追加\n  serpapi_api_key:\n    type: secret-input\n    required: true\n    label:\n      en_US: SerpApi API key\n      zh_Hans: SerpApi API key\n      ja_JP: SerpApi API key\n      pt_BR: chave de API SerpApi\n    placeholder:\n      en_US: Please input your SerpApi API key\n      zh_Hans: 请输入你的 SerpApi API key\n      ja_JP: SerpApi API keyを入力してください\n      pt_BR: Por favor, insira sua chave de API SerpApi\n    help:\n      en_US: Get your SerpApi API key from SerpApi\n      zh_Hans: 从 SerpApi 获取您的 SerpApi API key\n      ja_JP: SerpApiからSerpApi APIキーを取得する\n      pt_BR: Obtenha sua chave de API SerpApi da SerpApi\n    url: https://serpapi.com/manage-api-key\ntools:\n  - tools/google_search.yaml\nextra:\n  python:\n    source: google.py\n```\n\n----------------------------------------\n\nTITLE: Retrieve Knowledge from AWS Bedrock\nDESCRIPTION: This code snippet uses the boto3 library to interact with AWS Bedrock and retrieve knowledge. It configures the Bedrock client with AWS credentials and region, then it uses the `retrieve` method to query the knowledge base using a provided query and retrieval settings. It returns retrieved records with metadata, score, title and content. It requires boto3 library.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/how-to-connect-aws-bedrock.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\n\n\nclass ExternalDatasetService:\n    @staticmethod\n    def knowledge_retrieval(retrieval_setting: dict, query: str, knowledge_id: str):\n        # get bedrock client\n        client = boto3.client(\n            \"bedrock-agent-runtime\",\n            aws_secret_access_key=\"AWS_SECRET_ACCESS_KEY\",\n            aws_access_key_id=\"AWS_ACCESS_KEY_ID\",\n            # example: us-east-1\n            region_name=\"AWS_REGION_NAME\",\n        )\n        # fetch external knowledge retrieval\n        response = client.retrieve(\n            knowledgeBaseId=knowledge_id,\n            retrievalConfiguration={\n                \"vectorSearchConfiguration\": {\"numberOfResults\": retrieval_setting.get(\"top_k\"), \"overrideSearchType\": \"HYBRID\"}\n            },\n            retrievalQuery={\"text\": query},\n        )\n        # parse response\n        results = []\n        if response.get(\"ResponseMetadata\") and response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") == 200:\n            if response.get(\"retrievalResults\"):\n                retrieval_results = response.get(\"retrievalResults\")\n                for retrieval_result in retrieval_results:\n                    # filter out results with score less than threshold\n                    if retrieval_result.get(\"score\") < retrieval_setting.get(\"score_threshold\", .0):\n                        continue\n                    result = {\n                        \"metadata\": retrieval_result.get(\"metadata\"),\n                        \"score\": retrieval_result.get(\"score\"),\n                        \"title\": retrieval_result.get(\"metadata\").get(\"x-amz-bedrock-kb-source-uri\"),\n                        \"content\": retrieval_result.get(\"content\").get(\"text\"),\n                    }\n                    results.append(result)\n        return {\n            \"records\": results\n        }\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Admin Initialization Page (Server)\nDESCRIPTION: This command shows the URL for accessing the admin initialization page when Dify is deployed in a server environment. Replace `your_server_ip` with the actual server IP address.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# 服务器环境\nhttp://your_server_ip/install\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Content Base Class\nDESCRIPTION: Defines the `PromptMessageContent` base class, representing the content of a prompt message. It includes the `type` of content (e.g., text, image) and the `data` itself. This is a base class and should not be initialized directly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContent(BaseModel):\n    \"\"\"\n    Model class for prompt message content.\n    \"\"\"\n    type: PromptMessageContentType\n    data: str  # 内容数据\n```\n\n----------------------------------------\n\nTITLE: Generate Secret Key\nDESCRIPTION: Generates a random secret key and replaces the SECRET_KEY value in the .env file. This is important for security. The command uses openssl to generate a base64 encoded random string.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_40\n\nLANGUAGE: Bash\nCODE:\n```\nawk -v key=\"$(openssl rand -base64 42)\" '/^SECRET_KEY=/ {sub(/=.*/, \"=\" key)} 1' .env > temp_env && mv temp_env .env\n```\n\n----------------------------------------\n\nTITLE: Example Plugin Manifest YAML\nDESCRIPTION: This YAML snippet provides an example of a complete plugin manifest file. It defines various attributes such as version, type, author, labels, creation timestamp, icon, and resource permissions.  It also specifies plugin endpoints, meta information, architecture, and runtime configuration including language and entrypoint.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/manifest.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 0.0.1\ntype: \"plugin\"\nauthor: \"Yeuoly\"\nname: \"neko\"\nlabel:\n  en_US: \"Neko\"\ncreated_at: \"2024-07-12T08:03:44.658609186Z\"\nicon: \"icon.svg\"\nresource:\n  memory: 1048576\n  permission:\n    tool:\n      enabled: true\n    model:\n      enabled: true\n      llm: true\n    endpoint:\n      enabled: true\n    app:\n      enabled: true\n    storage: \n      enabled: true\n      size: 1048576\nplugins:\n  endpoints:\n    - \"provider/neko.yaml\"\nmeta:\n  version: 0.0.1\n  arch:\n    - \"amd64\"\n    - \"arm64\"\n  runner:\n    language: \"python\"\n    version: \"3.11\"\n    entrypoint: \"main\"\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Provider in agent.yaml (YAML)\nDESCRIPTION: This snippet defines the basic Agent provider information in the `agent.yaml` file. It includes descriptive information such as author, name, labels, description, and icon.  It also specifies which strategy files are included in the current provider. The `strategies` array points to YAML files defining specific Agent strategies.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/agent.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nidentity:\n  author: langgenius\n  name: agent\n  label:\n    en_US: Agent\n    zh_Hans: Agent\n    pt_BR: Agent\n  description:\n    en_US: Agent\n    zh_Hans: Agent\n    pt_BR: Agent\n  icon: icon.svg\nstrategies:\n  - strategies/function_calling.yaml\n```\n\n----------------------------------------\n\nTITLE: Starting Dify with Docker Compose\nDESCRIPTION: These commands navigate to the Docker directory within the Dify repository, copies the example environment file, and starts the Dify application using Docker Compose in detached mode. Ensure Docker and Docker Compose are installed. It requires being in the correct directory and having a valid .env file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_17\n\nLANGUAGE: Shell\nCODE:\n```\ncd dify/docker\ncp .env.example .env\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Starting Sandbox Service with Docker Compose\nDESCRIPTION: This command starts the sandbox service using Docker Compose, which is required for local deployments to ensure secure execution of code. It uses the `docker-compose.middleware.yaml` file to define the services and their configurations. Requires Docker to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_68\n\nLANGUAGE: docker\nCODE:\n```\ndocker-compose -f docker-compose.middleware.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Loop Termination Condition Example\nDESCRIPTION: Demonstrates examples of loop termination conditions that can be used to control the execution of a loop.  The termination condition is evaluated at the start of each iteration, and the loop continues as long as the condition is false.  Common examples include comparing a variable to a threshold value.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/loop.md#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nx < 50\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nerror_rate < 0.01\n```\n\n----------------------------------------\n\nTITLE: Configure .env File\nDESCRIPTION: This snippet renames the .env.example file to .env, enabling environment variable configuration for LocalAI.  It's crucial to adjust the THREADS variable in the .env file to a value not exceeding the CPU core count of the machine.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/localai.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ mv .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Dify Backend Directory Structure\nDESCRIPTION: This code snippet outlines the directory structure of the Dify backend, written in Python using the Flask framework. It details key components like controllers (API routes), core (application orchestration), models (database schemas), services (business logic), and tasks (async jobs).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/community/contribution.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n[api/]\n├── constants             // Constant settings used throughout code base.\n├── controllers           // API route definitions and request handling logic.           \n├── core                  // Core application orchestration, model integrations, and tools.\n├── docker                // Docker & containerization related configurations.\n├── events                // Event handling and processing\n├── extensions            // Extensions with 3rd party frameworks/platforms.\n├── fields                //field definitions for serialization/marshalling.\n├── libs                  // Reusable libraries and helpers.\n├── migrations            // Scripts for database migration.\n├── models                // Database models & schema definitions.\n├── services              // Specifies business logic.\n├── storage               // Private key storage.      \n├── tasks                 // Handling of async tasks and background jobs.\n└── tests\n```\n\n----------------------------------------\n\nTITLE: Configuring wrangler.toml\nDESCRIPTION: This code block shows the configuration settings required in the `wrangler.toml` file, including the application name, compatibility date, and the token used for API extension authentication with Dify. The TOKEN value should be a randomly generated string for security.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\nname = \"dify-extension-example\"\ncompatibility_date = \"2023-01-01\"\n\n[vars]\nTOKEN = \"bananaiscool\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Admin Initialization Page (Local)\nDESCRIPTION: This command shows the URL for accessing the admin initialization page when Dify is deployed in a local environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# 本地环境\nhttp://localhost/install\n```\n\n----------------------------------------\n\nTITLE: Install System Dependencies (Ubuntu/Debian)\nDESCRIPTION: This command updates the package list and installs necessary system components such as pkg-config, gcc, libseccomp-dev, git, and wget on Ubuntu/Debian based systems. These dependencies are required for compiling and running the DifySandbox.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/backend/sandbox/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get install pkg-config gcc libseccomp-dev git wget\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for claude-3-5-sonnet-20240620 Model\nDESCRIPTION: This YAML code snippet demonstrates the configuration for the claude-3-5-sonnet-20240620 model within a Dify plugin. It defines various properties such as model name, label, model type, features, model properties, parameter rules, and pricing details.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodel: claude-3-5-sonnet-20240620\nlabel:\n  en_US: claude-3-5-sonnet-20240620\nmodel_type: llm\nfeatures:\n  - agent-thought\n  - vision\n  - tool-call\n  - stream-tool-call\n  - document\nmodel_properties:\n  mode: chat\n  context_size: 200000\nparameter_rules:\n  - name: temperature\n    use_template: temperature\n  - name: top_p\n    use_template: top_p\n  - name: top_k\n    label:\n      zh_Hans: 取样数量\n      en_US: Top k\n    type: int\n    help:\n      zh_Hans: 仅从每个后续标记的前 K 个选项中采样。\n      en_US: Only sample from the top K options for each subsequent token.\n    required: false\n  - name: max_tokens\n    use_template: max_tokens\n    required: true\n    default: 8192\n    min: 1\n    max: 8192\n  - name: response_format\n    use_template: response_format\npricing:\n  input: '3.00'\n  output: '15.00'\n  unit: '0.000001'\n  currency: USD\n```\n\n----------------------------------------\n\nTITLE: Wikipedia Tool Provider YAML (No Credentials)\nDESCRIPTION: This YAML shows a tool provider configuration that does not require any credentials. It is an example how credentials are not mandatory for all tool providers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/quick-tool-integration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: Dify\n  name: wikipedia\n  label:\n    en_US: Wikipedia\n    zh_Hans: 维基百科\n    ja_JP: Wikipedia\n    pt_BR: Wikipedia\n  description:\n    en_US: Wikipedia is a free online encyclopedia, created and edited by volunteers around the world.\n    zh_Hans: 维基百科是一个由全世界的志愿者创建和编辑的免费在线百科全书。\n    ja_JP: Wikipediaは、世界中のボランティアによって作成、編集されている無料のオンライン百科事典です。\n    pt_BR: A Wikipédia é uma enciclopédia online gratuita, criada e editada por voluntários ao redor do mundo.\n  icon: icon.svg\ncredentials_for_provider:\n```\n\n----------------------------------------\n\nTITLE: Convert Array to Text using Template Node (Django)\nDESCRIPTION: This code snippet shows how to convert an array of strings to a single string using a Template Node with the Django template language. It uses the `join` filter to concatenate the elements of the `articleSections` array with newline characters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/iteration.md#_snippet_1\n\nLANGUAGE: django\nCODE:\n```\n{{ articleSections | join(\"\\n\") }}\n```\n\n----------------------------------------\n\nTITLE: Starting Web Service\nDESCRIPTION: This command starts the Dify web service using the start script defined in `package.json`.  It uses either npm, yarn or pnpm as the package manager.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_15\n\nLANGUAGE: Bash\nCODE:\n```\nnpm run start\n# または\nyarn start\n# または\npnpm start\n```\n\n----------------------------------------\n\nTITLE: Mapping Invoke Errors in Python\nDESCRIPTION: This Python snippet demonstrates how to map model invocation errors to unified error types, making it easier for Dify to handle different errors consistently. The `_invoke_error_mapping` property returns a dictionary that maps specific exception types to generic `InvokeError` types.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Manifest YAML Example\nDESCRIPTION: This code snippet shows an example of the `manifest.yaml` file that is required for the plugin automation process. It includes the `version`, `author`, and `name` fields.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/plugin-auto-publish-pr.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 0.0.x  # Version number\nauthor: your-github-username  # GitHub username/Author name\nname: your-plugin-name  # Plugin name\n```\n\n----------------------------------------\n\nTITLE: Accessing Docker Host from Container\nDESCRIPTION: This configuration allows accessing the Docker host from within a container. It involves replacing `localhost` with `host.docker.internal` in the Ollama service URL. This is useful when Dify is deployed in Docker and needs to communicate with Ollama running on the host machine.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_82\n\nLANGUAGE: HTTP\nCODE:\n```\nhttp://host.docker.internal:11434\n```\n\n----------------------------------------\n\nTITLE: JSON Schema for UI Generation\nDESCRIPTION: This snippet presents a JSON Schema for dynamically generating a UI. The schema is recursive, allowing for nested UI components with types like div, button, header, etc.  It includes attributes for customizing the UI components.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/how-to-use-json-schema-in-dify.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n        \"name\": \"ui\",\n        \"description\": \"Dynamically generated UI\",\n        \"strict\": true,\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"type\": {\n                    \"type\": \"string\",\n                    \"description\": \"The type of the UI component\",\n                    \"enum\": [\"div\", \"button\", \"header\", \"section\", \"field\", \"form\"]\n                },\n                \"label\": {\n                    \"type\": \"string\",\n                    \"description\": \"The label of the UI component, used for buttons or form fields\"\n                },\n                \"children\": {\n                    \"type\": \"array\",\n                    \"description\": \"Nested UI components\",\n                    \"items\": {\n                        \"$ref\": \"#\"\n                    }\n                },\n                \"attributes\": {\n                    \"type\": \"array\",\n                    \"description\": \"Arbitrary attributes for the UI component, suitable for any element\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\n                                \"type\": \"string\",\n                                \"description\": \"The name of the attribute, for example onClick or className\"\n                            },\n                            \"value\": {\n                                \"type\": \"string\",\n                                \"description\": \"The value of the attribute\"\n                            }\n                        },\n                      \"additionalProperties\": false,\n                      \"required\": [\"name\", \"value\"]\n                    }\n                }\n            },\n            \"required\": [\"type\", \"label\", \"children\", \"attributes\"],\n            \"additionalProperties\": false\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Stopping Nginx Service\nDESCRIPTION: This command is used to stop the Nginx web server.  It uses `service` to manage the Nginx service. It requires root privileges or being a member of the appropriate group to execute.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nservice nginx stop\n```\n\n----------------------------------------\n\nTITLE: User Prompt Message Class in Python\nDESCRIPTION: This code defines the User Prompt Message class, inheriting from PromptMessage. It specifies the role as USER.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass UserPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for user prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.USER\n```\n\n----------------------------------------\n\nTITLE: SearXNG Settings Configuration\nDESCRIPTION: This YAML configuration sets the SearXNG server to listen on all IP addresses (0.0.0.0) and enables JSON output format. This is required to allow external access and ensure Dify can process the search results.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/searxng.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  bind_address: \"0.0.0.0\"  # 外部アクセスを許可\n  port: 8080\n\nsearch:\n  formats:\n    - html\n    - json\n    - csv\n    - rss\n```\n\n----------------------------------------\n\nTITLE: Extracting JSON Data (Python)\nDESCRIPTION: This Python code snippet demonstrates how to extract a specific field ('data.name') from a JSON string obtained from an HTTP response within a Dify workflow.  It requires the `json` library for parsing the JSON string and returns a dictionary containing the extracted 'result'.  The input `http_response` is expected to be a valid JSON string.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/code.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef main(http_response: str) -> str:\n    import json\n    data = json.loads(http_response)\n    return {\n        # 注意在输出变量中声明result\n        'result': data['data']['name'] \n    }\n```\n\n----------------------------------------\n\nTITLE: Starting API Service with Flask\nDESCRIPTION: Starts the API service using Flask development server. It runs the Flask application on all available addresses (0.0.0.0) on port 5001 with debugging enabled.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run flask run --host 0.0.0.0 --port=5001 --debug\n```\n\n----------------------------------------\n\nTITLE: Install Optional Python Dependencies for Dify on WeChat\nDESCRIPTION: Installs optional Python dependencies that extend the functionality of the Dify on WeChat project. These are installed using pip3 based on the packages in requirements-optional.txt. An alternative command is provided to use the Aliyun mirror for faster installation within China.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements-optional.txt # 国内可以在该命令末尾添加 \"-i https://mirrors.aliyun.com/pypi/simple\" 参数，使用阿里云镜像源安装依赖\n```\n\n----------------------------------------\n\nTITLE: Verify Ollama Installation (Bash)\nDESCRIPTION: This command verifies that Ollama is installed correctly and displays the version number. It's essential to confirm that Ollama is running before proceeding with model deployment and Dify integration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n➜  ~ ollama -v\nollama version is 0.5.5\n```\n\n----------------------------------------\n\nTITLE: Reloading Systemd Daemon and Restarting Ollama on Linux\nDESCRIPTION: These commands reload the systemd daemon and restart the Ollama service after modifying its service configuration. This is essential for applying changes to environment variables and ensuring the Ollama service runs with the updated settings.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_84\n\nLANGUAGE: Shell\nCODE:\n```\nsystemctl daemon-reload\nsystemctl restart ollama\n```\n\n----------------------------------------\n\nTITLE: PromptMessageContent BaseModel Definition in Python\nDESCRIPTION: Defines the base class PromptMessageContent, using Pydantic's BaseModel. This class represents the content of a prompt message and includes fields for the content type (PromptMessageContentType) and the actual data (a string).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContent(BaseModel):\n    \"\"\"\n    Model class for prompt message content.\n    \"\"\"\n    type: PromptMessageContentType\n    data: str  # コンテンツデータ\n```\n\n----------------------------------------\n\nTITLE: Packaging the Bundle plugin project\nDESCRIPTION: This command packages the Bundle plugin project located in the `./bundle` directory into a single `bundle.difybndl` file.  This command assumes that `dify-plugin` is in the current working directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/bundle.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndify-plugin bundle package ./bundle\n```\n\n----------------------------------------\n\nTITLE: Using Localtunnel to Expose the Local Project to the Public Internet\nDESCRIPTION: This command uses Localtunnel to create a publicly accessible URL that forwards requests to the local server running on port 9000.  This allows Twilio to send messages to the chatbot application running locally.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-whatsapp.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnpx localtunnel --port 9000\n```\n\n----------------------------------------\n\nTITLE: Customizing Button Styles with CSS Variables\nDESCRIPTION: This CSS code snippet demonstrates how to customize the Dify Chatbot Bubble Button's style using CSS variables. It shows how to modify properties such as the button's distance from the bottom and right, background color, width, height, border radius, box shadow, and hover transform. These variables allow for fine-grained control over the button's appearance.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-publishing/embedding-in-websites.md#_snippet_1\n\nLANGUAGE: css\nCODE:\n```\n/* Button distance to bottom, default is `1rem` */\n--dify-chatbot-bubble-button-bottom\n\n/* Button distance to right, default is `1rem` */\n--dify-chatbot-bubble-button-right\n\n/* Button distance to left, default is `unset` */\n--dify-chatbot-bubble-button-left\n\n/* Button distance to top, default is `unset` */\n--dify-chatbot-bubble-button-top\n\n/* Button background color, default is `#155EEF` */\n--dify-chatbot-bubble-button-bg-color\n\n/* Button width, default is `50px` */\n--dify-chatbot-bubble-button-width\n\n/* Button height, default is `50px` */\n--dify-chatbot-bubble-button-height\n\n/* Button border radius, default is `25px` */\n--dify-chatbot-bubble-button-border-radius\n\n/* Button box shadow, default is `rgba(0, 0, 0, 0.2) 0px 4px 8px 0px)` */\n--dify-chatbot-bubble-button-box-shadow\n\n/* Button hover transform, default is `scale(1.1)` */\n--dify-chatbot-bubble-button-hover-transform\n```\n\n----------------------------------------\n\nTITLE: Text Completion Text Generation App Template\nDESCRIPTION: This is the prompt template for building text generation applications using text completion models in Dify. It uses context, pre-prompt and query to generate output.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nUse the following context as your learned knowledge, inside <context></context> XML tags.\n\n<context>\n{{#context#}}\n</context>\n\nWhen answering the user:\n- If you don't know, just say that you don't know.\n- If you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n\n{{pre_prompt}}\n{{query}}\n```\n\n----------------------------------------\n\nTITLE: Validate Provider Credentials (Python)\nDESCRIPTION: This Python code defines a function for validating provider credentials.  The function provides a template for implementing the credentials validation for the provider.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Frontend Project Structure Overview\nDESCRIPTION: This snippet shows the directory structure of the Dify frontend, which is built using Next.js and Typescript. It outlines the organization of the frontend codebase, covering layouts, pages, components, assets, configurations, and internationalization settings.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_27\n\nLANGUAGE: Text\nCODE:\n```\n[web/]\n├── app                   // layouts, pages, and components\n│   ├── (commonLayout)    // common layout used throughout the app\n│   ├── (shareLayout)     // layouts specifically shared across token-specific sessions\n│   ├── activate          // activate page\n│   ├── components        // shared by pages and layouts\n│   ├── install           // install page\n│   ├── signin            // signin page\n│   └── styles            // globally shared styles\n├── assets                // Static assets\n├── bin                   // scripts ran at build step\n├── config                // adjustable settings and options\n├── context               // shared contexts used by different portions of the app\n├── dictionaries          // Language-specific translate files\n├── docker                // container configurations\n├── hooks                 // Reusable hooks\n├── i18n                  // Internationalization configuration\n├── models                // describes data models & shapes of API responses\n├── public                // meta assets like favicon\n├── service               // specifies shapes of API actions\n├── test\n├── types                 // descriptions of function params and return values\n└── utils                 // Shared utility functions\n```\n\n----------------------------------------\n\nTITLE: Parameter Validation with Zod\nDESCRIPTION: This TypeScript code uses `zod` for parameter validation, defining the expected structure and types of the input parameters.  It restricts the 'point' parameter to specific values and defines the structure of the 'params' object, including optional fields and record types for inputs.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { zValidator } from \"@hono/zod-validator\";\n\nconst schema = z.object({\n  point: z.union([\n    z.literal(\"ping\"),\n    z.literal(\"app.external_data_tool.query\"),\n  ]), // 'point' を2つの特定の値に制限\n  params: z\n    .object({\n      app_id: z.string().optional(),\n      tool_variable: z.string().optional(),\n      inputs: z.record(z.any()).optional(),\n      query: z.any().optional(),  // 文字列または null\n    })\n    .optional(),\n});\n```\n\n----------------------------------------\n\nTITLE: Validate Provider Credentials Method\nDESCRIPTION: This Python code snippet shows the `validate_provider_credentials` method, which is used to validate the provider's credentials. It mentions that any validate_credentials method of model type can be chosen or a validate method can be implemented by yourself. If validation fails, an exception should be raised.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/new-provider.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Copying Environment Configuration File\nDESCRIPTION: This command copies the example environment configuration file (`.env.example`) to a new file named `.env`.  This allows users to customize the environment variables for their Dify deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Initializing a Bundle Project with Dify CLI\nDESCRIPTION: This command initializes a new Dify plugin bundle project in the current directory. It uses the `dify-plugin-darwin-arm64` binary.  The user will be prompted to enter information about the plugin, such as name, author, and description.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/bundle.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./dify-plugin-darwin-arm64 bundle init\n```\n\n----------------------------------------\n\nTITLE: Chat Model Text Generation App Template USER\nDESCRIPTION: This is the USER prompt template for building text generation applications using chat models in Dify. It defines the query variable that accepts user input, typically as a paragraph.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n{{Query}} // Input the query variable here, commonly in the form of a paragraph\n```\n\n----------------------------------------\n\nTITLE: Starting the API Server (Bash)\nDESCRIPTION: This command starts the Dify API server using Flask in debug mode, listening on all addresses (0.0.0.0) and port 5001. This allows the API to be accessed from other machines and provides debugging information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run flask run --host 0.0.0.0 --port=5001 --debug\n```\n\n----------------------------------------\n\nTITLE: Starting LiteLLM Proxy with Docker\nDESCRIPTION: This Docker command starts the LiteLLM Proxy server. It mounts the litellm_config.yaml file to the /app/config.yaml inside the container, maps port 4000 on the host to port 4000 in the container, and specifies the LiteLLM image.  The `--config /app/config.yaml --detailed_debug` arguments tell the proxy where to find the configuration file and to output detailed debug information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_76\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n    -v $(pwd)/litellm_config.yaml:/app/config.yaml \\\n    -p 4000:4000 \\\n    ghcr.io/berriai/litellm:main-latest \\\n    --config /app/config.yaml --detailed_debug\n```\n\n----------------------------------------\n\nTITLE: Ngrok Initialization Command\nDESCRIPTION: These commands initialize Ngrok by unzipping the downloaded file and adding the user's authentication token. Replace '/path/to/ngrok.zip' with the actual path to the downloaded Ngrok zip file, and 'あなたのトークン' with the Ngrok authentication token.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ unzip /path/to/ngrok.zip\n$ ./ngrok config add-authtoken あなたのトークン\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Content Base Class Definition\nDESCRIPTION: Defines a base class `PromptMessageContent` for message content. It includes the content `type` and `data`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContent(BaseModel):\n    \"\"\"\n    Model class for prompt message content.\n    \"\"\"\n    type: PromptMessageContentType\n    data: str\n```\n\n----------------------------------------\n\nTITLE: Resetting Password via Docker Compose - Flask\nDESCRIPTION: This command allows you to reset the password for the Dify application when deployed using Docker Compose. It executes the `flask reset-password` command within the `docker-api-1` container, prompting for the email address and new password.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-password\n```\n\n----------------------------------------\n\nTITLE: Response Syntax for External Knowledge Base API\nDESCRIPTION: Demonstrates the JSON response format from the external knowledge base. It contains a 'records' array, where each element represents a retrieved chunk of text from the knowledge base. Each record includes 'metadata' (path and description), 'score' (relevance score), 'title' (document title), and 'content' (text chunk).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_25\n\nLANGUAGE: json\nCODE:\n```\nHTTP/1.1 200\nContent-type: application/json\n{\n    \"records\": [{\\n                    \"metadata\": {\\n                            \"path\": \"s3://dify/knowledge.txt\",\\n                            \"description\": \"dify knowledge document\"\\n                    },\\n                    \"score\": 0.98,\\n                    \"title\": \"knowledge.txt\",\\n                    \"content\": \"This is the document for external knowledge.\"\\n            },\\n            {\\n                    \"metadata\": {\\n                            \"path\": \"s3://dify/introduce.txt\",\\n                            \"description\": \"dify introduce\"\\n                    },\\n                    \"score\": 0.66,\\n                    \"title\": \"introduce.txt\",\\n                    \"content\": \"The Innovation Engine for GenAI Applications\"\\n            }\\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables (Web)\nDESCRIPTION: Example environment variables for the web application, configured in .env.local. These settings control deployment environment, edition type, API prefixes, and Sentry integration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_14\n\nLANGUAGE: Text\nCODE:\n```\n# For production release, change this to PRODUCTION\nNEXT_PUBLIC_DEPLOY_ENV=DEVELOPMENT\n# The deployment edition, SELF_HOSTED or CLOUD\nNEXT_PUBLIC_EDITION=SELF_HOSTED\n# The base URL of console application, refers to the Console base URL of WEB service if console domain is\n# different from api or web app domain.\n# example: http://cloud.dify.ai/console/api\nNEXT_PUBLIC_API_PREFIX=http://localhost:5001/console/api\n# The URL for Web APP, refers to the Web App base URL of WEB service if web app domain is different from\n# console or api domain.\n# example: http://udify.app/api\nNEXT_PUBLIC_PUBLIC_API_PREFIX=http://localhost:5001/api\n\n# SENTRY\nNEXT_PUBLIC_SENTRY_DSN=\nNEXT_PUBLIC_SENTRY_ORG=\nNEXT_PUBLIC_SENTRY_PROJECT=\n```\n\n----------------------------------------\n\nTITLE: Install Plugins in Dify\nDESCRIPTION: This command installs plugins by running `flask install-plugins` within the docker-api container. It downloads and installs the plugins necessary for the latest community version, requiring access to `https://marketplace.dify.ai`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/dify-premium.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask install-plugins --workers=2\n```\n\n----------------------------------------\n\nTITLE: Resetting Admin Password via Docker Compose\nDESCRIPTION: This command executes the `flask reset-password` command within the `docker-api-1` container to reset the administrator password. It requires Docker Compose to be running and the `docker-api-1` container to be accessible.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/faq.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-password\n```\n\n----------------------------------------\n\nTITLE: Copying Environment Configuration (Bash)\nDESCRIPTION: This command copies the example environment variable configuration file (.env.example) to a new file named .env. This provides a template for configuring environment-specific settings for the Dify API service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Launching Stable Diffusion WebUI on Linux\nDESCRIPTION: Launches the Stable Diffusion WebUI on a Linux machine, enabling the API and listening for incoming requests. Requires navigating to the cloned repository directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/stable-diffusion.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd stable-diffusion-webui\n./webui.sh --api --listen\n```\n\n----------------------------------------\n\nTITLE: Run Llava with Ollama\nDESCRIPTION: This command runs the Llava model using Ollama. After successful launch, Ollama starts an API service on local port 11434, accessible at `http://localhost:11434`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_78\n\nLANGUAGE: Shell\nCODE:\n```\nollama run llava\n```\n\n----------------------------------------\n\nTITLE: MCP Server URL Configuration for Workflow Agent Strategy\nDESCRIPTION: This JSON configuration is provided as a template for configuring the MCP Agent Strategy in a Dify Workflow. The `url` value must be replaced with the Zapier MCP Server address.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/best-practice/how-to-use-mcp-zapier.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"server_name\": {\n    \"url\": \"https://actions.zapier.com/mcp/*******/sse\",\n    \"headers\": {},\n    \"timeout\": 5,\n    \"sse_read_timeout\": 300\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning LocalAI Repository and Navigating to Directory (Bash)\nDESCRIPTION: This snippet clones the LocalAI code repository from GitHub and then navigates to the 'langchain-chroma' example directory. This is the first step in setting up LocalAI for integration with Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/localai.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/go-skynet/LocalAI\n$ cd LocalAI/examples/langchain-chroma\n```\n\n----------------------------------------\n\nTITLE: Setting Ollama Host Environment Variable on Mac\nDESCRIPTION: This command sets the `OLLAMA_HOST` environment variable on macOS using `launchctl`. This is necessary when Ollama needs to be accessed from other services or containers. The environment variable makes the Ollama service accessible by specifying the host address.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_81\n\nLANGUAGE: Shell\nCODE:\n```\nlaunchctl setenv OLLAMA_HOST \"0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Migrate local files using Flask (Docker)\nDESCRIPTION: This command uploads local files to cloud storage within a Docker container. It uses `docker exec` to execute the flask command inside the `docker-api-1` container.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask upload-local-files-to-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Adding File Variable to Start Node\nDESCRIPTION: This snippet demonstrates how to add a file variable to the start node of a Dify application, enabling file uploads. It covers both single file and file list options, allowing users to upload one or multiple files respectively.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/file-upload.md#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Moderation Output Request Example JSON\nDESCRIPTION: Illustrates an example of the request body for the `app.moderation.output` extension point. It shows how to pass the application ID and LLM output for moderation. Contains the text \"I will kill you.\".\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation-extension.md#_snippet_6\n\nLANGUAGE: JSON\nCODE:\n```\n{\n        \"point\": \"app.moderation.output\",\n        \"params\": {\n            \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n            \"text\": \"I will kill you.\"\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Setting Ollama Host Environment Variable (Linux)\nDESCRIPTION: These lines configure the systemd service for Ollama to set the OLLAMA_HOST environment variable. The `Environment` line is added under the `[Service]` section of the `ollama.service` file.  The systemd daemon needs to be reloaded and the Ollama service restarted for the changes to apply.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_81\n\nLANGUAGE: Text\nCODE:\n```\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Downloading LLM and Embedding Models (Shell)\nDESCRIPTION: This code snippet provides shell commands to download example LLM and Embedding models for use with LocalAI.  It downloads `ggml-model-q4_0.bin` as the embedding model and `ggml-gpt4all-j.bin` as the LLM model, saving them into the `models` directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n$ wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O models/bert\n$ wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j\n```\n\n----------------------------------------\n\nTITLE: Install GPUStack on Windows (PowerShell)\nDESCRIPTION: This command installs GPUStack on Windows using PowerShell. It downloads the installation script and executes it, enabling GPUStack functionality within the Windows environment. Run PowerShell as administrator and avoid using PowerShell ISE.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/gpustack.md#_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n```\n\n----------------------------------------\n\nTITLE: Creating Text Messages in Python\nDESCRIPTION: This Python code snippet shows the interface for creating text messages in the Dify tool plugin system. It takes a text string as input and returns a ToolInvokeMessage. It requires the Dify plugin environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/tool.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef create_text_message(self, text: str) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Chat Model Text Generation App Template SYSTEM\nDESCRIPTION: This is the SYSTEM prompt template for building text generation applications using chat models in Dify. It uses context, pre_prompt, and instructs the model on how to respond to the user's query.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nUse the following context as your learned knowledge, inside <context></context> XML tags.\n\n<context>\n{{#context#}}\n</context>\n\nWhen answering the user:\n- If you don't know, just say that you don't know.\n- If you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n{{pre_prompt}}\n```\n\n----------------------------------------\n\nTITLE: LiteLLM Configuration YAML\nDESCRIPTION: This YAML file configures LiteLLM with different GPT-4 model configurations, including Azure OpenAI endpoints, API versions, and API keys.  It defines the `model_list` that LiteLLM will use to route requests. Replace the placeholders with your actual API keys and base URLs.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/litellm.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_list:\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/chatgpt-v-2\n      api_base: https://openai-gpt-4-test-v-1.openai.azure.com/\n      api_version: \"2023-05-15\"\n      api_key: \n  - model_name: gpt-4\n    litellm_params:\n      model: azure/gpt-4\n      api_key: \n      api_base: https://openai-gpt-4-test-v-2.openai.azure.com/\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/gpt-4\n      api_key: \n      api_base: https://openai-gpt-4-test-v-2.openai.azure.com/\n```\n\n----------------------------------------\n\nTITLE: Generating Secret Key (Bash)\nDESCRIPTION: This command generates a random base64 encoded secret key and replaces the SECRET_KEY value in the .env file. This key is essential for securing the Dify API.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nawk -v key=\"$(openssl rand -base64 42)\" '/^SECRET_KEY=/ {sub(/=.*/, \"=\" key)} 1' .env > temp_env && mv temp_env .env\n```\n\n----------------------------------------\n\nTITLE: Cloning and Configuring the Repository (Bash)\nDESCRIPTION: These commands clone the example GitHub repository for a Dify API extension and copy the example configuration file to the active configuration file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/crazywoola/dify-extension-workers.git\ncp wrangler.toml.example wrangler.toml\n```\n\n----------------------------------------\n\nTITLE: Pushing Plugin Files to GitHub in Bash\nDESCRIPTION: This snippet pushes the local Git repository's `main` branch to the remote GitHub repository. It assumes the local repository is already connected to the remote. It requires a connection to the remote GitHub repository and proper Git configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit branch -M main\ngit push -u origin main\n```\n\n----------------------------------------\n\nTITLE: OpenAI Model Configuration YAML\nDESCRIPTION: This YAML configuration file defines the schema for OpenAI family models, including model-specific credentials such as the API key, organization ID, and API base URL. This is specifically for fine-tuned models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_credential_schema:\n  model: # 微调模型名称\n    label:\n      en_US: Model Name\n      zh_Hans: 模型名称\n    placeholder:\n      en_US: Enter your model name\n      zh_Hans: 输入模型名称\n  credential_form_schemas:\n  - variable: openai_api_key\n    label:\n      en_US: API Key\n    type: secret-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入您的 API Key\n      en_US: Enter your API Key\n  - variable: openai_organization\n    label:\n        zh_Hans: 组织 ID\n        en_US: Organization\n    type: text-input\n    required: false\n    placeholder:\n      zh_Hans: 在此输入您的组织 ID\n      en_US: Enter your Organization ID\n  - variable: openai_api_base\n    label:\n      zh_Hans: API Base\n      en_US: API Base\n    type: text-input\n    required: false\n    placeholder:\n      zh_Hans: 在此输入您的 API Base\n      en_US: Enter your API Base\n```\n\n----------------------------------------\n\nTITLE: Running Frontend DockerHub Image\nDESCRIPTION: Runs the frontend service using a pre-built Docker image from DockerHub. The command maps port 3000 and sets environment variables for API endpoints. It requires Docker to be installed and running. The exposed port 3000 is used to access the application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/start-the-frontend-docker-container.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run -it -p 3000:3000 -e CONSOLE_API_URL=http://127.0.0.1:5001 -e APP_API_URL=http://127.0.0.1:5001 langgenius/dify-web:latest\n```\n\n----------------------------------------\n\nTITLE: Fetching Custom Model Schema in Python\nDESCRIPTION: This method fetches the customizable model schema. It takes model name and credentials as input and returns an AIModelEntity object or None if not supported.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> Optional[AIModelEntity]:\n      \"\"\"\n      Get customizable model schema\n\n      :param model: model name\n      :param credentials: model credentials\n      :return: model schema\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Web Environment Variables\nDESCRIPTION: Sets environment variables for web frontend configuration, including API prefixes and Sentry settings. Used for configuring the web application's connection to other services.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_47\n\nLANGUAGE: Text\nCODE:\n```\n# For production release, change this to PRODUCTION\nNEXT_PUBLIC_DEPLOY_ENV=DEVELOPMENT\n# The deployment edition, SELF_HOSTED\nNEXT_PUBLIC_EDITION=SELF_HOSTED\n# The base URL of console application, refers to the Console base URL of WEB service if console domain is\n# different from api or web app domain.\n# example: http://cloud.dify.ai/console/api\nNEXT_PUBLIC_API_PREFIX=http://localhost:5001/console/api\n# The URL for Web APP, refers to the Web App base URL of WEB service if web app domain is different from\n# console or api domain.\n# example: http://udify.app/api\nNEXT_PUBLIC_PUBLIC_API_PREFIX=http://localhost:5001/api\n\n# SENTRY\nNEXT_PUBLIC_SENTRY_DSN=\nNEXT_PUBLIC_SENTRY_ORG=\nNEXT_PUBLIC_SENTRY_PROJECT=\n```\n\n----------------------------------------\n\nTITLE: Running LiteLLM Proxy with Docker\nDESCRIPTION: This Docker command runs the LiteLLM proxy server, mounting the configuration file and exposing port 4000. It uses the latest `litellm:main-latest` image. The `--config` flag specifies the location of the configuration file within the container, and `--detailed_debug` enables detailed debugging output.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/litellm.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker run \\\n    -v $(pwd)/litellm_config.yaml:/app/config.yaml \\\n    -p 4000:4000 \\\n    ghcr.io/berriai/litellm:main-latest \\\n    --config /app/config.yaml --detailed_debug\n```\n\n----------------------------------------\n\nTITLE: Cloning the Dify repository with Git\nDESCRIPTION: This command clones the forked Dify repository from GitHub to your local machine. Replace <github_username> with your actual GitHub username. It allows you to work on the Dify codebase locally.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/community/contribution.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone git@github.com:<github_username>/dify.git\n```\n\n----------------------------------------\n\nTITLE: Install plugins in the new version\nDESCRIPTION: This command downloads and installs the necessary plugins for the latest Dify Community Edition, ensuring compatibility with the new version. It is executed within the `docker-api` container, relying on a working internet connection to access the plugin marketplace.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/migration/migrate-to-v1.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask install-plugins --workers=2\n```\n\n----------------------------------------\n\nTITLE: Chain-of-Thought JSON Schema Example\nDESCRIPTION: This JSON Schema is designed for capturing step-by-step mathematical reasoning. It defines an array of 'steps,' each with an 'explanation' and 'output,' along with a 'final_answer.' The 'strict' mode ensures that the LLM adheres strictly to the defined structure.  It requires all the fields to be filled and prohibits additional properties.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/extended-reading/how-to-use-json-schema-in-dify.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"math_reasoning\",\n    \"description\": \"数学的推論の手順と最終回答を記録します\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"steps\": {\n                \"type\": \"array\",\n                \"description\": \"推論ステップの配列\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"explanation\": {\n                            \"type\": \"string\",\n                            \"description\": \"推論ステップの説明\"\n                        },\n                        \"output\": {\n                            \"type\": \"string\",\n                            \"description\": \"推論ステップの出力\"\n                        }\n                    },\n                    \"required\": [\"explanation\", \"output\"],\n                    \"additionalProperties\": false\n                }\n            },\n            \"final_answer\": {\n                \"type\": \"string\",\n                \"description\": \"数学問題の最終回答\"\n            }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\"steps\", \"final_answer\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Knowledge Bases using Dify API (curl)\nDESCRIPTION: This snippet demonstrates how to retrieve a list of knowledge bases (datasets) using the Dify API. It takes pagination parameters (page and limit) and an API key for authentication.  The response provides a list of dataset objects with metadata.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request GET 'https://api.dify.ai/v1/datasets?page=1&limit=20' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Configuring App Info in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to configure application information such as title, description, copyright, privacy policy, and default language.  This configuration is located in `config/index.ts`. Modifying these values allows customization of the app's appearance and behavior.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-publishing/based-on-frontend-templates.md#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nexport const APP_INFO: AppInfo = {\n  \"title\": 'Chat APP',\n  \"description\": '',\n  \"copyright\": '',\n  \"privacy_policy\": '',\n  \"default_language\": 'zh-Hans'\n}\n\nexport const isShowPrompt = true\nexport const promptTemplate = ''\n```\n\n----------------------------------------\n\nTITLE: Checking if a number is enrolled and sending a response\nDESCRIPTION: This Python code checks if a WhatsApp number is in the `enrolled_numbers` list. If the number is not enrolled, it sends a message indicating that the user is not enrolled.  It uses the Twilio client to send the message.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/dify-on-whatsapp.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    # Check if the number is enrolled\n    if whatsapp_number not in enrolled_numbers:\n        message = client.messages.create(  \n            from_=f\"whatsapp:{twilio_number}\",  \n            body=\"You are not enrolled in this service.\",  \n            to=f\"whatsapp:{whatsapp_number}\"  \n        )\n        return \"\"\n```\n\n----------------------------------------\n\nTITLE: Dify Backend Directory Structure\nDESCRIPTION: This section outlines the directory structure of Dify's backend, written in Python using Flask, SQLAlchemy for ORM, and Celery for task queueing. It provides an overview of the different directories and their purposes, such as API route definitions, core application orchestration, database models, and task handling.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_27\n\nLANGUAGE: Text\nCODE:\n```\n[api/]\n├── constants             // Constant settings used throughout code base.\n├── controllers           // API route definitions and request handling logic.\n├── core                  // Core application orchestration, model integrations, and tools.\n├── docker                // Docker & containerization related configurations.\n├── events                // Event handling and processing\n├── extensions            // Extensions with 3rd party frameworks/platforms.\n├── fields                // field definitions for serialization/marshalling.\n├── libs                  // Reusable libraries and helpers.\n├── migrations            // Scripts for database migration.\n├── models                // Database models & schema definitions.\n├── services              // Specifies business logic.\n├── storage               // Private key storage.\n├── tasks                 // Handling of async tasks and background jobs.\n└── tests\n```\n\n----------------------------------------\n\nTITLE: Configuring Plugin Settings (YAML)\nDESCRIPTION: This YAML configuration defines the settings for the Slack Bot plugin. It includes fields for the Bot Token and the Dify App, allowing the user to specify which Dify App will handle the Slack messages. It also includes the option to allow retry attempts.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsettings:\n  - name: bot_token\n    type: secret-input\n    required: true\n    label:\n      en_US: Bot Token\n      zh_Hans: Bot Token\n      pt_BR: Token do Bot\n      ja_JP: Bot Token\n    placeholder:\n      en_US: Please input your Bot Token\n      zh_Hans: 请输入你的 Bot Token\n      pt_BR: Por favor, insira seu Token do Bot\n      ja_JP: ボットトークンを入力してください\n  - name: allow_retry\n    type: boolean\n    required: false\n    label:\n      en_US: Allow Retry\n      zh_Hans: 允许重试\n      pt_BR: Permitir Retentativas\n      ja_JP: 再試行を許可\n    default: false\n  - name: app\n    type: app-selector\n    required: true\n    label:\n      en_US: App\n      zh_Hans: 应用\n      pt_BR: App\n      ja_JP: アプリ\n    placeholder:\n      en_US: the app you want to use to answer Slack messages\n      zh_Hans: 你想要用来回答 Slack 消息的应用\n      pt_BR: o app que você deseja usar para responder mensagens do Slack\n      ja_JP: あなたが Slack メッセージに回答するために使用するアプリ\nendpoints:\n  - endpoints/slack.yaml\n```\n\n----------------------------------------\n\nTITLE: Tool Configuration - YAML\nDESCRIPTION: This YAML configuration demonstrates how to define a 'model-selector' type parameter named 'model' within a Dify Tool's parameter list.  The `scope` is set to `llm`, limiting the selectable models to LLM types. This allows the user to select the desired LLM through the UI.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: llm\n  author: Dify\n  label:\n    en_US: LLM\n    zh_Hans: LLM\n    pt_BR: LLM\ndescription:\n  human:\n    en_US: A tool for invoking a large language model\n    zh_Hans: 用于调用大型语言模型的工具\n    pt_BR: A tool for invoking a large language model\n  llm: A tool for invoking a large language model\nparameters:\n  - name: prompt\n    type: string\n    required: true\n    label:\n      en_US: Prompt string\n      zh_Hans: 提示字符串\n      pt_BR: Prompt string\n    human_description:\n      en_US: used for searching\n      zh_Hans: 用于搜索网页内容\n      pt_BR: used for searching\n    llm_description: key words for searching\n    form: llm\n  - name: model\n    type: model-selector\n    scope: llm\n    required: true\n    label:\n      en_US: Model\n      zh_Hans: 使用的模型\n      pt_BR: Model\n    human_description:\n      en_US: Model\n      zh_Hans: 使用的模型\n      pt_BR: Model\n    llm_description: which Model to invoke\n    form: form\nextra:\n  python:\n    source: tools/llm.py\n```\n\n----------------------------------------\n\nTITLE: JSON Schema for Math Reasoning\nDESCRIPTION: This JSON schema defines a structure for capturing mathematical reasoning steps and the final answer. The schema includes an array of steps, each containing an explanation and an output, along with a field for the final answer.  It enforces strict adherence to the defined structure, ensuring that the LLM provides outputs in the expected format, and that there are no additional properties allowed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/extended-reading/how-to-use-json-schema-in-dify.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"math_reasoning\",\n    \"description\": \"Records steps and final answer for mathematical reasoning\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"steps\": {\n                \"type\": \"array\",\n                \"description\": \"Array of reasoning steps\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"explanation\": {\n                            \"type\": \"string\",\n                            \"description\": \"Explanation of the reasoning step\"\n                        },\n                        \"output\": {\n                            \"type\": \"string\",\n                            \"description\": \"Output of the reasoning step\"\n                        }\n                    },\n                    \"required\": [\"explanation\", \"output\"],\n                    \"additionalProperties\": false\n                }\n            },\n            \"final_answer\": {\n                \"type\": \"string\",\n                \"description\": \"The final answer to the mathematical problem\"\n            }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\"steps\", \"final_answer\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Create a Model Provider File (xinference.yaml)\nDESCRIPTION: This YAML file defines the Xinference model provider, specifying supported model types (LLM, Text Embedding, Rerank), display labels, icons, help information, configurable methods, and the provider credential schema. It configures Xinference as a customizable model due to its local deployment and lack of predefined models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprovider: xinference  # Identifies the provider\nlabel:                # Display name; can set both en_US (English) and zh_Hans (Chinese). If zh_Hans is not set, en_US is used by default.\n  en_US: Xorbits Inference\nicon_small:           # Small icon; store in the _assets folder of this provider’s directory. The same multi-language logic applies as with label.\n  en_US: icon_s_en.svg\nicon_large:           # Large icon\n  en_US: icon_l_en.svg\nhelp:                 # Help information\n  title:\n    en_US: How to deploy Xinference\n    zh_Hans: 如何部署 Xinference\n  url:\n    en_US: https://github.com/xorbitsai/inference\n\nsupported_model_types:  # Model types Xinference supports: LLM/Text Embedding/Rerank\n- llm\n- text-embedding\n- rerank\n\nconfigurate_methods:     # Xinference is locally deployed and does not offer predefined models. Refer to its documentation to learn which model to use. Thus, we choose a customizable-model approach.\n- customizable-model\n\nprovider_credential_schema:\n  credential_form_schemas:\n```\n\n----------------------------------------\n\nTITLE: Escape Object as String - Python\nDESCRIPTION: This Python function `main` takes a list (assumed to contain a dictionary at index 0) as input, converts it into a JSON string, and wraps the string within `<answer>` tags. If the input list is empty, an empty dictionary is used. The function ensures that non-ASCII characters are correctly encoded and includes error handling to return an error message within `<answer>` tags if any exception occurs during processing.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/variable-assigner.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\ndef main(arg1: list) -> str:\n    try:\n        # Assume arg1[0] is the dictionary we need to process\n        context = arg1[0] if arg1 else {}\n        \n        # Construct the memory object\n        memory = {\"memory\": context}\n        \n        # Convert the object to a JSON string\n        json_str = json.dumps(memory, ensure_ascii=False, indent=2)\n        \n        # Wrap the JSON string in <answer> tags\n        result = f\"<answer>{json_str}</answer>\"\n        \n        return {\n            \"result\": result\n        }\n    except Exception as e:\n        return {\n            \"result\": f\"<answer>Error: {str(e)}</answer>\"\n        }\n```\n\n----------------------------------------\n\nTITLE: Checking WSL version using PowerShell\nDESCRIPTION: This PowerShell command checks the version and status of WSL distributions. It verifies that Ubuntu and Docker are running with WSL2.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_3\n\nLANGUAGE: PowerShell\nCODE:\n```\nwsl -l --verbose\n```\n\n----------------------------------------\n\nTITLE: Database Migration - Flask\nDESCRIPTION: This command migrates the database structure to the latest version when updating Dify from source code. It needs to be executed in the `api` directory after pulling the latest code. Requires Flask and database setup.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n`flask db upgrade`\n```\n\n----------------------------------------\n\nTITLE: Performing Database Migration (Bash)\nDESCRIPTION: Performs a database migration to upgrade the database schema to the latest version using Flask-Migrate. This ensures the database structure is compatible with the current application code.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\npoetry shell\nflask db upgrade\n```\n\n----------------------------------------\n\nTITLE: Interface Definition in YAML\nDESCRIPTION: Defines the path, method, and source code location for an endpoint interface using YAML. It specifies the path for the endpoint, the HTTP method to use, and the Python source code file implementing the endpoint logic.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/endpoint.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npath: \"/duck/<app_id>\"\nmethod: \"GET\"\nextra:\n  python:\n    source: \"endpoints/duck.py\"\n```\n\n----------------------------------------\n\nTITLE: Checking if WhatsApp Number is Enrolled\nDESCRIPTION: This Python code checks if the user's WhatsApp number is in the `enrolled_numbers` list. If not, it sends a message back to the user saying they are not enrolled in the service using the Twilio API.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-whatsapp.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    # Check if the number is enrolled\n    if whatsapp_number not in enrolled_numbers:\n        message = client.messages.create(  \n            from_=f\"whatsapp:{twilio_number}\",  \n            body=\"You are not enrolled in this service.\",  \n            to=f\"whatsapp:{whatsapp_number}\"  \n        )\n        return \"\"\n```\n\n----------------------------------------\n\nTITLE: Customizing Chatbot Style with iFrame\nDESCRIPTION: This snippet shows how to modify the style attribute within the iFrame code to customize the chatbot's appearance. In this example, a 2-pixel solid black border is added to the chatbot. This involves modifying the `style` attribute to include the `border` property.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/use-cases/how-to-integrate-dify-chatbot-to-your-wix-website.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n<iframe src=\"https://udify.app/chatbot/ez1pf83HVV3JgWO4\" style=\"width: 80%; height: 80%; min-height: 500px; border: 2px solid #000;\" frameborder=\"0\" allow=\"microphone\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: Packaging Dify Plugin\nDESCRIPTION: This command packages the Dify plugin into a single file (`.difypkg`) for distribution. The path to the plugin project (e.g., './basic_agent/') must be provided as an argument. This command is executed using the 'dify' command-line tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n# Replace ./basic_agent/ with your actual plugin project path.\n\ndify plugin package ./basic_agent/\n```\n\n----------------------------------------\n\nTITLE: Define System Prompt Message Model (Python)\nDESCRIPTION: Defines a Pydantic model `SystemPromptMessage` representing system messages, usually used for setting system commands given to the model. It includes the `role` attribute which is set to `PromptMessageRole.SYSTEM`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nclass SystemPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for system prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.SYSTEM\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response - Rate Limit Exceeded\nDESCRIPTION: This JSON snippet represents an error response indicating that the rate limit for the default-gpt-3.5-turbo model has been reached. The limit is 3 requests per minute. It suggests trying again after 20 seconds and provides links to the OpenAI help center and billing page to increase the rate limit.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/llms-use-faq.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"error\": \"Rate limit reached for default-gpt-3.5-turboin organization org-wDrZCxxxxxxxxxissoZb on requestsper min。 Limit: 3 / min. Please try again in 20s. Contact us through our help center   at help.openai.com   if you continue to haveissues. Please add a payment method toyour account to increase your rate limit.Visit https://platform.openai.com/account/billingto add a payment method.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining LLMResult Model (Python)\nDESCRIPTION: Defines the `LLMResult` model, representing the result of a Language Model. It includes the `model` used, the `prompt_messages` sent to the model, the `message` received, `usage` information, and an optional `system_fingerprint`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResult(BaseModel):\n    \"\"\"\n    Model class for llm result.\n    \"\"\"\n    model: str  # Actual used modele\n    prompt_messages: list[PromptMessage]  # prompt messages\n    message: AssistantPromptMessage  # response message\n    usage: LLMUsage  # usage info\n    system_fingerprint: Optional[str] = None  # request fingerprint, refer to OpenAI definition\n```\n\n----------------------------------------\n\nTITLE: Resetting Encryption Key Pair (Docker Compose)\nDESCRIPTION: Resets the encryption key pair used for encrypting large model keys when using Docker Compose. Executes a flask command within the docker-api container to regenerate the keys. Requires Docker and Docker Compose to be installed and running.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-encrypt-key-pair\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Example for User Info\nDESCRIPTION: This code adds a new field 'username' of type string to the JSON schema, defining it as a user's name and marking it as required. Used within the JSON Schema Editor.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/llm.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"username\",\n  \"type\": \"string\",\n  \"description\": \"user's name\",\n  \"required\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Running Ollama with Llava Model\nDESCRIPTION: This command launches the Ollama service and runs the Llava model, enabling multimodal capabilities. After running this command successfully, Ollama starts an API service on the local port 11434, accessible at `http://localhost:11434`. The user needs to have Ollama installed before running the command.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_77\n\nLANGUAGE: Shell\nCODE:\n```\nollama run llava\n```\n\n----------------------------------------\n\nTITLE: Starting Web Service with Yarn\nDESCRIPTION: Starts the web service using Yarn. This command initiates the frontend application, making it accessible in a browser.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_16\n\nLANGUAGE: Bash\nCODE:\n```\nyarn start\n```\n\n----------------------------------------\n\nTITLE: Complete Model Prompt Template for Text Generator Apps\nDESCRIPTION: This prompt template is designed for Complete models to build text generator applications. It combines context, pre-prompt, and a query. The template leverages XML tags to define the context and placeholders to allow for dynamically inserted information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/prompt-engineering/prompt-template.md#_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nUse the following context as your learned knowledge, inside <context></context> XML tags.\n\n<context>\n{{#context#}}\n</context>\n\nWhen answer to user:\n- If you don't know, just say that you don't know.\n- If you don't know when you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n\n{{pre_prompt}}\n{{query}}\n```\n\n----------------------------------------\n\nTITLE: PromptMessage Abstract Class Definition in Python\nDESCRIPTION: Defines the abstract base class PromptMessage, inheriting from both ABC and BaseModel.  It includes fields for the message role (PromptMessageRole), content (string or list of PromptMessageContent), and an optional name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessage(ABC, BaseModel):\n    \"\"\"\n    Model class for prompt message.\n    \"\"\"\n    role: PromptMessageRole  # メッセージロール\n    content: Optional[str | list[PromptMessageContent]] = None  # 2つのタイプ、文字列とコンテンツリストをサポート。コンテンツリストはマルチモーダルのニーズを満たすために使用されます。PromptMessageContentの説明を参照してください。\n    name: Optional[str] = None  # 名前、オプション\n```\n\n----------------------------------------\n\nTITLE: Setting a Storage Key-Value Pair\nDESCRIPTION: This snippet demonstrates the `set` method for storing a key-value pair. The value must be a byte string. The key is a string.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/persistent-storage.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef set(self, key: str, val: bytes) -> None:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Define Rerank Result Model (Python)\nDESCRIPTION: Defines a Pydantic model `RerankResult` representing the result of a reranking operation. It includes attributes for the `model` used and the list of `docs` (as `RerankDocument` objects) that were reranked.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nclass RerankResult(BaseModel):\n    \"\"\"\n    Model class for rerank result.\n    \"\"\"\n    model: str  # Actual model used\n    docs: list[RerankDocument]  # Reranked document list\n```\n\n----------------------------------------\n\nTITLE: Backup docker-compose YAML file (optional)\nDESCRIPTION: This snippet copies the `docker-compose.yaml` file to create a backup with a timestamp. It allows users to revert to a previous configuration if needed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/migration/migrate-to-v1.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ncp docker-compose.yaml docker-compose.yaml.$(date +%s).bak\n```\n\n----------------------------------------\n\nTITLE: Extract Plugins from Previous Community Version\nDESCRIPTION: This command executes the `flask extract-plugins` command within the docker-api container to extract the existing tools and model providers for migration to the new plugin environment. It requires a `poetry` environment within the container.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/dify-premium.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask extract-plugins --workers=20\n```\n\n----------------------------------------\n\nTITLE: Cloning Stable Diffusion WebUI Repository\nDESCRIPTION: Clones the Stable Diffusion WebUI repository from GitHub. This is the first step in setting up Stable Diffusion locally.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/stable-diffusion.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui\n```\n\n----------------------------------------\n\nTITLE: Install Xinference with pip\nDESCRIPTION: Installs Xinference with all dependencies using pip. This is the first step in setting up Xinference for local model deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/xinference.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install \"xinference[all]\"\n```\n\n----------------------------------------\n\nTITLE: Define TextEmbeddingResult Model\nDESCRIPTION: Defines the `TextEmbeddingResult` class, which represents the result of a text embedding call. It includes the model used, the embedding vectors, and usage information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nclass TextEmbeddingResult(BaseModel):\n    \"\"\"\n    Model class for text embedding result.\n    \"\"\"\n    model: str  # Actual model used\n    embeddings: list[list[float]]  # List of embedding vectors, corresponding to the input texts list\n    usage: EmbeddingUsage  # Usage information\n```\n\n----------------------------------------\n\nTITLE: Building Web Code\nDESCRIPTION: This command builds the Dify web frontend code using the build script defined in `package.json`. This prepares the code for deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_14\n\nLANGUAGE: Bash\nCODE:\n```\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: Anthropic Model Plugin Structure Example\nDESCRIPTION: Presents the directory structure for the Anthropic model plugin, specifically focusing on the 'llm' (Large Language Model) category and listing several Claude models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n- Anthropic\n  - llm\n    claude-3-5-sonnet-20240620\n    claude-3-haiku-20240307\n    claude-3-opus-20240229\n    claude-3-sonnet-20240229\n    claude-instant-1.2\n    claude-instant-1\n```\n\n----------------------------------------\n\nTITLE: Define Front-End Component Schema (schema.json)\nDESCRIPTION: Defines the JSON schema for the front-end component of the 'Cloud Service' moderation.  This schema describes the fields that will be displayed in the Dify UI for configuring the moderation service, including cloud provider selection, API endpoint, and API keys.  The schema includes labels for both English and Simplified Chinese.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/code-based-extension/moderation.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"label\": {\n        \"en-US\": \"Cloud Service\",\n        \"zh-Hans\": \"云服务\"\n    },\n    \"form_schema\": [\n        {\n            \"type\": \"select\",\n            \"label\": {\n                \"en-US\": \"Cloud Provider\",\n                \"zh-Hans\": \"云厂商\"\n            },\n            \"variable\": \"cloud_provider\",\n            \"required\": true,\n            \"options\": [\n                {\n                    \"label\": {\n                        \"en-US\": \"AWS\",\n                        \"zh-Hans\": \"亚马逊\"\n                    },\n                    \"value\": \"AWS\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Google Cloud\",\n                        \"zh-Hans\": \"谷歌云\"\n                    },\n                    \"value\": \"GoogleCloud\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Azure Cloud\",\n                        \"zh-Hans\": \"微软云\"\n                    },\n                    \"value\": \"Azure\"\n                }\n            ],\n            \"default\": \"GoogleCloud\",\n            \"placeholder\": \"\"\n        },\n        {\n            \"type\": \"text-input\",\n            \"label\": {\n                \"en-US\": \"API Endpoint\",\n                \"zh-Hans\": \"API Endpoint\"\n            },\n            \"variable\": \"api_endpoint\",\n            \"required\": true,\n            \"max_length\": 100,\n            \"default\": \"\",\n            \"placeholder\": \"https://api.example.com\"\n        },\n        {\n            \"type\": \"paragraph\",\n            \"label\": {\n                \"en-US\": \"API Key\",\n                \"zh-Hans\": \"API Key\"\n            },\n            \"variable\": \"api_keys\",\n            \"required\": true,\n            \"default\": \"\",\n            \"placeholder\": \"Paste your API key here\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Template Example\nDESCRIPTION: This is a general-purpose JSON Schema template demonstrating the basic structure and elements that can be defined. It includes string, number, array, and object types, along with descriptions, required fields, and the additionalProperties constraint. This schema can be customized to enforce specific data formats for LLM outputs in Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/extended-reading/how-to-use-json-schema-in-dify.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"template_schema\",\n    \"description\": \"JSON Schemaの汎用テンプレート\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"field1\": {\n                \"type\": \"string\",\n                \"description\": \"field1の説明\"\n            },\n            \"field2\": {\n                \"type\": \"number\",\n                \"description\": \"field2の説明\"\n            },\n            \"field3\": {\n                \"type\": \"array\",\n                \"description\": \"field3の説明\",\n                \"items\": {\n                    \"type\": \"string\"\n                }\n            },\n            \"field4\": {\n                \"type\": \"object\",\n                \"description\": \"field4の説明\",\n                \"properties\": {\n                    \"subfield1\": {\n                        \"type\": \"string\",\n                        \"description\": \"subfield1の説明\"\n                    }\n                },\n                \"required\": [\"subfield1\"],\n                \"additionalProperties\": false\n            }\n        },\n        \"required\": [\"field1\", \"field2\", \"field3\", \"field4\"],\n        \"additionalProperties\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: POST Request to Chat Messages API with Python\nDESCRIPTION: This Python script sends a POST request to the `/v1/chat-messages` endpoint using the `requests` library for conversational AI. It includes the authorization header, content type header, and a JSON payload with input parameters, response mode, conversation ID, and user ID.  It depends on the `requests` and `json` libraries. The `ENTER-YOUR-SECRET-KEY` should be replaced with the actual API key.  The `conversation_id` is used to continue an existing conversation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-publishing/developing-with-apis.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport requests\nimport json\n\nurl = 'https://api.dify.ai/v1/chat-messages'\nheaders = {\n    'Authorization': 'Bearer ENTER-YOUR-SECRET-KEY',\n    'Content-Type': 'application/json',\n}\ndata = {\n    \"inputs\": {},\n    \"query\": \"eh\",\n    \"response_mode\": \"streaming\",\n    \"conversation_id\": \"1c7e55fb-1ba2-4e10-81b5-30addcea2276\",\n    \"user\": \"abc-123\"\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nprint(response.text())\n```\n\n----------------------------------------\n\nTITLE: Convert string to object using JSON in Python\nDESCRIPTION: This Python code snippet takes a JSON string as input, parses it, extracts values from the 'memory' object, and constructs a new object with 'facts', 'preferences', and 'memories' fields. It handles JSON decoding errors and other exceptions, returning an error message if any occur. The input is expected to be a JSON string containing a 'memory' field, which itself is an object with 'facts', 'preferences', and 'memories' keys.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/variable-assigner.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\ndef main(arg1: str) -> object:\n    try:\n        # Parse the input JSON string\n        input_data = json.loads(arg1)\n        \n        # Extract the memory object\n        memory = input_data.get(\"memory\", {})\n        \n        # Construct the return object\n        result = {\n            \"facts\": memory.get(\"facts\", []),\n            \"preferences\": memory.get(\"preferences\", []),\n            \"memories\": memory.get(\"memories\", [])\n        }\n        \n        return {\n            \"mem\": result\n        }\n    except json.JSONDecodeError:\n        return {\n            \"result\": \"Error: Invalid JSON string\"\n        }\n    except Exception as e:\n        return {\n            \"result\": f\"Error: {str(e)}\"\n        }\n```\n\n----------------------------------------\n\nTITLE: Define LLMResultChunkDelta Model\nDESCRIPTION: Defines the `LLMResultChunkDelta` class, representing the delta of an LLM result chunk, used in streaming responses. It includes the index, message delta, usage information, and finish reason.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunkDelta(BaseModel):\n    \"\"\"\n    Model class for llm result chunk delta.\n    \"\"\"\n    index: int\n    message: AssistantPromptMessage  # response message\n    usage: Optional[LLMUsage] = None  # usage info\n    finish_reason: Optional[str] = None  # finish reason, only the last one returns\n```\n\n----------------------------------------\n\nTITLE: Implementing Google Tool Provider Class in Python\nDESCRIPTION: This Python code defines the GoogleProvider class, which handles the credential validation logic. The `_validate_credentials` method instantiates a GoogleSearchTool, forks the tool runtime with the provided credentials, and invokes it to validate the credentials. A ToolProviderCredentialValidationError is raised if the validation fails.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/quick-tool-integration.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom core.tools.entities.tool_entities import ToolInvokeMessage, ToolProviderType\nfrom core.tools.tool.tool import Tool\nfrom core.tools.provider.builtin_tool_provider import BuiltinToolProviderController\nfrom core.tools.errors import ToolProviderCredentialValidationError\n\nfrom core.tools.provider.builtin.google.tools.google_search import GoogleSearchTool\n\nfrom typing import Any, Dict\n\nclass GoogleProvider(BuiltinToolProviderController):\n    def _validate_credentials(self, credentials: Dict[str, Any]) -> None:\n        try:\n            # 1. 此处需要使用 GoogleSearchTool()实例化一个 GoogleSearchTool，它会自动加载 GoogleSearchTool 的 yaml 配置，但是此时它内部没有凭据信息\n            # 2. 随后需要使用 fork_tool_runtime 方法，将当前的凭据信息传递给 GoogleSearchTool\n            # 3. 最后 invoke 即可，参数需要根据 GoogleSearchTool 的 yaml 中配置的参数规则进行传递\n            GoogleSearchTool().fork_tool_runtime(\n                meta={\n                    \"credentials\": credentials,\n                }\n            ).invoke(\n                user_id='',\n                tool_Parameters={\n                    \"query\": \"test\",\n                    \"result_type\": \"link\"\n                },\n            )\n        except Exception as e:\n            raise ToolProviderCredentialValidationError(str(e))\n```\n\n----------------------------------------\n\nTITLE: Installing Xinference with pip\nDESCRIPTION: This command installs Xinference and all its dependencies using pip, Python's package installer. This is a prerequisite for deploying and using Xinference with Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install \"xinference[all]\"\n```\n\n----------------------------------------\n\nTITLE: Database Upgrade with Flask\nDESCRIPTION: This snippet shows how to upgrade the database structure to the latest version when updating a Dify instance deployed from source code. It executes the `flask db upgrade` command within the `api` directory to migrate the database schema.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/_book/zh_CN/learn-more/faq/install-faq.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nflask db upgrade\n```\n\n----------------------------------------\n\nTITLE: Upgrading Dify Instance\nDESCRIPTION: These commands are used to upgrade a self-hosted Dify instance. It clones the Dify repository, moves the docker files, removes the temporary directory, stops the existing containers, pulls the latest images and starts the containers again.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_62\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git /tmp/dify\nmv -f /tmp/dify/docker/* /dify/\nrm -rf /tmp/dify\ndocker-compose down\ndocker-compose pull\ndocker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Starting Dify Sandbox Service using Docker Compose\nDESCRIPTION: This command starts the Dify sandbox service using Docker Compose. It uses the `docker-compose.middleware.yaml` file to define the services and their configurations. The `-f` flag specifies the compose file, and the `up -d` command starts the services in detached mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_69\n\nLANGUAGE: Docker\nCODE:\n```\ndocker-compose -f docker-compose.middleware.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Modify Document Metadata (Assign Value) - Dify API - Bash\nDESCRIPTION: This snippet shows how to modify the metadata of a document in a Dify dataset by assigning values. It requires the dataset ID. The request includes headers for authorization (API key) and content type (application/json), along with the operation data in JSON format. 'operation_data' consists of 'document_id' and the list of 'metadata_list', each element consisting of metadata 'id', 'value' and 'name'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/metadata' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer {api_key}'\n--data '{\n    \"operation_data\":[\n        {\n            \"document_id\": \"3e928bc4-65ea-4201-87c8-cbcc5871f525\",\n            \"metadata_list\": [\n                    {\n                    \"id\": \"1887f5ec-966f-4c93-8c99-5ad386022f46\",\n                    \"value\": \"dify\",\n                    \"name\": \"test\"\n                }\n            ]\n        }\n    ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Link Message Creation in Dify (Python)\nDESCRIPTION: This Python code snippet defines the interface for creating a link message within the Dify tool plugin framework. It takes a link URL as input and returns a ToolInvokeMessage object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/tool.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef create_link_message(self, link: str) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Exposing local project to public access using Localtunnel\nDESCRIPTION: This command uses Localtunnel to expose a local server running on port 9000 to a publicly accessible URL. This is required because Twilio needs a public URL to send messages to. It assumes that Node.js and npm are installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/dify-on-whatsapp.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnpx localtunnel --port 9000\n```\n\n----------------------------------------\n\nTITLE: Prompt with Default Style Specification\nDESCRIPTION: This code snippet demonstrates a prompt that includes a default style (anime) for image generation using the stability_text2image tool. The agent will prioritize the specified style (anime) whenever the user provides an image description.  This ensures that generated images adhere to the desired aesthetic.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/workshop/basic/build-ai-image-generation-app.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nユーザーのプロンプトに従って、指定された内容をstability_text2imageを使用して描画してください。画像はアニメスタイルです。\n```\n\n----------------------------------------\n\nTITLE: Defining RerankResult Model in Python\nDESCRIPTION: Defines the `RerankResult` model, representing the result of a reranking operation. It includes the model name and a list of reranked documents.  Requires RerankDocument.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nclass RerankResult(BaseModel):\n    \"\"\"\n    Model class for rerank result.\n    \"\"\"\n    model: str  # Actual model used\n    docs: list[RerankDocument]  # Reranked document list\t\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Content Type Enum\nDESCRIPTION: Defines an enumeration for prompt message content types, including text and image. This enum is used to specify the type of content within a prompt message.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContentType(Enum):\n    \"\"\"\n    Enum class for prompt message content type.\n    \"\"\"\n    TEXT = 'text'\n    IMAGE = 'image'\n```\n\n----------------------------------------\n\nTITLE: Text2speech Model Invocation\nDESCRIPTION: Defines the interface for a text-to-speech model invocation. It takes text as input and converts it to an audio stream. Parameters include the model name, credentials, text content, a flag indicating whether the output should be streamed, and an optional user identifier.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, content_text: str, streaming: bool, user: Optional[str] = None):\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param content_text: text content to be translated\n      :param streaming: output is streaming\n      :param user: unique user id\n      :return: translated audio file\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Resetting Encryption Key Pair - Source Code - Flask\nDESCRIPTION: This command resets the encryption key pair when running Dify from source code.  It must be executed from within the `api` directory. It runs the `flask reset-encrypt-key-pair` command, prompting for input during the process.  This is to address \"File not found\" errors related to missing encryption keys.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nflask reset-encrypt-key-pair\n```\n\n----------------------------------------\n\nTITLE: Text Prompt Message Content Class\nDESCRIPTION: Defines a class for text-based prompt message content, inheriting from PromptMessageContent.  It sets the content type to TEXT by default. Used when including text within a multimodal prompt.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nclass TextPromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for text prompt message content.\n    \"\"\"\n    type: PromptMessageContentType = PromptMessageContentType.TEXT\n```\n\n----------------------------------------\n\nTITLE: Defining _invoke_error_mapping Property\nDESCRIPTION: This Python code defines the `_invoke_error_mapping` property. It maps model invocation errors to unified error types defined in the Runtime, allowing Dify to handle different errors appropriately.  The mapping is a dictionary where keys are Runtime error types and values are lists of model-specific exception types.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/predefined-model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@property\n    def _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n        \"\"\"\n        Map model invoke error to unified error\n        The key is the error type thrown to the caller\n        The value is the error type thrown by the model,\n        which needs to be converted into a unified error type for the caller.\n\n        :return: Invoke error mapping\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Run Frontend Docker Image from DockerHub\nDESCRIPTION: This command pulls the latest frontend Docker image from DockerHub and runs it. It maps port 3000 to port 3000 and sets environment variables CONSOLE_URL and APP_URL to point to the backend service running on localhost port 5001. This configuration is appropriate when the console and webapp are running on the same domain.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/start-the-frontend-docker-container.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run -it -p 3000:3000 -e CONSOLE_URL=http://127.0.0.1:5001 -e APP_URL=http://127.0.0.1:5001 langgenius/dify-web:latest\n```\n\n----------------------------------------\n\nTITLE: Adding Nux Dextop Repository on CentOS\nDESCRIPTION: This command adds the Nux Dextop repository to CentOS, which provides multimedia-related packages like FFmpeg. It uses `rpm` to install the repository package. Root or sudo privileges are required.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nsudo rpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-5.el7.nux.noarch.rpm\n```\n\n----------------------------------------\n\nTITLE: Defining TextEmbeddingResult Model in Python\nDESCRIPTION: Defines the `TextEmbeddingResult` model, representing the result of a text embedding operation. It includes the model name, a list of embedding vectors, and usage information. Requires EmbeddingUsage.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nclass TextEmbeddingResult(BaseModel):\n    \"\"\"\n    Model class for text embedding result.\n    \"\"\"\n    model: str  # Actual model used\n    embeddings: list[list[float]]  # List of embedding vectors, corresponding to the input texts list\n    usage: EmbeddingUsage  # Usage information\n```\n\n----------------------------------------\n\nTITLE: Model Plugin Directory Structure\nDESCRIPTION: This bash snippet demonstrates the basic directory structure for organizing model plugins within Dify, categorized by model vendor and model type. This hierarchical structure helps in managing different models provided by vendors like OpenAI, Anthropic, and Google.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n- 模型供应商\n    - 模型分类\n        - 具体模型\n```\n\n----------------------------------------\n\nTITLE: LLM Prompt - JSON Format Output\nDESCRIPTION: This prompt instructs the LLM to act as a teaching assistant, providing either a correct or incorrect JSON code sample based on user requirements. The focus is on generating JSON code snippets that can be used for demonstration or testing purposes. The LLM is constrained to only outputting JSON formatted content.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_76\n\nLANGUAGE: text\nCODE:\n```\nYou are a teaching assistant. According to the user's requirements, you only output a correct or incorrect sample code in json format.\n```\n\n----------------------------------------\n\nTITLE: Stop services and create data backup\nDESCRIPTION: This set of commands stops the running Docker Compose services and creates a compressed archive of the volumes directory, backing up all persistent data.  It's crucial to ensure data integrity during the upgrade process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/migration/migrate-to-v1.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\ntar -cvf volumes-$(date +%s).tgz volumes\n```\n\n----------------------------------------\n\nTITLE: Defining RerankDocument Model (Python)\nDESCRIPTION: Defines the `RerankDocument` model, representing a document in the reranking results. It contains the `index` (original index), the `text` content, and the `score` assigned to the document.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nclass RerankDocument(BaseModel):\n    \"\"\"\n    Model class for rerank document.\n    \"\"\"\n    index: int  # original index\n    text: str\n    score: float\n```\n\n----------------------------------------\n\nTITLE: Download LLM and Embedding Models\nDESCRIPTION: This snippet downloads example LLM (ggml-gpt4all-j) and embedding (all-MiniLM-L6-v2) models from Hugging Face and gpt4all.io, storing them in the 'models' directory.  These models are used for demonstration purposes and are compatible with all platforms for quick local deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/localai.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O models/bert\n$ wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j\n```\n\n----------------------------------------\n\nTITLE: Set Ollama Host Environment Variable in Systemd Config\nDESCRIPTION: This config snippet shows how to set the OLLAMA_HOST environment variable within the systemd service configuration file. Setting `OLLAMA_HOST=0.0.0.0` makes the Ollama service accessible from all IP addresses, allowing connections from other machines or Docker containers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/ollama.md#_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Run Flask API Service\nDESCRIPTION: Starts the Flask API service. This exposes the API endpoints that Dify relies on. The `--host 0.0.0.0` option makes the API accessible from any IP address. `--debug` enables debug mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_44\n\nLANGUAGE: Bash\nCODE:\n```\nflask run --host 0.0.0.0 --port=5001 --debug\n```\n\n----------------------------------------\n\nTITLE: Test Module Structure (Shell)\nDESCRIPTION: This shell output represents the directory structure for the test module, specifically for the Anthropic provider. It includes `test_llm.py` for testing the LLM functionality and `test_provider.py` for testing the provider itself. This illustrates the recommended way to organize tests for new providers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/new-provider.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n.\n├── __init__.py\n├── anthropic\n│   ├── __init__.py\n│   ├── test_llm.py       # LLM 测试\n│   └── test_provider.py  # 供应商测试\n```\n\n----------------------------------------\n\nTITLE: Python: validate_provider_credentials method\nDESCRIPTION: This Python code shows an example of `validate_provider_credentials` method, which used to Validate provider credentials.\n    You can choose any validate_credentials method of model type or implement validate method by yourself, such as: get model list api.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/new-provider.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Workflow Configuration\nDESCRIPTION: This code snippet defines a GitHub Actions workflow that automates plugin packaging, pushing to a forked repository, branch creation, and PR creation.  It is triggered on pushes to the `main` branch of the plugin source repository. It requires the `PLUGIN_ACTION` secret to be configured.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/plugin-auto-publish-pr.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# .github/workflows/auto-pr.yml\nname: Auto Create PR on Main Push\n\non:\n  push:\n    branches: [ main ]  # Trigger on push to main\n\njobs:\n  create_pr: # Renamed job for clarity\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Print working directory # Kept for debugging\n        run: |\n          pwd\n          ls -la\n\n      - name: Download CLI tool\n        run: |\n          # Create bin directory in runner temp\n          mkdir -p $RUNNER_TEMP/bin\n          cd $RUNNER_TEMP/bin\n\n          # Download CLI tool\n          wget https://github.com/langgenius/dify-plugin-daemon/releases/download/0.0.6/dify-plugin-linux-amd64\n          chmod +x dify-plugin-linux-amd64\n\n          # Show download location and file\n          echo \"CLI tool location:\"\n          pwd\n          ls -la dify-plugin-linux-amd64\n\n      - name: Get basic info from manifest # Changed step name and content\n        id: get_basic_info\n        run: |\n          PLUGIN_NAME=$(grep \"^name:\" manifest.yaml | cut -d' ' -f2)\n          echo \"Plugin name: $PLUGIN_NAME\"\n          echo \"plugin_name=$PLUGIN_NAME\" >> $GITHUB_OUTPUT\n\n          VERSION=$(grep \"^version:\" manifest.yaml | cut -d' ' -f2)\n          echo \"Plugin version: $VERSION\"\n          echo \"version=$VERSION\" >> $GITHUB_OUTPUT\n\n          # If the author's name is not your github username, you can change the author here\n          AUTHOR=$(grep \"^author:\" manifest.yaml | cut -d' ' -f2)\n          echo \"Plugin author: $AUTHOR\"\n          echo \"author=$AUTHOR\" >> $GITHUB_OUTPUT\n\n      - name: Package Plugin\n        id: package\n        run: |\n          # Use the downloaded CLI tool to package\n          cd $GITHUB_WORKSPACE\n          # Use variables for package name\n          PACKAGE_NAME=\"${{ steps.get_basic_info.outputs.plugin_name }}-${{ steps.get_basic_info.outputs.version }}.difypkg\"\n          # Use CLI from runner temp\n          $RUNNER_TEMP/bin/dify-plugin-linux-amd64 plugin package . -o \"$PACKAGE_NAME\"\n\n          # Show packaging result\n          echo \"Package result:\"\n          ls -la \"$PACKAGE_NAME\"\n          echo \"package_name=$PACKAGE_NAME\" >> $GITHUB_OUTPUT\n\n          # Show full file path and directory structure (kept for debugging)\n          echo \"\\\\nFull file path:\"\n          pwd\n          echo \"\\\\nDirectory structure:\"\n          tree || ls -R\n\n      - name: Checkout target repo\n        uses: actions/checkout@v3\n        with:\n          # Use author variable for repository\n          repository: ${{steps.get_basic_info.outputs.author}}/dify-plugins\n          path: dify-plugins\n          token: ${{ secrets.PLUGIN_ACTION }}\n          fetch-depth: 1 # Fetch only the last commit to speed up checkout\n          persist-credentials: true # Persist credentials for subsequent git operations\n\n      - name: Prepare and create PR\n        run: |\n          # Debug info (kept)\n          echo \"Debug: Current directory $(pwd)\"\n          # Use variable for package name\n          PACKAGE_NAME=\"${{ steps.get_basic_info.outputs.plugin_name }}-${{ steps.get_basic_info.outputs.version }}.difypkg\"\n          echo \"Debug: Package name: $PACKAGE_NAME\"\n          ls -la\n\n          # Move the packaged file to the target directory using variables\n          mkdir -p dify-plugins/${{ steps.get_basic_info.outputs.author }}/${{ steps.get_basic_info.outputs.plugin_name }}\n          mv \"$PACKAGE_NAME\" dify-plugins/${{ steps.get_basic_info.outputs.author }}/${{ steps.get_basic_info.outputs.plugin_name }}/\n\n          # Enter the target repository directory\n          cd dify-plugins\n\n          # Configure git\n          git config user.name \"GitHub Actions\"\n          git config user.email \"actions@github.com\"\n\n          # Ensure we are on the latest main branch\n          git fetch origin main\n          git checkout main\n          git pull origin main\n\n          # Create and switch to a new branch using variables and new naming convention\n          BRANCH_NAME=\"bump-${{ steps.get_basic_info.outputs.plugin_name }}-plugin-${{ steps.get_basic_info.outputs.version }}\"\n          git checkout -b \"$BRANCH_NAME\"\n\n          # Add and commit changes (using git add .)\n          git add .\n          git status # for debugging\n          # Use variables in commit message\n          git commit -m \"bump ${{ steps.get_basic_info.outputs.plugin_name }} plugin to version ${{ steps.get_basic_info.outputs.version }}\"\n\n          # Push to remote (use force just in case the branch existed before from a failed run)\n          git push -u origin \"$BRANCH_NAME\" --force\n\n          # Confirm branch has been pushed and wait for sync (GitHub API might need a moment)\n          git branch -a\n          echo \"Waiting for branch to sync...\"\n          sleep 10  # Wait 10 seconds for branch sync\n\n      - name: Create PR via GitHub API\n        env:\n          GH_TOKEN: ${{ secrets.PLUGIN_ACTION }} # Use the provided token for authentication\n        run: |\n          gh pr create \\\n            --repo langgenius/dify-plugins \\\n            --head \"${{ steps.get_basic_info.outputs.author }}:${{ steps.get_basic_info.outputs.plugin_name }}-${{ steps.get_basic_info.outputs.version }}\" \\\n            --base main \\\n            --title \"bump ${{ steps.get_basic_info.outputs.plugin_name }} plugin to version ${{ steps.get_basic_info.outputs.version }}\" \\\n            --body \"bump ${{ steps.get_basic_info.outputs.plugin_name }} plugin package to version ${{ steps.get_basic_info.outputs.version }}\n\n            Changes:\n            - Updated plugin package file\" || echo \"PR already exists or creation skipped.\" # Handle cases where PR already exists\n\n      - name: Print environment info # Kept for debugging\n        run: |\n          echo \"GITHUB_WORKSPACE: $GITHUB_WORKSPACE\"\n          echo \"Current directory contents:\"\n          ls -R\n```\n\n----------------------------------------\n\nTITLE: Verifying Docker Compose Installation\nDESCRIPTION: This command checks the installed Docker Compose version, confirming that Docker Compose has been successfully installed. It outputs the Docker Compose version information to the console.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose --version\n```\n\n----------------------------------------\n\nTITLE: Reload Systemd and Restart Ollama\nDESCRIPTION: These commands reload the `systemd` daemon and restart the Ollama service after modifying the service file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_82\n\nLANGUAGE: Shell\nCODE:\n```\nsystemctl daemon-reload\nsystemctl restart ollama\n```\n\n----------------------------------------\n\nTITLE: Configuring environment variables\nDESCRIPTION: This snippet demonstrates creating a .env file with the necessary environment variables.  These variables include the Twilio phone number, Account SID, Auth Token, Dify API server address, and Dify API key. These values are required for the application to connect to Twilio and Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/dify-on-whatsapp.md#_snippet_1\n\nLANGUAGE: env\nCODE:\n```\nTWILIO_NUMBER=+14155238886\nTWILIO_ACCOUNT_SID=<4で取得したTwilio Account SID>\nTWILIO_AUTH_TOKEN=<4で取得したTwilio Auth Token>\nDIFY_URL=<3で取得したDify APIサーバーアドレス>\nDIFY_API_KEY=<3で取得したDify APIキー>\n```\n\n----------------------------------------\n\nTITLE: JSON Schema for Math Reasoning (Chain-of-Thought)\nDESCRIPTION: This JSON Schema defines the structure for capturing step-by-step mathematical reasoning. It contains an array of 'steps', each with an 'explanation' and 'output', and a 'final_answer'.  The `steps` array enables capturing the chain of thought process while solving a math problem, and the `final_answer` represents the result.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/how-to-use-json-schema-in-dify.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"math_reasoning\",\n    \"description\": \"数学的推論の手順と最終回答を記録します\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"steps\": {\n                \"type\": \"array\",\n                \"description\": \"推論ステップの配列\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"explanation\": {\n                            \"type\": \"string\",\n                            \"description\": \"推論ステップの説明\"\n                        },\n                        \"output\": {\n                            \"type\": \"string\",\n                            \"description\": \"推論ステップの出力\"\n                        }\n                    },\n                    \"required\": [\"explanation\", \"output\"],\n                    \"additionalProperties\": false\n                }\n            },\n            \"final_answer\": {\n                \"type\": \"string\",\n                \"description\": \"数学問題の最終回答\"\n            }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\"steps\", \"final_answer\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: LLM Tool Invocation with OpenAI Python\nDESCRIPTION: This code demonstrates how to invoke an LLM (OpenAI's gpt-4o-mini) using the `LLMTool` within the Dify plugin. It constructs an `LLMModelConfig` with the provider, model name, mode, and completion parameters. The `prompt_messages` are constructed using SystemPromptMessage and UserPromptMessage, and the `query` is retrieved from `tool_parameters`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom typing import Any\n\nfrom dify_plugin import Tool\nfrom dify_plugin.entities.model.llm import LLMModelConfig\nfrom dify_plugin.entities.tool import ToolInvokeMessage\nfrom dify_plugin.entities.model.message import SystemPromptMessage, UserPromptMessage\n\nclass LLMTool(Tool):\n    def _invoke(self, tool_parameters: dict[str, Any]) -> Generator[ToolInvokeMessage]:\n        response = self.session.model.llm.invoke(\n            model_config=LLMModelConfig(\n                provider='openai',\n                model='gpt-4o-mini',\n                mode='chat',\n                completion_params={}\n            ),\n            prompt_messages=[\n                SystemPromptMessage(\n                    content='you are a helpful assistant'\n                ),\n                UserPromptMessage(\n                    content=tool_parameters.get('query')\n                )\n            ],\n            stream=True\n        )\n\n        for chunk in response:\n            if chunk.delta.message:\n                assert isinstance(chunk.delta.message.content, str)\n                yield self.create_text_message(text=chunk.delta.message.content)\n```\n\n----------------------------------------\n\nTITLE: Parameter Extractor Node Usage Example\nDESCRIPTION: This snippet demonstrates how to use the ParameterExtractor node to extract a person's name from a given text. It creates a ParameterConfig for the 'name' parameter, specifies an LLM model, provides a query string, and gives an instruction to guide the extraction process. The extracted name is then sent as a text message.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom dify_plugin.entities.tool import ToolInvokeMessage\nfrom dify_plugin import Tool\nfrom dify_plugin.entities.workflow_node import ModelConfig, ParameterConfig\n\nclass ParameterExtractorTool(Tool):\n    def _invoke(\n        self, tool_parameters: dict\n    ) -> Generator[ToolInvokeMessage, None, None]:\n        response = self.session.workflow_node.parameter_extractor.invoke(\n            parameters=[\n                ParameterConfig(\n                    name=\"name\",\n                    description=\"name of the person\",\n                    required=True,\n                    type=\"string\",\n                )\n            ],\n            model=ModelConfig(\n                provider=\"langgenius/openai/openai\",\n                name=\"gpt-4o-mini\",\n                completion_params={},\n            ),\n            query=\"My name is John Doe\",\n            instruction=\"Extract the name of the person\",\n        )\n\n        yield self.create_text_message(response.outputs[\"name\"])\n```\n\n----------------------------------------\n\nTITLE: LLM Invocation Method Signature\nDESCRIPTION: This python code snippet defines the signature of the _invoke method for calling a large language model within a Dify plugin. It specifies the input parameters, including model name, credentials, prompt messages, model parameters, tools, stop words, stream, and user ID, and the return type, which can be a full response or a stream response chunk generator.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            prompt_messages: list[PromptMessage], model_parameters: dict,\n            tools: Optional[list[PromptMessageTool]] = None, stop: Optional[list[str]] = None,\n            stream: bool = True, user: Optional[str] = None) \\\n        -> Union[LLMResult, Generator]:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param model_parameters: model parameters\n    :param tools: tools for tool calling\n    :param stop: stop words\n    :param stream: is stream response\n    :param user: unique user id\n    :return: full response or stream response chunk generator result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Copy Environment Variables File\nDESCRIPTION: Copies the example environment variable file to .env. This allows you to configure the API service with specific settings. The .env file should be customized to match the deployment environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_39\n\nLANGUAGE: Bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Initializing a new Bundle plugin project with dify in /usr/local/bin\nDESCRIPTION: This command initializes a new Dify Bundle plugin project in the current directory using the dify CLI tool. It assumes the 'dify' binary has been renamed and copied to the `/usr/local/bin` directory, making it accessible globally. The user will be prompted for plugin name, author information, and description.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/bundle.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify bundle init\n```\n\n----------------------------------------\n\nTITLE: AssistantPromptMessage Class Definition in Python\nDESCRIPTION: Defines the AssistantPromptMessage class, inheriting from PromptMessage. It sets the role to ASSISTANT and includes a tool_calls attribute, a list of ToolCall objects, representing the tool calls generated by the assistant.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nclass AssistantPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for assistant prompt message.\n    \"\"\"\n    class ToolCall(BaseModel):\n        \"\"\"\n        Model class for assistant prompt message tool call.\n        \"\"\"\n        class ToolCallFunction(BaseModel):\n            \"\"\"\n            Model class for assistant prompt message tool call function.\n            \"\"\"\n            name: str  # ツール名\n            arguments: str  # ツールパラメータ\n\n        id: str  # ツールID。OpenAI tool callの場合のみ有効で、ツール呼び出しのユニークIDです。同じツールを複数回呼び出すことができます。\n        type: str  # デフォルト function\n        function: ToolCallFunction  # ツール呼び出し情報\n\n    role: PromptMessageRole = PromptMessageRole.ASSISTANT\n    tool_calls: list[ToolCall] = []  # モデルの返信としてのツール呼び出し結果（`tools`を渡した場合のみ、モデルがツール呼び出しが必要と判断した場合に返されます）\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository (Specific Version)\nDESCRIPTION: This command clones the Dify repository from GitHub, specifically targeting the 0.15.3 branch. This ensures that a specific version of the Dify source code is obtained for deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git --branch 0.15.3\n```\n\n----------------------------------------\n\nTITLE: Cloning a Stable Diffusion Model\nDESCRIPTION: This command clones a Stable Diffusion model from Hugging Face using `git lfs`. This makes the model available to Stable Diffusion WebUI. It assumes that `git lfs` is installed and configured.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/stable-diffusion.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/JamesFlare/pastel-mix\n```\n\n----------------------------------------\n\nTITLE: RerankDocument Data Model Definition in Python\nDESCRIPTION: Defines a data model for representing a reranked document, including its original index, text content, and score. This model is used as part of the reranking process within the Dify platform.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nclass RerankDocument(BaseModel):\n    \"\"\"\n    Model class for rerank document.\n    \"\"\"\n    index: int  # 元の文書の順番\n    text: str  # 文書のテキスト内容\n    score: float  # スコア\n```\n\n----------------------------------------\n\nTITLE: Configure Plugin Settings in YAML\nDESCRIPTION: This YAML configuration file defines the settings required for the Slack Bot plugin, including the bot token, retry options, and the Dify app to be used for handling messages. The `app-selector` type allows the user to select a Dify app to forward Slack messages to.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsettings:\n  - name: bot_token\n    type: secret-input\n    required: true\n    label:\n      en_US: Bot Token\n      zh_Hans: Bot Token\n      pt_BR: Token do Bot\n      ja_JP: Bot Token\n    placeholder:\n      en_US: Please input your Bot Token\n      zh_Hans: 请输入你的 Bot Token\n      pt_BR: Por favor, insira seu Token do Bot\n      ja_JP: ボットトークンを入力してください\n  - name: allow_retry\n    type: boolean\n    required: false\n    label:\n      en_US: Allow Retry\n      zh_Hans: 允许重试\n      pt_BR: Permitir Retentativas\n      ja_JP: 再試行を許可\n    default: false\n  - name: app\n    type: app-selector\n    required: true\n    label:\n      en_US: App\n      zh_Hans: 应用\n      pt_BR: App\n      ja_JP: アプリ\n    placeholder:\n      en_US: the app you want to use to answer Slack messages\n      zh_Hans: 你想要用来回答 Slack 消息的应用\n      pt_BR: o app que você deseja usar para responder mensagens do Slack\n      ja_JP: あなたが Slack メッセージに回答するために使用するアプリ\nendpoints:\n  - endpoints/slack.yaml\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Configuration (yaml)\nDESCRIPTION: This Docker Compose override file configures environment variables for the `plugin_daemon` service to enable third-party signature verification and specify the path to the public key file. It sets `FORCE_VERIFYING_SIGNATURE`, `THIRD_PARTY_SIGNATURE_VERIFICATION_ENABLED`, and `THIRD_PARTY_SIGNATURE_VERIFICATION_PUBLIC_KEYS`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n  plugin_daemon:\n    environment:\n      FORCE_VERIFYING_SIGNATURE: true\n      THIRD_PARTY_SIGNATURE_VERIFICATION_ENABLED: true\n      THIRD_PARTY_SIGNATURE_VERIFICATION_PUBLIC_KEYS: /app/storage/public_keys/your_key_pair.public.pem\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Containers\nDESCRIPTION: This command starts the Docker containers defined in the docker-compose.yml file in detached mode.  It assumes that the current working directory is the docker directory. The docker compose file defines the services and dependencies needed to run the Dify-on-Dingtalk application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-dingtalk.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# 假设现在 pwd 是在 docker 文件夹下\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: System Prompt for Image Generation Agent\nDESCRIPTION: This prompt instructs the Agent to use the stability_text2image tool to draw content based on user input. It serves as a system-level instruction, informing the Agent to call the Stability tool when a user requests an image to be drawn.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/workshop/basic/build-ai-image-generation-app.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n根据用户的提示，使用工具 stability_text2image 绘画指定内容\n```\n\n----------------------------------------\n\nTITLE: Starting the Worker Service (Bash/Linux macOS)\nDESCRIPTION: This command starts the Celery worker service on Linux or macOS, which consumes asynchronous tasks from the queue. It specifies the app, concurrency, log level, and queues to process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run celery -A app.celery worker -P gevent -c 1 --loglevel INFO -Q dataset,generation,mail,ops_trace\n```\n\n----------------------------------------\n\nTITLE: Checking Docker Container Status\nDESCRIPTION: This command displays the status of all Docker containers defined in the Docker Compose configuration. It shows whether the containers are running and their respective ports.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose ps\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Web Interface with IP and Port - Bash\nDESCRIPTION: This code snippet shows how to access the Dify web interface when using IP address and port. Replace `your_server_ip` with the server's IP address and `8088` with the configured port.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/bt-panel.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# `IP+ポート`を介してアクセスする場合\nhttp://your_server_ip:8088/\n```\n\n----------------------------------------\n\nTITLE: Ngrok Initialization\nDESCRIPTION: These shell commands are for initializing Ngrok by unzipping the downloaded file and adding the authtoken.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/README.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n$ unzip /path/to/ngrok.zip\n$ ./ngrok config add-authtoken 你的Token\n```\n\n----------------------------------------\n\nTITLE: Configure Remote Debugging (.env)\nDESCRIPTION: This section configures the environment variables for remote debugging of the plugin. It specifies the installation method, remote host, port, and key.  The `REMOTE_INSTALL_KEY` is obtained from the Dify plugin management page.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: Define Prompt Message Tool Model (Python)\nDESCRIPTION: Defines a Pydantic model `PromptMessageTool` representing a tool that can be used in a prompt message. It includes attributes for the tool's `name`, `description`, and `parameters` (as a dictionary).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageTool(BaseModel):\n    \"\"\"\n    Model class for prompt message tool.\n    \"\"\"\n    name: str\n    description: str\n    parameters: dict\n```\n\n----------------------------------------\n\nTITLE: Packaging a Bundle Project\nDESCRIPTION: This command packages the bundle plugin project into a `bundle.difybndl` file in the current directory. The path to the bundle project directory is specified as an argument.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/bundle.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndify-plugin bundle package ./bundle\n```\n\n----------------------------------------\n\nTITLE: Concatenate Two Lists with Python in Dify\nDESCRIPTION: This Python code snippet demonstrates how to concatenate two lists (`knowledge1` and `knowledge2`) using the Dify Code Node. It returns a new list that is the concatenation of the two input lists, accessible via the 'result' key in the returned dictionary.  The inputs are two lists, and the output is a list.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_69\n\nLANGUAGE: Python\nCODE:\n```\ndef main(knowledge1: list, knowledge2: list) -> list:\n    return {\n        # Note to declare 'result' in the output variables\n        'result': knowledge1 + knowledge2\n    }\n```\n\n----------------------------------------\n\nTITLE: Start Dify with Docker Compose\nDESCRIPTION: This command navigates to the 'dify/docker' directory and starts all services defined in the docker-compose.yml file in detached mode. It's the primary command to bring up the Dify application and its dependencies.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\ncd dify/docker\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Start Xinference Locally\nDESCRIPTION: Starts Xinference in local deployment mode. This command initializes the Xinference server, making it accessible for model deployment and inference.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/xinference.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ xinference-local\n```\n\n----------------------------------------\n\nTITLE: CSP Whitelist Example\nDESCRIPTION: An example of the domains that are automatically included in the Content Security Policy (CSP) whitelist when the CSP_WHITELIST environment variable is enabled. These domains are commonly used for analytics, error reporting, and API access.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/environments.md#_snippet_3\n\nLANGUAGE: url\nCODE:\n```\n*.sentry.io http://localhost:* http://127.0.0.1:* https://analytics.google.com https://googletagmanager.com https://api.github.com\n```\n\n----------------------------------------\n\nTITLE: Merging Knowledge Data with Python\nDESCRIPTION: This Python code snippet merges two lists, `knowledge1` and `knowledge2`, into a single list. It takes two lists as input and returns a dictionary containing the concatenated list under the key 'result'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/code.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef main(knowledge1: list, knowledge2: list) -> list:\n    return {\n        # 出力変数にresultを宣言することに注意\n        'result': knowledge1 + knowledge2\n    }\n```\n\n----------------------------------------\n\nTITLE: manifest.yaml Example\nDESCRIPTION: Shows an example `manifest.yaml` file, which contains the `version`, `author`, and `name` fields. These are essential for the automated plugin publishing workflow to function correctly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/plugin-auto-publish-pr.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nversion: 0.0.x  # バージョン番号\nauthor: your-github-username  # GitHubユーザー名/作者名\nname: your-plugin-name  # プラグイン名\n```\n\n----------------------------------------\n\nTITLE: Uploading Private Key File to Cloud Storage (Docker)\nDESCRIPTION: These snippets demonstrate how to upload the private key file and local files to cloud storage using docker. This is needed to migrate from local storage to cloud storage.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/_book/zh_CN/learn-more/faq/install-faq.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask upload-private-key-file-to-cloud-storage\ndocker exec -it docker-api-1 flask upload-local-files-to-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Backup Docker Volumes\nDESCRIPTION: This snippet stops the Docker containers and creates a tar archive of the Docker volumes directory.  It is part of the data backup process when upgrading Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/dify-premium.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\ntar -cvf volumes-$(date +%s).tgz volumes\n```\n\n----------------------------------------\n\nTITLE: Update Document with Text - Dify API (JSON Response)\nDESCRIPTION: This is the JSON response received after successfully updating a document with text via the Dify API. It contains details about the updated document, such as its ID, position, data source type, name, creation timestamp, indexing status, and other relevant metadata.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"document\": {\n    \"id\": \"\",\n    \"position\": 1,\n    \"data_source_type\": \"upload_file\",\n    \"data_source_info\": {\n      \"upload_file_id\": \"\"\n    },\n    \"dataset_process_rule_id\": \"\",\n    \"name\": \"name.txt\",\n    \"created_from\": \"api\",\n    \"created_by\": \"\",\n    \"created_at\": 1695308667,\n    \"tokens\": 0,\n    \"indexing_status\": \"waiting\",\n    \"error\": null,\n    \"enabled\": true,\n    \"disabled_at\": null,\n    \"disabled_by\": null,\n    \"archived\": false,\n    \"display_status\": \"queuing\",\n    \"word_count\": 0,\n    \"hit_count\": 0,\n    \"doc_form\": \"text_model\"\n  },\n  \"batch\": \"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Credentials for Provider (YAML)\nDESCRIPTION: This YAML snippet defines the `credentials_for_provider` section in the tool vendor file. It specifies the API key required for the SerpApi service. It includes the type of input (secret-input), whether it is required, localized labels, placeholders, help text, and a URL to obtain the API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: Dify\n  name: google\n  label:\n    en_US: Google\n    zh_Hans: Google\n    pt_BR: Google\n  description:\n    en_US: Google\n    zh_Hans: GoogleSearch\n    pt_BR: Google\n  icon: icon.svg\n  tags:\n    - search\ncredentials_for_provider: #Add credentials_for_provider field\n  serpapi_api_key:\n    type: secret-input\n    required: true\n    label:\n      en_US: SerpApi API key\n      zh_Hans: SerpApi API key\n    placeholder:\n      en_US: Please input your SerpApi API key\n      zh_Hans: 请输入你的 SerpApi API key\n    help:\n      en_US: Get your SerpApi API key from SerpApi\n      zh_Hans: 从 SerpApi 获取您的 SerpApi API key\n    url: https://serpapi.com/manage-api-key\ntools:\n  - tools/google_search.yaml\nextra:\n  python:\n    source: google.py\n```\n\n----------------------------------------\n\nTITLE: Config.json Example\nDESCRIPTION: An example of the config.json file that needs to be created in the root directory of the project. It includes configuration options such as the Dify API base URL, API key, application type (chatbot, agent or workflow), and other WeChat-related settings.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dify_api_base\": \"https://api.dify.ai/v1\",\n  \"dify_api_key\": \"app-xxx\",\n  \"dify_app_type\": \"chatbot\",\n  \"channel_type\": \"wx\",\n  \"model\": \"dify\",\n  \"single_chat_prefix\": [\"\"],\n  \"single_chat_reply_prefix\": \"\",\n  \"group_chat_prefix\": [\"@bot\"],\n  \"group_name_white_list\": [\"ALL_GROUP\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Travel Assistant Example Output\nDESCRIPTION: Presents an example output for the travel assistant, demonstrating the desired format and content including hotel recommendations and daily itineraries. It's used to guide the Agent's writing style and ensure output aligns with expectations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/workshop/basic/travel-assistant.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n## 示例\n\n### 详细旅行计划\n\n**酒店推荐**\n1. 肯辛顿酒店 (了解更多：www.doylecollection.com/hotels/the-kensington-hotel)\n- 评分：4.6⭐\n- 价格：每晚约350美元\n- 简介：坐落在一座摄政时期的联排别墅中，这家优雅的酒店距离南肯辛顿地铁站5分钟步行路程，距离维多利亚和阿尔伯特博物馆10分钟步行路程。\n\n2. 伦勃朗酒店 (了解更多：www.sarova-rembrandthotel.com)\n- 评分：4.3⭐\n- 价格：每晚约130美元\n- 简介：建于1911年，最初是哈罗德百货公司（距离0.4英里）的公寓，这家现代化酒店坐落在维多利亚和阿尔伯特博物馆对面，距离南肯辛顿地铁站（直达希思罗机场）5分钟步行路程。\n\n**第1天 - 抵达和安顿**\n- **上午**：抵达机场。欢迎来到你的冒险之旅！我们的代表将在机场迎接你，确保你顺利入住。\n- **下午**：入住酒店，稍作休息，恢复精力。\n- **晚上**：在住宿周边进行轻松的步行游览，熟悉当地环境。发现附近的用餐选择，享受愉快的第一顿晚餐。\n\n**第2天 - 文化与自然之旅**\n- **上午**：从帝国理工学院开始你的一天，这是世界顶尖的学府之一。享受一次校园导览。\n- **下午**：选择参观自然历史博物馆（以其引人入胜的展览而闻名）或维多利亚和阿尔伯特博物馆（庆祝艺术和设计）。之后，在宁静的海德公园放松，也许还可以在蛇形湖上乘船游览。\n- **晚上**：探索当地美食。我们推荐你在传统的英国酒吧享用晚餐。\n\n**附加服务：**\n- **礼宾服务**：在你停留期间，我们的礼宾服务随时可以协助预订餐厅、购买门票、安排交通，以及满足任何特殊要求，以提升你的体验。\n- **24/7支持**：我们提供全天候支持，以解决你在旅行中可能遇到的任何问题或需求。\n\n祝你旅途愉快，满载丰富经历和美好回忆！\n\n```\n\n----------------------------------------\n\nTITLE: Getting VM IP Address\nDESCRIPTION: This command displays the IP addresses assigned to the network interfaces of the Linux VM. This is used to determine the VM's public IP address, which is needed to access SearXNG from Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nip addr show\n```\n\n----------------------------------------\n\nTITLE: Cloning a Git Repository\nDESCRIPTION: This command clones a forked Git repository to your local machine. It's essential for setting up the Dify project for development and allows you to make and contribute changes. Replace `<github_username>` with your GitHub username.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_25\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone git@github.com:<github_username>/dify.git\n```\n\n----------------------------------------\n\nTITLE: Define UserPromptMessage Class in Python\nDESCRIPTION: This code defines the `UserPromptMessage` class, which inherits from `PromptMessage`.  It sets the `role` attribute to `PromptMessageRole.USER`, indicating that this message represents a user's input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nclass UserPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for user prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.USER\n```\n\n----------------------------------------\n\nTITLE: Referencing File Variables in LLM Node\nDESCRIPTION: Demonstrates how to utilize file variables directly within an LLM node. This is suitable for specific file types like images, where the LLM can analyze the visual data after enabling the vision function in the LLM node. This assumes only image files are included, and filtering may be needed otherwise.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/file-upload.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Checking Docker Container Status\nDESCRIPTION: This command displays the status of the Docker containers managed by Docker Compose, ensuring all necessary services are running correctly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose ps\n```\n\n----------------------------------------\n\nTITLE: Starting SearXNG Docker Containers\nDESCRIPTION: This command starts the SearXNG, Redis, and Caddy Docker containers in detached mode.  It reads the configuration from the `docker-compose.yaml` file.  Requires Docker Compose to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/searxng.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Variable Configuration for Dify AI Agent\nDESCRIPTION: Defines variable configuration for user input, including destination, travel days, and budget.  These variables are designed to standardize user input and filter irrelevant details.  It specifies the variable key, type, and user-friendly field name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/workshop/basic/travel-assistant.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n| 变量Key       | 变量类型 | 字段名称 | 可选 |\n| ----------- | ---- | ---- | -- |\n| destination | 文本   | 目的地  | 是  |\n| day         | 文本   | 旅行天数 | 是  |\n| budget      | 文本   | 旅行预算 | 是  |\n\n```\n\n----------------------------------------\n\nTITLE: Package Plugin (CLI)\nDESCRIPTION: This command packages the plugin into a `.difypkg` file using the Dify plugin CLI. The `./neko` argument should be replaced with the actual path to the plugin's project directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# 将 ./neko 替换为插件项目的实际路径\n\ndify plugin package ./neko\n```\n\n----------------------------------------\n\nTITLE: Cloning LocalAI Repository (Shell)\nDESCRIPTION: This code snippet provides shell commands to clone the LocalAI repository from GitHub and navigate to the `langchain-chroma` example directory. This is the first step in deploying LocalAI for local LLM inference and embedding capabilities.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n$ git clone https://github.com/go-skynet/LocalAI\n$ cd LocalAI/examples/langchain-chroma\n```\n\n----------------------------------------\n\nTITLE: Invoke Error Mapping\nDESCRIPTION: Defines a mapping between model invocation errors and the unified InvokeError types. This allows the runtime to handle different errors consistently, enabling Dify to perform specific post-processing actions based on the type of error encountered.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/customizable-model.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Backup Docker Compose YAML (Optional) - Bash\nDESCRIPTION: This command creates a backup of the docker-compose.yaml file by appending a timestamp to the filename. It is an optional step to safeguard the original configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/migration/migrate-to-v1.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ncp docker-compose.yaml docker-compose.yaml.$(date +%s).bak\n```\n\n----------------------------------------\n\nTITLE: Define provider_credential_schema for Server URL and Model UID\nDESCRIPTION: This YAML defines the schema for obtaining the server URL and Model UID for a locally deployed Xinference instance. The server URL specifies the Xinference endpoint and the Model UID identifies the specific model to be used.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n  - variable: server_url\n    label:\n      zh_Hans: 服务器 URL\n      en_US: Server url\n    type: text-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入 Xinference 的服务器地址，如 https://example.com/xxx\n      en_US: Enter the url of your Xinference, for example https://example.com/xxx\n\n  - variable: model_uid\n    label:\n      zh_Hans: 模型 UID\n      en_US: Model uid\n    type: text-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入您的 Model UID\n      en_US: Enter the model uid\n```\n\n----------------------------------------\n\nTITLE: Chatflow App: Combined File Type Processing\nDESCRIPTION: Describes the configuration for processing both document and image files uploaded simultaneously in a Chatflow app. It utilizes list operator nodes to filter and extract different file types for specific LLM processing.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/additional-features.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n1. Enable Features and select 'image' and 'document' as the file types.\n2. Add two list operator nodes to extract image and document variables using 'filtering' condition.\n3. Extract the document file variable and pass it to the 'Document Extractor' node, and extract the image file variable and pass it to the LLM node.\n4. Add an 'Answer' node and fill in the output variable of the LLM node.\n```\n\n----------------------------------------\n\nTITLE: Cloning a Stable Diffusion Model using Git LFS\nDESCRIPTION: Clones a Stable Diffusion model using Git LFS. This example uses the pastel-mix model, but any compatible model can be used. This model will be placed in the `models` directory of `stable-diffusion-webui`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/stable-diffusion.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/JamesFlare/pastel-mix\n```\n\n----------------------------------------\n\nTITLE: Navigating to API Directory\nDESCRIPTION: Changes the current working directory to the 'api' directory. This is necessary to execute the subsequent API-related commands.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\ncd api\n```\n\n----------------------------------------\n\nTITLE: Creating Image Message in Dify (Python)\nDESCRIPTION: This code snippet demonstrates how to create an image message within the Dify `Tool` class. It takes an image URL as input and returns a `ToolInvokeMessage` object. The `save_as` parameter allows specifying a custom filename during the download.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/advanced-tool-integration.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    def create_image_message(self, image: str, save_as: str = '') -> ToolInvokeMessage:\n        \"\"\"\n            create an image message\n\n            :param image: the url of the image\n            :return: the image message\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Anthropic Model Plugin Example\nDESCRIPTION: This bash snippet shows an example of the Anthropic model plugin structure, focusing on LLMs. The structure includes a list of specific Claude models, showcasing how different models are organized under the llm category within the Anthropic vendor.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n- Anthropic\n    - llm\n        claude-3-5-sonnet-20240620\n        claude-3-haiku-20240307\n        claude-3-opus-20240229\n        claude-3-sonnet-20240229\n        claude-instant-1.2\n        claude-instant-1\n```\n\n----------------------------------------\n\nTITLE: Migrate local files using Flask (Local)\nDESCRIPTION: This command uploads the local files to the configured cloud storage. It is intended for use with local source code deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nflask upload-local-files-to-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Starting Worker Service (Linux/macOS Bash)\nDESCRIPTION: Starts the Celery worker service to consume asynchronous tasks from the queue on Linux or macOS. It uses the gevent pool and specifies the queues to listen on.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\ncelery -A app.celery worker -P gevent -c 1 --loglevel INFO -Q dataset,generation,mail,ops_trace\n```\n\n----------------------------------------\n\nTITLE: Set Python Version Globally\nDESCRIPTION: Sets the global Python version to 3.12 using pyenv. This ensures that all subsequent Python commands use the correct version. Requires pyenv to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_37\n\nLANGUAGE: Bash\nCODE:\n```\npyenv global 3.12\n```\n\n----------------------------------------\n\nTITLE: Navigating to API directory (Bash)\nDESCRIPTION: Changes the current directory to the 'api' directory. This is a necessary step to prepare the API server deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\ncd api\n```\n\n----------------------------------------\n\nTITLE: Simple Provider Credential Validation in Python\nDESCRIPTION: This shows a simple implementation of the `validate_provider_credentials` method for custom models. It simply passes without performing any validation, suitable for scenarios where credential validation is not required.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass XinferenceProvider(Provider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Install Xinference via PyPI\nDESCRIPTION: This command installs Xinference with all dependencies using pip. Xinference is used for serving language, speech recognition, and multimodal models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install \"xinference[all]\"\n```\n\n----------------------------------------\n\nTITLE: Check Docker Container ID - Bash\nDESCRIPTION: This command lists the running Docker containers, including their IDs, images, commands, status, ports, and names.  The output is used to identify the docker-api container for subsequent steps.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/migration/migrate-to-v1.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker ps\nCONTAINER ID   IMAGE                                       COMMAND                  CREATED       STATUS                 PORTS                                                                                                                             NAMES\n417241cd****   nginx:latest                                \"sh -c 'cp /docker-e…\"   3 hours ago   Up 3 hours             0.0.0.0:80->80/tcp, :::80->80/tcp, 0.0.0.0:443->443/tcp, :::443->443/tcp                                                          docker-nginx-1\nf84aa773****   langgenius/dify-api:1.0.0                   \"/bin/bash /entrypoi…\"   3 hours ago   Up 3 hours             5001/tcp                                                                                                                          docker-worker-1\na3cb19c2****   langgenius/dify-api:1.0.0                   \"/bin/bash /entrypoi…\"   3 hours ago   Up 3 hours             5001/tcp                                                                                                                          docker-api-1\n```\n\n----------------------------------------\n\nTITLE: Resetting Encryption Key Pair via Flask Command in Docker\nDESCRIPTION: This command is used to reset the encryption key pair for Dify when deployed using Docker Compose. It executes a Flask command within the `docker-api-1` container to reset the keys used for encrypting sensitive data, such as large model keys. This is necessary when the `api/storage/privkeys` file is lost or corrupted.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-encrypt-key-pair\n```\n\n----------------------------------------\n\nTITLE: Changing Button Background Color with CSS\nDESCRIPTION: This CSS code snippet demonstrates how to change the background color of the Dify Chatbot Bubble Button to `#ABCDEF` by overriding the `--dify-chatbot-bubble-button-bg-color` CSS variable.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-publishing/embedding-in-websites.md#_snippet_2\n\nLANGUAGE: css\nCODE:\n```\n#dify-chatbot-bubble-button {\n    --dify-chatbot-bubble-button-bg-color: #ABCDEF;\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking Large Language Model in Python\nDESCRIPTION: This method implements the core LLM invocation logic. It supports both streaming and synchronous returns. It takes model name, credentials, prompt messages, model parameters, optional tools, stop words, a stream flag, and an optional user identifier as input. It returns either a full LLMResult or a stream response chunk generator.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              prompt_messages: list[PromptMessage], model_parameters: dict,\n              tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,\n              stream: bool = True, user: Optional[str] = None) \\\n          -> Union[LLMResult, Generator]:\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param prompt_messages: prompt messages\n      :param model_parameters: model parameters\n      :param tools: tools for tool calling\n      :param stop: stop words\n      :param stream: is stream response\n      :param user: unique user id\n      :return: full response or stream response chunk generator result\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Checking VM Public IP Address\nDESCRIPTION: This command displays the IP addresses of the network interfaces on the Linux VM.  It is used to determine the public IP address needed to access the SearXNG instance from Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/searxng.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nip addr show\n```\n\n----------------------------------------\n\nTITLE: Setting Global Python Version\nDESCRIPTION: This command sets the global Python version to 3.12 using `pyenv`. This ensures that the subsequent commands use the correct Python environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\npyenv global 3.12\n```\n\n----------------------------------------\n\nTITLE: Upgrading Dify Instance\nDESCRIPTION: This snippet demonstrates the steps to upgrade the Dify instance by cloning the repository, moving the docker files, and restarting the Docker Compose setup. It assumes the existence of a `/dify` directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/dify-premium.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git /tmp/dify\nmv -f /tmp/dify/docker/* /dify/\nrm -rf /tmp/dify\ndocker-compose down\ndocker-compose pull\ndocker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: LLM Prompt for JSON Code Generation\nDESCRIPTION: This prompt is used to instruct an LLM to generate JSON code content, either correct or incorrect, based on user's requirements. This is used in the workflow to demonstrate error handling by intentionally generating invalid JSON.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_21\n\nLANGUAGE: Text\nCODE:\n```\nYou are a teaching assistant. According to the user's requirements, you only output a correct or incorrect sample code in json format.\n```\n\n----------------------------------------\n\nTITLE: Adding Docker GPG Key\nDESCRIPTION: This command downloads and adds the Docker GPG key to ensure the authenticity of packages from the Docker repository. It imports the key using gpg and saves it in the designated keyring.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n```\n\n----------------------------------------\n\nTITLE: Mapping Model Invoke Errors in Python\nDESCRIPTION: This property maps model invocation errors to unified error types for the caller. The key is the error type thrown to the caller, and the value is the error type thrown by the model, which needs to be converted into a unified error type.  This facilitates Dify's ability to handle different errors with appropriate follow-up actions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Adding a GitHub plugin dependency to a Dify Bundle\nDESCRIPTION: This command adds a plugin dependency from a GitHub repository to the Dify Bundle project. The --repo_pattern parameter specifies the GitHub repository, release version, and asset filename in the format organization-name/repository-name:release/attachment-name.  The current directory (.) is used as the target bundle project.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/bundle.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndify-plugin bundle append github . --repo_pattern=langgenius/openai:0.0.1/openai.difypkg\n```\n\n----------------------------------------\n\nTITLE: Jinja2 Template for Markdown Formatting\nDESCRIPTION: This Jinja2 template formats retrieved chunks and their metadata from a knowledge retrieval node into a markdown structure.  It iterates through a list of 'chunks' and generates markdown headings for each chunk's index, similarity score, title, and content. The template replaces newline characters in the content with double newlines for improved formatting.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_72\n\nLANGUAGE: jinja2\nCODE:\n```\n{% for item in chunks %}\n### Chunk {{ loop.index }}.\n### Similarity: {{ item.metadata.score | default('N/A') }}\n\n#### {{ item.title }}\n\n##### Content\n{{ item.content | replace('\\n', '\\n\\n') }}\n\n---\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Defining SystemPromptMessage Model (Python)\nDESCRIPTION: Defines the `SystemPromptMessage` model, representing system messages used for setting system commands for the model. It inherits from `PromptMessage` and defines the `role` field as `PromptMessageRole.SYSTEM`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nclass SystemPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for system prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.SYSTEM\n```\n\n----------------------------------------\n\nTITLE: YAML Model Configuration Example\nDESCRIPTION: This YAML configuration file defines the properties and parameters for a specific model (claude-2.1). It includes settings for model identification, display labels (with i18n support), model type (LLM), supported features (agent-thought), model properties (chat mode, context size), parameter rules (temperature, top_p, top_k, max_tokens_to_sample), and pricing information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/predefined-model.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodel: claude-2.1  # モデル識別子\n# モデル表示名。en_US英語、zh_Hans中国語の二つの言語を設定できます。zh_Hansが設定されていない場合、デフォルトでen_USが使用されます。\n# ラベルを設定しない場合、モデル識別子が使用されます。\nlabel:\n  en_US: claude-2.1\nmodel_type: llm  # モデルタイプ、claude-2.1はLLMです\nfeatures:  # サポートする機能、agent-thoughtはエージェント推論、visionは画像理解をサポート\n- agent-thought\nmodel_properties:  # モデルプロパティ\n  mode: chat  # LLMモード、completeはテキスト補完モデル、chatは対話モデル\n  context_size: 200000  # 最大コンテキストサイズ\nparameter_rules:  # モデル呼び出しパラメータルール、LLMのみ提供が必要\n- name: temperature  # 呼び出しパラメータ変数名\n  # デフォルトで5つの変数内容設定テンプレートが用意されています。temperature/top_p/max_tokens/presence_penalty/frequency_penalty\n  # use_template内でテンプレート変数名を設定すると、entities.defaults.PARAMETER_RULE_TEMPLATE内のデフォルト設定が使用されます\n  # 追加の設定パラメータを設定した場合、デフォルト設定を上書きします\n  use_template: temperature\n- name: top_p\n  use_template: top_p\n- name: top_k\n  label:  # 呼び出しパラメータ表示名\n    zh_Hans: 取样数量\n    en_US: Top k\n  type: int  # パラメータタイプ、float/int/string/booleanがサポートされています\n  help:  # ヘルプ情報、パラメータの作用を説明\n    zh_Hans: 仅从每个后续标记的前 K 个选项中采样。\n    en_US: Only sample from the top K options for each subsequent token.\n  required: false  # 必須かどうか、設定しない場合もあります\n- name: max_tokens_to_sample\n  use_template: max_tokens\n  default: 4096  # パラメータデフォルト値\n  min: 1  # パラメータ最小値、float/intのみ使用可能\n  max: 4096  # パラメータ最大値、float/intのみ使用可能\npricing:  # 価格情報\n  input: '8.00'  # 入力単価、つまりプロンプト単価\n  output: '24.00'  # 出力単価、つまり返答内容単価\n  unit: '0.000001'  # 価格単位、上記価格は100Kあたりの単価\n  currency: USD  # 価格通貨\n```\n\n----------------------------------------\n\nTITLE: Defining a JSON Object Array\nDESCRIPTION: This code snippet demonstrates how to define an array of JSON objects. The array contains multiple JSON objects, each representing a person with properties like name, age, and email. This type of array allows for storing structured data in a list format.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/what-is-array-variable.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n[\n    {\n        \"name\": \"Alice\",\n        \"age\": 30,\n        \"email\": \"alice@example.com\"\n    },\n    {\n        \"name\": \"Bob\",\n        \"age\": 25,\n        \"email\": \"bob@example.com\"\n    },\n    {\n        \"name\": \"Charlie\",\n        \"age\": 35,\n        \"email\": \"charlie@example.com\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Vector Database Migration with Flask\nDESCRIPTION: Migrates data between vector databases within a Dify deployment. This command is executed using the Flask CLI either directly in a source code deployment or within the `docker-api-1` container for Docker Compose deployments. The VECTOR_STORE variable must be set prior to running this command.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nflask vdb-migrate\n```\n\nLANGUAGE: python\nCODE:\n```\ndocker exec -it docker-api-1 flask vdb-migrate\n```\n\n----------------------------------------\n\nTITLE: Define AssistantPromptMessage Model\nDESCRIPTION: Defines the `AssistantPromptMessage` class, which represents a message returned by the model, typically used for `few-shots` or inputting chat history.  It includes nested models for tool calls and their functions.  The model includes the message role and a list of tool calls.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nclass AssistantPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for assistant prompt message.\n    \"\"\"\n    class ToolCall(BaseModel):\n        \"\"\"\n        Model class for assistant prompt message tool call.\n        \"\"\"\n        class ToolCallFunction(BaseModel):\n            \"\"\"\n            Model class for assistant prompt message tool call function.\n            \"\"\"\n            name: str  # tool name\n            arguments: str  # tool arguments\n\n        id: str  # Tool ID, effective only in OpenAI tool calls. It's the unique ID for tool invocation and the same tool can be called multiple times.\n        type: str  # default: function\n        function: ToolCallFunction  # tool call information\n\n    role: PromptMessageRole = PromptMessageRole.ASSISTANT\n    tool_calls: list[ToolCall] = []  # The result of tool invocation in response from the model (returned only when tools are input and the model deems it necessary to invoke a tool).\n```\n\n----------------------------------------\n\nTITLE: Verify Dify Plugin Scaffolding Tool Installation\nDESCRIPTION: This command verifies that the Dify plugin scaffolding tool is installed correctly by displaying its version number. It confirms that the tool is accessible from the terminal.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/initialize-development-tools.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./dify-plugin-darwin-arm64 version\n```\n\n----------------------------------------\n\nTITLE: Dify Documentation Structure Example\nDESCRIPTION: This code outlines the recommended structure for contributing new documentation content, particularly for sharing application scenarios. It includes sections for introduction, project principles, prerequisites, implementation in Dify, and a FAQ.  The content should be structured in Markdown format.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/community/docs-contribution.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n1. Introduction\n   - Application scenarios and problems addressed\n   - Key features and highlights\n   - Final results and demonstrations\n\n2. Project Principles / Process Overview\n\n3. Prerequisites (if any)\n   - Required resource list\n   - Tool and dependency requirements\n\n4. Implementation in the Dify Platform (Suggested Steps)\n   - Application creation and basic configurations\n   - Process-building guide\n   - Configuration details for key nodes\n\n5. FAQ\n```\n\n----------------------------------------\n\nTITLE: Creating a New Plugin Project with Dify CLI (Alternative)\nDESCRIPTION: This command initializes a new Dify plugin project using the `dify` CLI tool. It assumes that the binary file has been renamed to `dify` and copied to `/usr/local/bin`. It creates the basic project structure and files needed for plugin development.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: Migrating Vector Database\nDESCRIPTION: This snippet shows how to migrate the vector database to another vector database.  Modify the `.env` or `docker-compose.yaml` file to specify the new vector database and then execute the flask vdb-migrate command. Tested target databases: qdrant, milvus, analyticdb.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/_book/zh_CN/learn-more/faq/install-faq.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nflask vdb-migrate # or docker exec -it docker-api-1 flask vdb-migrate\n```\n\n----------------------------------------\n\nTITLE: Verify Docker Container Status\nDESCRIPTION: This command lists all running Docker containers, allowing verification that all Dify services (api, worker, web, weaviate, db, redis, nginx) are up and running as expected. It's used to confirm successful deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\ndocker compose ps\n```\n\n----------------------------------------\n\nTITLE: Tailing Docker Logs\nDESCRIPTION: This command tails the logs of a Docker container named `langchain-chroma-api-1`. It's used to monitor the container's output in real-time, typically during startup or operation to observe the application's behavior and identify any potential issues. The `-f` option ensures that the logs are followed continuously, displaying new output as it is generated.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_12\n\nLANGUAGE: Shell\nCODE:\n```\n$ docker logs -f langchain-chroma-api-1\n```\n\n----------------------------------------\n\nTITLE: Installing FFmpeg on CentOS\nDESCRIPTION: This command installs FFmpeg and its development libraries on CentOS. It uses `yum` to install the `ffmpeg` and `ffmpeg-devel` packages. This command requires that the EPEL and Nux Dextop repositories are enabled beforehand.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nsudo yum install ffmpeg ffmpeg-devel\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Main Page (Local)\nDESCRIPTION: This command shows the URL for accessing the main Dify page when deployed in a local environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# 本地环境\nhttp://localhost\n```\n\n----------------------------------------\n\nTITLE: Installing FastAPI dependencies\nDESCRIPTION: This shell command installs the necessary dependencies for the Python FastAPI example, including FastAPI itself, and uvicorn to run the application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npip install fastapi[all] uvicorn\n```\n\n----------------------------------------\n\nTITLE: Restarting Dify Services with Docker Compose\nDESCRIPTION: This snippet demonstrates how to restart Dify services using Docker Compose after making configuration changes. It ensures that the updated settings are applied to the running application. It requires Docker Compose to be installed and the `docker-compose.yml` file to be present in the current directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/faqs.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Configure Notion Public Integration Environment Variables\nDESCRIPTION: This code snippet shows the environment variables to be configured in the .env file when using public Notion integration with Dify. It specifies the integration type as 'public' and includes the client secret and client ID.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/create-knowledge-and-upload-documents/import-content-data/sync-from-notion.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nNOTION_INTEGRATION_TYPE=public\nNOTION_CLIENT_SECRET=your-client-secret\nNOTION_CLIENT_ID=your-client-id\n```\n\n----------------------------------------\n\nTITLE: Cloning SearXNG Docker Repository\nDESCRIPTION: This command clones the SearXNG Docker repository from GitHub, providing the necessary Docker Compose file and configurations. It then changes the current directory to the cloned repository.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/searxng/searxng-docker.git\ncd searxng-docker\n```\n\n----------------------------------------\n\nTITLE: Running the Application with Docker Compose\nDESCRIPTION: This command uses Docker Compose to build and start the application, including all necessary services defined in the docker-compose.yml file. It sets up the environment and dependencies for the chatbot.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-whatsapp.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Extract Data from JSON string with Python in Dify\nDESCRIPTION: This code snippet demonstrates how to extract a specific field (`data.name`) from a JSON string using the Dify Code Node. It uses the `json` library to parse the JSON string provided as input (`http_response`). The function returns a dictionary with the extracted data in the 'result' key. The input is a string, and the output is a dictionary.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_67\n\nLANGUAGE: Python\nCODE:\n```\ndef main(http_response: str) -> str:\n    import json\n    data = json.loads(http_response)\n    return {\n        # Note to declare 'result' in the output variables\n        'result': data['data']['name']\n    }\n```\n\n----------------------------------------\n\nTITLE: Vector Database Configuration (YAML)\nDESCRIPTION: This snippet shows how to configure the vector database in the `docker-compose.yaml` file.  This configures the vector store to be `weaviate`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/_book/zh_CN/learn-more/faq/install-faq.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n# The type of vector store to use. Supported values are `weaviate`, `qdrant`, `milvus`, `analyticdb`.\nVECTOR_STORE: weaviate\n```\n\n----------------------------------------\n\nTITLE: Stop Docker Services and Backup Volumes - Bash\nDESCRIPTION: This command stops the running Docker services defined in the docker-compose.yaml file and then creates a compressed archive of the 'volumes' directory. This directory typically contains persistent data used by the Dify application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/migration/migrate-to-v1.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\ntar -cvf volumes-$(date +%s).tgz volumes\n```\n\n----------------------------------------\n\nTITLE: RerankResult Class Definition in Python\nDESCRIPTION: Defines the RerankResult class encapsulating the result of reranking. Includes the model and a list of RerankDocument objects.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nclass RerankResult(BaseModel):\n    \"\"\"\n    Model class for rerank result.\n    \"\"\"\n    model: str  # 実際に使用したモデル\n    docs: list[RerankDocument]  # Rerankされたセグメントリスト\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack on Windows using PowerShell\nDESCRIPTION: This PowerShell command installs GPUStack on Windows. It downloads the installation script and executes it using Invoke-Expression and Invoke-WebRequest. Requires running PowerShell as administrator.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/gpustack.md#_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n```\n\n----------------------------------------\n\nTITLE: Restarting Dify Services with Docker Compose\nDESCRIPTION: This command restarts the Dify services using Docker Compose. It first stops the running containers and then starts them again in detached mode, ensuring the changes take effect.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/faq.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Configuring LocalAI .env File (Shell)\nDESCRIPTION: This code snippet provides a shell command to rename the `.env.example` file to `.env`.  The `.env` file contains environment variables used to configure LocalAI, such as the number of threads to use.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n$ mv .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Defining WhatsApp conversation IDs\nDESCRIPTION: This Python code defines a dictionary to store conversation IDs for each WhatsApp number. This is used to maintain the session state for each user interacting with the chatbot.  The conversation ID is used to correlate messages from the same user.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/dify-on-whatsapp.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconversation_ids = {}\n```\n\n----------------------------------------\n\nTITLE: Installing WSL using PowerShell\nDESCRIPTION: This PowerShell command installs the Windows Subsystem for Linux (WSL). The output lists the available Linux distributions and allows selection for installation. It's a prerequisite for running Docker-based applications on Windows.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_1\n\nLANGUAGE: PowerShell\nCODE:\n```\nwsl --install\n```\n\n----------------------------------------\n\nTITLE: Developing Locally (Bash)\nDESCRIPTION: These commands install the necessary dependencies and start the Cloudflare Worker in development mode for local testing. This allows developers to test their API extensions locally before deploying them to production.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Chat Model Text Generation USER Prompt - Dify\nDESCRIPTION: This USER prompt is for a text generation application utilizing a chat model within Dify.  The input query, typically a paragraph, is passed as a variable. The 'Query' variable should be replaced with the user's input paragraph.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n{{Query}} //这里输入查询的变量，常用的是输入段落形式的变量\n```\n\n----------------------------------------\n\nTITLE: Dify Frontend Directory Structure\nDESCRIPTION: This snippet illustrates the directory structure of the Dify frontend. It's based on a Typescript Next.js template and styled with Tailwind CSS. React-i18next is used for internationalization. The structure displays folders for layouts, pages, components and styling.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_16\n\nLANGUAGE: Text\nCODE:\n```\n[web/]\n├── app                   //layouts, pages, and components\n│   ├── (commonLayout)    //common layout used throughout the app\n│   ├── (shareLayout)     //layouts specifically shared across token-specific sessions\n│   ├── activate          //activate page\n│   ├── components        //shared by pages and layouts\n│   ├── install           //install page\n│   ├── signin            //signin page\n│   └── styles            //globally shared styles\n├── assets                // Static assets\n├── bin                   //scripts ran at build step\n├── config                //adjustable settings and options\n├── context               //shared contexts used by different portions of the app\n├── dictionaries          // Language-specific translate files\n├── docker                //container configurations\n├── hooks                 // Reusable hooks\n├── i18n                  // Internationalization configuration\n├── models                //describes data models & shapes of API responses\n├── public                //meta assets like favicon\n├── service               //specifies shapes of API actions\n├── test\n├── types                 //descriptions of function params and return values\n└── utils                 // Shared utility functions\n```\n\n----------------------------------------\n\nTITLE: Hierarchical Logging Example (Python)\nDESCRIPTION: This snippet provides an example of structuring logs hierarchically by setting the `parent` parameter in log calls. This can be useful when tracking complex workflows involving multiple rounds of logs.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfunction_call_round_log = self.create_log_message(\n    label=\"Function Call Round1 \",\n    data={},\n    metadata={},\n)\nyield function_call_round_log\n\nmodel_log = self.create_log_message(\n    label=f\"{params.model.model} Thought\",\n    data={},\n    metadata={\"start_at\": model_started_at, \"provider\": params.model.provider},\n    status=ToolInvokeMessage.LogMessage.LogStatus.START,\n    # add parent log\n    parent=function_call_round_log,\n)\nyield model_log\n```\n\n----------------------------------------\n\nTITLE: Reload Systemd and Restart Ollama (Bash)\nDESCRIPTION: These commands reload the systemd daemon configuration and restart the Ollama service. This applies the changes made to the service file, ensuring the environment variables are correctly set.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsystemctl daemon-reload\nsystemctl restart ollama\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Example for AI Generation\nDESCRIPTION: This code snippet represents a JSON schema generated by AI for user profiles. It includes fields for 'username' (string), 'age' (number), and 'interests' (array of strings). The 'required' array specifies that all three fields are mandatory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/llm.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"username\": {\n      \"type\": \"string\"\n    },\n    \"age\": {\n      \"type\": \"number\"\n    },\n    \"interests\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      }\n    }\n  },\n  \"required\": [\"username\", \"age\", \"interests\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Dify Frontend Directory Structure\nDESCRIPTION: This displays the directory structure of the Dify frontend, which is built using Next.js and styled with Tailwind CSS. It highlights key directories like `app` for layouts and components, `assets` for static files, and `i18n` for internationalization. Contributors can use this as a guide for frontend development and adding new features.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_62\n\nLANGUAGE: Text\nCODE:\n```\n[web/]\n├── app                   // レイアウト、ページ、およびコンポーネント\n│   ├── (commonLayout)    // アプリ全体で使用される共通レイアウト\n│   ├── (shareLayout)     // トークン固有のセッション間で共有されるレイアウト\n│   ├── activate          // アクティベートページ\n│   ├── components        // ページとレイアウトで共有されるコンポーネント\n│   ├── install           // インストールページ\n│   ├── signin            // サインインページ\n│   └── styles            // グローバルに共有されるスタイル\n├── assets                // 静的アセット\n├── bin                   // ビルドステップで実行されるスクリプト\n├── config                // 調整可能な設定とオプション\n├── context               // アプリの異なる部分で使用される共有コンテキスト\n├── dictionaries          // 言語固有の翻訳ファイル\n├── docker                // コンテナ設定\n├── hooks                 // 再利用可能なフック\n├── i18n                  // 国際化設定\n├── models                // データモデルとAPIレスポンスの形状を記述\n├── public                // ファビコンなどのメタアセット\n├── service               // APIアクションの形状を指定\n├── test\n├── types                 // 関数パラメータと戻り値の記述\n└── utils                 // 共有ユーティリティ関数\n```\n\n----------------------------------------\n\nTITLE: Cloning a Model from Hugging Face (Bash)\nDESCRIPTION: This command clones a specific model (pastel-mix in this example) from Hugging Face using git lfs. The model is then placed in the `models` directory of the Stable Diffusion WebUI, making it available for image generation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/stable-diffusion.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/JamesFlare/pastel-mix\n```\n\n----------------------------------------\n\nTITLE: Checking Dify version after moving to /usr/local/bin\nDESCRIPTION: This command checks the installed Dify version after the binary file has been renamed to 'dify' and moved to the /usr/local/bin directory. This allows the 'dify' command to be run from any terminal location.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/initialize-development-tools.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndify version\n```\n\n----------------------------------------\n\nTITLE: Creating a Twitter URL using Python\nDESCRIPTION: This Python function takes a user ID as a string and returns a dictionary containing a complete Twitter URL.  It integrates the prefix `https://twitter.com/` with the user ID. The input is a string representing the user ID, and the output is a dictionary with the 'url' key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ndef main(id: str) -> dict:\n    return {\n        \"url\": \"https://twitter.com/\"+id,\n    }\n```\n\n----------------------------------------\n\nTITLE: Installing Ubuntu using PowerShell\nDESCRIPTION: This PowerShell command installs the Ubuntu distribution for Windows Subsystem for Linux (WSL). This is done after the wsl has been installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_2\n\nLANGUAGE: PowerShell\nCODE:\n```\nwsl --install -d Ubuntu\n```\n\n----------------------------------------\n\nTITLE: SearXNG Settings Configuration\nDESCRIPTION: This YAML configuration sets the bind address to 0.0.0.0 to allow external access to the SearXNG server and configures the search formats to include HTML, JSON, CSV, and RSS.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  bind_address: \"0.0.0.0\"  # Allow external access\n  port: 8080\n\nsearch:\n  formats:\n    - html\n    - json\n    - csv\n    - rss\n```\n\n----------------------------------------\n\nTITLE: Cloning LocalAI Repository\nDESCRIPTION: This snippet clones the LocalAI code repository from GitHub and navigates to the langchain-chroma example directory.  It's a preliminary step for setting up LocalAI.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/go-skynet/LocalAI\n$ cd LocalAI/examples/langchain-chroma\n```\n\n----------------------------------------\n\nTITLE: External Data Tool Response Example\nDESCRIPTION: This JSON snippet shows an example response from the `external_data_tool` extension. In this case it returns weather information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/README.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"result\": \"City: London\\nTemperature: 10°C\\nRealFeel®: 8°C\\nAir Quality: Poor\\nWind Direction: ENE\\nWind Speed: 8 km/h\\nWind Gusts: 14 km/h\\nPrecipitation: Light rain\"\n}\n```\n\n----------------------------------------\n\nTITLE: Database Migration via Flask Command\nDESCRIPTION: This command is used to migrate the Dify database structure to the latest version after upgrading the application, ensuring compatibility with the new code. It executes a Flask command from the API directory, requiring a properly configured database connection. The command assumes that the Flask application context is already initialized.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nflask db upgrade\n```\n\n----------------------------------------\n\nTITLE: Accessing llms-full.txt via API with curl\nDESCRIPTION: This code snippet shows how to access the `llms-full.txt` file using a `curl` command. An HTTP GET request is made to the API endpoint with the API key included as a query parameter. Replace `YOUR_API_KEY` with the actual API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET 'http://llmstxt.firecrawl.dev/https://docs.dify.ai//full?FIRECRAWL_API_KEY=YOUR_API_KEY'\n```\n\n----------------------------------------\n\nTITLE: Invoking Tools within Dify Agent Plugin using Python\nDESCRIPTION: This code shows how to invoke a tool using `self.session.tool.invoke()` within a Dify Agent plugin. It demonstrates how to obtain tool parameters from the LLM output and invoke the appropriate tool instance using its provider type, provider name, tool name, and runtime parameters. The output of the invocation is a generator that needs to be parsed to extract the result.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/agent.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dify_plugin.entities.tool import ToolProviderType\n\nclass FunctionCallingAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        \"\"\"\n        Run FunctionCall agent application\n        \"\"\"\n        fc_params = FunctionCallingParams(**parameters)\n        \n        # tool_call_name and tool_call_args parameter is obtained from the output of LLM\n        tool_instances = {tool.identity.name: tool for tool in fc_params.tools} if fc_params.tools else {}\n        tool_instance = tool_instances[tool_call_name]\n        tool_invoke_responses = self.session.tool.invoke(\n            provider_type=ToolProviderType.BUILT_IN,\n            provider=tool_instance.identity.provider,\n            tool_name=tool_instance.identity.name,\n            # add the default value\n            parameters={**tool_instance.runtime_parameters, **tool_call_args},\n        )\n```\n\n----------------------------------------\n\nTITLE: Example Response Syntax\nDESCRIPTION: This is an example of the expected response when connecting to an external knowledge base. It returns a list of records from querying the knowledge base in JSON format.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_26\n\nLANGUAGE: JSON\nCODE:\n```\nHTTP/1.1 200\nContent-type: application/json\n{\n    \"records\": [{\\\n                    \"metadata\": {\\\n                            \"path\": \"s3://dify/knowledge.txt\",\\\n                            \"description\": \"dify knowledge document\"\\\n                    },\\\\n                    \"score\": 0.98,\\\\n                    \"title\": \"knowledge.txt\",\\\\n                    \"content\": \"This is the document for external knowledge.\"\\\n            },\\\\n            {\\\n                    \"metadata\": {\\\n                            \"path\": \"s3://dify/introduce.txt\",\\\n                            \"description\": \"dify introduce\"\\\n                    },\\\\n                    \"score\": 0.66,\\\\n                    \"title\": \"introduce.txt\",\\\\n                    \"content\": \"The Innovation Engine for GenAI Applications\"\\\n            }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: app.moderation.output Request Body Example (JSON)\nDESCRIPTION: This JSON payload represents a sample request body sent to the `app.moderation.output` extension point. It includes the extension point type, application ID, and the LLM's response text. This request is triggered by Dify when reviewing the LLM's generated content.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/moderation.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"point\": \"app.moderation.output\",\n    \"params\": {\n        \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n        \"text\": \"I will kill you.\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: POST Request to Chat Messages API with cURL\nDESCRIPTION: This cURL command sends a POST request to the `/v1/chat-messages` endpoint for conversational AI. It includes the authorization header, content type header, and a JSON payload with input parameters like query, response mode, conversation ID, and user ID.  The `ENTER-YOUR-SECRET-KEY` should be replaced with the actual API key. The `conversation_id` is used to continue an existing conversation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-publishing/developing-with-apis.md#_snippet_2\n\nLANGUAGE: cURL\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/chat-messages' \\\n--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"inputs\": {},\n    \"query\": \"eh\",\n    \"response_mode\": \"streaming\",\n    \"conversation_id\": \"1c7e55fb-1ba2-4e10-81b5-30addcea2276\",\n    \"user\": \"abc-123\"\n}'\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Installer with Domain - Bash\nDESCRIPTION: This code snippet shows how to access the Dify installer page when a domain name is configured. Replace `yourdomain` with the actual domain name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/bt-panel.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# ドメインを設定した場合\nhttp://yourdomain/install\n```\n\n----------------------------------------\n\nTITLE: Workspace structure example\nDESCRIPTION: This example illustrates the workspace-based structure of Dify, showing how multiple apps with various capabilities reside within a single workspace. The system bills on a per-workspace basis, calculated from the total resource consumption within that workspace. This example has no specific language assigned, it's plain text.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workspace/billing.md#_snippet_0\n\nLANGUAGE: Plaintext\nCODE:\n```\nWorkspace 1  \nApp 1(Prompt, RAG, LLM, Knowledge base, Logging & Annotation, API)\nApp 2(Prompt, RAG, LLM, Knowledge base, Logging & Annotation, API) \nApp 3(Prompt, RAG, LLM, Knowledge base, Logging & Annotation, API)\n...\nWorkspace 2\n```\n\n----------------------------------------\n\nTITLE: Initializing a Plugin Project\nDESCRIPTION: This command initializes a new plugin development project using the Dify plugin scaffolding tool.  It prompts for basic project information and uses the `extension` template.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: Upload Plugin Files to GitHub - Bash\nDESCRIPTION: This snippet pushes the local plugin files to the connected GitHub repository. It sets the default branch to `main` and pushes the code to the `origin` remote. The `-u` flag sets up tracking information for future pushes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit branch -M main\ngit push -u origin main\n```\n\n----------------------------------------\n\nTITLE: Installing Docker\nDESCRIPTION: This command installs Docker CE (Community Edition), the Docker CLI (Command Line Interface), and containerd.io, which is the container runtime. It updates the package lists again before installing to ensure the latest versions are available.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nsudo apt install docker-ce docker-ce-cli containerd.io\n```\n\n----------------------------------------\n\nTITLE: Bearer Authentication Logic\nDESCRIPTION: This TypeScript code snippet shows how to implement Bearer authentication using the `hono/bearer-auth` package. It retrieves the token from the environment variables and uses it to authenticate incoming requests.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bearerAuth } from \"hono/bearer-auth\";\n\n(c, next) => {\n    const auth = bearerAuth({ token: c.env.TOKEN });\n    return auth(c, next);\n},\n```\n\n----------------------------------------\n\nTITLE: Running the application using Docker Compose\nDESCRIPTION: This command starts the application using Docker Compose. It reads the docker-compose.yml file to build and run the necessary containers for the application.  This command assumes that Docker and Docker Compose are installed and configured correctly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/dify-on-whatsapp.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Parameter Extractor Node Endpoint Definition\nDESCRIPTION: This code snippet defines the endpoint for invoking the Parameter Extractor node.  It takes a list of ParameterConfig objects, a ModelConfig object, a query string, and an optional instruction string as input, and returns a NodeResponse object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self,\n    parameters: list[ParameterConfig],\n    model: ModelConfig,\n    query: str,\n    instruction: str = \"\",\n) -> NodeResponse:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Navigating to Web Directory\nDESCRIPTION: Changes the current working directory to the 'web' directory.  This is necessary to execute subsequent frontend-related commands.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\ncd web\n```\n\n----------------------------------------\n\nTITLE: JSON Validation Code Node\nDESCRIPTION: This Python code snippet defines a function `main` that attempts to parse a JSON string and returns the parsed object within a dictionary. It utilizes the `json.loads` function to parse the JSON.  If the JSON is invalid, the `json.loads` function will raise an exception, which should be handled appropriately in the workflow, ideally via error handling.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef main(json_str: str) -> dict:\n    obj = json.loads(json_str)\n    return {'result': obj}\n```\n\n----------------------------------------\n\nTITLE: Install Plugins - Bash\nDESCRIPTION: This command executes a Flask application within the docker-api container to install the extracted plugins. It requires Poetry to be installed within the container and access to the Dify marketplace. The `workers` parameter specifies the number of parallel processes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/migration/migrate-to-v1.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask install-plugins --workers=2\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloudflare Worker (TOML)\nDESCRIPTION: This TOML configuration file defines the Cloudflare Worker's name, compatibility date, and the token used for authentication. The token is stored as an environment variable for security.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\nname = \"dify-extension-example\"\ncompatibility_date = \"2023-01-01\"\n\n[vars]\nTOKEN = \"bananaiscool\"\n```\n\n----------------------------------------\n\nTITLE: Calculating Variance in Dify using Python\nDESCRIPTION: This Python code snippet calculates the variance of an array of numbers within a Dify workflow. It defines a function `main` that takes a list of numbers `x` as input and returns a dictionary containing the calculated variance under the key `result`. The function uses a list comprehension and the `sum` and `len` functions to perform the calculation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_67\n\nLANGUAGE: Python\nCODE:\n```\ndef main(x: list) -> float:\n    return {\n        # Note to declare 'result' in the output variables\n        'result': sum([(i - sum(x) / len(x)) ** 2 for i in x]) / len(x)\n    }\n```\n\n----------------------------------------\n\nTITLE: Extract Dify Plugins\nDESCRIPTION: This command enters the container terminal of the docker-api service and extracts all models and tools currently in use, generating a plugins.jsonl file. It utilizes poetry to run a flask command for the extraction process. The --workers parameter controls the number of parallel processes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/dify-premium.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask extract-plugins --workers=20\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Web Interface with IP and Port - Bash\nDESCRIPTION: This snippet shows how to access the main Dify web interface when accessing through IP and Port. Replace `your_server_ip` with the actual IP address and ensure the port (8088 in this example) matches the configured port during installation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/bt-panel.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# If you choose to access through `IP+Port`\nhttp://your_server_ip:8088/\n```\n\n----------------------------------------\n\nTITLE: PromptMessageRole Enum Definition in Python\nDESCRIPTION: Defines an enumeration `PromptMessageRole` representing the different roles a message can have in a prompt-based interaction. The roles include SYSTEM, USER, ASSISTANT, and TOOL.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageRole(Enum):\n    \"\"\"\n    Enum class for prompt message.\n    \"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    TOOL = \"tool\"\n```\n\n----------------------------------------\n\nTITLE: Cloning LocalAI Repository\nDESCRIPTION: This snippet shows how to clone the LocalAI code repository from GitHub and navigate to the relevant directory for Langchain Chroma example.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/go-skynet/LocalAI\n$ cd LocalAI/examples/langchain-chroma\n```\n\n----------------------------------------\n\nTITLE: Icon SVG Example\nDESCRIPTION: This SVG snippet is an example of the icon that is referred to in the tool provider YAML file.  It is placed in the `_assets` folder of the tool provider's module.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/quick-tool-integration.md#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"25\" viewBox=\"0 0 24 25\" fill=\"none\">\n  <path d=\"M22.501 12.7332C22.501 11.8699 22.4296 11.2399 22.2748 10.5865H12.2153V14.4832H18.12C18.001 15.4515 17.3582 16.9099 15.9296 17.8898L15.9096 18.0203L19.0902 20.435L19.3106 20.4565C21.3343 18.6249 22.501 15.9298 22.501 12.7332Z\" fill=\"#4285F4\"/>\n  <path d=\"M12.214 23C15.1068 23 17.5353 22.0666 19.3092 20.4567L15.9282 17.8899C15.0235 18.5083 13.8092 18.9399 12.214 18.9399C9.38069 18.9399 6.97596 17.1083 6.11874 14.5766L5.99309 14.5871L2.68583 17.0954L2.64258 17.2132C4.40446 20.6433 8.0235 23 12.214 23Z\" fill=\"#34A853\"/>\n  <path d=\"M6.12046 14.5766C5.89428 13.9233 5.76337 13.2233 5.76337 12.5C5.76337 11.7766 5.89428 11.0766 6.10856 10.4233L6.10257 10.2841L2.75386 7.7355L2.64429 7.78658C1.91814 9.20993 1.50146 10.8083 1.50146 12.5C1.50146 14.1916 1.91814 15.7899 2.64429 17.2132L6.12046 14.5766Z\" fill=\"#FBBC05\"/>\n  <path d=\"M12.2141 6.05997C14.2259 6.05997 15.583 6.91163 16.3569 7.62335L19.3807 4.73C17.5236 3.03834 15.1069 2 12.2141 2C8.02353 2 4.40447 4.35665 2.64258 7.78662L6.10686 10.4233C6.97598 7.89166 9.38073 6.05997 12.2141 6.05997Z\" fill=\"#EB4335\"/>\n</svg>\n```\n\n----------------------------------------\n\nTITLE: Starting Web Service with PNPM\nDESCRIPTION: Starts the web service using PNPM. This command initiates the frontend application, making it accessible in a browser.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_17\n\nLANGUAGE: Bash\nCODE:\n```\npnpm start\n```\n\n----------------------------------------\n\nTITLE: Copying .env file\nDESCRIPTION: This snippet copies the .env.example file to .env for configuring environment variables required by LocalAI.  It allows customizing settings such as the number of threads to use.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ mv .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Restart Dify on EC2 Instance\nDESCRIPTION: This script restarts a Dify instance running on an AWS EC2 instance. It stops the existing Docker containers and then restarts them using Docker Compose.  This is typically used after modifying the .env file or other configuration files.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_64\n\nLANGUAGE: shell\nCODE:\n```\ndocker-compose down\nocker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Clone LocalAI Repository\nDESCRIPTION: Clones the LocalAI code repository and navigates to the specified directory to prepare for LocalAI deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/localai.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/go-skynet/LocalAI\n$ cd LocalAI/examples/langchain-chroma\n```\n\n----------------------------------------\n\nTITLE: Accessing Parameter Extractor Node\nDESCRIPTION: This snippet shows how to access the parameter extractor node within a Dify plugin session. It provides the entry point to use for making requests to the parameter extraction functionality.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.session.workflow_node.parameter_extractor\n```\n\n----------------------------------------\n\nTITLE: Chat Model Conversational App Template SYSTEM\nDESCRIPTION: This is the SYSTEM prompt template for building conversational applications using chat models in Dify. It uses context, pre_prompt, and instructs the model on how to respond to the user's query.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nUse the following context as your learned knowledge, inside <context></context> XML tags.\n\n<context>\n{{#context#}}\n</context>\n\nWhen answering the user:\n- If you don't know, just say that you don't know.\n- If you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n{{pre_prompt}}\n```\n\n----------------------------------------\n\nTITLE: Backup Docker Compose YAML File\nDESCRIPTION: This snippet is an optional step for backing up the Docker Compose YAML file before upgrading Dify. It copies the current `docker-compose.yaml` to a backup file with a timestamp.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/dify-premium.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ncp docker-compose.yaml docker-compose.yaml.$(date +%s).bak\n```\n\n----------------------------------------\n\nTITLE: Example of English-Language Pre-Prompt Customization in Dify\nDESCRIPTION: This snippet provides an example of how to customize the initial prompt template to ensure the LLM's responses are in English. It modifies the `answer according to the language of the user's question` directive to `answer according to the language English`. This customization is performed within Dify's expert mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/prompt-engineering/prompt-engineering-1/README.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nWhen answer to user:\n- If you don't know, just say that you don't know.\n- If you don't know when you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language English.\n```\n\n----------------------------------------\n\nTITLE: Installing a Specific Python Version\nDESCRIPTION: This command uses `pyenv` to install Python version 3.12, which is a prerequisite for running the Dify API service. It ensures the correct Python version is available.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\npyenv install 3.12\n```\n\n----------------------------------------\n\nTITLE: Invoke LLM in Tool - Python\nDESCRIPTION: This code demonstrates how to invoke an LLM model (specifically, OpenAI's gpt-4o-mini) within a Dify Tool. It constructs an LLMModelConfig, provides prompt messages, and streams the response back to the user. The tool parameters are used to provide the user query.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom typing import Any\n\nfrom dify_plugin import Tool\nfrom dify_plugin.entities.model.llm import LLMModelConfig\nfrom dify_plugin.entities.tool import ToolInvokeMessage\nfrom dify_plugin.entities.model.message import SystemPromptMessage, UserPromptMessage\n\nclass LLMTool(Tool):\n    def _invoke(self, tool_parameters: dict[str, Any]) -> Generator[ToolInvokeMessage]:\n        response = self.session.model.llm.invoke(\n            model_config=LLMModelConfig(\n                provider='openai',\n                model='gpt-4o-mini',\n                mode='chat',\n                completion_params={}\n            ),\n            prompt_messages=[\n                SystemPromptMessage(\n                    content='you are a helpful assistant'\n                ),\n                UserPromptMessage(\n                    content=tool_parameters.get('query')\n                )\n            ],\n            stream=True\n        )\n\n        for chunk in response:\n            if chunk.delta.message:\n                assert isinstance(chunk.delta.message.content, str)\n                yield self.create_text_message(text=chunk.delta.message.content)\n```\n\n----------------------------------------\n\nTITLE: Invoking Large Language Model in Python\nDESCRIPTION: This method invokes a large language model. It supports both streaming and synchronous returns. It takes the model name, credentials, prompt messages, model parameters, tools, stop words, stream flag, and user ID as input.  The model parameters are defined by the `parameter_rules` in the model's YAML configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              prompt_messages: list[PromptMessage], model_parameters: dict,\n              tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,\n              stream: bool = True, user: Optional[str] = None) \\\n          -> Union[LLMResult, Generator]:\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param prompt_messages: prompt messages\n      :param model_parameters: model parameters\n      :param tools: tools for tool calling\n      :param stop: stop words\n      :param stream: is stream response\n      :param user: unique user id\n      :return: full response or stream response chunk generator result\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Example .bots.yaml Configuration\nDESCRIPTION: This is an example of the `.bots.yaml` file, which defines the binding between DingTalk robots and Dify applications. Each bot configuration includes DingTalk app credentials (client ID and secret), Dify application type, Dify API key, and a handler. It allows configuring multiple robots with different Dify applications.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-dingtalk.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nbots:\n  - name: 问答助手\n    # 钉钉应用凭证信息，上文 2.2. 步骤里记录的\n    dingtalk_app_client_id: <your-dingtalk-app-client-id>\n    dingtalk_app_client_secret: <your-dingtalk-app-client-secret>\n    # Dify 应用类型，上文 2.1. 开头提到的\n    dify_app_type: <chatbot or completion or workflow>\n    # Dify 应用 API 密钥，上文 2.1. 步骤里记录的\n    dify_app_api_key: <your-dify-api-key-per-app>\n    # 目前是固定值，不用动\n    handler: DifyAiCardBotHandler\n  - name: 问答助手 2\n    xxx 以下略 xxx\n```\n\n----------------------------------------\n\nTITLE: Request Body Structure for External Data Tool Query (app.external_data_tool.query)\nDESCRIPTION: Defines the structure of the request body for the `app.external_data_tool.query` extension point. It includes parameters like `app_id`, `tool_variable`, `inputs` (end-user provided variables), and `query` (end-user's current input).  The response provides the query result.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/external-data-tool.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"point\": \"app.external_data_tool.query\", // 拡張ポイントの種類。ここではapp.external_data_tool.queryに固定\n    \"params\": {\n        \"app_id\": string,  // アプリID\n        \"tool_variable\": string,  // 外部データツール変数名。対応する変数ツールの呼び出し元を示す\n        \"inputs\": {  // エンドユーザーが入力した変数値。キーが変数名、値が変数値\n            \"var_1\": \"value_1\",\n            \"var_2\": \"value_2\",\n            ...\n        },\n        \"query\": string | null  // エンドユーザーの現在の対話入力内容。対話型アプリの固定パラメータ\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Tool Provider YAML Configuration with Credentials\nDESCRIPTION: This YAML file configures the tool provider, including credentials for accessing the SerpApi service. It specifies the API key type, requirement, label, placeholder, help text, and URL.  It also defines the tools and the implementation code.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: Dify\n  name: google\n  label:\n    en_US: Google\n    zh_Hans: Google\n    pt_BR: Google\n  description:\n    en_US: Google\n    zh_Hans: GoogleSearch\n    pt_BR: Google\n  icon: icon.svg\n  tags:\n    - search\ncredentials_for_provider: #添加 credentials_for_provider 字段\n  serpapi_api_key:\n    type: secret-input\n    required: true\n    label:\n      en_US: SerpApi API key\n      zh_Hans: SerpApi API key\n    placeholder:\n      en_US: Please input your SerpApi API key\n      zh_Hans: 请输入你的 SerpApi API key\n    help:\n      en_US: Get your SerpApi API key from SerpApi\n      zh_Hans: 从 SerpApi 获取您的 SerpApi API key\n    url: https://serpapi.com/manage-api-key\ntools:\n  - tools/google_search.yaml\nextra:\n  python:\n    source: google.py\n```\n\n----------------------------------------\n\nTITLE: Defining Function Calling Parameters in Python\nDESCRIPTION: This code defines the data structures required for handling function calling parameters within the Dify Agent Plugin. It uses `pydantic.BaseModel` to define the structure of `FunctionCallingParams`, which includes the query string, model configuration (`AgentModelConfig`), a list of tool entities (`ToolEntity`), and the maximum number of iterations. This model is then used within the `FunctionCallingAgentStrategy` to process the parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/agent.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\n\nfrom pydantic import BaseModel\n\nclass FunctionCallingParams(BaseModel):\n    query: str\n    model: AgentModelConfig\n    tools: list[ToolEntity] | None\n    maximum_iterations: int = 3\n    \nclass FunctionCallingAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        \"\"\"\n        Run FunctionCall agent application\n        \"\"\"\n        fc_params = FunctionCallingParams(**parameters)\n```\n\n----------------------------------------\n\nTITLE: Configuring App ID and API Key in JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates how to configure the Dify App ID and API key within the WebApp Template. It's a crucial step to connect the template to your Dify application. These values should be replaced with your actual App ID and API Key to enable the application to function correctly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-publishing/based-on-frontend-templates.md#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nexport const APP_ID = ''\nexport const API_KEY = ''\n```\n\n----------------------------------------\n\nTITLE: Configure Notion Internal Integration in .env File\nDESCRIPTION: This code snippet shows how to configure the .env file for Notion integration using the 'internal' integration type.  It sets the integration type and provides the internal secret. This configuration is required for Dify to authenticate and access Notion data via an internal integration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/create-knowledge-and-upload-documents/import-content-data/sync-from-notion.md#_snippet_0\n\nLANGUAGE: env\nCODE:\n```\nNOTION_INTEGRATION_TYPE = internal or NOTION_INTEGRATION_TYPE = public\nNOTION_INTERNAL_SECRET=you-internal-secret\n```\n\n----------------------------------------\n\nTITLE: JSON Validation Code Node\nDESCRIPTION: This code snippet defines a function `main` that attempts to load a JSON string and returns the result within a dictionary. It's intended for validating JSON data within a Dify code node.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef main(json_str: str) -> dict:\n    obj = json.loads(json_str)\n    return {'result': obj}\n```\n\n----------------------------------------\n\nTITLE: Configuring Third-Party Signature Verification in Docker Compose (YAML)\nDESCRIPTION: This YAML snippet shows how to configure environment variables in the `plugin_daemon` service within a Docker Compose override file (`docker-compose.override.yaml`) to enable and configure third-party signature verification. It sets the `FORCE_VERIFYING_SIGNATURE`, `THIRD_PARTY_SIGNATURE_VERIFICATION_ENABLED`, and `THIRD_PARTY_SIGNATURE_VERIFICATION_PUBLIC_KEYS` environment variables.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n  plugin_daemon:\n    environment:\n      FORCE_VERIFYING_SIGNATURE: true\n      THIRD_PARTY_SIGNATURE_VERIFICATION_ENABLED: true\n      THIRD_PARTY_SIGNATURE_VERIFICATION_PUBLIC_KEYS: /app/storage/public_keys/your_key_pair.public.pem\n```\n\n----------------------------------------\n\nTITLE: Running Docker Compose\nDESCRIPTION: This snippet starts the application using Docker Compose, which manages the services defined in the docker-compose.yml file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-teams.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up\n```\n\n----------------------------------------\n\nTITLE: Bedrock Knowledge Retrieval Service\nDESCRIPTION: This service class handles the retrieval of knowledge from the AWS Bedrock Knowledge Base. It uses the boto3 library to interact with the Bedrock API, constructs the retrieval request, and parses the response.  It requires the boto3 library and AWS credentials to be configured. Replace the placeholder AWS credentials with your actual credentials.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/how-to-connect-aws-bedrock.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\n\n\nclass ExternalDatasetService:\n    @staticmethod\n    def knowledge_retrieval(retrieval_setting: dict, query: str, knowledge_id: str):\n        # get bedrock client\n        client = boto3.client(\n            \"bedrock-agent-runtime\",\n            aws_secret_access_key=\"AWS_SECRET_ACCESS_KEY\",\n            aws_access_key_id=\"AWS_ACCESS_KEY_ID\",\n            # example: us-east-1\n            region_name=\"AWS_REGION_NAME\",\n        )\n        # fetch external knowledge retrieval\n        response = client.retrieve(\n            knowledgeBaseId=knowledge_id,\n            retrievalConfiguration={\n                \"vectorSearchConfiguration\": {\"numberOfResults\": retrieval_setting.get(\"top_k\"), \"overrideSearchType\": \"HYBRID\"}\n            },\n            retrievalQuery={\"text\": query},\n        )\n        # parse response\n        results = []\n        if response.get(\"ResponseMetadata\") and response.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") == 200:\n            if response.get(\"retrievalResults\"):\n                retrieval_results = response.get(\"retrievalResults\")\n                for retrieval_result in retrieval_results:\n                    # filter out results with score less than threshold\n                    if retrieval_result.get(\"score\") < retrieval_setting.get(\"score_threshold\", .0):\n                        continue\n                    result = {\n                        \"metadata\": retrieval_result.get(\"metadata\"),\n                        \"score\": retrieval_result.get(\"score\"),\n                        \"title\": retrieval_result.get(\"metadata\").get(\"x-amz-bedrock-kb-source-uri\"),\n                        \"content\": retrieval_result.get(\"content\").get(\"text\"),\n                    }\n                    results.append(result)\n        return {\n            \"records\": results\n        }\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify-on-Dingtalk Repository\nDESCRIPTION: This command clones the Dify-on-Dingtalk project repository from GitHub. Alternatively, a Gitee mirror repository can be used if network issues are encountered with GitHub.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-dingtalk.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/zfanswer/dify-on-dingtalk.git\n# 有网络问题的可以从 gitee 镜像仓库拉取\n# git clone https://gitee.com/zfanswer/dify-on-dingtalk.git\n```\n\n----------------------------------------\n\nTITLE: API request for llms-full.txt\nDESCRIPTION: This code snippet shows how to fetch the `llms-full.txt` file using a `curl` command.  An active Firecrawl API key is needed for a successful request.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET 'http://llmstxt.firecrawl.dev/https://docs.dify.ai//full?FIRECRAWL_API_KEY=YOUR_API_KEY'\n```\n\n----------------------------------------\n\nTITLE: Upgrade Dify to Version 1.0.0\nDESCRIPTION: This snippet checks out the `1.0.0` branch and uses Docker Compose to upgrade the Dify version.  It switches the git branch and starts the updated containers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/dify-premium.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout 1.0.0 # 1.0.0 ブランチに切り替える\ncd docker\ndocker compose -f docker-compose.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Customizing iFrame Style - Adding Border\nDESCRIPTION: This code snippet demonstrates how to add a border to the Dify chatbot iFrame by modifying the `style` attribute.  A 2-pixel solid black border is added to the iFrame element. This customization allows for enhanced visual integration into the Wix website.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/how-to-integrate-dify-chatbot-to-your-wix-website.md#_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<iframe src=\"https://udify.app/chatbot/ez1pf83HVV3JgWO4\" style=\"width: 80%; height: 80%; min-height: 500px; border: 2px solid #000;\" frameborder=\"0\" allow=\"microphone\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: Creating a Variable Message in Dify (Python)\nDESCRIPTION: This code snippet demonstrates how to create a variable message in a Dify tool plugin for returning non-streaming output variables.  It takes the variable name (`variable_name`) and its value (`variable_value`) as input. If multiple variables with the same name are created, the last one will override previous ones.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/tool.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_variable_message(self, variable_name: str, variable_value: Any) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring Input Values for the Chatbot\nDESCRIPTION: This JavaScript code snippet demonstrates how to configure input values for the Dify Chatbot using the `inputs` option in the `window.difyChatbotConfig` object. It shows an example of setting a value for the `name` input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-publishing/embedding-in-websites.md#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.difyChatbotConfig = {\n    // 他の設定項目...\n    inputs: {\n        name: 'apple',\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Upgrade Dify Instance (AWS AMI) via Git and Docker\nDESCRIPTION: This snippet updates a Dify instance running on AWS AMI by cloning the latest code from the GitHub repository, moving the Docker configurations, and restarting the Docker containers. This ensures the instance is running the newest version of Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_64\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git /tmp/dify\nmv -f /tmp/dify/docker/* /dify/\nrm -rf /tmp/dify\ndocker-compose down\ndocker-compose pull\ndocker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Logging in Python\nDESCRIPTION: This code shows an example implementation of agent logging within the `FunctionCallingAgentStrategy`. It creates a log message with a \"start\" status when the agent starts thinking and then updates the log message to a \"Success\" status after the LLM invocation is complete. This allows developers to track the agent's thinking process and task execution in a structured way.  The code invokes an LLM model to create a response.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/agent.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass FunctionCallingAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        thinking_log = self.create_log_message(\n            data={\"Query\": parameters.get(\"query\")},\n            label=\"Thinking\",\n            status=AgentInvokeMessage.LogMessage.LogStatus.START,\n        )\n\n        yield thinking_log\n\n        llm_response = self.session.model.llm.invoke(\n            model_config=LLMModelConfig(\n                provider=\"openai\",\n                model=\"gpt-4o-mini\",\n                mode=\"chat\",\n                completion_params={},\n            ),\n            prompt_messages=[\n                SystemPromptMessage(content=\"you are a helpful assistant\"),\n                UserPromptMessage(content=parameters.get(\"query\")),\n            ],\n            stream=False,\n            tools=[],\n        )\n\n        thinking_log = self.finish_log_message(log=thinking_log)\n        yield thinking_log\n        yield self.create_text_message(text=llm_response.message.content)\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Role Enum Definition\nDESCRIPTION: This code snippet defines an enumeration for different roles in a prompt message, including SYSTEM, USER, ASSISTANT, and TOOL. These roles are used to classify the origin and purpose of a message within a conversation or prompt.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageRole(Enum):\n    \"\"\"\n    Enum class for prompt message.\n    \"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    TOOL = \"tool\"\n```\n\n----------------------------------------\n\nTITLE: Resetting Dify Password (Docker Compose)\nDESCRIPTION: Resets the Dify user password when using Docker Compose deployment.  Executes a flask command within the docker-api container to initiate the password reset process.  Requires Docker and Docker Compose to be installed and running.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-password\n```\n\n----------------------------------------\n\nTITLE: Adding Dify Scheduler Subscription to QingLong\nDESCRIPTION: This command adds the Dify Scheduler repository as a subscription to the QingLong panel. This allows the panel to access and manage the scheduler's scripts and configurations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/dify-schedule.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nql repo https://github.com/leochen-g/dify-schedule.git \"ql_\" \"utils\" \"sdk\"\n```\n\n----------------------------------------\n\nTITLE: Run Frontend Docker from Local Image\nDESCRIPTION: This command runs a Docker container from the locally built 'dify-web' image. It maps port 3000 of the container to port 3000 on the host and sets the CONSOLE_URL and APP_URL environment variables to point to the backend service running on localhost.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/start-the-frontend-docker-container.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run -it -p 3000:3000 -e CONSOLE_URL=http://127.0.0.1:5001 -e APP_URL=http://127.0.0.1:5001 dify-web\n```\n\n----------------------------------------\n\nTITLE: Image Prompt Message Content Class in Python\nDESCRIPTION: This snippet defines an `ImagePromptMessageContent` class inheriting from `PromptMessageContent` with a default type of 'image' and a detail resolution setting.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nclass ImagePromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for image prompt message content.\n    \"\"\"\n    class DETAIL(Enum):\n        LOW = 'low'\n        HIGH = 'high'\n\n    type: PromptMessageContentType = PromptMessageContentType.IMAGE\n    detail: DETAIL = DETAIL.LOW  # Resolution\n```\n\n----------------------------------------\n\nTITLE: Text Prompt Message Content Definition\nDESCRIPTION: Defines a class `TextPromptMessageContent` that inherits from `PromptMessageContent` and specifies the `type` as text.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nclass TextPromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for text prompt message content.\n    \"\"\"\n    type: PromptMessageContentType = PromptMessageContentType.TEXT\n```\n\n----------------------------------------\n\nTITLE: Image Prompt Message Content Class in Python\nDESCRIPTION: This code defines the Image Prompt Message Content class, inheriting from PromptMessageContent. It specifies that the content type is IMAGE and includes a DETAIL enum for specifying the resolution (LOW or HIGH).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass ImagePromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for image prompt message content.\n    \"\"\"\n    class DETAIL(Enum):\n        LOW = 'low'\n        HIGH = 'high'\n\n    type: PromptMessageContentType = PromptMessageContentType.IMAGE\n    detail: DETAIL = DETAIL.LOW  # Resolution\n```\n\n----------------------------------------\n\nTITLE: Start Docker Compose Service\nDESCRIPTION: This command starts the sandbox service using Docker Compose with the `docker-compose.middleware.yaml` configuration file. The `-f` flag specifies the Compose file, and the `up -d` command starts the services in detached mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_70\n\nLANGUAGE: Docker\nCODE:\n```\ndocker-compose -f docker-compose.middleware.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Chat Model Text Generation SYSTEM Prompt - Dify\nDESCRIPTION: This SYSTEM prompt is for building a text generation application using a chat model in Dify. Similar to the dialog template, it uses context within XML tags and instructs the LLM to avoid mentioning the source and answer in the user's language. The pre_prompt is also utilized.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nUse the following context as your learned knowledge, inside <context></context> XML tags.\n\n<context>\n{{#context#}}\n</context>\n\nWhen answer to user:\n- If you don't know, just say that you don't know.\n- If you don't know when you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n{{pre_prompt}}\n```\n\n----------------------------------------\n\nTITLE: Content Moderation Output Request Body Example\nDESCRIPTION: Illustrates the structure of the HTTP POST request body sent to the `app.moderation.output` extension point for reviewing LLM output content. It includes the extension point type, application ID, and the LLM's response text.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"point\": \"app.moderation.output\",\n    \"params\": {\n        \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n        \"text\": \"I will kill you.\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Customizing Chatbot Position\nDESCRIPTION: This code snippet demonstrates how to customize the position of the Dify Chatbot within the webpage. By modifying the `position`, `bottom`, and `right` styles, the chatbot is fixed to the bottom-right corner of the page, 20 pixels from the bottom and right edges.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/how-to-integrate-dify-chatbot-to-your-wix-website.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n<iframe src=\"https://udify.app/chatbot/1yS3gohroW1sKyLc\" style=\"width: 100%; height: 100%; min-height: 700px; position: fixed; bottom: 20px; right: 20px;\" frameborder=\"0\" allow=\"microphone\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: Get Cloudflare Worker Logs (Bash)\nDESCRIPTION: This bash script is used to get the Cloudflare Worker Logs.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nwrangler tail\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Web Interface (Server)\nDESCRIPTION: This URL is used to access the Dify web interface on a server environment. Replace `your_server_ip` with the actual IP address of the server.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhttp://your_server_ip\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Configuration for Plugin Daemon (YAML)\nDESCRIPTION: This YAML snippet shows how to configure the `plugin_daemon` service in a Docker Compose override file to enable third-party signature verification. It sets environment variables to enable signature verification (`THIRD_PARTY_SIGNATURE_VERIFICATION_ENABLED`) and specifies the path to the public key file (`THIRD_PARTY_SIGNATURE_VERIFICATION_PUBLIC_KEYS`).  `FORCE_VERIFYING_SIGNATURE` is also set to true.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n  plugin_daemon:\n    environment:\n      FORCE_VERIFYING_SIGNATURE: true\n      THIRD_PARTY_SIGNATURE_VERIFICATION_ENABLED: true\n      THIRD_PARTY_SIGNATURE_VERIFICATION_PUBLIC_KEYS: /app/storage/public_keys/your_key_pair.public.pem\n```\n\n----------------------------------------\n\nTITLE: LiteLLM Config YAML Example\nDESCRIPTION: This YAML configuration file is used by LiteLLM Proxy to define the models it will proxy.  It specifies the model name, litellm parameters (model, api_base, api_version, and api_key) for each model. This file is crucial for LiteLLM to connect to different LLM providers like Azure OpenAI.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_75\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_list:\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/chatgpt-v-2\n      api_base: https://openai-gpt-4-test-v-1.openai.azure.com/\n      api_version: \"2023-05-15\"\n      api_key:\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/gpt-4\n      api_key:\n      api_base: https://openai-gpt-4-test-v-2.openai.azure.com/\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/gpt-4\n      api_key:\n      api_base: https://openai-gpt-4-test-v-2.openai.azure.com/\n```\n\n----------------------------------------\n\nTITLE: Configuring Initial Prompt Template (Pre-prompt)\nDESCRIPTION: This snippet demonstrates an example of a pre-prompt used to guide the LLM's responses.  It sets the persona, constraints, and desired output format for a customer service assistant interacting with users regarding iPhone-related queries. The {{Format}} variable is intended to be dynamically populated.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/prompt-engineering/prompt-engineering-expert-mode.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nWhen answer to user:\n- If you don't know, just say that you don't know.\n- If you don't know when you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n\nYou are a customer service assistant for Apple Inc., and you can provide iPhone consultation services to users.\nWhen answering, you need to list the detailed specifications of the iPhone, and you must output this information in a vertical {{Format}} table. If the list is too long, it should be transposed.\nYou are allowed to take a long time to think to generate a more reasonable output.\nNote: You currently have knowledge of only a subset of iPhone models, not all of them.\n```\n\n----------------------------------------\n\nTITLE: Prompt with Content Restriction\nDESCRIPTION: This code snippet presents a structured prompt incorporating content restrictions to prevent the AI agent from generating inappropriate or irrelevant content. The agent will refuse to generate images based on requests that are not related to image drawing tasks, responding with a predefined message. It relies on understanding task and constraint sections.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/workshop/basic/build-ai-image-generation-app.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n## タスク\nユーザーのプロンプトに従って、指定された内容をstability_text2imageを使用して描画してください。画像はアニメスタイルです。\n\n## 制約\nもしユーザーが描画に関係のないコンテンツを要求した場合、「申し訳ありませんが、その内容は理解できません。」と返答してください。\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Template Definition\nDESCRIPTION: This JSON Schema defines a generic template with fields of various types (string, number, array, object) and their descriptions. It enforces strict adherence to the schema, requiring all defined fields and preventing additional properties.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/how-to-use-json-schema-in-dify.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"template_schema\",\n    \"description\": \"A generic template for JSON Schema\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"field1\": {\n                \"type\": \"string\",\n                \"description\": \"Description of field1\"\n            },\n            \"field2\": {\n                \"type\": \"number\",\n                \"description\": \"Description of field2\"\n            },\n            \"field3\": {\n                \"type\": \"array\",\n                \"description\": \"Description of field3\",\n                \"items\": {\n                    \"type\": \"string\"\n                }\n            },\n            \"field4\": {\n                \"type\": \"object\",\n                \"description\": \"Description of field4\",\n                \"properties\": {\n                    \"subfield1\": {\n                        \"type\": \"string\",\n                        \"description\": \"Description of subfield1\"\n                    }\n                },\n                \"required\": [\"subfield1\"],\n                \"additionalProperties\": false\n            }\n        },\n        \"required\": [\"field1\", \"field2\", \"field3\", \"field4\"],\n        \"additionalProperties\": false\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implement Speech-to-Text Model Invocation in Python\nDESCRIPTION: This code snippet demonstrates how to implement the `_invoke` method for a speech-to-text model, inheriting from the `Speech2TextModel` base class.  It defines the expected parameters: `model` (model name), `credentials` (model credentials), `file` (audio file), and `user` (unique user id). The method should return the transcribed text from the given audio file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, file: IO[bytes], user: Optional[str] = None) -> str:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param file: audio file\n    :param user: unique user id\n    :return: text for given audio file\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Function Calling Parameters in Python\nDESCRIPTION: This code defines the data structures for handling function calling parameters within an agent. It includes the `FunctionCallingParams` class, which inherits from Pydantic's `BaseModel`, and the `FunctionCallingAgentStrategy` class, which inherits from `AgentStrategy`. The `FunctionCallingParams` encapsulates query, model, and tools information, while `FunctionCallingAgentStrategy` is used to execute a FunctionCall agent application, extracting parameters from a dictionary.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/agent.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\n\nfrom pydantic import BaseModel\n\nclass FunctionCallingParams(BaseModel):\n    query: str\n    model: AgentModelConfig\n    tools: list[ToolEntity] | None\n    maximum_iterations: int = 3\n    \nclass FunctionCallingAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        \"\"\"\n        Run FunctionCall agent application\n        \"\"\"\n        fc_params = FunctionCallingParams(**parameters)\n```\n\n----------------------------------------\n\nTITLE: Cloning a Git Repository\nDESCRIPTION: This code snippet demonstrates how to clone a forked repository using Git.  Replace <github_username> with your actual GitHub username.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/community/contribution.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone git@github.com:<github_username>/dify.git\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Docker Directory\nDESCRIPTION: This command changes the current directory to the 'docker' directory within the Dify source code, where the Docker Compose configuration files are located. This is necessary to execute subsequent Docker commands.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/docker-compose.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd dify/docker\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository with Git\nDESCRIPTION: This command clones the forked Dify repository from GitHub to your local machine. Replace <github_username> with your GitHub username. This allows you to work on a local copy of the repository.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_27\n\nLANGUAGE: git\nCODE:\n```\ngit clone git@github.com:<github_username>/dify.git\n```\n\n----------------------------------------\n\nTITLE: Creating Public Keys Directory (bash)\nDESCRIPTION: This command creates a directory to store public keys within the plugin daemon's volume. This ensures the public keys used for signature verification are accessible to the Dify plugin daemon. Requires bash shell.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmkdir docker/volumes/plugin_daemon/public_keys\n```\n\n----------------------------------------\n\nTITLE: Starting Middleware Services\nDESCRIPTION: This command starts the PostgreSQL, Redis, and Weaviate middleware services using Docker Compose, if they are not already running locally. It utilizes the `docker-compose.middleware.yaml` file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ncd docker\ndocker compose -f docker-compose.middleware.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Inspect Docker Containers\nDESCRIPTION: This command lists the running Docker containers, providing information such as container ID, image, command, and ports. It's used to identify the docker-api container ID.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/dify-premium.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker ps\n```\n\n----------------------------------------\n\nTITLE: Verifying a Plugin Signature in Dify (Bash)\nDESCRIPTION: This command verifies the signature of a Dify plugin file using a public key. It checks if the plugin was signed with the corresponding private key. The command takes the signed plugin file (`.signed.difypkg`) and the public key (`.pem`) as input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndify signature verify your_plugin_project.signed.difypkg -p your_key_pair.public.pem\n```\n\n----------------------------------------\n\nTITLE: Endpoint Group Configuration Definition YAML\nDESCRIPTION: This YAML snippet defines the configuration for an Endpoint group. It specifies settings such as an API key (with type, requirement, labels, and placeholder texts in multiple languages) and a list of endpoint interface definitions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/endpoint.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsettings:\n  api_key:\n    type: secret-input\n    required: true\n    label:\n      en_US: API key\n      zh_Hans: API key\n      ja_Jp: API key\n      pt_BR: API key\n    placeholder:\n      en_US: Please input your API key\n      zh_Hans: 请输入你的 API key\n      ja_Jp: あなたの API key を入れてください\n      pt_BR: Por favor, insira sua chave API\nendpoints:\n  - endpoints/duck.yaml\n  - endpoints/neko.yaml\n```\n\n----------------------------------------\n\nTITLE: Starting Stable Diffusion WebUI on Windows (Bash)\nDESCRIPTION: These commands navigate to the Stable Diffusion WebUI directory and start the WebUI with API and listen flags enabled. These flags allow external applications like Dify to interact with the WebUI.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/stable-diffusion.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd stable-diffusion-webui\n./webui.bat --api --listen\n```\n\n----------------------------------------\n\nTITLE: Convert Array to Text with Template Node - Jinja2\nDESCRIPTION: This Jinja2 code snippet shows how to convert an array (`articleSections`) to a text string, using newline characters as separators. The `join` filter is used for the conversion. The output is a single string with elements from the array joined by newline characters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_75\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ articleSections | join(\"/n\") }}\n```\n\n----------------------------------------\n\nTITLE: Dify Backend Directory Structure\nDESCRIPTION: This section outlines the directory structure of Dify's backend, which is written in Python using the Flask framework. It provides an overview of each directory and its purpose within the application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/community/contribution.md#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\n[api/]\n├── constants             // コードベース全体で使用される定数設定。\n├── controllers           // APIルート定義とリクエスト処理ロジック。           \n├── core                  // コアアプリケーションオーケストレーション、モデル統合、ツール。\n├── docker                // Dockerおよびコンテナ化関連の設定。\n├── events                // イベント処理と処理\n├── extensions            // サードパーティフレームワーク/プラットフォームとの拡張機能。\n├── fields                // シリアライズ/マーシャリングのためのフィールド定義。\n├── libs                  // 再利用可能なライブラリとヘルパー。\n├── migrations            // データベース移行のためのスクリプト。\n├── models                // データベースモデルとスキーマ定義。\n├── services              // ビジネスロジックを指定。\n├── storage               // 秘密鍵保管。      \n├── tasks                 // 非同期タスクとバックグラウンドジョブの処理。\n└── tests\n```\n\n----------------------------------------\n\nTITLE: Call Dify AI Application API using Curl\nDESCRIPTION: This code snippet demonstrates how to call a Dify AI application's API using a curl command. It requires the API secret key and optionally a conversation ID.  The query parameter specifies the user's question, and the response_mode parameter requests a streaming response. Ensure you replace 'ENTER-YOUR-SECRET-KEY' with your actual Dify API key and optionally populate 'conversation_id' with the previous conversation's ID to maintain context. The `user` field can be any unique identifier.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/build-an-notion-ai-assistant.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/chat-messages' \\\n--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"inputs\": {},\n    \"query\": \"eh\",\n    \"response_mode\": \"streaming\",\n    \"conversation_id\": \"\",\n    \"user\": \"abc-123\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Initializing the Agent Plugin Template (Bash)\nDESCRIPTION: This command initializes the Agent plugin development template, creating a code folder containing all necessary resources. The user will be prompted to enter information such as plugin name, author, and description. The selected language should be Python, and the template type should be agent-strategy.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: Embedding Dify Chatbot iFrame\nDESCRIPTION: This code snippet demonstrates how to embed a Dify chatbot into a webpage using an iFrame.  The `src` attribute specifies the URL of the chatbot, and the `style` attribute defines the dimensions and other visual properties. The `allow=\"microphone\"` attribute enables microphone access for the chatbot.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/how-to-integrate-dify-chatbot-to-your-wix-website.md#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe src=\"https://udify.app/chatbot/ez1pf83HVV3JgWO4\" style=\"width: 100%; height: 100%; min-height: 700px\" frameborder=\"0\" allow=\"microphone\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: Successful Response Example\nDESCRIPTION: This snippet shows the JSON format for a successful response from the External Knowledge API. It includes a list of 'records', where each record contains the 'content', 'score', 'title', and 'metadata' of a retrieved document. The 'metadata' contains attributes specific to that document.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/external-knowledge-api-documentation.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"records\": [{\n                    \"metadata\": {\n                            \"path\": \"s3://dify/knowledge.txt\",\n                            \"description\": \"dify knowledge document\"\n                    },\n                    \"score\": 0.98,\n                    \"title\": \"knowledge.txt\",\n                    \"content\": \"This is the document for external knowledge.\"\n            },\n            {\n                    \"metadata\": {\n                            \"path\": \"s3://dify/introduce.txt\",\n                            \"description\": \"dify introduce\"\n                    },\n                    \"score\": 0.66,\n                    \"title\": \"introduce.txt\",\n                    \"content\": \"The Innovation Engine for GenAI Applications\"\n            }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Exposing Ollama on a Network\nDESCRIPTION: Ollama binds to 127.0.0.1 port 11434 by default. To expose it on a network, you need to change the bind address using the `OLLAMA_HOST` environment variable. This allows Ollama to be accessed from other machines on the network.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_85\n\nLANGUAGE: text\nCODE:\n```\nOllama binds 127.0.0.1 port 11434 by default. Change the bind address with the `OLLAMA_HOST` environment variable.\n```\n\n----------------------------------------\n\nTITLE: Building Frontend Code\nDESCRIPTION: Builds the frontend code using NPM. This compiles the source code into optimized static assets for deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_14\n\nLANGUAGE: Bash\nCODE:\n```\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: API request for llms.txt\nDESCRIPTION: This code snippet demonstrates how to retrieve the `llms.txt` file via an API request using `curl`. It requires a Firecrawl API key to access the content.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET 'http://llmstxt.firecrawl.dev/https://docs.dify.ai/?FIRECRAWL_API_KEY=YOUR_API_KEY'\n```\n\n----------------------------------------\n\nTITLE: Deploying OpenLLM Model with Docker\nDESCRIPTION: This command deploys an OpenLLM model using Docker. It pulls the OpenLLM image, starts the server, and exposes it on port 3333. The specified model (facebook/opt-1.3b) is used with the PyTorch backend.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_19\n\nLANGUAGE: Docker\nCODE:\n```\ndocker run --rm -it -p 3333:3000 ghcr.io/bentoml/openllm start facebook/opt-1.3b --backend pt\n```\n\n----------------------------------------\n\nTITLE: Defining ToolPromptMessage Model (Python)\nDESCRIPTION: Defines the `ToolPromptMessage` model, representing tool messages conveying the results of tool execution to the model. It inherits from `PromptMessage` and includes fields for `role` and `tool_call_id`. The `content` field from the base class is used to store the results of the tool execution.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nclass ToolPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for tool prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.TOOL\n    tool_call_id: str  # Tool invocation ID. If OpenAI tool call is not supported, the name of the tool can also be inputted.\n```\n\n----------------------------------------\n\nTITLE: Text Prompt Message Content Class in Python\nDESCRIPTION: This snippet defines a `TextPromptMessageContent` class inheriting from `PromptMessageContent` with a default type of 'text'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nclass TextPromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for text prompt message content.\n    \"\"\"\n    type: PromptMessageContentType = PromptMessageContentType.TEXT\n```\n\n----------------------------------------\n\nTITLE: Example Password Reset Interaction\nDESCRIPTION: This example shows the interaction within the docker container when running the password reset command. It includes the prompts for email and the new password and the confirmation message.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/faq.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndify@my-pc:~/hello/dify/docker$ docker compose up -d\n[+] Running 9/9\n ✔ Container docker-web-1         Started                                                              0.1s \n ✔ Container docker-sandbox-1     Started                                                              0.1s \n ✔ Container docker-db-1          Started                                                              0.1s \n ✔ Container docker-redis-1       Started                                                              0.1s \n ✔ Container docker-weaviate-1    Started                                                              0.1s \n ✔ Container docker-ssrf_proxy-1  Started                                                              0.1s \n ✔ Container docker-api-1         Started                                                              0.1s \n ✔ Container docker-worker-1      Started                                                              0.1s \n ✔ Container docker-nginx-1       Started                                                              0.1s \ndify@my-pc:~/hello/dify/docker$ docker exec -it docker-api-1 flask reset-password\nNone of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\nsagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\nsagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\nEmail: hello@dify.ai\nNew password: newpassword4567\nPassword confirm: newpassword4567\nPassword reset successfully.\n```\n\n----------------------------------------\n\nTITLE: OpenAI Model Credential Schema YAML Configuration\nDESCRIPTION: This YAML snippet defines the model credential schema for OpenAI, which allows for custom model configurations like fine-tuned models. It includes fields for the model name, API key, organization ID, and API base URL, demonstrating how to configure credentials for individual models within a provider. It specifies the API key as a required secret input and the others as optional text inputs.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/new-provider.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nmodel_credential_schema:\n  model: # 微调模型名称\n    label:\n      en_US: Model Name\n      zh_Hans: 模型名称\n    placeholder:\n      en_US: Enter your model name\n      zh_Hans: 输入模型名称\n  credential_form_schemas:\n  - variable: openai_api_key\n    label:\n      en_US: API Key\n    type: secret-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入你的 API Key\n      en_US: Enter your API Key\n  - variable: openai_organization\n    label:\n        zh_Hans: 组织 ID\n        en_US: Organization\n    type: text-input\n    required: false\n    placeholder:\n      zh_Hans: 在此输入你的组织 ID\n      en_US: Enter your Organization ID\n  - variable: openai_api_base\n    label:\n      zh_Hans: API Base\n      en_US: API Base\n    type: text-input\n    required: false\n    placeholder:\n      zh_Hans: 在此输入你的 API Base\n      en_US: Enter your API Base\n```\n\n----------------------------------------\n\nTITLE: Edit Ollama Systemd Service on Linux\nDESCRIPTION: This command opens the systemd service file for Ollama, allowing you to edit the service configuration. This is used to set environment variables like OLLAMA_HOST, making the Ollama service accessible on the network.  After editing, the systemd daemon needs to be reloaded and the Ollama service restarted.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/ollama.md#_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Dify Backend Directory Structure\nDESCRIPTION: This snippet outlines the directory structure of the Dify backend. It's written in Python and utilizes the Flask framework, SQLAlchemy ORM, and Celery task queue. It shows the purpose of each folder like API route definitions, database models, and task handling.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_15\n\nLANGUAGE: Text\nCODE:\n```\n[api/]\n├── constants             // Constant settings used throughout code base.\n├── controllers           // API route definitions and request handling logic.\n├── core                  // Core application orchestration, model integrations, and tools.\n├── docker                // Docker & containerization related configurations.\n├── events                // Event handling and processing\n├── extensions            // Extensions with 3rd party frameworks/platforms.\n├── fields                //field definitions for serialization/marshalling.\n├── libs                  // Reusable libraries and helpers.\n├── migrations            // Scripts for database migration.\n├── models                // Database models & schema definitions.\n├── services              // Specifies business logic.\n├── storage               // Private key storage.\n├── tasks                 // Handling of async tasks and background jobs.\n└── tests\n```\n\n----------------------------------------\n\nTITLE: Checking dify-plugin-darwin-arm64 version\nDESCRIPTION: This command executes the Dify plugin scaffolding tool to verify its installation and display the installed version number. It confirms that the tool is correctly installed and accessible.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/initialize-development-tools.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./dify-plugin-darwin-arm64 version\n```\n\n----------------------------------------\n\nTITLE: Chat Interface Specification\nDESCRIPTION: Defines the signature of the `invoke` method for chat interface calls. It outlines the expected input parameters: `app_id` (application identifier), `inputs` (input data dictionary), `response_mode` (specifies streaming or blocking response), `conversation_id` (identifier for the ongoing conversation), and `files` (list of file objects). It returns a generator for streaming (`Generator[dict, None, None]`) or a dictionary (`dict`).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    def invoke(\n        self,\n        app_id: str,\n        inputs: dict,\n        response_mode: Literal[\"streaming\", \"blocking\"],\n        conversation_id: str,\n        files: list,\n    ) -> Generator[dict, None, None] | dict:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Calling Text Completion API with Python\nDESCRIPTION: This Python script uses the `requests` library to call the `/v1/completion-messages` endpoint for text completion.  It sets the `Authorization` and `Content-Type` headers and sends a JSON payload containing inputs (including the text to complete), `response_mode`, and `user`.  It requires the `requests` and `json` libraries. Replace `ENTER-YOUR-SECRET-KEY` with your actual API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-publishing/developing-with-apis.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport requests\nimport json\n\nurl = \"https://api.dify.ai/v1/completion-messages\"\n\nheaders = {\n    'Authorization': 'Bearer ENTER-YOUR-SECRET-KEY',\n    'Content-Type': 'application/json',\n}\n\ndata = {\n    \"inputs\": {\"text\": 'Hello, how are you?'},\n    \"response_mode\": \"streaming\",\n    \"user\": \"abc-123\"\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Chat App Invocation Example\nDESCRIPTION: Demonstrates a complete endpoint implementation that invokes a Workflow-type Dify application. It defines an `Endpoint` class with an `_invoke` method that retrieves the `app_id` from request values and then calls `self.session.app.workflow.invoke` with appropriate parameters for streaming mode. The response is formatted as HTML.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom typing import Mapping\nfrom werkzeug import Request, Response\nfrom dify_plugin import Endpoint\n\nclass Duck(Endpoint):\n    def _invoke(self, r: Request, values: Mapping, settings: Mapping) -> Response:\n        \"\"\"\n        Invokes the endpoint with the given request.\n        \"\"\"\n        app_id = values[\"app_id\"]\n\n        def generator():\n            response = self.session.app.workflow.invoke(\n                app_id=app_id, inputs={}, response_mode=\"streaming\", files=[]\n            )\n\n            for data in response:\n                yield f\"{json.dumps(data)} <br>\"\n\n        return Response(generator(), status=200, content_type=\"text/html\")\n```\n\n----------------------------------------\n\nTITLE: Customizing Dify Chatbot Button Style with containerProps (Class)\nDESCRIPTION: This JavaScript snippet demonstrates how to customize the Dify chatbot bubble button's appearance using the `containerProps` option with CSS classes.  It shows how to apply custom CSS classes to the button.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-publishing/embedding-in-websites.md#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.difyChatbotConfig = {\n    // ... 其他配置\n    containerProps: {\n        className: 'dify-chatbot-bubble-button-custom my-custom-class',\n    },\n};\n```\n\n----------------------------------------\n\nTITLE: API Call Example with curl\nDESCRIPTION: Demonstrates how to retrieve llms.txt or llms-full.txt using curl and a Firecrawl API key. The first command retrieves 'llms.txt', while the second retrieves 'llms-full.txt'. Ensure to replace 'YOUR_API_KEY' with your actual Firecrawl API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET 'http://llmstxt.firecrawl.dev/https://docs.dify.ai/?FIRECRAWL_API_KEY=YOUR_API_KEY'\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET 'http://llmstxt.firecrawl.dev/https://docs.dify.ai//full?FIRECRAWL_API_KEY=YOUR_API_KEY'\n```\n\n----------------------------------------\n\nTITLE: Repository Structure Example\nDESCRIPTION: Example directory structure for the forked dify-plugins repository, showing the organization required for plugin contributions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/plugin-auto-publish-pr.md#_snippet_0\n\nLANGUAGE: TEXT\nCODE:\n```\ndify-plugins/\n└── your-author-name\n    └── plugin-name\n```\n\n----------------------------------------\n\nTITLE: Dify Frontend Directory Structure\nDESCRIPTION: This outlines the directory structure of the Dify frontend, based on Next.js.  It includes components, assets, configuration, and utility functions. It uses Typescript and Tailwind CSS.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_14\n\nLANGUAGE: Text\nCODE:\n```\n[web/]\n├── app                   //layouts, pages, and components\n│   ├── (commonLayout)    //common layout used throughout the app\n│   ├── (shareLayout)     //layouts specifically shared across token-specific sessions\n│   ├── activate          //activate page\n│   ├── components        //shared by pages and layouts\n│   ├── install           //install page\n│   ├── signin            //signin page\n│   └── styles            //globally shared styles\n├── assets                // Static assets\n├── bin                   //scripts ran at build step\n├── config                //adjustable settings and options\n├── context               //shared contexts used by different portions of the app\n├── dictionaries          // Language-specific translate files\n├── docker                //container configurations\n├── hooks                 // Reusable hooks\n├── i18n                  // Internationalization configuration\n├── models                //describes data models & shapes of API responses\n├── public                //meta assets like favicon\n├── service               //specifies shapes of API actions\n├── test\n├── types                 //descriptions of function params and return values\n└── utils                 // Shared utility functions\n```\n\n----------------------------------------\n\nTITLE: Running the Plugin\nDESCRIPTION: This command starts the plugin using the Python interpreter. This allows the plugin to be deployed after remote debugging configurations have been properly setup.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m main\n```\n\n----------------------------------------\n\nTITLE: Define RerankResult Model\nDESCRIPTION: Defines the `RerankResult` class, which represents the result of a reranking call. It includes the model used and the reranked document list.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nclass RerankResult(BaseModel):\n    \"\"\"\n    Model class for rerank result.\n    \"\"\"\n    model: str  # Actual model used\n    docs: list[RerankDocument]  # Reranked document list\n```\n\n----------------------------------------\n\nTITLE: Grant Execute Permission to Dify Plugin CLI - Bash\nDESCRIPTION: This command grants execute permissions to the downloaded Dify plugin CLI tool. This is required to run the tool on macOS and other Unix-like systems. The command changes the file permissions to allow the user to execute the file as a program.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/initialize-development-tools.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nchmod +x dify-plugin-darwin-arm64\n```\n\n----------------------------------------\n\nTITLE: Starting Worker Service (Windows Bash)\nDESCRIPTION: Starts the Celery worker service to consume asynchronous tasks from the queue on Windows. It uses the solo pool and disables gossip and mingle, specifying the queues to listen on.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\ncelery -A app.celery worker -P solo --without-gossip --without-mingle -Q dataset,generation,mail,ops_trace --loglevel INFO\n```\n\n----------------------------------------\n\nTITLE: Starting the API Service\nDESCRIPTION: This command starts the Dify API service using Flask. It binds the service to all addresses (0.0.0.0) and specifies the port (5001) and enables debug mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run flask run --host 0.0.0.0 --port=5001 --debug\n```\n\n----------------------------------------\n\nTITLE: Initializing Git Repository in Bash\nDESCRIPTION: This snippet initializes a local Git repository, adds all files, and commits them with an initial message.  It is a prerequisite for pushing the plugin code to a remote GitHub repository. It requires Git to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit init\ngit add .\ngit commit -m \"Initial commit: Add plugin files\"\n```\n\n----------------------------------------\n\nTITLE: Integrate History Messages (Python)\nDESCRIPTION: This Python code shows how to incorporate history messages when invoking the LLM.  The `history_prompt_messages` from the model configuration are combined with the current user query to provide the model with conversational context.  The prompt_messages parameter is populated by combining history and current message.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n\n        chunks: Generator[LLMResultChunk, None, None] | LLMResult = (\n            self.session.model.llm.invoke(\n                model_config=LLMModelConfig(**params.model.model_dump(mode=\"json\")),\n                # 添加历史消息\n                prompt_messages=params.model.history_prompt_messages\n                + [UserPromptMessage(content=params.query)],\n                tools=[\n                    self._convert_tool_to_prompt_message_tool(tool)\n                    for tool in params.tools\n                ],\n                stop=params.model.completion_params.get(\"stop\", [])\n                if params.model.completion_params\n                else [],\n                stream=True,\n            )\n        )\n        ...\n```\n\n----------------------------------------\n\nTITLE: Updating Dify with Git and Docker Compose\nDESCRIPTION: These commands update the Dify application by pulling the latest changes from the Git repository and rebuilding the Docker containers. This assumes the user is already in the `dify/docker` directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncd dify/docker\ndocker compose down\ngit pull origin main\ndocker compose pull\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Change Directory API\nDESCRIPTION: Navigates to the api directory, where the Dify API service code is located. This is a prerequisite for installing dependencies and starting the API service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_38\n\nLANGUAGE: Bash\nCODE:\n```\ncd api\n```\n\n----------------------------------------\n\nTITLE: Anthropic Model Plugin Structure\nDESCRIPTION: This example demonstrates the model plugin structure for Anthropic, showing the 'llm' category and various Claude models available.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n- Anthropic\n  - llm\n    claude-3-5-sonnet-20240620\n    claude-3-haiku-20240307\n    claude-3-opus-20240229\n    claude-3-sonnet-20240229\n    claude-instant-1.2\n    claude-instant-1\n```\n\n----------------------------------------\n\nTITLE: LiteLLM Configuration YAML\nDESCRIPTION: This YAML configuration defines a model list for LiteLLM. It specifies the model name as gpt-4 and provides connection parameters for an Azure OpenAI instance, including the API base URL, API version, and API key. This configuration allows LiteLLM to access and manage the specified language models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_77\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_list:\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/chatgpt-v-2\n      api_base: https://openai-gpt-4-test-v-1.openai.azure.com/\n      api_version: \"2023-05-15\"\n      api_key:\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/gpt-4\n      api_key:\n      api_base: https://openai-gpt-4-test-v-2.openai.azure.com/\n  - model_name: gpt-4\n    litellm_params:\n      model: azure/gpt-4\n      api_key:\n      api_base: https://openai-gpt-4-test-v-2.openai.azure.com/\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Documentation Repository\nDESCRIPTION: This command clones the Dify documentation repository from GitHub to your local machine. Replace `<your-github-account>` with your GitHub username.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_64\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/<your-github-account>/dify-docs.git\n```\n\n----------------------------------------\n\nTITLE: LLM Prompt for JSON Code Generation\nDESCRIPTION: This prompt instructs the LLM to generate either correct or incorrect JSON code samples based on user requirements. It's used in the LLM node within the workflow to simulate scenarios where the code node might receive invalid JSON.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/error-handling/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nYou are a teaching assistant. According to the user's requirements, you only output a correct or incorrect sample code in json format.\n```\n\n----------------------------------------\n\nTITLE: Prompt with Task and Constraints\nDESCRIPTION: This prompt includes both a task instruction and constraints, demonstrating how to handle user requests and prevent the Agent from generating inappropriate or irrelevant content. It also showcases how to format prompts using markdown-like structures.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/workshop/basic/build-ai-image-generation-app.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n## Task\nDraw the specified content according to the user's prompt using stability_text2image, the picture is in anime style.\n\n## Constraints\nIf the user requests content unrelated to drawing, reply: \"Sorry, I don't understand what you're saying.\"\n```\n\n----------------------------------------\n\nTITLE: Dify Frontend Directory Structure\nDESCRIPTION: This snippet shows the directory structure of the Dify frontend, built using Next.js with TypeScript and styled with Tailwind CSS.  It describes the purpose of each directory within the frontend architecture, covering layouts, pages, components, assets, configurations, and API service specifications.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_15\n\nLANGUAGE: Text\nCODE:\n```\n[web/]\n├── app                   //layouts, pages, and components\n│   ├── (commonLayout)    //common layout used throughout the app\n│   ├── (shareLayout)     //layouts specifically shared across token-specific sessions\n│   ├── activate          //activate page\n│   ├── components        //shared by pages and layouts\n│   ├── install           //install page\n│   ├── signin            //signin page\n│   └── styles            //globally shared styles\n├── assets                // Static assets\n├── bin                   //scripts ran at build step\n├── config                //adjustable settings and options\n├── context               //shared contexts used by different portions of the app\n├── dictionaries          // Language-specific translate files\n├── docker                //container configurations\n├── hooks                 // Reusable hooks\n├── i18n                  // Internationalization configuration\n├── models                //describes data models & shapes of API responses\n├── public                //meta assets like favicon\n├── service               //specifies shapes of API actions\n├── test\n├── types                 //descriptions of function params and return values\n└── utils                 // Shared utility functions\n```\n\n----------------------------------------\n\nTITLE: Restarting Dify after Configuration Changes\nDESCRIPTION: These commands restart the Dify Docker containers after modifying the environment variables in the .env file.  The 'down' command stops the running containers and the 'up -d' command recreates and starts the containers in detached mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/docker-compose.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Google Search Tool YAML Configuration\nDESCRIPTION: This YAML file defines the configuration for the Google Search tool, including identity, description, parameters, and extra information such as the Python source file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: google_search\n  author: Dify\n  label:\n    en_US: GoogleSearch\n    zh_Hans: 谷歌搜索\n    pt_BR: GoogleSearch\ndescription:\n  human:\n    en_US: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.\n    zh_Hans: 一个用于执行 Google SERP 搜索并提取片段和网页的工具。输入应该是一个搜索查询。\n    pt_BR: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.\n  llm: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.\nparameters:\n  - name: query\n    type: string\n    required: true\n    label:\n      en_US: Query string\n      zh_Hans: 查询语句\n      pt_BR: Query string\n    human_description:\n      en_US: used for searching\n      zh_Hans: 用于搜索网页内容\n      pt_BR: used for searching\n    llm_description: key words for searching\n    form: llm\nextra:\n  python:\n    source: tools/google_search.py\n```\n\n----------------------------------------\n\nTITLE: Calculating Variance of an Array with Python\nDESCRIPTION: This Python code snippet calculates the variance of a given list of numbers. It defines a `main` function that takes a list `x` as input and returns a dictionary containing the calculated variance under the key 'result'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\ndef main(x: list) -> float:\n    return {\n        # Note to declare 'result' in the output variables\n        'result': sum([(i - sum(x) / len(x)) ** 2 for i in x]) / len(x)\n    }\n```\n\n----------------------------------------\n\nTITLE: Disable Plugin Signature Verification in Dify\nDESCRIPTION: This snippet demonstrates how to disable plugin signature verification in Dify by adding a field to the `.env` configuration file. This allows the installation of plugins not listed on the Dify Marketplace. Note that this action may introduce security risks and is recommended only for testing or sandbox environments.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/faq.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ndocker compose down\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Define PromptMessageContentType Enum in Python\nDESCRIPTION: This code defines an Enum class `PromptMessageContentType` which specifies the type of content within a prompt message. It currently supports 'text' and 'image' content types. This enum is used to distinguish different types of message payloads.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContentType(Enum):\n    \"\"\"\n    Enum class for prompt message content type.\n    \"\"\"\n    TEXT = 'text'\n    IMAGE = 'image'\n```\n\n----------------------------------------\n\nTITLE: Navigating to the API Directory\nDESCRIPTION: Changes the current directory to the 'api' directory. This is a prerequisite for setting up the API service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_21\n\nLANGUAGE: Bash\nCODE:\n```\ncd api\n```\n\n----------------------------------------\n\nTITLE: Response Syntax Example\nDESCRIPTION: Shows the expected response format in JSON, including the `records` array with `content`, `score`, `title`, and `metadata` for each retrieved document. The example provides sample values for demonstration purposes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_24\n\nLANGUAGE: JSON\nCODE:\n```\nHTTP/1.1 200\nContent-type: application/json\n{\n    \"records\": [{\n                    \"metadata\": {\n                            \"path\": \"s3://dify/knowledge.txt\",\n                            \"description\": \"dify knowledge document\"\n                    },\n                    \"score\": 0.98,\n                    \"title\": \"knowledge.txt\",\n                    \"content\": \"This is the document for external knowledge.\"\n            },\n            {\n                    \"metadata\": {\n                            \"path\": \"s3://dify/introduce.txt\",\n                            \"description\": \"dify introduce\"\n                    },\n                    \"score\": 0.66,\n                    \"title\": \"introduce.txt\",\n                    \"content\": \"The Innovation Engine for GenAI Applications\"\n            }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Update PostgreSQL Configuration for Docker\nDESCRIPTION: Updates the `pg_hba.conf` file inside the `docker-db-1` container to allow connections from a specific network segment. This resolves database connection errors.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/faqs.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-db-1 sh -c \"echo 'host all all 172.19.0.0/16 trust' >> /var/lib/postgresql/data/pgdata/pg_hba.conf\"\ndocker-compose restart\n```\n\n----------------------------------------\n\nTITLE: User Prompt Message Class\nDESCRIPTION: Defines the `UserPromptMessage` class, a subclass of `PromptMessage`, representing a message from the user. It sets the role to USER.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass UserPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for user prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.USER\n```\n\n----------------------------------------\n\nTITLE: Define Wikipedia Tool Provider (No Credentials)\nDESCRIPTION: This YAML configuration defines the identity of the Wikipedia tool provider. It includes the author, name, multi-language labels, description, and icon.  Since Wikipedia requires no API keys, the `credentials_for_provider` section is absent.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/quick-tool-integration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: Dify\n  name: wikipedia\n  label:\n    en_US: Wikipedia\n    zh_Hans: 维基百科\n    ja_JP: Wikipedia\n    pt_BR: Wikipedia\n  description:\n    en_US: Wikipedia is a free online encyclopedia, created and edited by volunteers around the world.\n    zh_Hans: 维基百科是一个由全世界的志愿者创建和编辑的免费在线百科全书。\n    ja_JP: Wikipediaは、世界中のボランティアによって作成、編集されている無料のオンライン百科事典です。\n    pt_BR: A Wikipédia é uma enciclopédia online gratuita, criada e editada por voluntários ao redor do mundo.\n  icon: icon.svg\ncredentials_for_provider:\n```\n\n----------------------------------------\n\nTITLE: Initializing a Dify Plugin Project\nDESCRIPTION: This command initializes a new Dify plugin project using the dify-plugin-darwin-arm64 binary. It is assumed the binary is in the current directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./dify-plugin-darwin-arm64 plugin init\n```\n\n----------------------------------------\n\nTITLE: Cloning the Sample GitHub Repository\nDESCRIPTION: This command clones the provided GitHub repository containing a sample API extension, which can be modified to create custom extensions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/crazywoola/dify-extension-workers.git\ncp wrangler.toml.example wrangler.toml\n```\n\n----------------------------------------\n\nTITLE: Defining Frontend Component Schema (JSON)\nDESCRIPTION: This JSON schema defines the structure and properties of the frontend form used to configure the 'Weather Search' external data tool. It specifies the input fields, labels, options, and default values for the temperature unit selection.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/code-based-extension/external-data-tool.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"label\": {\n        \"en-US\": \"Weather Search\",\n        \"zh-Hans\": \"天気検索\"\n    },\n    \"form_schema\": [\n        {\n            \"type\": \"select\",\n            \"label\": {\n                \"en-US\": \"Temperature Unit\",\n                \"zh-Hans\": \"温度単位\"\n            },\n            \"variable\": \"temperature_unit\",\n            \"required\": true,\n            \"options\": [\n                {\n                    \"label\": {\n                        \"en-US\": \"Fahrenheit\",\n                        \"zh-Hans\": \"華氏度\"\n                    },\n                    \"value\": \"fahrenheit\"\n                },\n                {\n                    \"label\": {\n                        \"en-US\": \"Centigrade\",\n                        \"zh-Hans\": \"摂氏度\"\n                    },\n                    \"value\": \"centigrade\"\n                }\n            ],\n            \"default\": \"centigrade\",\n            \"placeholder\": \"Please select temperature unit\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Query Logic (Python - Template)\nDESCRIPTION: This Python code provides a template for implementing the query function within a custom external data tool.  It demonstrates the structure and parameter passing.  The user must implement their own logic in place of the provided comments.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/code-based-extension/external-data-tool.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nfrom core.external_data_tool.base import ExternalDataTool\n\n\nclass WeatherSearch(ExternalDataTool):\n    \"\"\"\n    The name of custom type must be unique, keep the same with directory and file name.\n    \"\"\"\n    name: str = \"weather_search\"\n\n    @classmethod\n    def validate_config(cls, tenant_id: str, config: dict) -> None:\n        \"\"\"\n        schema.json validation. It will be called when user save the config.\n\n        :param tenant_id: the id of workspace\n        :param config: the variables of form config\n        :return:\n        \"\"\"\n\n        # implement your own logic here\n\n    def query(self, inputs: dict, query: Optional[str] = None) -> str:\n        \"\"\"\n        Query the external data tool.\n\n        :param inputs: user inputs\n        :param query: the query of chat app\n        :return: the tool query result\n        \"\"\"\n       \n        # implement your own logic here\n        return \"your own data.\"\n```\n\n----------------------------------------\n\nTITLE: Customizing Button Styles using containerProps (Inline Styles)\nDESCRIPTION: This JavaScript code snippet demonstrates how to customize the Dify Chatbot Bubble Button's style using the `containerProps` option with inline styles. It sets the background color, width, height, and border radius of the button. The snippet illustrates setting styles using both an object and a string for the `style` attribute.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-publishing/embedding-in-websites.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.difyChatbotConfig = {\n    // ... other configurations\n    containerProps: {\n        style: {\n            backgroundColor: '#ABCDEF',\n            width: '60px',\n            height: '60px',\n            borderRadius: '30px',\n        },\n        // For minor style overrides, you can also use a string value for the `style` attribute:\n        // style: 'background-color: #ABCDEF; width: 60px;',\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Global Python Version with pyenv\nDESCRIPTION: Sets the global Python version to 3.12 using pyenv. This activates the specified Python environment for subsequent commands.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_20\n\nLANGUAGE: Bash\nCODE:\n```\npyenv global 3.12\n```\n\n----------------------------------------\n\nTITLE: Example Request Body for Knowledge Retrieval\nDESCRIPTION: This code snippet shows an example JSON request body for querying the external knowledge base using the `/retrieval` endpoint. It includes the `knowledge_id`, `query`, and `retrieval_setting` parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/external-knowledge-api-documentation.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"knowledge_id\": \"your-knowledge-id\",\n    \"query\": \"your question\",\n    \"retrieval_setting\":{\n        \"top_k\": 2,\n        \"score_threshold\": 0.5\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a String Array\nDESCRIPTION: This code snippet demonstrates how to define a string array variable. The array contains a list of strings enclosed in square brackets, with each string separated by a comma.  Each string element is enclosed in double quotes. This type of array is suitable for storing and processing textual data.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/what-is-array-variable.md#_snippet_1\n\nLANGUAGE: N/A\nCODE:\n```\n[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\"]\n```\n\n----------------------------------------\n\nTITLE: Upgrade Dify version using Git and Docker Compose\nDESCRIPTION: This series of commands updates the Dify codebase to version 1.0.0 using Git and then uses Docker Compose to rebuild and start the services.  It involves fetching the latest changes, checking out the 1.0.0 branch, and updating the environment configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/migration/migrate-to-v1.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit fetch origin\ngit checkout 1.0.0 # 1.0.0 ブランチに切り替える\ncd docker\nnano .env # .env.example ファイルと同期するように環境構成ファイルを変更する\ndocker compose -f docker-compose.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Clone Dify on WeChat Repository\nDESCRIPTION: Clones the Dify on WeChat repository from GitHub to the local machine. This is the first step to integrate Dify with WeChat.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/hanfangyuan4396/dify-on-wechat\ncd dify-on-wechat/\n```\n\n----------------------------------------\n\nTITLE: Dify Backend Directory Structure\nDESCRIPTION: This outlines the directory structure of the Dify backend, written in Python using the Flask framework. It provides an overview of each directory's purpose within the backend architecture, including API handling, core application logic, database interactions, and task management.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_14\n\nLANGUAGE: Text\nCODE:\n```\n[api/]\n├── constants             // Constant settings used throughout code base.\n├── controllers           // API route definitions and request handling logic.\n├── core                  // Core application orchestration, model integrations, and tools.\n├── docker                // Docker & containerization related configurations.\n├── events                // Event handling and processing\n├── extensions            // Extensions with 3rd party frameworks/platforms.\n├── fields                //field definitions for serialization/marshalling.\n├── libs                  // Reusable libraries and helpers.\n├── migrations            // Scripts for database migration.\n├── models                // Database models & schema definitions.\n├── services              // Specifies business logic.\n├── storage               // Private key storage.\n├── tasks                 // Handling of async tasks and background jobs.\n└── tests\n```\n\n----------------------------------------\n\nTITLE: Starting Dify with Docker Compose\nDESCRIPTION: These commands navigate to the `dify/docker` directory, copy the `.env.example` file to `.env`, and start the Dify application using Docker Compose in detached mode. This sets up the Dify environment and runs the necessary services.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncd dify/docker\ncp .env.example .env\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Packaging a Dify Bundle project\nDESCRIPTION: This command packages the Dify Bundle project into a single bundle.difybndl file. The command takes the path to the bundle project directory as an argument (./bundle in this case). The resulting bundle file contains all the necessary plugin dependencies.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/bundle.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndify-plugin bundle package ./bundle\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response: Unrecognized Request Argument\nDESCRIPTION: This error message indicates that the server received an unrecognized request argument, specifically 'functions'. This can occur when using Azure OpenAI keys and the model deployment has failed or the gpt-3.5-turbo model version is older than 0613.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/llms-use-faq.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"error\": \"Unrecognized request argument supplied:functions\"\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Chunks from Dify Knowledge Base\nDESCRIPTION: Retrieves relevant chunks from the Dify knowledge base based on a given query and retrieval model configuration. It requires the dataset ID as a path parameter, a valid API key in the Authorization header, and a JSON payload containing the query and retrieval model parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/retrieve' \\\n--header 'Authorization: Bearer {api_key}'\\\n--header 'Content-Type: application/json'\\\n--data-raw '{\n    \"query\": \"test\",\n    \"retrieval_model\": {\n        \"search_method\": \"keyword_search\",\n        \"reranking_enable\": false,\n        \"reranking_mode\": null,\n        \"reranking_model\": {\n            \"reranking_provider_name\": \"\",\n            \"reranking_model_name\": \"\"\n        },\n        \"weights\": null,\n        \"top_k\": 1,\n        \"score_threshold_enabled\": false,\n        \"score_threshold\": null\n    }\n}'\n```\n\n----------------------------------------\n\nTITLE: Starting Dify Docker Containers (Compose V2)\nDESCRIPTION: This command starts the Dify Docker containers in detached mode using Docker Compose V2.  The `-d` flag runs the containers in the background.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Web Directory\nDESCRIPTION: Changes the current directory to the 'web' directory. This is a prerequisite for installing frontend dependencies and building the web application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\ncd web\n```\n\n----------------------------------------\n\nTITLE: Enable/Disable Built-in Metadata Field - Dify API - Bash\nDESCRIPTION: This snippet demonstrates how to enable or disable a built-in metadata field in a Dify knowledge base using a DELETE request. It requires the dataset ID and the action (enable/disable). The request includes a header for authorization (API key).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata/built-in/{action}' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Tests\nDESCRIPTION: This section describes the directory structure for the tests related to the newly added provider. It includes the `__init__.py`, a folder with the provider's name (e.g., `anthropic`), and within that folder, `__init__.py`, the LLM tests (`test_llm.py`), and the provider tests (`test_provider.py`).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/new-provider.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n.\n├── __init__.py\n├── anthropic\n│   ├── __init__.py\n│   ├── test_llm.py       # LLMテスト\n│   └── test_provider.py  # プロバイダーテスト\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository\nDESCRIPTION: Clones the Dify repository from GitHub for local development and deployment. Uses git to download the project source code.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd dify/docker\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Example Response - Add/Modify Metadata Field - Dify API - JSON\nDESCRIPTION: This JSON represents the successful response after adding or modifying a metadata field. It contains the ID, type, and name of the metadata field.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": \"9f63c91b-d60e-4142-bb0c-c81a54dc2db5\",\n    \"type\": \"string\",\n    \"name\": \"test\"\n}\n```\n\n----------------------------------------\n\nTITLE: Appending a Package Plugin Dependency\nDESCRIPTION: This command appends a plugin dependency from a local package to the bundle project. The `--package_path` argument specifies the path to the plugin package file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/bundle.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndify-plugin bundle append package . --package_path=./openai.difypkg\n```\n\n----------------------------------------\n\nTITLE: Extract Plugins using Flask (Poetry)\nDESCRIPTION: This snippet executes a Flask command within the `docker-api` container to extract plugin information. It requires `poetry` to be installed and generates a `plugins.jsonl` file containing the plugin data.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/migration/migrate-to-v1.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask extract-plugins --workers=20\n```\n\n----------------------------------------\n\nTITLE: Example of LLM Response with Stop Sequences Applied\nDESCRIPTION: This code shows the output generated by the LLM after applying the 'Human1:' stop sequence as defined previously. The stop sequence limits the LLM to a single response, preventing it from generating subsequent turns in a conversation.  This is a demonstration of how to use stop sequences to limit the output length from LLMs.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/prompt-engineering/prompt-engineering-1/README.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nAssistant1: Soil is yellow.\n```\n\n----------------------------------------\n\nTITLE: Clone Dify Docs Repository\nDESCRIPTION: This command clones the Dify documentation repository from GitHub to a local machine.  Replace `<your-github-account>` with the actual GitHub username where the repository was forked.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_65\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/<your-github-account>/dify-docs.git\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Content Type Enum Definition\nDESCRIPTION: Defines an enumeration `PromptMessageContentType` representing the types of content a message can have: text or image.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContentType(Enum):\n    \"\"\"\n    Enum class for prompt message content type.\n    \"\"\"\n    TEXT = 'text'\n    IMAGE = 'image'\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response - Invalid request error\nDESCRIPTION: This error indicates that one or more parameters in the request are invalid, in this case, the temperature parameter for the Anthropic model.  Each model has different acceptable ranges for its parameters, and the settings should be adjusted accordingly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/llms-use-faq.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"error\": \"Anthropic: Error code: 400 - f'error': f'type': \\\"invalid request error, 'message': 'temperature: range: -1 or 0..1)'\"\n}\n```\n\n----------------------------------------\n\nTITLE: Chat Model Conversational App Template USER\nDESCRIPTION: This is the USER prompt template for building conversational applications using chat models in Dify. It defines the query variable that accepts user input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n{{Query}} // Input the query variable here\n```\n\n----------------------------------------\n\nTITLE: Implementing Bearer Auth (TypeScript)\nDESCRIPTION: This TypeScript snippet demonstrates how to implement Bearer authentication using the `hono/bearer-auth` library. It defines a middleware function that authenticates requests based on the provided token.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { bearerAuth } from \"hono/bearer-auth\";\n\n(c, next) => {\n    const auth = bearerAuth({ token: c.env.TOKEN });\n    return auth(c, next);\n},\n```\n\n----------------------------------------\n\nTITLE: FFmpeg Installation - CentOS\nDESCRIPTION: These commands install FFmpeg on CentOS. It involves enabling the EPEL and NUX Dextop repositories, updating the yum packages, and then installing FFmpeg.  Requires root privileges.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n`sudo yum install epel-release`\n```\n\nLANGUAGE: shell\nCODE:\n```\n`sudo rpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-5.el7.nux.noarch.rpm`\n```\n\nLANGUAGE: shell\nCODE:\n```\n`sudo yum update`\n```\n\nLANGUAGE: shell\nCODE:\n```\n`sudo yum install ffmpeg ffmpeg-devel`\n```\n\n----------------------------------------\n\nTITLE: Deploying GPUStack on Windows using PowerShell\nDESCRIPTION: This PowerShell command downloads and executes the GPUStack installation script. It uses `Invoke-WebRequest` to fetch the content from the specified URL and `Invoke-Expression` to execute it. Ensure PowerShell ISE is not used, and the script is executed as an administrator.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/gpustack.md#_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n```\n\n----------------------------------------\n\nTITLE: Configuring Dify Service API in LangBot (JSON)\nDESCRIPTION: This JSON snippet configures the dify-service-api settings within LangBot. It specifies the base URL for the Dify API, the application type (chat, agent, or workflow), options for handling thinking tips, and API keys with timeouts for each application type.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/connect-dify-to-various-im-platforms-by-using-langbot.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dify-service-api\": {\n        \"base-url\": \"https://api.dify.ai/v1\",\n        \"app-type\": \"chat\",\n        \"options\": {\n            \"convert-thinking-tips\": \"original\"\n        },\n        \"chat\": {\n            \"api-key\": \"app-1234567890\",\n            \"timeout\": 120\n        },\n        \"agent\": {\n            \"api-key\": \"app-1234567890\",\n            \"timeout\": 120\n        },\n        \"workflow\": {\n            \"api-key\": \"app-1234567890\",\n            \"output-key\": \"summary\",\n            \"timeout\": 120\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: LLM Invoke Method Definition (Python)\nDESCRIPTION: This Python code defines the structure for invoking the LLM model. The key parameters include `model_config` (containing model information), `prompt_messages` (the prompt or input for the model), `tools` (tool information for function calling), `stop` (a list of stop sequences), and `stream` (whether to support streaming output).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n        self,\n        model_config: LLMModelConfig,\n        prompt_messages: list[PromptMessage],\n        tools: list[PromptMessageTool] | None = None,\n        stop: list[str] | None = None,\n        stream: bool = True,\n    ) -> Generator[LLMResultChunk, None, None] | LLMResult:...\n```\n\n----------------------------------------\n\nTITLE: Knowledge Retrieval Formatting with Jinja2\nDESCRIPTION: This Jinja2 template formats retrieved chunks of knowledge, including similarity scores, titles, and content, into markdown.  It iterates through a list of `chunks` and accesses their metadata (specifically 'score'), title, and content. The `replace` filter ensures proper newline handling in the content. The template expects a variable named `chunks` to be passed in.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/template.md#_snippet_0\n\nLANGUAGE: Plain\nCODE:\n```\n{% raw %}\n{% for item in chunks %}\n### Chunk {{ loop.index }}. \n### Similarity: {{ item.metadata.score | default('N/A') }}\n\n#### {{ item.title }}\n\n##### Content\n{{ item.content | replace('\\n', '\\n\\n') }}\n\n---\n{% endfor %}\n{% endraw %}\n```\n\n----------------------------------------\n\nTITLE: Endpoint Group Configuration Example YAML\nDESCRIPTION: Defines an Endpoint group configuration with settings for an API key and specifies the endpoints to be used.  The 'settings' map defines the configurations required for the endpoint, including the data type, requirement status, labels for different languages, and placeholders. The 'endpoints' list specifies the YAML files that define the actual endpoints.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/endpoint.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsettings:\n  api_key:\n    type: secret-input\n    required: true\n    label:\n      en_US: API key\n      zh_Hans: API key\n      ja_Jp: API key\n      pt_BR: API key\n    placeholder:\n      en_US: Please input your API key\n      zh_Hans: 请输入你的 API key\n      ja_Jp: あなたの API key を入れてください\n      pt_BR: Por favor, insira sua chave API\nendpoints:\n  - endpoints/duck.yaml\n  - endpoints/neko.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Function Calling Parameters in Python\nDESCRIPTION: This code snippet defines a data model (`FunctionCallingParams`) to encapsulate parameters for a function calling agent, including query, model configuration (`AgentModelConfig`), a list of tools (`ToolEntity`), and the maximum number of iterations. It also shows how to access these parameters within the `_invoke` method of an `AgentStrategy` subclass.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/agent.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dify_plugin.interfaces.agent import AgentModelConfig, AgentStrategy, ToolEntity\n\nclass FunctionCallingParams(BaseModel):\n    query: str\n    model: AgentModelConfig\n    tools: list[ToolEntity] | None\n    maximum_iterations: int = 3\n    \n class FunctionCallingAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        \"\"\"\n        Run FunctionCall agent application\n        \"\"\"\n        fc_params = FunctionCallingParams(**parameters)\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Web Interface (Local)\nDESCRIPTION: This URL is used to access the Dify web interface on a local environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhttp://localhost\n```\n\n----------------------------------------\n\nTITLE: Inspecting Docker Container IPs using Docker CLI\nDESCRIPTION: This command retrieves the IP addresses of the Docker containers 'docker-web-1' and 'docker-api-1'. It uses 'docker ps' to list container IDs, 'docker inspect' to get details, and 'awk' to format the output. The script assumes Docker is installed and running. The intended use is to resolve 502 Bad Gateway errors related to Nginx misconfiguration by getting the IP addresses of containers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/llms-use-faq.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\ndocker ps -q | xargs -n 1 docker inspect --format '{{ .Name }}: {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'\n```\n\n----------------------------------------\n\nTITLE: Deploying Locally for Testing (Bash)\nDESCRIPTION: These commands install the dependencies and deploy the API extension locally for testing using npm.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: OpenAI TTS Error Message\nDESCRIPTION: This error message indicates that FFmpeg is not installed, which is a prerequisite for using OpenAI's text-to-speech (TTS) feature in Dify when running from source code.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n[openai] Error: ffmpeg is not installed\n```\n\n----------------------------------------\n\nTITLE: Setting Global Python Version with Pyenv\nDESCRIPTION: Sets the global Python version to 3.12 using Pyenv. This ensures that all subsequent Python commands use the specified version.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\npyenv global 3.12\n```\n\n----------------------------------------\n\nTITLE: Render HTML Form\nDESCRIPTION: This HTML snippet shows an example of a basic HTML form with various input fields such as text, password, content (textarea), date, time, datetime, select, and checkbox. It includes attributes like `data-format` to specify the format (JSON by default), `data-options` for select options, and `data-tip` for checkbox tooltips.  It uses the `form` tag and various input types to create a user interface for data input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/template.md#_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<form data-format=\"json\"> // Default to text\n  <label for=\"username\">Username:</label>\n  <input type=\"text\" name=\"username\" />\n  <label for=\"password\">Password:</label>\n  <input type=\"password\" name=\"password\" />\n  <label for=\"content\">Content:</label>\n  <textarea name=\"content\"></textarea>\n  <label for=\"date\">Date:</label>\n  <input type=\"date\" name=\"date\" />\n  <label for=\"time\">Time:</label>\n  <input type=\"time\" name=\"time\" />\n  <label for=\"datetime\">Datetime:</label>\n  <input type=\"datetime\" name=\"datetime\" />\n  <label for=\"select\">Select:</label>\n  <input type=\"select\" name=\"select\" data-options='[\"hello\",\"world\"]'/>\n  <input type=\"checkbox\" name=\"check\" data-tip=\"By checking this means you agreed\"/>\n  <button data-size=\"small\" data-variant=\"primary\">Login</button>\n</form>\n```\n\n----------------------------------------\n\nTITLE: Installing Docker on Ubuntu\nDESCRIPTION: These commands install Docker on an Ubuntu system. It updates the package list, installs required packages, adds the Docker GPG key and repository, and then installs Docker CE, Docker CLI, and containerd.io.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/searxng.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# パッケージリストの更新\nsudo apt update\n\n# 必要なパッケージのインストール\nsudo apt install apt-transport-https ca-certificates curl software-properties-common\n\n# Docker GPGキーの追加\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\n# Docker公式リポジトリの追加\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n\n# Dockerのインストール\nsudo apt update\nsudo apt install docker-ce docker-ce-cli containerd.io\n```\n\n----------------------------------------\n\nTITLE: Defining Enrolled Numbers for Whitelisting\nDESCRIPTION: This Python code defines a list of WhatsApp numbers that are allowed to use the chatbot. Any number not in this list will receive a message indicating they are not enrolled in the service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-whatsapp.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nenrolled_numbers = ['+14155238886']\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Admin Initialization Page (Server)\nDESCRIPTION: This URL is used to access the administrator initialization page on a server environment. Replace `your_server_ip` with the actual IP address of the server.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhttp://your_server_ip/install\n```\n\n----------------------------------------\n\nTITLE: Copying Environment Configuration File (API)\nDESCRIPTION: Copies the example environment configuration file to .env. This is a necessary step for configuring the API service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_22\n\nLANGUAGE: Bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Define LLMResultChunk Model\nDESCRIPTION: Defines the `LLMResultChunk` class, representing a chunk of an LLM result, used in streaming responses.  It includes the model, prompt messages, system fingerprint and the delta.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunk(BaseModel):\n    \"\"\"\n    Model class for llm result chunk.\n    \"\"\"\n    model: str  # Actual used modele\n    prompt_messages: list[PromptMessage]  # prompt messages\n    system_fingerprint: Optional[str] = None  # request fingerprint, refer to OpenAI definition\n    delta: LLMResultChunkDelta\n```\n\n----------------------------------------\n\nTITLE: Configuring .env File\nDESCRIPTION: This snippet shows the structure of the .env file which is used to store sensitive information like API keys and app credentials for the bot.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-teams.md#_snippet_1\n\nLANGUAGE: env\nCODE:\n```\nMicrosoftAppType=MultiTenant\nMicrosoftAppId=< 在 (4) 获取的 Client ID>\nMicrosoftAppPassword=< 在 (4) 获取的 Client Secret>\nMicrosoftAppTenantId=< 在 (4) 获取的 Tenant ID>\n\nAPI_ENDPOINT=< 在 (3) 获取的 Dify API 服务器地址 >\nAPI_KEY=< 在 (3) 获取的 Dify API 密钥 >\n```\n\n----------------------------------------\n\nTITLE: Testing SearXNG Integration\nDESCRIPTION: This curl command tests the SearXNG service by sending a search query for 'apple'. It specifies JSON as the format and 'general' as the category. The command expects a JSON response containing search results related to 'apple'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"http://<your-linux-vm-ip>:8081/search?q=apple&format=json&categories=general\"\n```\n\n----------------------------------------\n\nTITLE: Reloading systemd and restarting Ollama (Linux) (Bash)\nDESCRIPTION: These commands reload the systemd daemon and restart the Ollama service on Linux, which is necessary after modifying the Ollama service configuration file to make Ollama accessible from within a Docker container if running both Dify and Ollama within Docker. The first command tells systemd to reload its configuration files. The second command restarts the Ollama service for changes to take effect.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsystemctl daemon-reload\nsystemctl restart ollama\n```\n\n----------------------------------------\n\nTITLE: Install System Dependencies (CentOS)\nDESCRIPTION: This command installs required system components such as pkgconfig, gcc, libseccomp-devel, git, and wget on CentOS based systems. These packages are essential for building and running DifySandbox.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/backend/sandbox/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum install pkgconfig gcc libseccomp-devel git wget\n```\n\n----------------------------------------\n\nTITLE: Cloning the Dify Repository using Git\nDESCRIPTION: This command clones the Dify repository from GitHub to the local machine. It is the first step to deploying Dify locally using Docker.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/workshop/intermediate/twitter-chatflow.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\n----------------------------------------\n\nTITLE: app.moderation.input Request Body Example (JSON)\nDESCRIPTION: This JSON payload represents a sample request body sent to the `app.moderation.input` extension point.  It includes the extension point type, application ID, user input variables, and the user's current query. This request is sent by Dify when content review is enabled for user inputs.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/moderation.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"point\": \"app.moderation.input\",\n    \"params\": {\n        \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n        \"inputs\": {\n            \"var_1\": \"I will kill you.\",\n            \"var_2\": \"I will fuck you.\"\n        },\n        \"query\": \"Happy everydays.\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running OpenLLM with Docker\nDESCRIPTION: This command starts an OpenLLM server using Docker, exposing port 3333 for communication. It deploys the `facebook/opt-1.3b` model with the PyTorch backend. This is intended for demonstration purposes, and a more appropriate model should be selected for production use.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm -it -p 3333:3000 ghcr.io/bentoml/openllm start facebook/opt-1.3b --backend pt\n```\n\n----------------------------------------\n\nTITLE: URLを取得 (Python)\nDESCRIPTION: ウェブページのリンクとユーザーエージェント（任意）を渡すと、Difyがウェブページ情報をスクレイピングするツールです。`user_agent`はオプションで、指定しない場合はDifyのデフォルトが使用されます。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/advanced-tool-integration.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef get_url(self, url: str, user_agent: str = None) -> str:\n    \"\"\"\n        URLを取得\n    \"\"\" スクレイピング結果\n```\n\n----------------------------------------\n\nTITLE: Moderation _invoke method definition in Python\nDESCRIPTION: Defines the _invoke method for ModerationModel, used for moderating text content using a specified model and credentials. It takes the model name, credentials, text to moderate, and an optional user ID as input. The method returns a boolean; False indicates the text is safe, while True indicates otherwise.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            text: str, user: Optional[str] = None) \\\n        -> bool:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param text: text to moderate\n    :param user: unique user id\n    :return: false if text is safe, true otherwise\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Cloning the Dify Repository using Git\nDESCRIPTION: This snippet shows how to clone the Dify repository from GitHub using the git command. It's a prerequisite for setting up Dify locally. It downloads the entire project source code to your local machine.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/workshop/intermediate/twitter-chatflow.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\n----------------------------------------\n\nTITLE: Restarting Dify After Customization\nDESCRIPTION: These commands are used to restart Dify after making changes to the environment variables in the .env file. This ensures that the changes are applied correctly to the running application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_63\n\nLANGUAGE: shell\nCODE:\n```\ndocker-compose down\nocker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Dify Frontend Directory Structure\nDESCRIPTION: This section outlines the directory structure of Dify's frontend, which uses Next.js and Tailwind CSS. It describes the purpose of each directory and its role in building the user interface.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/community/contribution.md#_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\n[web/]\n├── app                   // レイアウト、ページ、およびコンポーネント\n│   ├── (commonLayout)    // アプリ全体で使用される共通レイアウト\n│   ├── (shareLayout)     // トークン固有のセッション間で共有されるレイアウト \n│   ├── activate          // アクティベートページ\n│   ├── components        // ページとレイアウトで共有されるコンポーネント\n│   ├── install           // インストールページ\n│   ├── signin            // サインインページ\n│   └── styles            // グローバルに共有されるスタイル\n├── assets                // 静的アセット\n├── bin                   // ビルドステップで実行されるスクリプト\n├── config                // 調整可能な設定とオプション \n├── context               // アプリの異なる部分で使用される共有コンテキスト\n├── dictionaries          // 言語固有の翻訳ファイル \n├── docker                // コンテナ設定\n├── hooks                 // 再利用可能なフック\n├── i18n                  // 国際化設定\n├── models                // データモデルとAPIレスポンスの形状を記述\n├── public                // ファビコンなどのメタアセット\n├── service               // APIアクションの形状を指定\n├── test                  \n├── types                 // 関数パラメータと戻り値の記述\n└── utils                 // 共有ユーティリティ関数\n```\n\n----------------------------------------\n\nTITLE: SSRF Proxy Configuration - Squid\nDESCRIPTION: Example Squid configuration rules to restrict access to sensitive IP addresses within a local network while allowing access to the rest of the local network. Prevents SSRF attacks.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_10\n\nLANGUAGE: squid\nCODE:\n```\nacl restricted_ip dst 192.168.101.19\nacl localnet src 192.168.101.0/24\n\nhttp_access deny restricted_ip\nhttp_access allow localnet\nhttp_access deny all\n```\n\n----------------------------------------\n\nTITLE: Define EmbeddingUsage Model\nDESCRIPTION: Defines the `EmbeddingUsage` class, which represents the usage information for an embedding call. It extends `ModelUsage` and includes token counts, pricing details, currency, and latency.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nclass EmbeddingUsage(ModelUsage):\n    \"\"\"\n    Model class for embedding usage.\n    \"\"\"\n    tokens: int  # Number of tokens used\n    total_tokens: int  # Total number of tokens used\n    unit_price: Decimal  # Unit price\n    price_unit: Decimal  # Price unit, i.e., the unit price based on how many tokens\n    total_price: Decimal  # Total cost\n    currency: str  # Currency unit\n    latency: float  # Request latency (s)\n```\n\n----------------------------------------\n\nTITLE: Rerank Document Class Definition\nDESCRIPTION: This code snippet defines the RerankDocument class, representing a document in a reranking result. It contains the original index, the document text, and the reranking score.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nclass RerankDocument(BaseModel):\n    \"\"\"\n    Model class for rerank document.\n    \"\"\"\n    index: int  # 元の順番\n    text: str  # ドキュメントテキスト\n    score: float  # スコア\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Role Enum in Python\nDESCRIPTION: This snippet defines an Enum class `PromptMessageRole` for different message roles (system, user, assistant, tool) in a prompt.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageRole(Enum):\n    \"\"\"\n    Enum class for prompt message.\n    \"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    TOOL = \"tool\"\n```\n\n----------------------------------------\n\nTITLE: Tag Plugin Code\nDESCRIPTION: Tags the plugin code with a version number. This is recommended for later packaging purposes and helps in managing different versions of the plugin.  The command creates an annotated tag `v0.0.1` with a message and then pushes the tag to the remote repository.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit tag -a v0.0.1 -m \"Release version 0.0.1\"\ngit push origin v0.0.1\n```\n\n----------------------------------------\n\nTITLE: LLMUsage Class Definition in Python\nDESCRIPTION: Defines the `LLMUsage` class as a Pydantic BaseModel representing usage information for a language model, inheriting from `ModelUsage`. It includes details about prompt tokens, completion tokens, pricing, and latency.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nclass LLMUsage(ModelUsage):\n    \"\"\"\n    Model class for llm usage.\n    \"\"\"\n    prompt_tokens: int  # prompt 使用 tokens\n    prompt_unit_price: Decimal  # prompt 单价\n    prompt_price_unit: Decimal  # prompt 价格单位，即单价基于多少 tokens \n    prompt_price: Decimal  # prompt 费用\n    completion_tokens: int  # 回复使用 tokens\n    completion_unit_price: Decimal  # 回复单价\n    completion_price_unit: Decimal  # 回复价格单位，即单价基于多少 tokens \n    completion_price: Decimal  # 回复费用\n    total_tokens: int  # 总使用 token 数\n    total_price: Decimal  # 总费用\n    currency: str  # 货币单位\n    latency: float  # 请求耗时(s)\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response - Unrecognized request argument\nDESCRIPTION: This error indicates that the backend does not recognize the 'functions' argument, which is used in the context of chat completions with function calling capabilities. This might occur due to version mismatches between the frontend and backend, or if the Azure OpenAI key is used without properly deployed models that supports function call, specially gpt-3.5-turbo model version must be 0613 or above.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/llms-use-faq.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"error\": \"Unrecognized request argument supplied:functions\"\n}\n```\n\n----------------------------------------\n\nTITLE: LLM Invocation Method in Python\nDESCRIPTION: Illustrates the `_invoke` method for invoking a large language model, handling both streaming and synchronous responses. It shows how to differentiate between stream and sync responses by utilizing generator functions with the `yield` keyword.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, stream: bool, **kwargs) \\\n        -> Union[LLMResult, Generator]:\n    if stream:\n          return self._handle_stream_response(**kwargs)\n    return self._handle_sync_response(**kwargs)\n\ndef _handle_stream_response(self, **kwargs) -> Generator:\n    for chunk in response:\n          yield chunk\ndef _handle_sync_response(self, **kwargs) -> LLMResult:\n    return LLMResult(**response)\n```\n\n----------------------------------------\n\nTITLE: LLM Result Chunk Class Definition\nDESCRIPTION: This code snippet defines the LLMResultChunk class, representing a single chunk of a streaming LLM result. It contains the model name, prompt messages, system fingerprint, and the delta (change) in this chunk.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunk(BaseModel):\n    \"\"\"\n    Model class for llm result chunk.\n    \"\"\"\n    model: str  # 使用モデル\n    prompt_messages: list[PromptMessage]  # プロンプトメッセージリスト\n    system_fingerprint: Optional[str] = None  # リクエスト指紋（OpenAIの定義に準拠）\n    delta: LLMResultChunkDelta  # 各イテレーションで変化する内容\n```\n\n----------------------------------------\n\nTITLE: Viewing Docker Container Logs\nDESCRIPTION: This command retrieves the logs of a specific Docker container.  Replace `< 容器 ID>` with the actual container ID to view the service logs and check for any errors or startup information. This command helps in verifying if the service has started successfully.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-dingtalk.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndocker logs < 容器 ID>\n```\n\n----------------------------------------\n\nTITLE: Check and Extract Tool Calls (Python)\nDESCRIPTION: The `check_tool_calls` method checks if the LLM result chunk contains tool calls. The `extract_tool_calls` method extracts the tool call ID, name, and arguments from the LLM result chunk, parsing the arguments from JSON format.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n    def check_tool_calls(self, llm_result_chunk: LLMResultChunk) -> bool:\n        \"\"\"\n        Check if there is any tool call in llm result chunk\n        \"\"\"\n        return bool(llm_result_chunk.delta.message.tool_calls)\n\n    def extract_tool_calls(\n        self, llm_result_chunk: LLMResultChunk\n    ) -> list[tuple[str, str, dict[str, Any]]]:\n        \"\"\"\n        Extract tool calls from llm result chunk\n\n        Returns:\n            List[Tuple[str, str, Dict[str, Any]]]: [(tool_call_id, tool_call_name, tool_call_args)]\n        \"\"\"\n        tool_calls = []\n        for prompt_message in llm_result_chunk.delta.message.tool_calls:\n            args = {}\n            if prompt_message.function.arguments != \"\":\n                args = json.loads(prompt_message.function.arguments)\n\n            tool_calls.append(\n                (\n                    prompt_message.id,\n                    prompt_message.function.name,\n                    args,\n                )\n            )\n\n        return tool_calls\n```\n\n----------------------------------------\n\nTITLE: DallE3 Tool Invoke (Python)\nDESCRIPTION: DallE3を使用して画像を生成し、生成された画像を変数プールに保存する例です。生成された画像は、`save_as`パラメータを介して変数プールに保存されます。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/advanced-tool-integration.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, List, Union\nfrom core.tools.entities.tool_entities import ToolInvokeMessage\nfrom core.tools.tool.builtin_tool import BuiltinTool\n\nfrom base64 import b64decode\n\nfrom openai import OpenAI\n\nclass DallE3Tool(BuiltinTool):\n    def _invoke(self, \n                user_id: str, \n               tool_Parameters: Dict[str, Any], \n        ) -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:\n        \"\"\"\n            invoke tools\n        \"\"\"\n        client = OpenAI(\n            api_key=self.runtime.credentials['openai_api_key'],\n        )\n\n        # プロンプト\n        prompt = tool_Parameters.get('prompt', '')\n        if not prompt:\n            return self.create_text_message('プロンプトを入力してください')\n\n        # openapi dalle3を呼び出す\n        response = client.images.generate(\n            prompt=prompt, model='dall-e-3',\n            size='1024x1024', n=1, style='vivid', quality='standard',\n            response_format='b64_json'\n        )\n\n        result = []\n        for image in response.data:\n            # すべての画像をsave_asパラメータを介して変数プールに保存し、変数名をself.VARIABLE_KEY.IMAGE.valueに設定します。後続の新しい画像が生成される場合、前の画像が上書きされます。\n            result.append(self.create_blob_message(blob=b64decode(image.b64_json), \n                                                   meta={ 'mime_type': 'image/png' },\n                                                    save_as=self.VARIABLE_KEY.IMAGE.value))\n\n        return result\n```\n\n----------------------------------------\n\nTITLE: Convert Knowledge Retrieval Output to Markdown using Jinja2\nDESCRIPTION: This Jinja2 template takes a list of chunks (presumably from a knowledge retrieval node) and formats them into a structured Markdown output.  It iterates through the chunks, displaying their index, similarity score (if available), title, and content. Newlines within the content are replaced with double newlines for better formatting in Markdown.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/template.md#_snippet_0\n\nLANGUAGE: Plain\nCODE:\n```\n{% for item in chunks %}\n### Chunk {{ loop.index }}. \n### Similarity: {{ item.metadata.score | default ('N/A') }}\n\n#### {{ item.title }}\n\n##### Content\n{{ item.content | replace ('\\n', '\\n\\n') }}\n\n---\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Create Empty Knowledge Base - Dify API (JSON Response)\nDESCRIPTION: This is the JSON response received after successfully creating an empty knowledge base via the Dify API. It contains details about the created knowledge base, such as its ID, name, description, permission settings, data source type, indexing technique, and creation timestamp.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"\",\n  \"name\": \"name\",\n  \"description\": null,\n  \"provider\": \"vendor\",\n  \"permission\": \"only_me\",\n  \"data_source_type\": null,\n  \"indexing_technique\": null,\n  \"app_count\": 0,\n  \"document_count\": 0,\n  \"word_count\": 0,\n  \"created_by\": \"\",\n  \"created_at\": 1695636173,\n  \"updated_by\": \"\",\n  \"updated_at\": 1695636173,\n  \"embedding_model\": null,\n  \"embedding_model_provider\": null,\n  \"embedding_available\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Configuration\nDESCRIPTION: This is a docker-compose.yml configuration file for deploying the Dify on WeChat application using Docker. It defines the service, image, container name, and environment variables required for the application to run. It is important to correctly configure the DIFY_API_BASE, DIFY_API_KEY, and DIFY_APP_TYPE environment variables to match the Dify application settings.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '2.0'\nservices:\n  dify-on-wechat:\n    image: hanfangyuan/dify-on-wechat\n    container_name: dify-on-wechat\n    security_opt:\n      - seccomp:unconfined\n    environment:\n      DIFY_API_BASE: 'https://api.dify.ai/v1'\n      DIFY_API_KEY: 'app-xx'\n      DIFY_APP_TYPE: 'chatbot'\n      MODEL: 'dify'\n      SINGLE_CHAT_PREFIX: '[\"\"]'\n      SINGLE_CHAT_REPLY_PREFIX: '\"\"'\n      GROUP_CHAT_PREFIX: '[\"@bot\"]'\n      GROUP_NAME_WHITE_LIST: '[\"ALL_GROUP\"]'\n```\n\n----------------------------------------\n\nTITLE: Convert Array to Text with Code Node - Python\nDESCRIPTION: This Python code snippet demonstrates how to convert an array (list) to a text string by joining the elements with newline characters. The function `main` takes a list named `articleSections` as input, joins its elements using '/n', and returns the result in a dictionary under the key \"result\". This is useful for converting the array output of an iteration node into a single text output.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_72\n\nLANGUAGE: Python\nCODE:\n```\ndef main(articleSections: list):\n    data = articleSections\n    return {\n        \"result\": \"/n\".join(data)\n    }\n```\n\n----------------------------------------\n\nTITLE: Shell Output Example\nDESCRIPTION: An example of the output displayed after running docker compose commands.  Demonstrates the status of the containers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_12\n\nLANGUAGE: Shell\nCODE:\n```\n[+] Running 11/11\n ✔ Network docker_ssrf_proxy_network  Created                                                                 0.1s \n ✔ Network docker_default             Created                                                                 0.0s \n ✔ Container docker-redis-1           Started                                                                 2.4s \n ✔ Container docker-ssrf_proxy-1      Started                                                                 2.8s \n ✔ Container docker-sandbox-1         Started                                                                 2.7s \n ✔ Container docker-web-1             Started                                                                 2.7s \n ✔ Container docker-weaviate-1        Started                                                                 2.4s \n ✔ Container docker-db-1              Started                                                                 2.7s \n ✔ Container docker-api-1             Started                                                                 6.5s \n ✔ Container docker-worker-1          Started                                                                 6.4s \n ✔ Container docker-nginx-1           Started                                                                 7.1s\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Base Class in Python\nDESCRIPTION: This snippet defines an abstract base class `PromptMessage` for all role message bodies, with role, content, and name attributes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessage(ABC, BaseModel):\n    \"\"\"\n    Model class for prompt message.\n    \"\"\"\n    role: PromptMessageRole\n    content: Optional[str | list[PromptMessageContent]] = None  # Supports two types: string and content list. The content list is designed to meet the needs of multimodal inputs. For more details, see the PromptMessageContent explanation.\n    name: Optional[str] = None\n```\n\n----------------------------------------\n\nTITLE: Setting Notion Internal Integration Environment Variables\nDESCRIPTION: These environment variables are required to configure Dify to use Notion's internal integration.  NOTION_INTEGRATION_TYPE should be set to 'インターナル' and NOTION_INTERNAL_SECRET should be set to the internal integration secret obtained from Notion.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/create-knowledge-and-upload-documents/import-content-data/sync-from-notion.md#_snippet_0\n\nLANGUAGE: env\nCODE:\n```\n**NOTION_INTEGRATION_TYPE** = インターナル または **NOTION_INTEGRATION_TYPE** = パブリック\n\n**NOTION_INTERNAL_SECRET**=you-internal-secret\n```\n\n----------------------------------------\n\nTITLE: Modifying PostgreSQL pg_hba.conf for Database Connection\nDESCRIPTION: This command modifies the `pg_hba.conf` file within the `docker-db-1` container to allow connections from a specific network segment. This is needed to fix database connection errors. It requires that Dify is deployed using Docker Compose and the `docker-db-1` container is running.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/faqs.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-db-1 sh -c \"echo 'host all all 172.19.0.0/16 trust' >> /var/lib/postgresql/data/pgdata/pg_hba.conf\"\ndocker-compose restart\n```\n\n----------------------------------------\n\nTITLE: Upgrading Dify Database (Source Code)\nDESCRIPTION: Upgrades the Dify database schema to the latest version when deploying from source code. Executes a flask command from the api directory to apply database migrations.  Requires Flask and the necessary database migration tools to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nflask db upgrade\n```\n\n----------------------------------------\n\nTITLE: Docker Compose PS Example\nDESCRIPTION: An example of the output displayed after running docker compose ps command.  Demonstrates the status of the containers and the ports used.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nNAME                IMAGE                              COMMAND                  SERVICE             CREATED             STATUS              PORTS\ndocker-api-1        langgenius/dify-api:0.3.2          \"/entrypoint.sh\"         api                 4 seconds ago       Up 2 seconds        80/tcp, 5001/tcp\ndocker-db-1         postgres:15-alpine                 \"docker-entrypoint.s…\"   db                  4 seconds ago       Up 2 seconds        0.0.0.0:5432->5432/tcp\ndocker-nginx-1      nginx:latest                       \"/docker-entrypoint.…\"   nginx               4 seconds ago       Up 2 seconds        0.0.0.0:80->80/tcp\ndocker-redis-1      redis:6-alpine                     \"docker-entrypoint.s…\"   redis               4 seconds ago       Up 3 seconds        6379/tcp\ndocker-weaviate-1   semitechnologies/weaviate:1.18.4   \"/bin/weaviate --hos…\"   weaviate            4 seconds ago       Up 3 seconds        \ndocker-web-1        langgenius/dify-web:0.3.2          \"/entrypoint.sh\"         web                 4 seconds ago       Up 3 seconds        80/tcp, 3000/tcp\ndocker-worker-1     langgenius/dify-api:0.3.2          \"/entrypoint.sh\"         worker              4 seconds ago       Up 2 seconds        80/tcp, 5001/tcp\n```\n\n----------------------------------------\n\nTITLE: JSON Verification Code Node\nDESCRIPTION: This code snippet defines a function `main` that takes a JSON string as input and attempts to parse it using `json.loads`. It returns a dictionary containing the parsed JSON object under the key 'result'.  This code is intended to be run within a Dify Code node.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef main(json_str: str) -> dict:\n    obj = json.loads(json_str)\n    return {'result': obj}\n```\n\n----------------------------------------\n\nTITLE: Speech-to-Text Model Invocation Interface in Python\nDESCRIPTION: This snippet defines the `_invoke` method for a speech-to-text model. It takes a model name, credentials, an audio file, and an optional user ID as input, and returns the transcribed text.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, file: IO[bytes], user: Optional[str] = None) -> str:\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param file: audio file\n      :param user: unique user id\n      :return: text for given audio file\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: External Data Tool Query Request Body\nDESCRIPTION: This JSON payload represents the request body sent to the API endpoint to query an external data tool. It includes the extension point type, application ID, tool variable name, user input variables, and the user's query. The 'point' field is fixed to 'app.external_data_tool.query'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/external-data-tool.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"point\": \"app.external_data_tool.query\", // 扩展点类型，此处固定为 app.external_data_tool.query\n    \"params\": {\n        \"app_id\": string,  // 应用 ID\n        \"tool_variable\": string,  // 外部数据工具变量名称，表示对应变量工具调用来源\n        \"inputs\": {  // 终端用户传入变量值，key 为变量名，value 为变量值\n            \"var_1\": \"value_1\",\n            \"var_2\": \"value_2\",\n            ...\n        },\n        \"query\": string | null  // 终端用户当前对话输入内容，对话型应用固定参数。\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Ollama Host Environment Variable on Linux\nDESCRIPTION: This configuration sets the `OLLAMA_HOST` environment variable within the systemd service configuration for Ollama on Linux. This makes the Ollama service accessible by specifying the host address. The change requires reloading the systemd daemon and restarting the Ollama service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_83\n\nLANGUAGE: Systemd\nCODE:\n```\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Customizing iFrame Style - Positioning\nDESCRIPTION: This code snippet shows how to fix the position of the Dify chatbot iFrame to the bottom-right corner of the webpage. The `style` attribute is modified to include `position: fixed;`, `bottom: 20px;`, and `right: 20px;`, ensuring the chatbot remains visible in the bottom right, 20 pixels from the bottom and right edges, even while scrolling.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/how-to-integrate-dify-chatbot-to-your-wix-website.md#_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<iframe src=\"https://udify.app/chatbot/ez1pf83HVV3JgWO4\" style=\"width: 100%; height: 100%; min-height: 700px; position: fixed; bottom: 20px; right: 20px;\" frameborder=\"0\" allow=\"microphone\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: Add Dify Schedule Subscription to Qinglong Panel (Bash)\nDESCRIPTION: This command adds a subscription task to Qinglong Panel to schedule Dify workflows. It retrieves the Dify scheduling project from GitHub. The parameters 'ql_', 'utils', and 'sdk' filter the files to be included.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-schedule.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nql repo https://github.com/leochen-g/dify-schedule.git \"ql_\" \"utils\" \"sdk\"\n```\n\n----------------------------------------\n\nTITLE: Install Python Dependencies for Dify on WeChat\nDESCRIPTION: Installs the core Python dependencies required for the Dify on WeChat project. It uses pip3 to install the packages listed in the requirements.txt file. An optional argument is provided for using the Aliyun mirror for faster installation in China.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt  # 国内可以在该命令末尾添加 \"-i https://mirrors.aliyun.com/pypi/simple\" 参数，使用阿里云镜像源安装依赖\n```\n\n----------------------------------------\n\nTITLE: Build the Code (Bash)\nDESCRIPTION: Builds the Dify web application using npm. This command compiles the source code and prepares it for deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: Example .env Configuration\nDESCRIPTION: This is an example of the `.env` file, which contains global configuration variables like log level, default maximum workers, Dify API URL, conversation remain time, and DingTalk AI card template ID.  This configuration is used by the Dify-on-Dingtalk service to connect to Dify and DingTalk.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-dingtalk.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# 日志级别\nLOG_LEVEL=INFO\n# 默认每个机器人后台监听线程数，调大可加大可问答并发\n# 不过不要太大根据 cpu 配置适当调整\nDEFAULT_MAX_WORKERS=2\n# Dify API 服务器访问地址，上文 2.1. 步骤里记录的\nDIFY_OPEN_API_URL=\"https://api.dify.ai/v1\"\n# 用户各自上下文维持时间，默认 15 minutes，只对 chatbot app 有效\nDIFY_CONVERSATION_REMAIN_TIME=15\n# 钉钉 AI 卡片模板 ID，上文 2.3. 步骤里记录的\nDINGTALK_AI_CARD_TEMPLATE_ID=\"<your-dingtalk-ai-card-temp-id>\"\n```\n\n----------------------------------------\n\nTITLE: View Docker Container IDs\nDESCRIPTION: This command displays a list of running Docker containers, including their IDs, images, commands, status, and ports. It is used to identify the `docker-api` container ID for subsequent steps.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/migration/migrate-to-v1.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker ps\nCONTAINER ID   IMAGE                                       COMMAND                  CREATED       STATUS                 PORTS                                                                                                                             NAMES\n417241cd****   nginx:latest                                \"sh -c 'cp /docker-e…\"   3 hours ago   Up 3 hours             0.0.0.0:80->80/tcp, :::80->80/tcp, 0.0.0.0:443->443/tcp, :::443->443/tcp                                                          docker-nginx-1\nf84aa773****   langgenius/dify-api:1.0.0                   \"/bin/bash /entrypoi…\"   3 hours ago   Up 3 hours             5001/tcp                                                                                                                          docker-worker-1\na3cb19c2****   langgenius/dify-api:1.0.0                   \"/bin/bash /entrypoi…\"   3 hours ago   Up 3 hours             5001/tcp                                                                                                                          docker-api-1\n```\n\n----------------------------------------\n\nTITLE: Dify Backend Directory Structure\nDESCRIPTION: This shows the directory structure of the Dify backend, which is written in Python and uses the Flask framework. It provides an overview of the different modules and their responsibilities, such as API controllers, core application logic, database models, and task processing. Understanding this structure is crucial for backend development and contribution.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_61\n\nLANGUAGE: Text\nCODE:\n```\n[api/]\n├── constants             // コードベース全体で使用される定数設定。\n├── controllers           // APIルート定義とリクエスト処理ロジック。\n├── core                  // コアアプリケーションオーケストレーション、モデル統合、ツール。\n├── docker                // Dockerおよびコンテナ化関連の設定。\n├── events                // イベント処理と処理\n├── extensions            // サードパーティフレームワーク/プラットフォームとの拡張機能。\n├── fields                // シリアライズ/マーシャリングのためのフィールド定義。\n├── libs                  // 再利用可能なライブラリとヘルパー。\n├── migrations            // データベース移行のためのスクリプト。\n├── models                // データベースモデルとスキーマ定義。\n├── services              // ビジネスロジックを指定。\n├── storage               // 秘密鍵保管。\n├── tasks                 // 非同期タスクとバックグラウンドジョブの処理。\n└── tests\n```\n\n----------------------------------------\n\nTITLE: Define AssistantPromptMessage Class in Python\nDESCRIPTION: This code defines the `AssistantPromptMessage` class, which inherits from `PromptMessage`. It includes nested classes `ToolCall` and `ToolCallFunction` to represent tool calls made by the assistant. The `role` is set to `PromptMessageRole.ASSISTANT`, and it includes a `tool_calls` attribute which is a list of `ToolCall` objects.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nclass AssistantPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for assistant prompt message.\n    \"\"\"\n    class ToolCall(BaseModel):\n        \"\"\"\n        Model class for assistant prompt message tool call.\n        \"\"\"\n        class ToolCallFunction(BaseModel):\n            \"\"\"\n            Model class for assistant prompt message tool call function.\n            \"\"\"\n            name: str  # tool name\n            arguments: str  # tool arguments\n\n        id: str  # Tool ID, effective only in OpenAI tool calls. It's the unique ID for tool invocation and the same tool can be called multiple times.\n        type: str  # default: function\n        function: ToolCallFunction  # tool call information\n\n    role: PromptMessageRole = PromptMessageRole.ASSISTANT\n    tool_calls: list[ToolCall] = []  # The result of tool invocation in response from the model (returned only when tools are input and the model deems it necessary to invoke a tool).\n```\n\n----------------------------------------\n\nTITLE: Backend Directory Structure\nDESCRIPTION: This snippet shows the directory structure of the Dify backend. The backend is written in Python and uses the Flask framework, SQLAlchemy ORM, and Celery for task queueing.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_60\n\nLANGUAGE: Text\nCODE:\n```\n[api/]\n├── constants             // コードベース全体で使用される定数設定。\n├── controllers           // APIルート定義とリクエスト処理ロジック。\n├── core                  // コアアプリケーションオーケストレーション、モデル統合、ツール。\n├── docker                // Dockerおよびコンテナ化関連の設定。\n├── events                // イベント処理と処理\n├── extensions            // サードパーティフレームワーク/プラットフォームとの拡張機能。\n├── fields                // シリアライズ/マーシャリングのためのフィールド定義。\n├── libs                  // 再利用可能なライブラリとヘルパー。\n├── migrations            // データベース移行のためのスクリプト。\n├── models                // データベースモデルとスキーマ定義。\n├── services              // ビジネスロジックを指定。\n├── storage               // 秘密鍵保管。\n├── tasks                 // 非同期タスクとバックグラウンドジョブの処理。\n└── tests\n```\n\n----------------------------------------\n\nTITLE: Rerank Result Class Definition\nDESCRIPTION: This code snippet defines the RerankResult class, representing the result of a reranking operation. It contains the model name and a list of RerankDocument objects.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nclass RerankResult(BaseModel):\n    \"\"\"\n    Model class for rerank result.\n    \"\"\"\n    model: str  # 使用モデル\n    docs: list[RerankDocument]  # リランク後のドキュメントリスト\n```\n\n----------------------------------------\n\nTITLE: Localtunnel Command\nDESCRIPTION: This snippet uses localtunnel to expose a local server running on port 3978 to the public internet, creating a publicly accessible URL for Azure Bot to communicate with the bot application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-teams.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpx localtunnel --port 3978\n```\n\n----------------------------------------\n\nTITLE: User Prompt Message Definition\nDESCRIPTION: Defines a class `UserPromptMessage` that inherits from `PromptMessage` and specifies the `role` as user.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nclass UserPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for user prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.USER\n```\n\n----------------------------------------\n\nTITLE: Verifying Go Installation\nDESCRIPTION: Verifies that Go is installed correctly and displays the installed Go version. This confirms the successful installation of the Go environment required for DifySandbox.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/backend/sandbox/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngo version  # 显示安装的 Go 版本\n```\n\n----------------------------------------\n\nTITLE: Starting Dify Docker Containers (Compose V1)\nDESCRIPTION: This command starts the Dify Docker containers in detached mode using Docker Compose V1.  The `-d` flag runs the containers in the background.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for OpenAI Models\nDESCRIPTION: This bash code snippet shows the directory structure for OpenAI models, including llm, text_embedding, moderation, speech2text, and tts models. It demonstrates how to organize different model types within the plugin.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n├── models\n│   ├── common_openai.py\n│   ├── llm\n│   │   ├── _position.yaml\n│   │   ├── chatgpt-4o-latest.yaml\n│   │   ├── gpt-3.5-turbo.yaml\n│   │   ├── gpt-4-0125-preview.yaml\n│   │   ├── gpt-4-turbo.yaml\n│   │   ├── gpt-4o.yaml\n│   │   ├── llm.py\n│   │   ├── o1-preview.yaml\n│   │   └── text-davinci-003.yaml\n│   ├── moderation\n│   │   ├── moderation.py\n│   │   └── text-moderation-stable.yaml\n│   ├── speech2text\n│   │   ├── speech2text.py\n│   │   └── whisper-1.yaml\n│   ├── text_embedding\n│   │   ├── text-embedding-3-large.yaml\n│   │   └── text_embedding.py\n│   └── tts\n│       ├── tts-1-hd.yaml\n│   │   ├── tts-1.yaml\n│   │   └── tts.py\n```\n\n----------------------------------------\n\nTITLE: Customizing Squid Proxy Configuration\nDESCRIPTION: This plaintext configuration shows how to customize the Squid proxy behavior to allow access to a specific network (192.168.101.0/24) while denying access to a specific IP address (192.168.101.19) within that network. It uses access control lists (ACLs) and http_access rules to define the proxy's behavior. This example requires modification of the `squid.conf` file within the Dify deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nacl restricted_ip dst 192.168.101.19\nacl localnet src 192.168.101.0/24\n\nhttp_access deny restricted_ip\nhttp_access allow localnet\nhttp_access deny all\n```\n\n----------------------------------------\n\nTITLE: Creating a URL from Twitter User ID (Python)\nDESCRIPTION: This Python code defines a function `main` that takes a Twitter user ID as a string input and returns a dictionary containing the complete Twitter profile URL. The function concatenates a predefined base URL (`https://twitter.com/`) with the provided user ID to construct the profile URL. The expected input is a string representing the Twitter user ID, and the output is a dictionary containing the 'url' key with the complete Twitter profile URL as its value.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/workshop/intermediate/twitter-chatflow.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef main(id: str) -> dict:\n    return {\n        \"url\": \"https://twitter.com/\"+id,\n    }\n```\n\n----------------------------------------\n\nTITLE: LLM Invocation with Stream and Sync Handling (Python)\nDESCRIPTION: This Python code demonstrates the `_invoke` method for handling LLM calls, differentiating between streaming and synchronous responses. It utilizes helper functions `_handle_stream_response` and `_handle_sync_response` to process the responses accordingly, with the stream response returning a generator and the sync response returning an `LLMResult` object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, stream: bool, **kwargs) \\\n        -> Union[LLMResult, Generator]:\n    if stream:\n          return self._handle_stream_response(**kwargs)\n    return self._handle_sync_response(**kwargs)\n\ndef _handle_stream_response(self, **kwargs) -> Generator:\n    for chunk in response:\n          yield chunk\ndef _handle_sync_response(self, **kwargs) -> LLMResult:\n    return LLMResult(**response)\n```\n\n----------------------------------------\n\nTITLE: Defining Server URL and Model UID in YAML\nDESCRIPTION: Configures input fields for the Xinference model's server URL and model UID, which are required for connecting to the locally deployed Xinference instance.  Includes labels and placeholders in both English and Chinese.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n<strong>  - variable: server_url\n</strong>    label:\n      zh_Hans: サーバーURL\n      en_US: Server url\n    type: text-input\n    required: true\n    placeholder:\n      zh_Hans: Xinferenceのサーバーアドレスをここに入力してください（例：https://example.com/xxx）\n      en_US: Enter the url of your Xinference, for example https://example.com/xxx\n<strong>  - variable: model_uid\n</strong>    label:\n      zh_Hans: モデルUID\n      en_US: Model uid\n    type: text-input\n    required: true\n    placeholder:\n      zh_Hans: モデルUIDを入力してください\n      en_US: Enter the model uid\n```\n\n----------------------------------------\n\nTITLE: Configure MCP Server URL in Workflow Agent Strategy\nDESCRIPTION: This code snippet shows the JSON structure used to specify the Zapier MCP Server address within a Dify workflow's agent strategy.  The `url` field is replaced with the actual Zapier endpoint, allowing the workflow agent to utilize Zapier's connected services to execute tasks according to the prompt instructions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/best-practice/how-to-use-mcp-zapier.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"server_name\": {\n    \"url\": \"https://actions.zapier.com/mcp/*******/sse\",\n    \"headers\": {},\n    \"timeout\": 5,\n    \"sse_read_timeout\": 300\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Response Body for Knowledge Retrieval\nDESCRIPTION: This code snippet illustrates an example JSON response body returned by the Dify API after a successful knowledge retrieval. It includes a list of `records` containing the `content`, `score`, `title`, and `metadata` of the retrieved knowledge.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/external-knowledge-api-documentation.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"records\": [{\n                    \"metadata\": {\n                            \"path\": \"s3://dify/knowledge.txt\",\n                            \"description\": \"dify知識ドキュメント\"\n                    },\n                    \"score\": 0.98,\n                    \"title\": \"knowledge.txt\",\n                    \"content\": \"これは外部知識のドキュメントです。\"\n            },\n            {\n                    \"metadata\": {\n                            \"path\": \"s3://dify/introduce.txt\",\n                            \"description\": \"dify紹介\"\n                    },\n                    \"score\": 0.66,\n                    \"title\": \"introduce.txt\",\n                    \"content\": \"GenAIアプリケーションのイノベーションエンジン\"\n            }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Get Number of Tokens Function\nDESCRIPTION: This Python function calculates the number of tokens for a given prompt. If the model does not provide its own tokenization, it can return 0. It receives model, credentials, prompt messages, and optional tools as parameters.  If the model does not provide a means of calculating the token count, a zero value should be returned.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/customizable-model.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                     tools: Optional[list[PromptMessageTool]] = None) -> int:\n      \"\"\"\n      Get number of tokens for given prompt messages\n\n      :param model: model name\n      :param credentials: model credentials\n      :param prompt_messages: prompt messages\n      :param tools: tools for tool calling\n      :return:\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Update Document with File - Dify API (cURL)\nDESCRIPTION: This cURL command updates a document in an existing Dify knowledge base by uploading a file. It requires the dataset ID, document ID, and API key. The request uses `multipart/form-data` to send the file and the configuration data, including document name, indexing technique, and processing rules.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/update-by-file' \\\n--header 'Authorization: Bearer {api_key}' \\\n--form 'data=\"{\"name\":\"Dify\",\"indexing_technique\":\"high_quality\",\"process_rule\":{\"rules\":{\"pre_processing_rules\":[{\"id\":\"remove_extra_spaces\",\"enabled\":true},{\"id\":\"remove_urls_emails\",\"enabled\":true}],\"segmentation\":{\"separator\":\"###\",\"max_tokens\":500}},\"mode\":\"custom\"}}\";type=text/plain' \\\n--form 'file=@\"/path/to/file\"'\n```\n\n----------------------------------------\n\nTITLE: Edit Systemd Service for Ollama on Linux\nDESCRIPTION: This command opens the systemd service configuration file for Ollama on Linux. It allows modifying the service settings, including setting environment variables.  Modifying the service is necessary for persistent configuration changes and to allow access to Ollama from other machines or containers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/ollama.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsystemctl edit ollama.service\n```\n\n----------------------------------------\n\nTITLE: Calculate Variance of Array with Python in Dify\nDESCRIPTION: This Python code snippet calculates the variance of a numerical array (`x`) using the Dify Code Node. It computes the mean of the array and then calculates the sum of squared differences from the mean, dividing by the number of elements. The result is returned in a dictionary under the 'result' key. The input is a list of numbers, and the output is a float.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_68\n\nLANGUAGE: Python\nCODE:\n```\ndef main(x: list) -> float:\n    return {\n        # Note to declare 'result' in the output variables\n        'result': sum([(i - sum(x) / len(x)) ** 2 for i in x]) / len(x)\n    }\n```\n\n----------------------------------------\n\nTITLE: Cloudflare Worker for Content Moderation (TypeScript)\nDESCRIPTION: This TypeScript code demonstrates a Cloudflare Worker that implements content moderation using keyword matching. It defines API endpoint validation using `zod`, implements bearer authentication, and contains the core logic for filtering user input and LLM output based on predefined keyword lists. This code demonstrates the implementation of `app.moderation.input` and `app.moderation.output` extension points.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/moderation.md#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Hono } from \"hono\";\nimport { bearerAuth } from \"hono/bearer-auth\";\nimport { z } from \"zod\";\nimport { zValidator } from \"@hono/zod-validator\";\nimport { generateSchema } from '@anatine/zod-openapi';\n\ntype Bindings = {\n  TOKEN: string;\n};\n\nconst app = new Hono<{ Bindings: Bindings }>();\n\n// API形式の検証 ⬇️\nconst schema = z.object({\n  point: z.union([\n    z.literal(\"ping\"),\n    z.literal(\"app.external_data_tool.query\"),\n    z.literal(\"app.moderation.input\"),\n    z.literal(\"app.moderation.output\"),\n  ]), // 'point'の値を特定の値に制限\n  params: z\n    .object({\n      app_id: z.string().optional(),\n      tool_variable: z.string().optional(),\n      inputs: z.record(z.any()).optional(),\n      query: z.any(),\n      text: z.any()\n    })\n    .optional(),\n});\n\n// OpenAPIスキーマを生成\napp.get(\"/\", (c) => {\n  return c.json(generateSchema(schema));\n});\n\napp.post(\n  \"/\",\n  (c, next) => {\n    const auth = bearerAuth({ token: c.env.TOKEN });\n    return auth(c, next);\n  },\n  zValidator(\"json\", schema),\n  async (c) => {\n    const { point, params } = c.req.valid(\"json\");\n    if (point === \"ping\") {\n      return c.json({\n        result: \"pong\",\n      });\n    }\n    // ⬇️ ここにロジックを実装 ⬇️\n    // point === \"app.external_data_tool.query\"\n    else if (point === \"app.moderation.input\"){\n    // 入力チェック ⬇️\n    const inputkeywords = [\"入力フィルターテスト1\", \"入力フィルターテスト2\", \"入力フィルターテスト3\"];\n\n    if (inputkeywords.some(keyword => params.query.includes(keyword)))\n      {\n      return c.json({\n        \"flagged\": true,\n        \"action\": \"direct_output\",\n        \"preset_response\": \"入力に不適切なコンテンツが含まれています。別の質問を試してください！\"\n      });\n    } else {\n      return c.json({\n        \"flagged\": false,\n        \"action\": \"direct_output\",\n        \"preset_response\": \"入力に問題はありません\"\n      });\n    }\n    // 入力チェック完了 \n    }\n    \n    else {\n      // 出力チェック ⬇️\n      const outputkeywords = [\"出力フィルターテスト1\", \"出力フィルターテスト2\", \"出力フィルターテスト3\"]; \n\n  if (outputkeywords.some(keyword => params.text.includes(keyword)))\n    {\n      return c.json({\n        \"flagged\": true,\n        \"action\": \"direct_output\",\n        \"preset_response\": \"出力に機密情報が含まれており、システムによってフィルタリングされました。別の質問をしてください！\"\n      });\n    }\n  \n  else {\n    return c.json({\n      \"flagged\": false,\n      \"action\": \"direct_output\",\n      \"preset_response\": \"出力に問題はありません\"\n    });\n  };\n    }\n    // 出力チェック完了 \n  }\n);\n\nexport default app;\n```\n\n----------------------------------------\n\nTITLE: Start Xinference in Local Deployment Mode\nDESCRIPTION: This command starts the Xinference server in local mode.  It initializes the server and worker, providing an endpoint for accessing the deployed models. The endpoint URL is displayed in the output.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/xinference.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ xinference-local\n```\n\n----------------------------------------\n\nTITLE: Frontend Directory Structure\nDESCRIPTION: This snippet shows the directory structure of the Dify frontend. The frontend is built using Next.js with Tailwind CSS for styling and React-i18next for internationalization.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_61\n\nLANGUAGE: Text\nCODE:\n```\n[web/]\n├── app                   // レイアウト、ページ、およびコンポーネント\n│   ├── (commonLayout)    // アプリ全体で使用される共通レイアウト\n│   ├── (shareLayout)     // トークン固有のセッション間で共有されるレイアウト\n│   ├── activate          // アクティベートページ\n│   ├── components        // ページとレイアウトで共有されるコンポーネント\n│   ├── install           // インストールページ\n│   ├── signin            // サインインページ\n│   └── styles            // グローバルに共有されるスタイル\n├── assets                // 静的アセット\n├── bin                   // ビルドステップで実行されるスクリプト\n├── config                // 調整可能な設定とオプション\n├── context               // アプリの異なる部分で使用される共有コンテキスト\n├── dictionaries          // 言語固有の翻訳ファイル\n├── docker                // コンテナ設定\n├── hooks                 // 再利用可能なフック\n├── i18n                  // 国際化設定\n├── models                // データモデルとAPIレスポンスの形状を記述\n├── public                // ファビコンなどのメタアセット\n├── service               // APIアクションの形状を指定\n├── test\n├── types                 // 関数パラメータと戻り値の記述\n└── utils                 // 共有ユーティリティ関数\n```\n\n----------------------------------------\n\nTITLE: Run Database Migrations\nDESCRIPTION: Applies database migrations to update the database schema. This ensures that the database is up-to-date with the latest code changes. Requires Flask and the database to be configured correctly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_43\n\nLANGUAGE: Bash\nCODE:\n```\npoetry shell\n```\n\nLANGUAGE: Bash\nCODE:\n```\nflask db upgrade\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository with Git\nDESCRIPTION: This command clones the forked Dify repository from GitHub to the local machine. Replace `<github_username>` with your actual GitHub username. This allows developers to work on a local copy of the codebase.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_60\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone git@github.com:<github_username>/dify.git\n```\n\n----------------------------------------\n\nTITLE: Mapping Model Invoke Errors in Python\nDESCRIPTION: This property defines a mapping between specific exception types raised during model invocation and unified InvokeError types defined in the Runtime. This allows Dify to perform appropriate follow-up actions based on the type of error encountered during model invocation, such as connection errors, rate limits, or authorization failures.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Input Moderation Request Body Example (JSON)\nDESCRIPTION: Illustrates the JSON structure expected in the HTTP POST request body when using the `app.moderation.input` extension point. It shows the `point` (fixed value), `app_id`, `inputs` (user-submitted variables), and `query` (user input) parameters. It is used by Dify when content moderation for inputs is enabled.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/moderation.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"point\": \"app.moderation.input\",\n    \"params\": {\n        \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n        \"inputs\": {\n            \"var_1\": \"I will kill you.\",\n            \"var_2\": \"I will fuck you.\"\n        },\n        \"query\": \"Happy everydays.\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: User Prompt Message Class in Python\nDESCRIPTION: This snippet defines a `UserPromptMessage` class inheriting from `PromptMessage` with a default role of 'user'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nclass UserPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for user prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.USER\n```\n\n----------------------------------------\n\nTITLE: Install Xinference via PyPI\nDESCRIPTION: This command installs Xinference with all optional dependencies using pip. This allows you to utilize Xinference for various tasks, including language model inference and embedding generation. Ensure you have Python and pip installed before running this command.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install \"xinference[all]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Poetry\nDESCRIPTION: This command installs the required Python dependencies for the Dify API service using Poetry. It first ensures the correct Python environment is selected, and then installs all the dependencies defined in the `poetry.lock` or `pyproject.toml` file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\npoetry env use 3.12\npoetry install\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response: Prompt Too Long\nDESCRIPTION: This snippet shows an error response indicating that the prompt (query or prefix) is too long.  It suggests reducing the prefix prompt, shrinking the max token value, or switching to a model with a larger token limit.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/llms-use-faq.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"Query or prefix prompt is too long, you can reduce the preix prompt, or shrink the max token, or switch to a llm with a larger token limit size\": null\n}\n```\n\n----------------------------------------\n\nTITLE: SSRF Proxy Squid Configuration Example\nDESCRIPTION: Example Squid configuration to restrict access to a specific IP address within a local network. Defines an ACL for the restricted IP and a local network, then denies access to the restricted IP while allowing access to the rest of the local network.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_14\n\nLANGUAGE: squid\nCODE:\n```\nacl restricted_ip dst 192.168.101.19\nacl localnet src 192.168.101.0/24\n\nhttp_access deny restricted_ip\nhttp_access allow localnet\nhttp_access deny all\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Content Base Class\nDESCRIPTION: Defines a base class for prompt message content, which includes the content type and the actual data. This class is meant for parameter declaration and cannot be initialized directly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContent(BaseModel):\n    \"\"\"\n    Model class for prompt message content.\n    \"\"\"\n    type: PromptMessageContentType\n    data: str\n```\n\n----------------------------------------\n\nTITLE: Dify Backend Directory Structure\nDESCRIPTION: This snippet illustrates the directory structure of the Dify backend, written in Python using Flask. It shows key directories such as controllers, core, models, services, and tasks, providing an overview of the backend's organization and components.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_28\n\nLANGUAGE: plaintext\nCODE:\n```\n[api/]\n├── constants             // Constant settings used throughout code base.\n├── controllers           // API route definitions and request handling logic.\n├── core                  // Core application orchestration, model integrations, and tools.\n├── docker                // Docker & containerization related configurations.\n├── events                // Event handling and processing\n├── extensions            // Extensions with 3rd party frameworks/platforms.\n├── fields                // field definitions for serialization/marshalling.\n├── libs                  // Reusable libraries and helpers.\n├── migrations            // Scripts for database migration.\n├── models                // Database models & schema definitions.\n├── services              // Specifies business logic.\n├── storage               // Private key storage.\n├── tasks                 // Handling of async tasks and background jobs.\n└── tests\n```\n\n----------------------------------------\n\nTITLE: BLOBメッセージを作成 (Python)\nDESCRIPTION: ファイルの生データ（blob）とメタデータを渡して、DifyにBLOBメッセージを生成させるインターフェースです。ファイルタイプが分かる場合は`mime_type`を渡すことが推奨されます。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/advanced-tool-integration.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef create_blob_message(self, blob: bytes, meta: dict = None, save_as: str = '') -> ToolInvokeMessage:\n    \"\"\"\n        BLOBメッセージを作成\n\n        :param blob: BLOBデータ\n        :return: BLOBメッセージ\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Cloning the Bot Code\nDESCRIPTION: This snippet clones the necessary code from a GitHub repository to create a basic chatbot.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-teams.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/somethingwentwell/dify-teams-bot\n```\n\n----------------------------------------\n\nTITLE: Workflow System Variables (Workflow)\nDESCRIPTION: This section defines system variables available in Dify's Workflow applications. These variables, prefixed with `sys`, provide access to user data, application IDs, workflow IDs, and workflow run IDs.  They are designed to be globally accessible within the workflow and are intended for advanced users with development capabilities for tracking and recording information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_79\n\n\n\n----------------------------------------\n\nTITLE: Define LLMUsage Model\nDESCRIPTION: Defines the `LLMUsage` class, which represents the usage information for an LLM call. It extends `ModelUsage` and includes token counts, pricing details, currency, and latency.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nclass LLMUsage(ModelUsage):\n    \"\"\"\n    Model class for LLM usage.\n    \"\"\"\n    prompt_tokens: int  # Tokens used for prompt\n    prompt_unit_price: Decimal  # Unit price for prompt\n    prompt_price_unit: Decimal  # Price unit for prompt, i.e., the unit price based on how many tokens\n    prompt_price: Decimal  # Cost for prompt\n    completion_tokens: int  # Tokens used for response\n    completion_unit_price: Decimal  # Unit price for response\n    completion_price_unit: Decimal  # Price unit for response, i.e., the unit price based on how many tokens\n    completion_price: Decimal  # Cost for response\n    total_tokens: int  # Total number of tokens used\n    total_price: Decimal  # Total cost\n    currency: str  # Currency unit\n    latency: float  # Request latency (s)\n```\n\n----------------------------------------\n\nTITLE: Verifying Docker Compose Installation\nDESCRIPTION: This command checks the installed version of Docker Compose, confirming a successful installation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/searxng.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose --version\n```\n\n----------------------------------------\n\nTITLE: Install Go 1.20.6 on Ubuntu/Debian\nDESCRIPTION: Installs Go version 1.20.6 on Ubuntu/Debian. DifySandbox requires Go to be installed for building the project. Version 1.20.6 or higher is recommended.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/backend/sandbox/README.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nsudo apt-get install -y golang-1.20.6\n```\n\n----------------------------------------\n\nTITLE: Handling Streaming and Synchronous Responses (Python)\nDESCRIPTION: This Python code demonstrates how to handle both streaming and synchronous responses from an LLM. It uses the `stream` parameter to determine which handler to call. The `_handle_stream_response` function yields chunks of data for streaming responses, while the `_handle_sync_response` function returns a complete LLMResult for synchronous responses.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/predefined-model.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, stream: bool, **kwargs) \\\n        -> Union[LLMResult, Generator]:\n    if stream:\n          return self._handle_stream_response(**kwargs)\n    return self._handle_sync_response(**kwargs)\n\ndef _handle_stream_response(self, **kwargs) -> Generator:\n    for chunk in response:\n          yield chunk\ndef _handle_sync_response(self, **kwargs) -> LLMResult:\n    return LLMResult(**response)\n```\n\n----------------------------------------\n\nTITLE: JSON Schema for Math Reasoning\nDESCRIPTION: This snippet defines a JSON Schema for recording steps and the final answer for mathematical reasoning. It includes an array of reasoning steps, each with an explanation and output, and a final answer field.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/how-to-use-json-schema-in-dify.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"math_reasoning\",\n    \"description\": \"Records steps and final answer for mathematical reasoning\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"steps\": {\n                \"type\": \"array\",\n                \"description\": \"Array of reasoning steps\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"explanation\": {\n                            \"type\": \"string\",\n                            \"description\": \"Explanation of the reasoning step\"\n                        },\n                        \"output\": {\n                            \"type\": \"string\",\n                            \"description\": \"Output of the reasoning step\"\n                        }\n                    },\n                    \"required\": [\"explanation\", \"output\"],\n                    \"additionalProperties\": false\n                }\n            },\n            \"final_answer\": {\n                \"type\": \"string\",\n                \"description\": \"The final answer to the mathematical problem\"\n            }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\"steps\", \"final_answer\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: POST Request to Completion Messages API with cURL\nDESCRIPTION: This cURL command sends a POST request to the `/v1/completion-messages` endpoint for text completion. It includes the authorization header with a bearer token, the content type header, and a JSON payload with input parameters, response mode, and user ID.  The `ENTER-YOUR-SECRET-KEY` should be replaced with the actual API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-publishing/developing-with-apis.md#_snippet_0\n\nLANGUAGE: cURL\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/completion-messages' \\\n--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"inputs\": {},\n    \"response_mode\": \"streaming\",\n    \"user\": \"abc-123\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Creating BLOB Message in Dify (Python)\nDESCRIPTION: This snippet explains how to create a file BLOB message using the `Tool` class in Dify. It takes raw byte data (`blob`) and metadata (`meta`) as input.  The `meta` dictionary should ideally include the `mime_type` to correctly identify the file type; otherwise, it defaults to `octet/stream`. It returns a `ToolInvokeMessage`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/advanced-tool-integration.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    def create_blob_message(self, blob: bytes, meta: dict = None, save_as: str = '') -> ToolInvokeMessage:\n        \"\"\"\n            create a blob message\n\n            :param blob: the blob\n            :return: the blob message\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Output Schema in YAML\nDESCRIPTION: This YAML snippet defines the output schema for a Dify tool plugin, specifying the structure and data types of the output variables. In this case, it defines a simple tool with a single output variable named 'name' of type string.  The schema must be adhered to when setting output variables within the tool's implementation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/tool.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: author\n  name: tool\n  label:\n    en_US: label\n    zh_Hans: 标签\n    ja_JP: レベル\n    pt_BR: etiqueta\ndescription:\n  human:\n    en_US: description\n    zh_Hans: 描述\n    ja_JP: 説明\n    pt_BR: descrição\n  llm: description\noutput_schema:\n  type: object\n  properties:\n    name:\n      type: string\n```\n\n----------------------------------------\n\nTITLE: OpenAI Model Plugin Structure\nDESCRIPTION: This snippet shows the directory structure for the OpenAI model plugin, including categories like 'llm', 'moderation', 'speech2text', 'text_embedding', and 'tts', along with specific models under each category.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n├── models\n│ ├── llm\n│ │ ├── chatgpt-4o-latest\n│ │ ├── gpt-3.5-turbo\n│ │ ├── gpt-4-0125-preview\n│ │ ├── gpt-4-turbo\n│ │ ├── gpt-4o\n│ │ ├── llm\n│ │ ├── o1-preview\n│ │ └── text-davinci-003\n│ ├── moderation\n│ │ ├── moderation\n│ │ └── text-moderation-stable\n│ ├── speech2text\n│ │ ├── speech2text\n│ │ └── whisper-1\n│ ├── text_embedding\n│ │ ├── text-embedding-3-large\n│ │ └── text_embedding\n│ └── tts\n│ ├── tts-1-hd\n│ ├── tts-1\n│ └── tts\n```\n\n----------------------------------------\n\nTITLE: Invoke LLM with Model Selector - Python\nDESCRIPTION: This code snippet shows how to invoke an LLM using the 'model-selector' parameter defined in the YAML configuration. The `model_config` is directly retrieved from the `tool_parameters`, simplifying the model invocation process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom typing import Any\n\nfrom dify_plugin import Tool\nfrom dify_plugin.entities.model.llm import LLMModelConfig\nfrom dify_plugin.entities.tool import ToolInvokeMessage\nfrom dify_plugin.entities.model.message import SystemPromptMessage, UserPromptMessage\n\nclass LLMTool(Tool):\n    def _invoke(self, tool_parameters: dict[str, Any]) -> Generator[ToolInvokeMessage]:\n        response = self.session.model.llm.invoke(\n            model_config=tool_parameters.get('model'),\n            prompt_messages=[\n                SystemPromptMessage(\n                    content='you are a helpful assistant'\n                ),\n                UserPromptMessage(\n                    content=tool_parameters.get('query')\n                )\n            ],\n            stream=True\n        )\n\n        for chunk in response:\n            if chunk.delta.message:\n                assert isinstance(chunk.delta.message.content, str)\n                yield self.create_text_message(text=chunk.delta.message.content)\n```\n\n----------------------------------------\n\nTITLE: Handling Stream and Sync Responses in Python\nDESCRIPTION: Illustrates how to handle both streaming and synchronous responses within the `_invoke` method. It demonstrates the use of separate handler functions, `_handle_stream_response` and `_handle_sync_response`, and leverages Python's `yield` keyword to return a generator for streaming responses. The `LLMResult` class is used for synchronous responses.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/predefined-model.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, stream: bool, **kwargs) \\\n            -> Union[LLMResult, Generator]:\n        if stream:\n              return self._handle_stream_response(**kwargs)\n        return self._handle_sync_response(**kwargs)\n\ndef _handle_stream_response(self, **kwargs) -> Generator:\n        for chunk in response:\n              yield chunk\ndef _handle_sync_response(self, **kwargs) -> LLMResult:\n        return LLMResult(**response)\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response: OpenAI Connection Error\nDESCRIPTION: This JSON displays an error related to communication with OpenAI. The error occurs when the maximum number of retries is exceeded while attempting to establish a new connection, likely due to a temporary failure in name resolution. This is often caused by proxy settings in the environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/llms-use-faq.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"error\": \"Error communicating with OpenAl: HTTPSConnectionPool(host='api.openai.com', port=443): Max retriesexceeded with url: /v1/chat/completions (Caused byNewConnectionError( <urllib3.connection.HTTPSConnection object at 0x7f0462ed7af0>; Failed toestablish a new connection: [Errno -3] Temporary failure in name resolution'))\"\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response: Internal Server Error\nDESCRIPTION: This JSON snippet represents an error response from the server, indicating an internal server error that prevented the completion of the request. It suggests either the server is overloaded or there's an error in the application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/llms-use-faq.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"error\": \"The server encountered an internal error and was unable to complete your request。Either the server is overloaded or there is an error in the application\"\n}\n```\n\n----------------------------------------\n\nTITLE: Stability API Key Input\nDESCRIPTION: This step involves inputting the Stability API key into the Dify platform to enable image generation capabilities.  The API key is obtained from the Stability AI platform and pasted into the Dify tool settings.  A valid API key is required for the integration to work.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/workshop/basic/build-ai-image-generation-app.md#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Run Frontend Docker from DockerHub\nDESCRIPTION: This command pulls the latest Dify web frontend image from DockerHub and runs it as a container. It maps port 3000 of the container to port 3000 on the host and sets the CONSOLE_URL and APP_URL environment variables to point to the backend service running on localhost.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/start-the-frontend-docker-container.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run -it -p 3000:3000 -e CONSOLE_URL=http://127.0.0.1:5001 -e APP_URL=http://127.0.0.1:5001 langgenius/dify-web:latest\n```\n\n----------------------------------------\n\nTITLE: Uploading Local Files to Cloud Storage (Docker Compose)\nDESCRIPTION: Uploads local files to cloud storage when using Docker Compose. Requires cloud storage configuration (e.g., aliyun-oss) to be set up.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask upload-local-files-to-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Deploying the API Extension\nDESCRIPTION: These commands are used to install the dependencies and deploy the API extension using npm.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\nnpm run deploy\n```\n\n----------------------------------------\n\nTITLE: API Response Example\nDESCRIPTION: This JSON snippet shows the expected structure of the API response from an extension. The content varies depending on the extension point.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/README.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    ...  // API 返回的内容，不同扩展点返回见不同模块的规范设计\n}\n```\n\n----------------------------------------\n\nTITLE: Text Embedding Invoke Entry Point Python\nDESCRIPTION: This snippet shows how to access the text embedding functionality in the Dify plugin session.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.text_embedding\n```\n\n----------------------------------------\n\nTITLE: Example Stable Diffusion Model Response\nDESCRIPTION: This JSON snippet shows an example response from the Stable Diffusion API when querying available models. It includes the model's title, `model_name`, hash, SHA256 checksum, filename, and configuration. The `model_name` field is crucial for configuring Dify to use the selected model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/stable-diffusion.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"title\": \"pastel-mix/pastelmix-better-vae-fp32.ckpt [943a810f75]\",\n        \"model_name\": \"pastel-mix_pastelmix-better-vae-fp32\",\n        \"hash\": \"943a810f75\",\n        \"sha256\": \"943a810f7538b32f9d81dc5adea3792c07219964c8a8734565931fcec90d762d\",\n        \"filename\": \"/home/takatost/stable-diffusion-webui/models/Stable-diffusion/pastel-mix/pastelmix-better-vae-fp32.ckpt\",\n        \"config\": null\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose Services\nDESCRIPTION: This command starts the Docker Compose services defined in the `docker-compose.yaml` file in detached mode (-d), running the SearXNG, Redis, and Caddy containers in the background.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Moderation Invoke Entry Point Python\nDESCRIPTION: This snippet shows how to access the moderation functionality in the Dify plugin session.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.moderation\n```\n\n----------------------------------------\n\nTITLE: Clone Dify Documentation Repository via Git\nDESCRIPTION: This command clones the Dify documentation repository from GitHub to a local machine, allowing contributors to make changes and submit pull requests. Replace `<your-github-account>` with the user's GitHub username.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_66\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/<your-github-account>/dify-docs.git\n```\n\n----------------------------------------\n\nTITLE: Install Python Version\nDESCRIPTION: Installs Python 3.12 using pyenv. This ensures the correct Python version is available for running the Dify API service. Requires pyenv to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_36\n\nLANGUAGE: Bash\nCODE:\n```\npyenv install 3.12\n```\n\n----------------------------------------\n\nTITLE: Deploying OpenLLM Model via Docker\nDESCRIPTION: This command deploys a specified model using OpenLLM via Docker.  It runs an OpenLLM container, maps port 3333 of the host machine to port 3000 of the container, and starts the specified model with a specified backend. This allows for running inference on the model locally.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/openllm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -it -p 3333:3000 ghcr.io/bentoml/openllm start facebook/opt-1.3b --backend pt\n```\n\n----------------------------------------\n\nTITLE: Running Database Migrations (API)\nDESCRIPTION: Runs database migrations using Flask DB upgrade. This ensures that the database schema is up-to-date.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_25\n\nLANGUAGE: Bash\nCODE:\n```\npoetry shell\nflask db upgrade\n```\n\n----------------------------------------\n\nTITLE: User Prompt Message Class Definition\nDESCRIPTION: This code snippet defines the UserPromptMessage class, which represents a message from the user. It inherits from PromptMessage and sets the role to USER.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass UserPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for user prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.USER\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Content Type Enum in Python\nDESCRIPTION: This snippet defines an Enum class `PromptMessageContentType` for message content types (text, image).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContentType(Enum):\n    \"\"\"\n    Enum class for prompt message content type.\n    \"\"\"\n    TEXT = 'text'\n    IMAGE = 'image'\n```\n\n----------------------------------------\n\nTITLE: Install FastAPI dependencies\nDESCRIPTION: This command installs FastAPI and Uvicorn, which are necessary to run the Python code example.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install fastapi[all] uvicorn\n```\n\n----------------------------------------\n\nTITLE: Upgrade Dify via Git and Docker Compose\nDESCRIPTION: This snippet clones the Dify repository, moves the Docker configuration files, removes the temporary directory, and restarts Dify using Docker Compose. It's used to upgrade the Dify instance within the EC2 environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/dify-premium.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git /tmp/dify\nmv -f /tmp/dify/docker/* /dify/\nrm -rf /tmp/dify\ndocker-compose down\ndocker-compose pull\ndocker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Embedding Dify Chatbot iFrame Code\nDESCRIPTION: This code snippet demonstrates the basic iFrame code required to embed a Dify Chatbot into a website. The `src` attribute specifies the URL of the chatbot, and the `style` attribute defines the initial dimensions and appearance. The `allow` attribute specifies which browser features the iframe can use, in this case the microphone.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/use-cases/how-to-integrate-dify-chatbot-to-your-wix-website.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n<iframe src=\"https://udify.app/chatbot/ez1pf83HVV3JgWO4\" style=\"width: 100%; height: 100%; min-height: 700px\" frameborder=\"0\" allow=\"microphone\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: Define SystemPromptMessage Model\nDESCRIPTION: Defines the `SystemPromptMessage` class, which represents system messages, usually used for setting system commands given to the model. It inherits from `PromptMessage` and sets the message role to `SYSTEM`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nclass SystemPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for system prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.SYSTEM\n```\n\n----------------------------------------\n\nTITLE: Log Message Creation and Completion\nDESCRIPTION: This Python code snippet demonstrates how to create and finish log messages using the `create_log_message` and `finish_log_message` methods. It logs the start and finish of a model invocation, including relevant data and metadata.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel_log = self.create_log_message(\n            label=f\"{params.model.model} Thought\",\n            data={},\n            metadata={\"start_at\": model_started_at, \"provider\": params.model.provider},\n            status=ToolInvokeMessage.LogMessage.LogStatus.START,\n        )\nyield model_log\nself.session.model.llm.invoke(...)\nyield self.finish_log_message(\n    log=model_log,\n    data={\n        \"output\": response,\n        \"tool_name\": tool_call_names,\n        \"tool_input\": tool_call_inputs,\n    },\n    metadata={\n        \"started_at\": model_started_at,\n        \"finished_at\": time.perf_counter(),\n        \"elapsed_time\": time.perf_counter() - model_started_at,\n        \"provider\": params.model.provider,\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Install Dify Plugins\nDESCRIPTION: This command installs the extracted plugins into the latest Community Edition.  It continues within the `docker-api-1` container and uses `poetry` to execute a flask command that downloads and installs the necessary plugins.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/dify-premium.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask install-plugins --workers=2\n```\n\n----------------------------------------\n\nTITLE: Defining Wikipedia Tool Provider YAML (No Credentials)\nDESCRIPTION: This YAML file defines the Wikipedia tool provider, which doesn't require any credentials. It includes the provider's author, name, labels, descriptions, and icon.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/quick-tool-integration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: Dify\n  name: wikipedia\n  label:\n    en_US: Wikipedia\n    zh_Hans: 维基百科\n    ja_JP: Wikipedia\n    pt_BR: Wikipedia\n  description:\n    en_US: Wikipedia is a free online encyclopedia, created and edited by volunteers around the world.\n    zh_Hans: 维基百科是一个由全世界的志愿者创建和编辑的免费在线百科全书。\n    ja_JP: Wikipediaは、世界中のボランティアによって作成、編集されている無料のオンライン百科事典です。\n    pt_BR: A Wikipédia é uma enciclopédia online gratuita, criada e editada por voluntários ao redor do mundo.\n  icon: icon.svg\ncredentials_for_provider:\n```\n\n----------------------------------------\n\nTITLE: Integrating History Messages into Model Calls\nDESCRIPTION: This Python snippet shows how to incorporate conversation history into the model call by concatenating `history_prompt_messages` with the current query using `UserPromptMessage`. This enables the model to consider previous turns in the conversation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n\n        chunks: Generator[LLMResultChunk, None, None] | LLMResult = (\n            self.session.model.llm.invoke(\n                model_config=LLMModelConfig(**params.model.model_dump(mode=\"json\")),\n                # Add history messages\n                prompt_messages=params.model.history_prompt_messages\n                + [UserPromptMessage(content=params.query)],\n                tools=[\n                    self._convert_tool_to_prompt_message_tool(tool)\n                    for tool in params.tools\n                ],\n                stop=params.model.completion_params.get(\"stop\", [])\n                if params.model.completion_params\n                else [],\n                stream=True,\n            )\n        )\n        ...\n```\n\n----------------------------------------\n\nTITLE: Celery Broker URL Configuration (Redis Sentinel)\nDESCRIPTION: This snippet illustrates the format for configuring the Celery broker URL when using Redis Sentinel for high availability.  It includes placeholders for the Sentinel username, password, host, port, and database number. Multiple Sentinel instances can be specified, separated by semicolons. The database number should differ from those used for Session Redis and the Redis cache.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/environments.md#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nsentinel://<sentinel_username>:<sentinel_password>@<sentinel_host>:<sentinel_port>/<redis_database>\n```\n\n----------------------------------------\n\nTITLE: Customizing Button Styles with containerProps (Inline Styles)\nDESCRIPTION: This JavaScript code snippet demonstrates how to customize the button's styles using the `containerProps` option in the `window.difyChatbotConfig` object. It uses inline styles to change the background color, width, height, and border radius of the button.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-publishing/embedding-in-websites.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.difyChatbotConfig = {\n    // ... 他の設定\n    containerProps: {\n        style: {\n            backgroundColor: '#ABCDEF',\n            width: '60px',\n            height: '60px',\n            borderRadius: '30px',\n        },\n        // ちょっとしたスタイル変更の場合、style 属性に文字列を使用することもできます。\n        // style: 'background-color: #ABCDEF; width: 60px;',\n    },\n};\n```\n\n----------------------------------------\n\nTITLE: Moderation Input Response Body JSON\nDESCRIPTION: Defines the structure for the API response of the `app.moderation.input` extension point. It indicates whether the content was flagged, the action to take, a preset response, the input variables, and the original query. The `flagged` bool indicates if the input requires action. `action` will determine next steps (direct output, override).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation-extension.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"flagged\": bool,\n    \"action\": string,\n    \"preset_response\": string,\n    \"inputs\": {\n        \"var_1\": \"value_1\",\n        \"var_2\": \"value_2\",\n        ...\n    },\n    \"query\": string | null\n}\n```\n\n----------------------------------------\n\nTITLE: Error Mapping for Model Invocation (Python)\nDESCRIPTION: This Python code demonstrates how to map model invocation errors to Runtime-defined `InvokeError` types. The mapping allows Dify to handle different error scenarios appropriately, such as connection errors, server unavailability, rate limits, authorization failures, and bad requests.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing get_num_tokens method\nDESCRIPTION: This Python code snippet shows the `get_num_tokens` method, which is used to get the number of tokens for given prompt messages. If the model does not provide pre-computed tokens, the method can return 0. It accepts model name, credentials, prompt messages, and tools as input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/predefined-model.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                       tools: Optional[list[PromptMessageTool]] = None) -> int:\n        \"\"\"\n        Get number of tokens for given prompt messages\n\n        :param model: model name\n        :param credentials: model credentials\n        :param prompt_messages: prompt messages\n        :param tools: tools for tool calling\n        :return:\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring App Information in config/index.ts JavaScript\nDESCRIPTION: This JavaScript code block illustrates the configuration of app information within the `config/index.ts` file of a Dify WebApp template. It defines properties such as the application title, description, copyright, privacy policy link, and default language.  It is used to customize the appearance and behavior of the web application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-publishing/based-on-frontend-templates.md#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nexport const APP_INFO: AppInfo = {\n  \"title\": 'Chat APP',\n  \"description\": '',\n  \"copyright\": '',\n  \"privacy_policy\": '',\n  \"default_language\": 'zh-Hans'\n}\n\nexport const isShowPrompt = true\nexport const promptTemplate = ''\n```\n\n----------------------------------------\n\nTITLE: Installing Web Dependencies\nDESCRIPTION: This command installs the required dependencies for the Dify web frontend using pnpm. It first installs pnpm globally if it is not already installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_13\n\nLANGUAGE: Bash\nCODE:\n```\nnpm i -g pnpm\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Requesting Chat App in Endpoint (Python)\nDESCRIPTION: Demonstrates how to request a Chat type App within a plugin's `Endpoint` and return the results. It uses the `self.session.app.workflow.invoke` method with `response_mode` set to \"streaming\" and converts the streaming response to an HTML response.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom typing import Mapping\nfrom werkzeug import Request, Response\nfrom dify_plugin import Endpoint\n\nclass Duck(Endpoint):\n    def _invoke(self, r: Request, values: Mapping, settings: Mapping) -> Response:\n        \"\"\"\n        Invokes the endpoint with the given request.\n        \"\"\"\n        app_id = values[\"app_id\"]\n        def generator():\n            response = self.session.app.workflow.invoke(\n                app_id=app_id, inputs={}, response_mode=\"streaming\", files=[]\n            )\n            for data in response:\n                yield f\"{json.dumps(data)} <br>\"\n        return Response(generator(), status=200, content_type=\"text/html\")\n```\n\n----------------------------------------\n\nTITLE: Starting Dify using Docker Compose\nDESCRIPTION: This snippet demonstrates how to start Dify using Docker Compose. It involves navigating to the docker directory within the Dify project, copying the example environment file, and then using `docker compose up` to build and start the Dify containers. This assumes Docker and Docker Compose are already installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/workshop/intermediate/twitter-chatflow.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ncd dify/docker\ncp .env.example .env\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Creating Twitter URL in Python\nDESCRIPTION: This Python code snippet defines a function `main` that takes a Twitter user ID (a string) as input and returns a dictionary containing the full Twitter URL. The URL is constructed by concatenating the base URL with the user ID.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef main(id: str) -> dict:\n    return {\n        \"url\": \"https://twitter.com/\"+id,\n    }\n```\n\n----------------------------------------\n\nTITLE: Implement Google Provider Code in Python\nDESCRIPTION: This Python code defines the GoogleProvider class, which inherits from BuiltinToolProviderController. The _validate_credentials method validates the provided credentials by instantiating a GoogleSearchTool, forking its runtime to pass the credentials, and invoking it with test parameters. If the invocation fails, it raises a ToolProviderCredentialValidationError exception. This code relies on core.tools modules, the GoogleSearchTool implementation, and the typing module.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/quick-tool-integration.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom core.tools.entities.tool_entities import ToolInvokeMessage, ToolProviderType\nfrom core.tools.tool.tool import Tool\nfrom core.tools.provider.builtin_tool_provider import BuiltinToolProviderController\nfrom core.tools.errors import ToolProviderCredentialValidationError\n\nfrom core.tools.provider.builtin.google.tools.google_search import GoogleSearchTool\n\nfrom typing import Any, Dict\n\nclass GoogleProvider(BuiltinToolProviderController):\n    def _validate_credentials(self, credentials: Dict[str, Any]) -> None:\n        try:\n            # 1. Here you need to instantiate a GoogleSearchTool with GoogleSearchTool(), it will automatically load the yaml configuration of GoogleSearchTool, but at this time it does not have credential information inside\n            # 2. Then you need to use the fork_tool_runtime method to pass the current credential information to GoogleSearchTool\n            # 3. Finally, invoke it, the parameters need to be passed according to the parameter rules configured in the yaml of GoogleSearchTool\n            GoogleSearchTool().fork_tool_runtime(\n                meta={\n                    \"credentials\": credentials,\n                }\n            ).invoke(\n                user_id='',\n                tool_parameters={\n                    \"query\": \"test\",\n                    \"result_type\": \"link\"\n                },\n            )\n        except Exception as e:\n            raise ToolProviderCredentialValidationError(str(e))\n```\n\n----------------------------------------\n\nTITLE: Input Moderation Response Example (JSON)\nDESCRIPTION: Demonstrates the JSON response structure for the `app.moderation.input` extension. The response includes `flagged` (boolean indicating violation of rules), `action` (specifying either `direct_output` or `overridden`), `preset_response` (a pre-defined response if `action` is `direct_output`), `inputs` (overridden user input variables if `action` is `overridden`), and `query` (overridden user query if `action` is `overridden`).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/moderation.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"direct_output\",\n    \"preset_response\": \"Your content violates our usage policy.\"\n  }\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"overridden\",\n    \"inputs\": {\n        \"var_1\": \"I will *** you.\",\n        \"var_2\": \"I will *** you.\"\n    },\n    \"query\": \"Happy everydays.\"\n  }\n```\n\n----------------------------------------\n\nTITLE: Extract Plugins\nDESCRIPTION: This snippet demonstrates how to extract plugins from the previous Dify version using `poetry` inside the `docker-api` container.  It requires `poetry` to be installed and accessible within the container. `a3cb19c2****` is a placeholder for the actual container ID.  The `--workers` argument specifies the number of parallel processes to use during extraction. If there's a prompt for input, the instructions say to click the \"回车\" (enter) key to skip.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/dify-premium.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it a3cb19c2**** bash\npoetry run flask extract-plugins --workers=20\n```\n\n----------------------------------------\n\nTITLE: Assistant Prompt Message Class\nDESCRIPTION: Defines the `AssistantPromptMessage` class, representing a message from the assistant. It includes support for `tool_calls`, representing function calls suggested by the model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nclass AssistantPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for assistant prompt message.\n    \"\"\"\n    class ToolCall(BaseModel):\n        \"\"\"\n        Model class for assistant prompt message tool call.\n        \"\"\"\n        class ToolCallFunction(BaseModel):\n            \"\"\"\n            Model class for assistant prompt message tool call function.\n            \"\"\"\n            name: str  # 工具名称\n            arguments: str  # 工具参数\n\n        id: str  # 工具 ID，仅在 OpenAI tool call 生效，为工具调用的唯一 ID，同一个工具可以调用多次\n        type: str  # 默认 function\n        function: ToolCallFunction  # 工具调用信息\n\n    role: PromptMessageRole = PromptMessageRole.ASSISTANT\n    tool_calls: list[ToolCall] = []  # 模型回复的工具调用结果（仅当传入 tools，并且模型认为需要调用工具时返回）\n```\n\n----------------------------------------\n\nTITLE: Getting Dataset Metadata List with cURL\nDESCRIPTION: This cURL command retrieves the list of metadata fields for a Dify dataset. It requires the dataset ID.  It returns a JSON object containing an array of metadata objects, each with ID, type, name, and use_count.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Compose\nDESCRIPTION: This command downloads and installs Docker Compose, a tool for defining and running multi-container Docker applications. It fetches the latest version from the GitHub releases page and makes it executable.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsudo curl -L \"https://github.com/docker/compose/releases/download/2.32.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\n```\n\n----------------------------------------\n\nTITLE: Docker Connection Error Message\nDESCRIPTION: This is an example error message that may appear when using Docker to deploy both Dify and Ollama. It shows a connection refused error, indicating that the Ollama service is not accessible from within the Docker container.  The error stems from localhost referring to the container itself instead of the host machine.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_78\n\nLANGUAGE: Text\nCODE:\n```\nhttpconnectionpool(host=127.0.0.1, port=11434): max retries exceeded with url:/cpi/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8562812c20>: fail to establish a new connection:[Errno 111] Connection refused'))\n\nhttpconnectionpool(host=localhost, port=11434): max retries exceeded with url:/cpi/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8562812c20>: fail to establish a new connection:[Errno 111] Connection refused'))\n```\n\n----------------------------------------\n\nTITLE: Set Ollama Host Environment Variable on Mac\nDESCRIPTION: This command sets the OLLAMA_HOST environment variable on macOS using `launchctl`. This is necessary when Ollama is run as a macOS application and needs to be accessible from other machines or containers. The value \"0.0.0.0\" allows access from any IP address.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/ollama.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlaunchctl setenv OLLAMA_HOST \"0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: テキストメッセージを作成 (Python)\nDESCRIPTION: テキストメッセージを渡して、Difyにテキストメッセージを生成させるインターフェースです。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/advanced-tool-integration.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef create_text_message(self, text: str, save_as: str = '') -> ToolInvokeMessage:\n    \"\"\"\n        テキストメッセージを作成\n\n        :param text: メッセージのテキスト\n        :return: テキストメッセージ\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Start Web Application\nDESCRIPTION: Starts the web application using NPM, Yarn or PNPM.  This command serves the built application and makes it accessible through a web browser. Requires the application to be built first using `npm run build`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_33\n\nLANGUAGE: JavaScript\nCODE:\n```\nnpm run start\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\nyarn start\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\npnpm start\n```\n\n----------------------------------------\n\nTITLE: System Prompt Message Class Definition\nDESCRIPTION: This code snippet defines the SystemPromptMessage class, representing a system message. It inherits from PromptMessage and sets the role to SYSTEM.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nclass SystemPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for system prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.SYSTEM\n```\n\n----------------------------------------\n\nTITLE: Setting Stop Sequences in Model Parameters\nDESCRIPTION: This code snippet illustrates setting the `Stop_Sequences` parameter in the model configuration.  The value `Human1:` is input as a stop sequence.  The Tab key should be pressed after inputting the stop sequence value. This configuration instructs the LLM to stop generating text before the next occurrence of `Human1:`. Note the tab key press isn't representable directly in the json output.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/prompt-engineering/prompt-engineering-expert-mode.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nThe soil is yellow.\n```\n\n----------------------------------------\n\nTITLE: Overriding Default Button Styles with CSS Variables\nDESCRIPTION: This CSS code snippet demonstrates how to override the default styles of the Dify Chatbot Bubble Button using CSS variables. It includes variables for customizing the button's position, background color, width, height, border-radius, box-shadow, and hover transformation.  These can be adjusted by setting the variables within the scope of the button's ID.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-publishing/embedding-in-websites.md#_snippet_1\n\nLANGUAGE: css\nCODE:\n```\n/* ボタンの下端からの距離、デフォルトは `1rem` */\n--dify-chatbot-bubble-button-bottom\n\n/* ボタンの右端からの距離、デフォルトは `1rem` */\n--dify-chatbot-bubble-button-right\n\n/* ボタンの左端からの距離、デフォルトは `unset` */\n--dify-chatbot-bubble-button-left\n\n/* ボタンの上端からの距離、デフォルトは `unset` */\n--dify-chatbot-bubble-button-top\n\n/* ボタンの背景色、デフォルトは `#155EEF` */\n--dify-chatbot-bubble-button-bg-color\n\n/* ボタンの幅、デフォルトは `50px` */\n--dify-chatbot-bubble-button-width\n\n/* ボタンの高さ、デフォルトは `50px` */\n--dify-chatbot-bubble-button-height\n\n/* ボタンの角丸、デフォルトは `25px` */\n--dify-chatbot-bubble-button-border-radius\n\n/* ボタンのボックスシャドウ、デフォルトは `rgba(0, 0, 0, 0.2) 0px 4px 8px 0px)` */\n--dify-chatbot-bubble-button-box-shadow\n\n/* ボタンホバー時の変形、デフォルトは `scale(1.1)` */\n--dify-chatbot-bubble-button-hover-transform\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Installation Page\nDESCRIPTION: These commands provides the URLs to access the Dify installation page, either using a domain name or IP address and port number. It allows a user to setup an administrator account.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/bt-panel.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# 使用域名\nhttp://yourdomain/install\n\n# 使用IP+端口\n\nhttp://your_server_ip:8088/install\n```\n\n----------------------------------------\n\nTITLE: Image Prompt Message Content Class Definition\nDESCRIPTION: This code snippet defines the class for image-based prompt message content, inheriting from PromptMessageContent. It allows specifying the image detail (resolution) and accepts image data as URL or base64 encoded string.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass ImagePromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for image prompt message content.\n    \"\"\"\n    class DETAIL(Enum):\n        LOW = 'low'\n        HIGH = 'high'\n\n    type: PromptMessageContentType = PromptMessageContentType.IMAGE\n    detail: DETAIL = DETAIL.LOW  # 解像度\n```\n\n----------------------------------------\n\nTITLE: Defining LLMResultChunk Model in Python\nDESCRIPTION: Defines the `LLMResultChunk` model, representing an individual chunk of a streaming LLM response. It includes the model name, prompt messages, an optional system fingerprint, and the delta of the chunk.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunk(BaseModel):\n    \"\"\"\n    Model class for llm result chunk.\n    \"\"\"\n    model: str  # Actual used modele\n    prompt_messages: list[PromptMessage]  # prompt messages\n    system_fingerprint: Optional[str] = None  # request fingerprint, refer to OpenAI definition\n    delta: LLMResultChunkDelta\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository with Git\nDESCRIPTION: This command clones the Dify repository from GitHub, allowing users to access the Dify source code and related files. It's a prerequisite for local deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\n----------------------------------------\n\nTITLE: Configuring Endpoint Path and Method (YAML)\nDESCRIPTION: This YAML configuration specifies the endpoint for the Slack Bot plugin. The `path` is set to \"/\" and the `method` is set to POST, indicating that the plugin will handle POST requests at the root path.  It also specifies the python source file to be used.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npath: \"/\"\nmethod: \"POST\"\nextra:\n  python:\n    source: \"endpoints/slack.py\"\n```\n\n----------------------------------------\n\nTITLE: Connecting to Remote GitHub Repository in Bash\nDESCRIPTION: This snippet adds a remote origin to the local Git repository, pointing to a specific GitHub repository. Replace `<your-username>` and `<repository-name>` with your actual GitHub username and repository name. It requires a GitHub repository to exist.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add origin https://github.com/<your-username>/<repository-name>.git\n```\n\n----------------------------------------\n\nTITLE: Speech2Text _invoke method definition in Python\nDESCRIPTION: Defines the _invoke method for Speech2TextModel, which handles the conversion of an audio file to text using a specified model and credentials. It takes the model name, credentials, audio file (as a byte stream), and an optional user ID as input. It returns the transcribed text from the audio file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            file: IO[bytes], user: Optional[str] = None) \\\n        -> str:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param file: audio file\n    :param user: unique user id\n    :return: text for given audio file\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens (Python)\nDESCRIPTION: This Python code shows how to retrieve the number of tokens for a given prompt. If the model does not provide a native method, it defaults to returning 0, or can use `self._get_num_tokens_by_gpt2(text: str)` as an alternative approach for estimating token count.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                 tools: Optional[list[PromptMessageTool]] = None) -> int:\n  \"\"\"\n  Get number of tokens for given prompt messages\n\n  :param model: model name\n  :param credentials: model credentials\n  :param prompt_messages: prompt messages\n  :param tools: tools for tool calling\n  :return:\n  \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Updating Document via File using Dify API (curl)\nDESCRIPTION: This snippet demonstrates how to update a document in a Dify knowledge base by uploading a new file. It requires the dataset ID, document ID, and API key. The request includes form data for the file and processing rules.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/update_by_file' \\\n--header 'Authorization: Bearer {api_key}' \\\n--form 'data=\"{\"name\":\"Dify\",\"indexing_technique\":\"high_quality\",\"process_rule\":{\"rules\":{\"pre_processing_rules\":[{\"id\":\"remove_extra_spaces\",\"enabled\":true},{\"id\":\"remove_urls_emails\",\"enabled\":true}],\"segmentation\":{\"separator\":\"###\",\"max_tokens\":500}},\"mode\":\"custom\"}}\";type=text/plain' \\\n--form 'file=@\"/path/to/file\"'\n```\n\n----------------------------------------\n\nTITLE: Adding New Segments to Document using Dify API (curl)\nDESCRIPTION: This snippet shows how to add new segments to an existing document in a Dify knowledge base.  It uses the `segments` endpoint and requires the dataset ID, document ID, and API key.  The request body includes an array of segment objects with content, answer, and keywords.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"segments\": [{\"content\": \"1\",\"answer\": \"1\",\"keywords\": [\"a\"]}]}'\n```\n\n----------------------------------------\n\nTITLE: Xinference Provider Class Definition (Python)\nDESCRIPTION: This Python snippet shows how to define a minimal provider class for Xinference, which supports customizable models.  It inherits from the `Provider` base class and implements an empty `validate_provider_credentials` method to satisfy the abstract class requirements, as this provider doesn't require provider-level credentials. This prevents instantiation errors with abstract classes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/new-provider.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass XinferenceProvider(Provider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Chatflow App: Document File Processing\nDESCRIPTION: Describes the configuration for processing uploaded document files in a Chatflow app. It involves enabling the file upload feature, selecting 'document' as the file type, using a Document Extractor node to process the `sys.files` variable, and then passing the extracted content to an LLM node.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/additional-features.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n1. Enable Features and select 'document' as the file type.\n2. Select the `sys.files` variable as the input variable for the Document Extractor node.\n3. Add an LLM node and select the Document Extractor node's output variable in the system prompt.\n4. Add a 'Answer' node and fill in the output variable of the LLM node.\n```\n\n----------------------------------------\n\nTITLE: Google Provider Implementation (Python)\nDESCRIPTION: This Python code implements the Google Provider, which is responsible for validating the credentials required for the Google Search tool. It uses the `ToolProvider` class from the `dify_plugin` library and the `ToolProviderCredentialValidationError` exception to handle credential validation failures.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any\n\nfrom dify_plugin import ToolProvider\nfrom dify_plugin.errors.tool import ToolProviderCredentialValidationError\nfrom tools.google_search import GoogleSearchTool\n\nclass GoogleProvider(ToolProvider):\n    def _validate_credentials(self, credentials: dict[str, Any]) -> None:\n        try:\n            for _ in GoogleSearchTool.from_credentials(credentials).invoke(\n                tool_parameters={\"query\": \"test\", \"result_type\": \"link\"},\n            ):\n                pass\n        except Exception as e:\n            raise ToolProviderCredentialValidationError(str(e))\n```\n\n----------------------------------------\n\nTITLE: Mapping Model Invoke Errors (Python)\nDESCRIPTION: This Python code defines the `_invoke_error_mapping` property, which maps model invocation errors to unified error types. This allows Dify to handle different errors with specific follow-up actions. The mapping includes errors like InvokeConnectionError, InvokeServerUnavailableError, InvokeRateLimitError, InvokeAuthorizationError, and InvokeBadRequestError.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/predefined-model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Directory Structure for External Data Tool (Weather Search) Python\nDESCRIPTION: This code snippet shows the recommended directory structure for a custom 'Weather Search' external data tool in Dify. It includes the `__init__.py`, `weather_search.py`, and `schema.json` files, which are essential components for the tool's functionality and integration with the Dify platform. This structure is placed under `api/core/external_data_tool`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/code-based-extension/external-data-tool.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n.\\\n└── api\n    └── core\n        └── external_data_tool\n            └── weather_search\n                ├── __init__.py\n                ├── weather_search.py\n                └── schema.json\n```\n\n----------------------------------------\n\nTITLE: Setting Stop Sequences in Dify Expert Mode\nDESCRIPTION: This snippet demonstrates how to set stop sequences in Dify's expert mode to prevent LLMs from generating excessive content. The example shows how to define `Human1:` as a stop sequence to limit the LLM's response to a single turn in a few-shot scenario.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/prompt-engineering/prompt-engineering-1/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nHuman1: 天是什么颜色\n\nAssistant1: 天是蓝色的\n\nHuman1: 火是什么颜色\n\nAssistant1: 火是红色的\n\nHuman1: 土是什么颜色\n\nAssistant1: \n```\n\n----------------------------------------\n\nTITLE: Downloading LLM and Embedding Models\nDESCRIPTION: This snippet downloads pre-trained LLM and embedding models for LocalAI from Hugging Face and gpt4all.io. These models are used for local deployment and inference.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O models/bert\n$ wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j\n```\n\n----------------------------------------\n\nTITLE: Tool Invocation Example\nDESCRIPTION: This Python code snippet shows an example of how to invoke tools using the `session.tool.invoke()` method. It retrieves tool instances from the provided parameters and then calls the tool with the appropriate provider type, provider, tool name, and parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntool_instances = (\n    {tool.identity.name: tool for tool in params.tools} if params.tools else {}\n)\nfor tool_call_id, tool_call_name, tool_call_args in tool_calls:\n    tool_instance = tool_instances[tool_call_name]\n    self.session.tool.invoke(\n        provider_type=ToolProviderType.BUILT_IN,\n        provider=tool_instance.identity.provider,\n        tool_name=tool_instance.identity.name,\n        parameters={**tool_instance.runtime_parameters, **tool_call_args},\n    )\n```\n\n----------------------------------------\n\nTITLE: FFmpeg Installation - Ubuntu\nDESCRIPTION: These commands install FFmpeg on Ubuntu using the apt-get package manager.  This is required for OpenAI TTS (text-to-speech) functionality. The steps involve updating the package list and then installing the ffmpeg package.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n`sudo apt-get update`\n```\n\nLANGUAGE: shell\nCODE:\n```\n`sudo apt-get install ffmpeg`\n```\n\n----------------------------------------\n\nTITLE: Math Reasoning JSON Schema Example\nDESCRIPTION: This JSON Schema is designed for mathematical reasoning, specifying the structure for recording steps and the final answer. It includes an array of reasoning steps with explanations and outputs, along with a field for the final answer.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/how-to-use-json-schema-in-dify.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"math_reasoning\",\n    \"description\": \"Records steps and final answer for mathematical reasoning\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"steps\": {\n                \"type\": \"array\",\n                \"description\": \"Array of reasoning steps\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"explanation\": {\n                            \"type\": \"string\",\n                            \"description\": \"Explanation of the reasoning step\"\n                        },\n                        \"output\": {\n                            \"type\": \"string\",\n                            \"description\": \"Output of the reasoning step\"\n                        }\n                    },\n                    \"required\": [\"explanation\", \"output\"],\n                    \"additionalProperties\": false\n                }\n            },\n            \"final_answer\": {\n                \"type\": \"string\",\n                \"description\": \"The final answer to the mathematical problem\"\n            }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\"steps\", \"final_answer\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Granting Execution Permissions to Dify Plugin Tool (macOS)\nDESCRIPTION: This command grants execution permissions to the downloaded Dify plugin scaffolding tool for macOS. It is a necessary step to make the tool executable from the command line.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/package-plugin-file-and-publish.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nchmod +x dify-plugin-darwin-arm64\n```\n\n----------------------------------------\n\nTITLE: Check Go Version\nDESCRIPTION: This command verifies the installed Go version.  It helps ensure that the correct Go version is installed before proceeding with the DifySandbox installation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/backend/sandbox/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngo version  # インストールされたGoバージョンを表示\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies (Bash)\nDESCRIPTION: Installs the required Python dependencies using Poetry after activating the environment with `poetry env use 3.12`. Poetry manages the project's dependencies.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\npoetry env use 3.12\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Validate Provider Credentials Python\nDESCRIPTION: This Python code snippet defines a method for validating provider credentials. It takes a dictionary of credentials as input and raises an exception if the validation fails. This method is part of the ModelProvider base class and needs to be implemented by specific provider classes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: ImagePromptMessageContent Class Definition in Python\nDESCRIPTION: Defines the ImagePromptMessageContent class, which inherits from PromptMessageContent. It includes the content type as IMAGE and provides a DETAIL enum (LOW, HIGH) for specifying image resolution. The data field should contain either a URL or a base64 encoded string of the image.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclass ImagePromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for image prompt message content.\n    \"\"\"\n    class DETAIL(Enum):\n        LOW = 'low'\n        HIGH = 'high'\n\n    type: PromptMessageContentType = PromptMessageContentType.IMAGE\n    detail: DETAIL = DETAIL.LOW  # 解像度\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Debugging (.env)\nDESCRIPTION: This shows the environment variables used to configure remote debugging for the plugin.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: Embedding Usage Class Definition\nDESCRIPTION: This code snippet defines the EmbeddingUsage class, which extends ModelUsage, representing the usage information for a text embedding operation. It includes details on token counts, prices, and latency.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nclass EmbeddingUsage(ModelUsage):\n    \"\"\"\n    Model class for embedding usage.\n    \"\"\"\n    tokens: int  # 使用トークン数\n    total_tokens: int  # 総使用トークン数\n    unit_price: Decimal  # 単価\n    price_unit: Decimal  # 価格単位（単価が適用されるトークン数）\n    total_price: Decimal  # 総料金\n    currency: str  # 通貨単位\n    latency: float  # リクエスト処理時間（秒）\n```\n\n----------------------------------------\n\nTITLE: Verifying Ollama Installation (Bash)\nDESCRIPTION: This command checks the installed version of Ollama to ensure it's properly installed and functional. It's a basic verification step after the initial Ollama installation. The expected output is the Ollama version number.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n➜  ~ ollama -v\nollama version is 0.5.5\n```\n\n----------------------------------------\n\nTITLE: System Prompt with Constraints\nDESCRIPTION: This structured prompt divides instructions for the Agent, specifying the task of generating images using stability_text2image in an anime style and adding a constraint to reject unrelated requests. This helps prevent the model from hallucinating or engaging in irrelevant conversations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/workshop/basic/build-ai-image-generation-app.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n## 任务\n根据用户的提示，使用工具 stability_text2image 绘画指定内容，画面是二次元风格。\n\n## 约束\n如果用户在请求和绘画无关的内容，回复：“对不起，我不明白你在说什么”\n```\n\n----------------------------------------\n\nTITLE: Output Moderation Request Body Example (JSON)\nDESCRIPTION: Shows the expected JSON structure for the HTTP POST request body of the `app.moderation.output` extension point. The parameters include the `point` (fixed value), `app_id`, and `text` (the LLM's response, potentially chunked if streaming). Used when content moderation for outputs is enabled.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/moderation.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"point\": \"app.moderation.output\",\n    \"params\": {\n        \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n        \"text\": \"I will kill you.\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Getting Customizable Model Schema (Python)\nDESCRIPTION: This code snippet demonstrates how to retrieve a customizable model schema for a Large Language Model (LLM) in Dify. This is an optional method that allows suppliers to support custom LLMs by providing a way to retrieve model rules. It returns an `AIModelEntity` or None if customization is not supported.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> Optional[AIModelEntity]:\n    \"\"\"\n    Get customizable model schema\n\n    :param model: model name\n    :param credentials: model credentials\n    :return: model schema\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Text-to-Speech Model Invocation\nDESCRIPTION: This code snippet defines the interface for invoking a text-to-speech model. It takes a model name, credentials, text content, streaming option, and user ID as input, and returns a translated audio file. The credentials parameter is defined within vendor YAML settings.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, content_text: str, streaming: bool, user: Optional[str] = None):\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param content_text: text content to be translated\n    :param streaming: output is streaming\n    :param user: unique user id\n    :return: translated audio file\n    \"\"\"        \n```\n\n----------------------------------------\n\nTITLE: Creating Document via Text using Dify API (curl)\nDESCRIPTION: This snippet shows how to create a new document in a Dify knowledge base by providing the text content directly. It requires the dataset ID and an API key for authorization. The request includes the document name, text, indexing technique, and processing rule.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/document/create-by-text' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"name\": \"text\",\"text\": \"text\",\"indexing_technique\": \"high_quality\",\"process_rule\": {\"mode\": \"automatic\"}}'\n```\n\n----------------------------------------\n\nTITLE: Add Knowledge Base Metadata Field - Dify API - Bash\nDESCRIPTION: This snippet shows how to add a metadata field to a knowledge base in Dify using a POST request. It requires the dataset ID. The request includes headers for authorization (API key) and content type (application/json), along with the metadata definition (type and name) in JSON format.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer {api_key}' \\\n--data '{\n    \"type\":\"string\",\n    \"name\":\"test\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Workflow App File Upload with sys.files variable\nDESCRIPTION: Enables image upload functionality in Workflow apps. When enabled, users can upload images, which are stored in the `sys.files` variable. A LLM node with VISION capability enabled can then access the image data, and the output is passed to an END node.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/additional-features.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nUsers upload images which are stored in `sys.files` variable.\nAdd LLM node, select a large model with vision capabilities, enable VISION and select the `sys.files` variable.\nSelect the output variable of the LLM node in the END node.\n```\n\n----------------------------------------\n\nTITLE: Customizing Button Styles using containerProps (CSS Classes)\nDESCRIPTION: This JavaScript code snippet shows how to customize the Dify Chatbot Bubble Button's style by applying CSS classes using the `containerProps` option. It sets the `className` attribute to include custom CSS classes, allowing for more complex styling through external stylesheets.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-publishing/embedding-in-websites.md#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.difyChatbotConfig = {\n    // ... other configurations\n    containerProps: {\n        className: 'dify-chatbot-bubble-button-custom my-custom-class',\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Provider Credentials Validation (Python)\nDESCRIPTION: This Python snippet demonstrates the structure of the `validate_provider_credentials` method, used for validating the provider's API key. This method is vital for pre-defined model providers, as it ensures that the credentials supplied by the user are correct.  It is inherited from `__base.model_provider.ModelProvider`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/new-provider.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking the LLM (Python)\nDESCRIPTION: This Python code shows the method definition for invoking an LLM using the `invoke` method. The method takes `model_config`, `prompt_messages`, optional `tools` and `stop` parameters, and a `stream` flag. It returns a generator of `LLMResultChunk` or `LLMResult` depending on the streaming setting.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n        self,\n        model_config: LLMModelConfig,\n        prompt_messages: list[PromptMessage],\n        tools: list[PromptMessageTool] | None = None,\n        stop: list[str] | None = None,\n        stream: bool = True,\n    ) -> Generator[LLMResultChunk, None, None] | LLMResult:...\n```\n\n----------------------------------------\n\nTITLE: Dify Backend Directory Structure\nDESCRIPTION: This shows the directory structure of the Dify backend, outlining key components such as controllers, core, models, and services.  It uses Python and Flask framework.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_13\n\nLANGUAGE: Text\nCODE:\n```\n[api/]\n├── constants             // Constant settings used throughout code base.\n├── controllers           // API route definitions and request handling logic.\n├── core                  // Core application orchestration, model integrations, and tools.\n├── docker                // Docker & containerization related configurations.\n├── events                // Event handling and processing\n├── extensions            // Extensions with 3rd party frameworks/platforms.\n├── fields                //field definitions for serialization/marshalling.\n├── libs                  // Reusable libraries and helpers.\n├── migrations            // Scripts for database migration.\n├── models                // Database models & schema definitions.\n├── services              // Specifies business logic.\n├── storage               // Private key storage.\n├── tasks                 // Handling of async tasks and background jobs.\n└── tests\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Example - AI Generated\nDESCRIPTION: This example demonstrates how to generate a JSON schema with AI, showcasing nested structures like arrays. The schema defines a user profile with a username (string), age (number), and interests (array of strings), all marked as required fields.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/llm.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"username\": {\n      \"type\": \"string\"\n    },\n    \"age\": {\n      \"type\": \"number\"\n    },\n    \"interests\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      }\n    }\n  },\n  \"required\": [\n    \"username\",\n    \"age\",\n    \"interests\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Reloading Systemd and Restarting Ollama (Linux)\nDESCRIPTION: These commands reload the systemd daemon and restart the Ollama service after modifying the service configuration file. This ensures that the changes to the environment variables take effect.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_82\n\nLANGUAGE: Shell\nCODE:\n```\nsystemctl daemon-reload\nsystemctl restart ollama\n```\n\n----------------------------------------\n\nTITLE: Xinference Provider Class\nDESCRIPTION: This Python code defines a placeholder `XinferenceProvider` class for custom model providers. It inherits from the `Provider` base class and implements an empty `validate_provider_credentials` method.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass XinferenceProvider(Provider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Stopping Apache Service\nDESCRIPTION: This command is used to stop the Apache web server.  It uses `service` to manage the Apache2 service. It requires root privileges or being a member of the appropriate group to execute.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nservice apache2 stop\n```\n\n----------------------------------------\n\nTITLE: Set Ollama Host Environment Variable on Mac\nDESCRIPTION: This command sets the `OLLAMA_HOST` environment variable on macOS using `launchctl`. This is necessary when running Ollama as a macOS application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_79\n\nLANGUAGE: Shell\nCODE:\n```\nlaunchctl setenv OLLAMA_HOST \"0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Run Dify on WeChat Application\nDESCRIPTION: Executes the Dify on WeChat application using Python. This command starts the application, which then listens for messages on WeChat and interacts with the Dify API to generate responses. The specific command might vary based on the operating system.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd dify-on-wechat\npython3 app.py   # windows 环境下该命令通常为 python app.py\n```\n\n----------------------------------------\n\nTITLE: Define ImagePromptMessageContent Class in Python\nDESCRIPTION: This code defines the `ImagePromptMessageContent` class, inheriting from `PromptMessageContent`. It includes a nested `DETAIL` enum to specify the image resolution (LOW or HIGH). It sets the `type` to `PromptMessageContentType.IMAGE` and includes a `detail` attribute with a default value of `DETAIL.LOW`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nclass ImagePromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for image prompt message content.\n    \"\"\"\n    class DETAIL(Enum):\n        LOW = 'low'\n        HIGH = 'high'\n\n    type: PromptMessageContentType = PromptMessageContentType.IMAGE\n    detail: DETAIL = DETAIL.LOW  # Resolution\n```\n\n----------------------------------------\n\nTITLE: Database Upgrade with Flask\nDESCRIPTION: Upgrades the database structure for a Dify deployment running from source code.  This is required after pulling the latest code to ensure the database schema is compatible with the updated application. Executed from the `api` directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nflask db upgrade\n```\n\n----------------------------------------\n\nTITLE: CSP Whitelist Example URLs\nDESCRIPTION: This code snippet provides a list of example URLs that are automatically included in the Content Security Policy (CSP) whitelist when CSP is enabled. These URLs are commonly used by Dify for services such as error reporting, analytics, and API access. No manual changes are needed unless additional domains need to be whitelisted.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/environments.md#_snippet_2\n\nLANGUAGE: url\nCODE:\n```\n*.sentry.io http://localhost:* http://127.0.0.1:* https://analytics.google.com https://googletagmanager.com https://api.github.com\n```\n\n----------------------------------------\n\nTITLE: Uploading Private Key File to Cloud Storage (Flask Command)\nDESCRIPTION: Uploads the private key file to cloud storage.  Requires cloud storage configuration (e.g., aliyun-oss) to be set up.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nflask upload-private-key-file-to-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Delete Knowledge Base - Dify API (JSON Response)\nDESCRIPTION: This is the JSON response received after successfully deleting a knowledge base via the Dify API. A `204 No Content` status code indicates successful deletion.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n204 No Content\n```\n\n----------------------------------------\n\nTITLE: Moderation Model Invocation\nDESCRIPTION: This code snippet defines the interface for invoking a moderation model. It takes a model name, credentials, text content, and user ID as input, and returns a boolean value indicating whether the text is safe or not. Credentials parameter is based on the provider or model credential schema.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            text: str, user: Optional[str] = None) \\\n        -> bool:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param text: text to moderate\n    :param user: unique user id\n    :return: false if text is safe, true otherwise\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Model Types in YAML\nDESCRIPTION: Defines the available model types for the Xinference provider (text-generation, embeddings, reranking) within the `provider_credential_schema` using a select input.  Includes labels in both English and Chinese.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprovider_credential_schema:\n  credential_form_schemas:\n  - variable: model_type\n    type: select\n    label:\n      en_US: Model type\n      zh_Hans: 模型类型 (モデルタイプ)\n    required: true\n    options:\n<strong>    - value: text-generation\n</strong>      label:\n        en_US: Language Model\n        zh_Hans: 语言模型 (言語モデル)\n<strong>    - value: embeddings\n</strong>      label:\n        en_US: Text Embedding\n<strong>    - value: reranking\n</strong>      label:\n        en_US: Rerank\n```\n\n----------------------------------------\n\nTITLE: Define Server URL Credential Schema (YAML)\nDESCRIPTION: This YAML code defines the schema for specifying the Xinference server URL, including labels, a text-input type, a requirement status, and placeholders for both English and Chinese.  The `server_url` variable is crucial for Dify to connect to the Xinference deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/customizable-model.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n  - variable: server_url\n    label:\n      zh_Hans: 服务器URL\n      en_US: Server url\n    type: text-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入Xinference的服务器地址，如 https://example.com/xxx\n      en_US: Enter the url of your Xinference, for example https://example.com/xxx\n```\n\n----------------------------------------\n\nTITLE: Example API Response for Weather Query\nDESCRIPTION: This JSON payload illustrates an example response containing weather information for London. The 'result' field provides a formatted string including temperature, RealFeel, air quality, wind conditions, and precipitation details. This information would be used to provide context to the LLM.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/external-data-tool.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"result\": \"City: London\\nTemperature: 10°C\\nRealFeel®: 8°C\\nAir Quality: Poor\\nWind Direction: ENE\\nWind Speed: 8 km/h\\nWind Gusts: 14 km/h\\nPrecipitation: Light rain\"\n}\n```\n\n----------------------------------------\n\nTITLE: Initialize External Data Tool Directory Structure - Python\nDESCRIPTION: Defines the directory structure for a custom external data tool in Dify.  This involves creating a main directory under `api/core/external_data_tool`, and files for initialization (`__init__.py`), the main tool logic (`weather_search.py`), and the frontend schema (`schema.json`). This setup is crucial for Dify to recognize and utilize the new tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/code-based-extension/external-data-tool.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.\\\n└── api\n    └── core\n        └── external_data_tool\n            └── weather_search\n                ├── __init__.py\n                ├── weather_search.py\n                └── schema.json\n```\n\n----------------------------------------\n\nTITLE: Accessing llms.txt via API with curl\nDESCRIPTION: This code snippet demonstrates how to access the `llms.txt` file using a `curl` command. It sends an HTTP GET request to the specified URL, including an API key as a query parameter. The API key is required for authentication and authorization.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET 'http://llmstxt.firecrawl.dev/https://docs.dify.ai/?FIRECRAWL_API_KEY=YOUR_API_KEY'\n```\n\n----------------------------------------\n\nTITLE: API Request Example\nDESCRIPTION: This example shows the structure of a POST request that Dify sends to the API endpoint when the external data tool is queried. It contains the 'point', which identifies the extension point, and 'params', which include information such as app_id, tool_variable, inputs, and the query itself.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/README.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"point\": \"app.external_data_tool.query\",\n    \"params\": {\n        \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n        \"tool_variable\": \"weather_retrieve\",\n        \"inputs\": {\n            \"location\": \"London\"\n        },\n        \"query\": \"How's the weather today?\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Check for Tool Calls in LLM Result Chunk in Python\nDESCRIPTION: The `check_tool_calls` function verifies if a given `LLMResultChunk` contains any tool call information.  It specifically checks if the `tool_calls` attribute within the message delta of the chunk is present and not empty, returning a boolean indicating the presence of tool calls.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n    def check_tool_calls(self, llm_result_chunk: LLMResultChunk) -> bool:\n        \"\"\"\n        Check if there is any tool call in llm result chunk\n        \"\"\"\n        return bool(llm_result_chunk.delta.message.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response: Rate Limit Reached\nDESCRIPTION: This snippet shows an error response indicating that the OpenAI API rate limit has been reached for the default-gpt-3.5-turbo model.  It suggests waiting 20 seconds or adding a payment method to increase the rate limit.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/llms-use-faq.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"Rate limit reached for default-gpt-3.5-turboin organization org-wDrZCxxxxxxxxxissoZb on requestsper min。 Limit: 3 / min. Please try again in 20s. Contact us through our help center   at help.openai.com   if you continue to haveissues. Please add a payment method toyour account to increase your rate limit.Visit https://platform.openai.com/account/billingto add a payment method.\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for LLM Prompt in Python\nDESCRIPTION: This method gets the number of tokens for given prompt messages. It takes the model name, credentials, prompt messages, and tools as input. If the model does not provide a pre-calculated tokens interface, it can directly return 0.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                     tools: Optional[list[PromptMessageTool]] = None) -> int:\n      \"\"\"\n      Get number of tokens for given prompt messages\n\n      :param model: model name\n      :param credentials: model credentials\n      :param prompt_messages: prompt messages\n      :param tools: tools for tool calling\n      :return:\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama as Systemd Service (Linux)\nDESCRIPTION: These commands are used to edit the Ollama systemd service file and reload the systemd daemon to apply the changes. The `Environment` line within the `[Service]` section sets the OLLAMA_HOST environment variable, allowing Ollama to be accessed from other containers or devices on the network.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Simple Prompt for Image Generation\nDESCRIPTION: This code snippet is a basic prompt instructing the AI agent to generate an image based on the user's input using the stability_text2image tool. It serves as a fundamental instruction for the agent to interpret user requests and utilize the specified tool for image creation. The agent expects a user prompt describing the desired image.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/workshop/basic/build-ai-image-generation-app.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nユーザーのプロンプトに従って、指定された内容を stability_text2image を使用して描画してください。\n```\n\n----------------------------------------\n\nTITLE: Cloudflare Workers Route for Dify Tools\nDESCRIPTION: This route can be imported into Dify to provide an OpenAPI-compatible interface documentation. It is part of the dify-tools-worker project and helps to quickly deploy custom tools on Cloudflare Workers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_2\n\nLANGUAGE: URL\nCODE:\n```\nhttps://difytoolsworker.yourname.workers.dev/doc\n```\n\n----------------------------------------\n\nTITLE: Installing system dependencies on CentOS\nDESCRIPTION: Installs required system components on CentOS Linux distributions for DifySandbox, including package configuration tools, a C compiler, and necessary libraries for security and version control.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/backend/sandbox/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum install pkgconfig gcc libseccomp-devel git wget\n```\n\n----------------------------------------\n\nTITLE: Example Response from Stable Diffusion API (sd-models)\nDESCRIPTION: Example JSON response from the `/sdapi/v1/sd-models` endpoint, containing the model name and other metadata. The `model_name` field is required for integration with Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/stable-diffusion.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"title\": \"pastel-mix/pastelmix-better-vae-fp32.ckpt [943a810f75]\",\n        \"model_name\": \"pastel-mix_pastelmix-better-vae-fp32\",\n        \"hash\": \"943a810f75\",\n        \"sha256\": \"943a810f7538b32f9d81dc5adea3792c07219964c8a8734565931fcec90d762d\",\n        \"filename\": \"/home/takatost/stable-diffusion-webui/models/Stable-diffusion/pastel-mix/pastelmix-better-vae-fp32.ckpt\",\n        \"config\": null\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Starting Xinference\nDESCRIPTION: This command starts the Xinference service, making it accessible on the network by specifying the host. It starts a worker on the default endpoint (http://127.0.0.1:9997).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/practical-implementation-of-building-llm-applications-using-a-full-set-of-open-source-tools.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\n$ xinference -H 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for OpenAI Models\nDESCRIPTION: This bash snippet shows the directory structure for OpenAI models, including different types of models like llm, text_embedding, moderation, speech2text, and tts. Each model type has its dedicated directory with specific configuration and code files, indicating a well-organized model integration setup.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n├── models\n│   ├── common_openai.py\n│   ├── llm\n│   │   ├── _position.yaml\n│   │   ├── chatgpt-4o-latest.yaml\n│   │   ├── gpt-3.5-turbo.yaml\n│   │   ├── gpt-4-0125-preview.yaml\n│   │   ├── gpt-4-turbo.yaml\n│   │   ├── gpt-4o.yaml\n│   │   ├── llm.py\n│   │   ├── o1-preview.yaml\n│   │   └── text-davinci-003.yaml\n│   ├── moderation\n│   │   ├── moderation.py\n│   │   └── text-moderation-stable.yaml\n│   ├── speech2text\n│   │   ├── speech2text.py\n│   │   └── whisper-1.yaml\n│   ├── text_embedding\n│   │   ├── text-embedding-3-large.yaml\n│   │   └── text_embedding.py\n│   └── tts\n│       ├── tts-1-hd.yaml\n│       ├── tts-1.yaml\n│       └── tts.py\n```\n\n----------------------------------------\n\nTITLE: Sample Travel Assistant Output\nDESCRIPTION: Provides example output of the travel assistant application, including hotel recommendations, daily itineraries, and additional services. This sample demonstrates the desired format and content the AI agent should generate.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/workshop/basic/travel-assistant.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n## サンプル\n\n### 詳細な旅行計画\n\n**ホテルのおすすめ**\n1. ケンジントンホテル（詳細はこちら：www.doylecollection.com/hotels/the-kensington-hotel）\n   - 評価：4.6⭐\n   - 価格：1泊約350ドル\n   - 紹介：摂政時代の連棟住宅に位置し、南ケンジントン駅まで徒歩5分、ヴィクトリア＆アルバート博物館まで徒歩10分の優雅なホテルです。\n\n2. レンブラントホテル（詳細はこちら：www.sarova-rembrandthotel.com）\n   - 評価：4.3⭐\n   - 価格：1泊約130ドル\n   - 紹介：1911年に建てられた元ハロッズ百貨店のアパートで、現代的なホテルはヴィクトリア＆アルバート博物館の向かいにあり、南ケンジントン駅（ヒースロー空港行き直通）まで徒歩5分です。\n\n**1日目 - 到着と落ち着き**\n- **午前**：空港に到着。冒険の旅の始まりです！弊社の代表が空港でお迎えし、スムーズなチェックインをサポートいたします。\n- **午後**：ホテルにチェックインし、少し休んでエネルギーを回復します。\n- **夜**：周辺の宿泊施設を散策し、地元の雰囲気に浸ってください。近くの飲食店を見つけて、初めての楽しい夕食をお楽しみください。\n\n**2日目 - 文化と自然の旅**\n- **午前**：世界でもトップクラスの学府である帝国カレッジから1日をスタートしましょう。キャンパスツアーをお楽しみください。\n- **午後**：自然史博物館（その魅力的な展示で知られています）またはヴィクトリア＆アルバート博物館（芸術とデザインを称える）を訪れる選択肢があります。その後、静かなハイドパークでリラックスし、蛇形湖でボートに乗ることもできます。\n- **夜**：地元の料理を探索しましょう。伝統的なイギリスのパブで夕食をお召し上がりいただくことをお勧めします。\n\n**追加サービス：**\n- **コンシェルジュサービス**：滞在中に、レストランの予約、チケット購入、交通手配、特別なリクエストへの対応を行うコンシェルジュサービスをご利用いただけます。\n- **24時間サポート**：旅行中に発生する可能性のある問題や要望に対応するため、24時間対応のサポートを提供しています。\n\n楽しい旅を！豊かな経験と素晴らしい思い出でいっぱいになりますように！\n```\n\n----------------------------------------\n\nTITLE: Installing API Dependencies with Poetry\nDESCRIPTION: Installs the API service dependencies using Poetry, a Python dependency management tool. This ensures that all required packages are installed in the correct versions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\npoetry env use 3.12\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Starting the Web Service\nDESCRIPTION: Starts the web service using npm, yarn, or pnpm.  This command launches the web application, making it accessible via a web browser.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_16\n\nLANGUAGE: Bash\nCODE:\n```\nnpm run start\n# or\nyarn start\n# or\npnpm start\n```\n\n----------------------------------------\n\nTITLE: Direct Error Mapping (Python)\nDESCRIPTION: This code snippet shows an alternative approach to mapping invocation errors by directly throwing the corresponding InvokeError exceptions.  This can simplify the error mapping process by removing the need for explicit mapping in some cases.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    return {\n        InvokeConnectionError: [\n          InvokeConnectionError\n        ],\n        InvokeServerUnavailableError: [\n          InvokeServerUnavailableError\n        ],\n        InvokeRateLimitError: [\n          InvokeRateLimitError\n        ],\n        InvokeAuthorizationError: [\n          InvokeAuthorizationError\n        ],\n        InvokeBadRequestError: [\n          InvokeBadRequestError\n        ],\n    }\n```\n\n----------------------------------------\n\nTITLE: Streaming and Synchronous Response Handling - YAML\nDESCRIPTION: These functions handle streaming and synchronous responses from the LLM.  `_invoke` determines whether to stream the response or handle it synchronously. `_handle_stream_response` yields chunks from the response, while `_handle_sync_response` returns the full LLMResult.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndef _invoke(self, stream: bool, **kwargs) -> Union[LLMResult, Generator]:\n    if stream:\n        return self._handle_stream_response(**kwargs)\n    return self._handle_sync_response(**kwargs)\n\ndef _handle_stream_response(self, **kwargs) -> Generator:\n    for chunk in response:\n        yield chunk\n\ndef _handle_sync_response(self, **kwargs) -> LLMResult:\n    return LLMResult(**response)\n```\n\n----------------------------------------\n\nTITLE: SearXNG settings.yml Configuration\nDESCRIPTION: This YAML configuration file sets SearXNG server settings, specifically binding the server to all IP addresses (0.0.0.0) on port 8080 and enabling JSON output format.  This allows the server to be accessible externally and to return search results in JSON format, which is useful for APIs.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  bind_address: \"0.0.0.0\"  # 允许外部访问\n  port: 8080\n\nsearch:\n  formats:\n    - html\n    - json\n    - csv\n    - rss\n```\n\n----------------------------------------\n\nTITLE: Extract Tool Calls from LLM Result\nDESCRIPTION: Extracts tool call information from an LLM result chunk. The information extracted includes the tool call ID, tool call name, and tool call arguments. This function parses the arguments from JSON format.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n    def extract_tool_calls(\n        self, llm_result_chunk: LLMResultChunk\n    ) -> list[tuple[str, str, dict[str, Any]]]:\n        \"\"\"\n        Extract tool calls from llm result chunk\n\n        Returns:\n            List[Tuple[str, str, Dict[str, Any]]]: [(tool_call_id, tool_call_name, tool_call_args)]\n        \"\"\"\n        tool_calls = []\n        for prompt_message in llm_result_chunk.delta.message.tool_calls:\n            args = {}\n            if prompt_message.function.arguments != \"\":\n                args = json.loads(prompt_message.function.arguments)\n\n            tool_calls.append(\n                (\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Agent Parameters (Python)\nDESCRIPTION: This Python code shows how to access the agent parameters within the `BasicAgentAgentStrategy` class. The `_invoke` method receives a dictionary of parameters, which are then unpacked into a `BasicParams` object for easier access. The AgentStrategy class serves as the base class for strategy implementations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n```\n\n----------------------------------------\n\nTITLE: Resetting Encryption Key Pair - Docker Compose - Flask\nDESCRIPTION: This command resets the encryption key pair used for encrypting large model keys in Dify when deployed with Docker Compose.  It runs the `flask reset-encrypt-key-pair` command inside the `docker-api-1` container.  This is used to recover from 'File not found' errors related to missing encryption keys.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-encrypt-key-pair\n```\n\n----------------------------------------\n\nTITLE: JSON文字列をオブジェクトに変換 (Python)\nDESCRIPTION: 入力されたJSON文字列を解析し、オブジェクトに変換します。入力文字列には、'memory'キーが含まれている必要があります。オブジェクトから 'facts', 'preferences', 'memories' を抽出し、辞書形式で返します。エラー処理を含み、不正なJSON文字列が入力された場合、または例外が発生した場合は、エラーメッセージを返します。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/variable-assigner.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\ndef main(arg1: str) -> object:\n    try:\n        # Parse the input JSON string\n        input_data = json.loads(arg1)\n        \n        # Extract the memory object\n        memory = input_data.get(\"memory\", {})\n        \n        # Construct the return object\n        result = {\n            \"facts\": memory.get(\"facts\", []),\n            \"preferences\": memory.get(\"preferences\", []),\n            \"memories\": memory.get(\"memories\", [])\n        }\n        \n        return {\n            \"mem\": result\n        }\n    except json.JSONDecodeError:\n        return {\n            \"result\": \"Error: Invalid JSON string\"\n        }\n    except Exception as e:\n        return {\n            \"result\": f\"Error: {str(e)}\"\n        }\n\n```\n\n----------------------------------------\n\nTITLE: Moderation Invoke Interface\nDESCRIPTION: Defines the `_invoke` interface for moderation models, which evaluates text content for safety. It accepts the model name, credentials, text to moderate, and user ID as input, returning a boolean indicating whether the text is safe (False) or not (True).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            text: str, user: Optional[str] = None) \\\n        -> bool:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param text: text to moderate\n    :param user: unique user id\n    :return: false if text is safe, true otherwise\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining LLMResultChunkDelta Model (Python)\nDESCRIPTION: Defines the `LLMResultChunkDelta` model, representing a delta in a streaming LLM result.  It contains the `index`, the `message` delta as an `AssistantPromptMessage`, optional `usage` information, and an optional `finish_reason` that appears only on the last chunk.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunkDelta(BaseModel):\n    \"\"\"\n    Model class for llm result chunk delta.\n    \"\"\"\n    index: int\n    message: AssistantPromptMessage  # response message\n    usage: Optional[LLMUsage] = None  # usage info\n    finish_reason: Optional[str] = None  # finish reason, only the last one returns\n```\n\n----------------------------------------\n\nTITLE: Update Document by Text with Dify API\nDESCRIPTION: This snippet shows how to update an existing document's content in a Dify knowledge base using text via the API. It requires the `dataset_id` and `document_id` of the document to be updated, along with a valid API key. The request includes the updated document name and text content.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/update-by-text' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"name\": \"name\",\"text\": \"text\"}'\n```\n\n----------------------------------------\n\nTITLE: Starting PostgreSQL/Redis/Weaviate with Docker Compose\nDESCRIPTION: Deploys PostgreSQL, Redis, and Weaviate (if not already present locally) using Docker Compose.  First it changes the directory to the docker folder, then copies the middleware environment example file and starts the services in detached mode based on the middleware configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ncd docker\ncp middleware.env.example middleware.env\ndocker compose -f docker-compose.middleware.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Unsafe File Access (Python)\nDESCRIPTION: This is an example of a dangerous Python code snippet that attempts to read the contents of the `/etc/passwd` file. This is highly discouraged due to security risks. The snippet highlights the importance of security measures to prevent unauthorized file access within the code node environment. The `open` function can be exploited if file access is not restricted.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/code.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef main() -> dict:\n    return {\n        \"result\": open(\"/etc/passwd\").read(),\n    }\n```\n\n----------------------------------------\n\nTITLE: Start Xinference Locally\nDESCRIPTION: This command starts the Xinference server locally, making it accessible for model inference. The server endpoint is displayed upon successful startup, typically on localhost port 9997.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ xinference-local\n2023-08-20 19:21:05,265 xinference   10148 INFO     Xinference successfully started. Endpoint: http://127.0.0.1:9997\n2023-08-20 19:21:05,266 xinference.core.supervisor 10148 INFO     Worker 127.0.0.1:37822 has been added successfully\n2023-08-20 19:21:05,267 xinference.deploy.worker 10148 INFO     Xinference worker successfully started.\n```\n\n----------------------------------------\n\nTITLE: Starting API Server (Bash)\nDESCRIPTION: Starts the Flask API server, binding it to all addresses (0.0.0.0) on port 5001 in debug mode. This allows the API to be accessed from other machines.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\nflask run --host 0.0.0.0 --port=5001 --debug\n```\n\n----------------------------------------\n\nTITLE: Enabling History Messages (YAML)\nDESCRIPTION: This YAML code shows the configuration needed to enable history messages for the Agent Plugin, by adding 'history-messages' in the features section of `strategies/agent.yaml`. This ensures the model can maintain context across multiple interactions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: basic_agent  # Agent戦略名\n  author: novice     # 作者\n  label:\n    en_US: BasicAgent  # 英語ラベル\ndescription:\n  en_US: BasicAgent    # 英語説明\nfeatures:\n  - history-messages   # 履歴メッセージ機能を有効化\n...\n```\n\n----------------------------------------\n\nTITLE: Poetry Use Python Version\nDESCRIPTION: Configures poetry to use the specified python version (3.12). This ensures that the correct python interpreter is used for dependency installation and script execution.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_41\n\nLANGUAGE: Bash\nCODE:\n```\npoetry env use 3.12\n```\n\n----------------------------------------\n\nTITLE: Navigating to Dify Docker Directory\nDESCRIPTION: This command changes the current directory to the `docker` directory within the cloned Dify source code. This is necessary to execute subsequent Docker Compose commands.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd dify/docker\n```\n\n----------------------------------------\n\nTITLE: LLM Prompt for JSON Generation\nDESCRIPTION: This prompt instructs the LLM to output code in JSON format, specifying that the LLM should act as a teaching assistant providing either correct or incorrect sample code based on user requirements.  The prompt is intended for use within an LLM node in a Dify workflow application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_21\n\nLANGUAGE: text\nCODE:\n```\nYou are a teaching assistant. According to the user's requirements, you only output a correct or incorrect sample code in json format.\n```\n\n----------------------------------------\n\nTITLE: Text-to-Speech Invoke Interface\nDESCRIPTION: Defines the `_invoke` interface for text-to-speech models, which translates text content into an audio stream. It takes the model name, credentials, text content, streaming flag, and user ID as input, returning the translated audio file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, content_text: str, streaming: bool, user: Optional[str] = None):\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param content_text: text content to be translated\n    :param streaming: output is streaming\n    :param user: unique user id\n    :return: translated audio file\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Concatenating Lists (Python)\nDESCRIPTION: This Python code snippet concatenates two lists, `knowledge1` and `knowledge2`. It takes both lists as input and returns a dictionary containing a 'result' key, whose value is the concatenation of the two input lists. It assumes both inputs are valid lists of same type.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/code.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef main(knowledge1: list, knowledge2: list) -> list:\n    return {\n        # 注意在输出变量中声明result\n        'result': knowledge1 + knowledge2\n    }\n```\n\n----------------------------------------\n\nTITLE: Setting Authorization Header for API Request\nDESCRIPTION: This code snippet demonstrates how to set the `Authorization` header in an HTTP request to authenticate with the Dify external knowledge base API. It uses the `Bearer` scheme with an API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/external-knowledge-api-documentation.md#_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nAuthorization: Bearer {API_KEY}\n```\n\n----------------------------------------\n\nTITLE: Modify Nginx Ports in Docker Compose\nDESCRIPTION: Specifies the ports that Nginx will expose for HTTP and HTTPS traffic. These values should be set in the `.env` file used by Docker Compose.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/faqs.md#_snippet_3\n\nLANGUAGE: docker\nCODE:\n```\nEXPOSE_NGINX_PORT=80\nEXPOSE_NGINX_SSL_PORT=443\n```\n\n----------------------------------------\n\nTITLE: Converting Array to Text using Python Code Node\nDESCRIPTION: This code snippet demonstrates how to convert an array of strings into a single string, separated by newline characters, using a Python code node. It takes a list called `articleSections` as input and returns a dictionary containing the joined string under the key `result`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/iteration.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef main(articleSections: list):\n    data = articleSections\n    return {\n        \"result\": \"\\n\".join(data)\n    }\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository (Bash)\nDESCRIPTION: Clones the Dify repository from GitHub. This is the first step in setting up Dify from local source code.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\n----------------------------------------\n\nTITLE: Defining RerankDocument Model in Python\nDESCRIPTION: Defines the `RerankDocument` model, representing a reranked document. It includes the original index, the text of the document, and a score.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nclass RerankDocument(BaseModel):\n    \"\"\"\n    Model class for rerank document.\n    \"\"\"\n    index: int  # original index\n    text: str\n    score: float\n```\n\n----------------------------------------\n\nTITLE: Configure Notion Public Integration in .env File\nDESCRIPTION: This code snippet demonstrates the configuration required in the .env file for Notion integration using the 'public' integration type. It defines the integration type, client secret, and client ID. These settings are necessary for Dify to properly authenticate with Notion using a public integration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/create-knowledge-and-upload-documents/import-content-data/sync-from-notion.md#_snippet_1\n\nLANGUAGE: env\nCODE:\n```\nNOTION_INTEGRATION_TYPE=public\nNOTION_CLIENT_SECRET=you-client-secret\nNOTION_CLIENT_ID=you-client-id\n```\n\n----------------------------------------\n\nTITLE: Implementing an Endpoint in Python\nDESCRIPTION: Demonstrates how to implement an endpoint in Python by creating a subclass of `dify_plugin.Endpoint` and implementing the `_invoke` method. It shows how to access request parameters, settings, and return a werkzeug Response object, which supports streaming.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/endpoint.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom typing import Mapping\nfrom werkzeug import Request, Response\nfrom dify_plugin import Endpoint\n\nclass Duck(Endpoint):\n    def _invoke(self, r: Request, values: Mapping, settings: Mapping) -> Response:\n        \"\"\"\n        Invokes the endpoint with the given request.\n        \"\"\"\n        app_id = values[\"app_id\"]\n        def generator():\n            yield f\"{app_id} <br>\"\n        return Response(generator(), status=200, content_type=\"text/html\")\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Invocation in Python\nDESCRIPTION: This Python code snippet demonstrates the `_invoke` method for an LLM. It handles both synchronous and streaming responses, using `_handle_stream_response` for streaming and `_handle_sync_response` for synchronous processing.  The `stream` parameter determines the response type.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/predefined-model.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, stream: bool, **kwargs) \\\n            -> Union[LLMResult, Generator]:\n        if stream:\n              return self._handle_stream_response(**kwargs)\n        return self._handle_sync_response(**kwargs)\n\n    def _handle_stream_response(self, **kwargs) -> Generator:\n        for chunk in response:\n              yield chunk\n    def _handle_sync_response(self, **kwargs) -> LLMResult:\n        return LLMResult(**response)\n```\n\n----------------------------------------\n\nTITLE: Creating a Blob Message in Dify (Python)\nDESCRIPTION: This snippet shows how to create a blob message for returning file data within a Dify tool plugin.  It requires the raw file data as a bytes object (`blob`) and optional metadata (`meta`) as a dictionary. If `mime_type` is not specified in the meta, Dify uses `octet/stream` as the default.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/tool.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef create_blob_message(self, blob: bytes, meta: dict = None) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Xinference Provider Class (Python)\nDESCRIPTION: Implements the `XinferenceProvider` class as a placeholder for custom model providers. It inherits from the `Provider` base class and includes an empty `validate_provider_credentials` method. This allows the class to be instantiated without requiring a full implementation of credential validation, as the actual validation is handled elsewhere.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass XinferenceProvider(Provider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Invoking Text Embedding Model in Python\nDESCRIPTION: This function invokes a text embedding model to generate embeddings for a list of text inputs. It takes the model name, credentials, a list of texts to embed, and an optional user identifier as input. The function returns a TextEmbeddingResult entity containing the generated embeddings.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            texts: list[str], user: Optional[str] = None) \\\n        -> TextEmbeddingResult:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param texts: texts to embed\n    :param user: unique user id\n    :return: embeddings result\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing a Tool Provider in Python\nDESCRIPTION: This code snippet demonstrates how to implement a tool provider class in Python using the `dify_plugin` library. It defines a `GoogleProvider` class that inherits from `ToolProvider` and validates the provided credentials using the `GoogleSearchTool`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any\n\nfrom dify_plugin import ToolProvider\nfrom dify_plugin.errors.tool import ToolProviderCredentialValidationError\nfrom tools.google_search import GoogleSearchTool\n\nclass GoogleProvider(ToolProvider):\n    def _validate_credentials(self, credentials: dict[str, Any]) -> None:\n        try:\n            for _ in GoogleSearchTool.from_credentials(credentials).invoke(\n                tool_parameters={\"query\": \"test\", \"result_type\": \"link\"},\n            ):\n                pass\n        except Exception as e:\n            raise ToolProviderCredentialValidationError(str(e))\n```\n\n----------------------------------------\n\nTITLE: Implementing Parameter Validation (TypeScript)\nDESCRIPTION: This TypeScript snippet demonstrates how to implement parameter validation using the `zod` library. It defines a schema for validating the `point` and `params` request parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { zValidator } from \"@hono/zod-validator\";\n\nconst schema = z.object({\n  point: z.union([\n    z.literal(\"ping\"),\n    z.literal(\"app.external_data_tool.query\"),\n  ]), // Restricts 'point' to two specific values\n  params: z\n    .object({\n      app_id: z.string().optional(),\n      tool_variable: z.string().optional(),\n      inputs: z.record(z.any()).optional(),\n      query: z.any().optional(),  // string or null\n    })\n    .optional(),\n});\n\n```\n\n----------------------------------------\n\nTITLE: Model Invocation Error Mapping in Python\nDESCRIPTION: Defines a property to map model invocation errors to unified error types. This allows Dify to handle different errors consistently with appropriate follow-up actions. The key is the error type thrown to the caller, and the value is the error type thrown by the model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invoke error to unified error\n    The key is the error type thrown to the caller\n    The value is the error type thrown by the model,\n    which needs to be converted into a unified error type for the caller.\n\n    :return: Invoke error mapping\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for LLM in Python\nDESCRIPTION: This Python code defines the `get_num_tokens` method for pre-calculating the number of tokens for given prompt messages. It takes the model name, credentials, prompt messages, and tools as input and returns the number of tokens.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                   tools: Optional[list[PromptMessageTool]] = None) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param tools: tools for tool calling\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Agent Application for WeChat\nDESCRIPTION: This code snippet demonstrates the configuration needed in the `config.json` file to integrate a Dify agent application with WeChat. It sets the API base URL, API key, application type, channel type, and other parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n  {\n    \"dify_api_base\": \"https://api.dify.ai/v1\",\n    \"dify_api_key\": \"app-xxx\",\n    \"dify_app_type\": \"agent\",\n    \"channel_type\": \"wx\",\n    \"model\": \"dify\",\n    \"single_chat_prefix\": [\"\"],\n    \"single_chat_reply_prefix\": \"\",\n    \"group_chat_prefix\": [\"@bot\"],\n    \"group_name_white_list\": [\"ALL_GROUP\"]\n }\n\n```\n\n----------------------------------------\n\nTITLE: Parameter Extractor Node Endpoint\nDESCRIPTION: This defines the endpoint signature for invoking the ParameterExtractor node. It specifies the parameters needed such as the list of `ParameterConfig`, the `ModelConfig`, the `query` string, and the optional `instruction` string. It also indicates that the function returns a `NodeResponse` object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self,\n    parameters: list[ParameterConfig],\n    model: ModelConfig,\n    query: str,\n    instruction: str = \"\",\n) -> NodeResponse:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Vendor Metadata (YAML)\nDESCRIPTION: This YAML snippet defines the metadata for a tool vendor (Google in this case). It includes the author, name, label (localized for English and Chinese), description (localized), icon, and tags. The icon must be an attachment resource in the `_assets` folder.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  author: Your-name\n  name: google\n  label:\n    en_US: Google\n    zh_Hans: Google\n  description:\n    en_US: Google\n    zh_Hans: Google\n  icon: icon.svg\n  tags:\n    - search\n```\n\n----------------------------------------\n\nTITLE: Adding Knowledge Base Metadata Field with cURL\nDESCRIPTION: This cURL command adds a new metadata field to the Dify knowledge base. It requires the dataset ID and the metadata field details (type and name). The request body contains the metadata field type and name. It returns the ID, type and name of the newly added metadata field.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer {api_key}' \\\n--data '{\n    \"type\":\"string\",\n    \"name\":\"test\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Creating Blob Messages in Python\nDESCRIPTION: This Python code snippet showcases the interface for creating blob (binary large object) messages within the Dify tool plugin. It accepts the raw file data as bytes and optional metadata (including mime_type) as input, returning a ToolInvokeMessage. Dify defaults to 'octet/stream' if mime_type is not provided.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/tool.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef create_blob_message(self, blob: bytes, meta: dict = None) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Example Container IP Output\nDESCRIPTION: This is example output from the `docker inspect` command showing the IP address assignments for the `docker-web-1` and `docker-api-1` containers, these addresses are used in the Nginx configuration. Note that these are examples and should be replaced by the IPs retrieved from the live environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n/docker-web-1: 172.19.0.5\n/docker-api-1: 172.19.0.7\n```\n\n----------------------------------------\n\nTITLE: Defining LLMResultChunk Model (Python)\nDESCRIPTION: Defines the `LLMResultChunk` model, representing a chunk in a streaming LLM result. It includes the `model` used, the `prompt_messages` sent, an optional `system_fingerprint`, and the `delta` representing the change from the previous chunk.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunk(BaseModel):\n    \"\"\"\n    Model class for llm result chunk.\n    \"\"\"\n    model: str  # Actual used modele\n    prompt_messages: list[PromptMessage]  # prompt messages\n    system_fingerprint: Optional[str] = None  # request fingerprint, refer to OpenAI definition\n    delta: LLMResultChunkDelta\n```\n\n----------------------------------------\n\nTITLE: Prompt for Math Reasoning\nDESCRIPTION: This prompt instructs the AI to act as a math assistant and provide step-by-step solutions and final answers to math problems.  It emphasizes providing explanations for each step and including the equation in the output field.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/how-to-use-json-schema-in-dify.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nあなたは数学の助教です。数学問題が提示された際に、ステップバイステップの解法と最終回答を出力することが目標です。各ステップでは、出力欄に方程式を記入し、説明欄には推論の詳細を記述してください。\n```\n\n----------------------------------------\n\nTITLE: Provider Credential Validation in Python\nDESCRIPTION: This Python code snippet demonstrates how to validate provider credentials. The `validate_provider_credentials` method receives a dictionary of credentials and should raise an exception if the validation fails. The credentials are defined by the `provider_credential_schema` in the provider's YAML configuration file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Restart Dify Instance (AWS AMI) via Docker Compose\nDESCRIPTION: This snippet restarts the Dify Docker containers using `docker-compose`, which is necessary after modifying environment variables or making configuration changes. This ensures that the changes are applied and Dify is running with the updated settings.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_65\n\nLANGUAGE: shell\nCODE:\n```\ndocker-compose down\nocker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Disable Plugin Signature Verification in Dify\nDESCRIPTION: This code snippet demonstrates how to disable plugin signature verification in Dify by adding a line to the `.env` configuration file. This allows the installation of plugins that are not verified in the Dify Marketplace. This setting should be used with caution and plugins from unknown sources should be thoroughly tested before being used in a production environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/plugins.md#_snippet_0\n\nLANGUAGE: env\nCODE:\n```\nFORCE_VERIFYING_SIGNATURE=false\n```\n\n----------------------------------------\n\nTITLE: Moderation Model Invocation Python\nDESCRIPTION: This Python snippet defines the `_invoke` method for a moderation model. It takes a model name, credentials, text content, and an optional user ID as input. It returns a boolean indicating whether the text is safe.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n            text: str, user: Optional[str] = None) \\\n        -> bool:\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param text: text to moderate\n    :param user: unique user id\n    :return: false if text is safe, true otherwise\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Running OpenLLM Model with Docker\nDESCRIPTION: This command starts an OpenLLM server within a Docker container, deploying the specified model (facebook/opt-1.3b) and binding the container's port 3000 to the host's port 3333.  The `--rm` flag ensures the container is removed upon exit.  The `--backend pt` specifies the PyTorch backend.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_20\n\nLANGUAGE: docker\nCODE:\n```\ndocker run --rm -it -p 3333:3000 ghcr.io/bentoml/openllm start facebook/opt-1.3b --backend pt\n```\n\n----------------------------------------\n\nTITLE: Setting LangBot Runner to Dify Service API (JSON)\nDESCRIPTION: This snippet demonstrates how to configure the LangBot's provider.json file to use the Dify Service API as the runner, enabling communication between LangBot and the Dify platform. It involves setting the \"runner\" key to \"dify-service-api\".\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/connect-dify-to-various-im-platforms-by-using-langbot.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"runner\": \"dify-service-api\"\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response - Internal Server Error\nDESCRIPTION: This JSON snippet represents an error response from the server, indicating an internal server error that prevented the completion of the request. The server might be overloaded or have an error in the application code.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/llms-use-faq.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"error\": \"The server encountered an internal error and was unable to complete your request。Either the server is overloaded or there is an error in the application\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Middleware Services (Bash)\nDESCRIPTION: Navigates to the docker directory, copies the middleware environment file, and starts the PostgreSQL, Redis, and Weaviate services using Docker Compose. This step is necessary before enabling business services.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ncd docker\ncp middleware.env.example middleware.env\ndocker compose -f docker-compose.middleware.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: LLMResultChunk Class Definition in Python\nDESCRIPTION: Defines the LLMResultChunk class which represents a chunk of a streamed LLM result.  It includes the model used, the list of prompt messages, system fingerprint, and the LLMResultChunkDelta.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunk(BaseModel):\n    \"\"\"\n    Model class for llm result chunk.\n    \"\"\"\n    model: str  # 実際に使用したモデル\n    prompt_messages: list[PromptMessage]  # プロンプトメッセージのリスト\n    system_fingerprint: Optional[str] = None  # リクエスト指紋。OpenAIのこのパラメータの定義を参照。\n    delta: LLMResultChunkDelta  # 各イテレーションの変更が存在する内容\n```\n\n----------------------------------------\n\nTITLE: NPM Build Web Application\nDESCRIPTION: Builds the web application for deployment. Transforms the source code into optimized, production-ready static assets. Requires NPM and the web application's package.json file to be present.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_32\n\nLANGUAGE: JavaScript\nCODE:\n```\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: Copying environment file (Bash)\nDESCRIPTION: Copies the example environment file (.env.example) to .env. This file is then customized with the correct configuration values.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Install Go 1.20.6\nDESCRIPTION: This command installs Go version 1.20.6 on Ubuntu/Debian systems.  Go is required to build and run parts of the DifySandbox.  It is recommended to use Go 1.20.6 or later.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/backend/sandbox/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install -y golang-1.20.6\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Commands\nDESCRIPTION: Commands to start and monitor the Dify on WeChat application using Docker Compose. The first command navigates to the docker directory, the second starts the Docker container in detached mode, and the third command displays the logs of the running container, including the QR code needed for WeChat login.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncd dify-on-wechat/docker       # 进入 docker 目录\ndocker compose up -d           # 启动 docker 容器\ndocker logs -f dify-on-wechat  # 查看二维码并登录\n```\n\n----------------------------------------\n\nTITLE: Data Concatenation in Dify Workflow (Python)\nDESCRIPTION: This Python snippet illustrates how to concatenate two lists representing data from different knowledge bases within a Dify workflow. It takes two lists, `knowledge1` and `knowledge2`, as input and returns a combined list as the 'result' output variable.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/code.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef main(knowledge1: list, knowledge2: list) -> list:\n    return {\n        # Note to declare 'result' in the output variables\n        'result': knowledge1 + knowledge2\n    }\n```\n\n----------------------------------------\n\nTITLE: Invoking Text Embedding Model in Python\nDESCRIPTION: This method invokes a text embedding model. It takes the model name, credentials, texts to embed, and user ID as input. It returns a TextEmbeddingResult entity. It's capable of batch processing texts.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              texts: list[str], user: Optional[str] = None) \\\n          -> TextEmbeddingResult:\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param texts: texts to embed\n      :param user: unique user id\n      :return: embeddings result\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define LLMResult Model\nDESCRIPTION: Defines the `LLMResult` class, which represents the result of an LLM call. It includes the model used, prompt messages, the response message, usage information, and an optional system fingerprint.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResult(BaseModel):\n    \"\"\"\n    Model class for llm result.\n    \"\"\"\n    model: str  # Actual used modele\n    prompt_messages: list[PromptMessage]  # prompt messages\n    message: AssistantPromptMessage  # response message\n    usage: LLMUsage  # usage info\n    system_fingerprint: Optional[str] = None  # request fingerprint, refer to OpenAI definition\n```\n\n----------------------------------------\n\nTITLE: Define ToolPromptMessage Class in Python\nDESCRIPTION: This code defines the `ToolPromptMessage` class, inheriting from `PromptMessage`. It represents messages from a tool.  It includes attributes for `role` (set to `PromptMessageRole.TOOL`) and `tool_call_id` (the ID of the tool invocation).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nclass ToolPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for tool prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.TOOL\n    tool_call_id: str  # Tool invocation ID. If OpenAI tool call is not supported, the name of the tool can also be inputted.\n```\n\n----------------------------------------\n\nTITLE: Convert Array to Text using Code Node (Python)\nDESCRIPTION: This code snippet demonstrates how to convert an array (list) of strings to a single string with newline separators using a Code Node in a workflow. It takes a list called `articleSections` as input and returns a dictionary with a `result` key, where the value is the joined string.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/iteration.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef main(articleSections: list):\n    data = articleSections\n    return {\n        \"result\": \"\\n\".join(data)\n    }\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response: Anthropic Invalid Request Error\nDESCRIPTION: This error message signifies an invalid request to the Anthropic model, specifically related to the 'temperature' parameter being outside the allowed range (-1 or 0..1). This indicates the parameter settings need to be adjusted according to the specific model's requirements.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/llms-use-faq.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"error\": \"Anthropic: Error code: 400 - f'error': f'type': \\\"invalid request error, 'message': 'temperature: range: -1 or 0..1)'\"\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Document Segment using Dify API (curl)\nDESCRIPTION: This snippet shows how to delete a specific segment from a document in a Dify knowledge base. It requires the dataset ID, document ID, segment ID, and API key.  A successful deletion is indicated by a JSON response with the \"result\": \"success\".\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments/{segment_id}' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Restart Dify Services using Docker Compose\nDESCRIPTION: This snippet demonstrates how to restart the Dify services using Docker Compose after modifying the `.env` file.  The commands first navigate to the docker directory, then stop and remove the existing containers using `docker compose down`, and finally start the services in detached mode using `docker compose up -d`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/faq.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ndocker compose down\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Get Number of Tokens Method Signature\nDESCRIPTION: This python code shows the signature of the get_num_tokens method, which is used to get the number of tokens for given prompt messages. If a model does not provide this functionality, the method should return 0.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/integrate-the-predefined-model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                   tools: Optional[list[PromptMessageTool]] = None) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param tools: tools for tool calling\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Build Frontend Docker Image from Source\nDESCRIPTION: This command builds a Docker image named 'dify-web' from the Dockerfile located in the 'web' directory of the source code. It assumes that you have navigated to the root directory of the project.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/start-the-frontend-docker-container.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ncd web && docker build . -t dify-web\n```\n\n----------------------------------------\n\nTITLE: Running Frontend Docker Image (Local Build)\nDESCRIPTION: Runs the frontend service using a locally built Docker image. The command maps port 3000 and sets environment variables for the API endpoints. It requires Docker to be installed and the frontend image to be built. The exposed port 3000 is used to access the application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/start-the-frontend-docker-container.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ndocker run -it -p 3000:3000 -e CONSOLE_API_URL=http://127.0.0.1:5001 -e APP_API_URL=http://127.0.0.1:5001 dify-web\n```\n\n----------------------------------------\n\nTITLE: Image Prompt Message Content Class\nDESCRIPTION: Defines a class for image-based prompt message content, inheriting from PromptMessageContent. It includes the image data (URL or base64) and resolution detail. Used when including images within a multimodal prompt.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nclass ImagePromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for image prompt message content.\n    \"\"\"\n    class DETAIL(Enum):\n        LOW = 'low'\n        HIGH = 'high'\n\n    type: PromptMessageContentType = PromptMessageContentType.IMAGE\n    detail: DETAIL = DETAIL.LOW  # Resolution\n```\n\n----------------------------------------\n\nTITLE: Customizing Chatbot Style with Border\nDESCRIPTION: This code snippet shows how to customize the style of the Dify Chatbot button by modifying the `style` attribute in the iframe code.  Specifically, it adds a 2-pixel wide black solid border to the chatbot's iframe.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/how-to-integrate-dify-chatbot-to-your-wix-website.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n<iframe src=\"https://udify.app/chatbot/1yS3gohroW1sKyLc\" style=\"width: 80%; height: 80%; min-height: 500px; border: 2px solid #000;\" frameborder=\"0\" allow=\"microphone\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: Moderation Model Invocation\nDESCRIPTION: Defines the interface for a moderation model invocation. It checks if the given text is safe or not. The interface takes the model name, credentials, and text as input. An optional user identifier can also be provided.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict,\n              text: str, user: Optional[str] = None) \\\n          -> bool:\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param text: text to moderate\n      :param user: unique user id\n      :return: false if text is safe, true otherwise\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Starting the Worker Service (Linux/MacOS)\nDESCRIPTION: Starts the Celery worker service on Linux/MacOS. This service consumes asynchronous tasks related to dataset processing, generation, and other background operations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_27\n\nLANGUAGE: Bash\nCODE:\n```\ncelery -A app.celery worker -P gevent -c 1 -Q dataset,generation,mail,ops_trace --loglevel INFO\n```\n\n----------------------------------------\n\nTITLE: OpenAI Model Plugin Structure Example\nDESCRIPTION: Demonstrates the directory structure for the OpenAI model plugin, which encompasses multiple model types such as LLM, moderation, speech-to-text, text embedding, and TTS (Text-to-Speech).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n├── models\n│ ├── llm\n│ │ ├── chatgpt-4o-latest\n│ │ ├── gpt-3.5-turbo\n│ │ ├── gpt-4-0125-preview\n│ │ ├── gpt-4-turbo\n│ │ ├── gpt-4o\n│ │ ├── llm\n│ │ ├── o1-preview\n│ │ └── text-davinci-003\n│ ├── moderation\n│ │ ├── moderation\n│ │ └── text-moderation-stable\n│ ├── speech2text\n│ │ ├── speech2text\n│ │ └── whisper-1\n│ ├── text_embedding\n│ │ ├── text-embedding-3-large\n│ │ └── text_embedding\n│ └── tts\n│ ├── tts-1-hd\n│ ├── tts-1\n│ └── tts\n```\n\n----------------------------------------\n\nTITLE: LLM Result Chunk Delta Class Definition\nDESCRIPTION: This code snippet defines the LLMResultChunkDelta class, representing the delta (change) in each chunk of a streaming LLM result. It contains the index, message, usage information (only in the last chunk), and finish reason (only in the last chunk).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunkDelta(BaseModel):\n    \"\"\"\n    Model class for llm result chunk delta.\n    \"\"\"\n    index: int  # 順番\n    message: AssistantPromptMessage  # 返信メッセージ\n    usage: Optional[LLMUsage] = None  # トークン及び費用情報（最後のチャンクのみ）\n    finish_reason: Optional[str] = None  # 終了理由（最後のチャンクのみ）\n```\n\n----------------------------------------\n\nTITLE: Disable Plugin Signature Verification in Dify\nDESCRIPTION: This snippet shows how to disable plugin signature verification in the Dify platform. This allows the installation of unverified plugins, which can be useful in development or testing environments. However, it's crucial to only use this method for trusted plugins, as unverified plugins may pose security risks.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/faq.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nFORCE_VERIFYING_SIGNATURE=false\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens (Python)\nDESCRIPTION: This Python code defines the `get_num_tokens` method, which calculates the number of tokens for given prompt messages. If the model doesn't provide a native token counting interface, it can return 0.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/predefined-model.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],\n                       tools: Optional[list[PromptMessageTool]] = None) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param prompt_messages: prompt messages\n    :param tools: tools for tool calling\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Containers with Compose V2\nDESCRIPTION: This command starts the Dify application's Docker containers using Docker Compose V2. The `-d` option runs the containers in detached mode (in the background).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Resetting Password via Flask Command in Docker\nDESCRIPTION: This command is used to reset the password for the Dify application when deployed using Docker Compose. It executes a Flask command within the `docker-api-1` container to initiate the password reset process. You need to enter the account email and twice the new password to complete the reset.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-password\n```\n\n----------------------------------------\n\nTITLE: Creating a Blob Message in Dify (Python)\nDESCRIPTION: This snippet demonstrates how to create a blob message in Dify, which is used to return raw file data. It accepts the file's raw data (bytes) and optional metadata (like MIME type) and returns a ToolInvokeMessage. If the mime_type is not provided, Dify defaults to `octet/stream`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/advanced-tool-integration.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    def create_blob_message(self, blob: bytes, meta: dict = None, save_as: str = '') -> ToolInvokeMessage:\n        \"\"\"\n            create a blob message\n\n            :param blob: the blob\n            :return: the blob message\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository with Git\nDESCRIPTION: This command clones the forked Dify repository from GitHub to your local machine. Replace `<github_username>` with your GitHub username.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_13\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone git@github.com:<github_username>/dify.git\n```\n\n----------------------------------------\n\nTITLE: Installing ntwork Dependency using pip\nDESCRIPTION: This command is used to install the `ntwork` dependency from a local wheel file.  Replace `your-path/ntwork-0.1.3-cp38-cp38-win_amd64.whl` with the actual path to the downloaded `.whl` file.  This is needed to integrate with older versions of WeCom.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\npip install your-path/ntwork-0.1.3-cp38-cp38-win_amd64.whl\n```\n\n----------------------------------------\n\nTITLE: UserPromptMessage Class Definition in Python\nDESCRIPTION: Defines the UserPromptMessage class, inheriting from PromptMessage.  It sets the role to USER, representing a message originating from the user.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass UserPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for user prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.USER\n```\n\n----------------------------------------\n\nTITLE: Text Summarization Tool in Dify (Python)\nDESCRIPTION: This code demonstrates how to use the text summarization tool provided by Dify. It takes a `user_id` and the `content` to be summarized as input and returns the summarized text. It relies on Dify's default model for the current workspace to perform the summarization.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/advanced-tool-integration.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    def summary(self, user_id: str, content: str) -> str:\n        \"\"\"\n            summary the content\n\n            :param user_id: the user id\n            :param content: the content\n            :return: the summary\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Sign a Dify Plugin (Bash)\nDESCRIPTION: This command adds a signature to a Dify plugin using a private key.  It requires the plugin file and the private key file as arguments. The signed plugin file will be created in the same directory as the original plugin with a `signed` suffix. Make sure to specify the correct paths to both the plugin and private key files.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify signature sign your_plugin_project.difypkg -p your_key_pair.private.pem\n```\n\n----------------------------------------\n\nTITLE: Package a Dify Plugin\nDESCRIPTION: This snippet shows how to package a Dify plugin using the `dify plugin package` command. Replace `./basic_agent/` with the actual path to your plugin project directory. This command creates a `.difypkg` file, which is the final plugin package that can be uploaded and distributed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n# 将 ./basic_agent 替换为插件项目的实际路径\ndify plugin package ./basic_agent/\n```\n\n----------------------------------------\n\nTITLE: Workflow Invoke Specification Python\nDESCRIPTION: Defines the invoke method for workflow applications within Dify. It outlines the parameters required: app_id, inputs, response_mode, and files. The return type is either a generator of dictionaries (for streaming) or a single dictionary (for blocking).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/app.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self,\n    app_id: str,\n    inputs: dict,\n    response_mode: Literal[\"streaming\", \"blocking\"],\n    files: list,\n) -> Generator[dict, None, None] | dict:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Start LocalAI with Docker Compose\nDESCRIPTION: This snippet starts LocalAI using Docker Compose, building the necessary images. It then tails the logs to monitor the build process until completion. This command assumes Docker and Docker Compose are already installed and configured on the system.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/localai.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# start with docker-compose\n$ docker-compose up -d --build\n\n# tail the logs & wait until the build completes\n$ docker logs -f langchain-chroma-api-1\n```\n\n----------------------------------------\n\nTITLE: Validating Provider Credentials in Python\nDESCRIPTION: This Python code defines the `validate_provider_credentials` method that must be implemented by model providers.  It validates the provider's credentials using a dictionary. A failed validation should raise a `CredentialsValidateFailedError` exception.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Deploying GPUStack on Linux or MacOS\nDESCRIPTION: This command downloads and executes a script to install GPUStack as a service on systemd or launchd-based systems. It leverages `curl` to fetch the installation script from the provided URL and pipes it to `sh` for execution.  This is a one-line installation command.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/gpustack.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | sh -s -\n```\n\n----------------------------------------\n\nTITLE: Debugging History Messages (Python)\nDESCRIPTION: This Python code snippet shows how to print the history messages for debugging purposes, inside the `_invoke` method of the `BasicAgentAgentStrategy` class. It helps you verify if the history messages are correctly being received by the agent.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n        print(f\"history_messages: {params.model.history_prompt_messages}\")\n        ...\n```\n\n----------------------------------------\n\nTITLE: Extracting JSON Data with Python\nDESCRIPTION: This Python code snippet extracts the 'data.name' field from a JSON string obtained from an HTTP node. It uses the `json` library to parse the string and returns a dictionary containing the extracted value under the key 'result'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/code.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef main(http_response: str) -> str:\n    import json\n    data = json.loads(http_response)\n    return {\n        # 出力変数にresultを宣言することに注意\n        'result': data['data']['name']\n    }\n```\n\n----------------------------------------\n\nTITLE: Mapping Model Invoke Errors in Python\nDESCRIPTION: This property maps model invocation errors to unified error types. The key is the error type thrown to the caller, and the value is the error type thrown by the model. This allows Dify to handle different errors in a consistent manner.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@property\n    def _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n        \"\"\"\n        Map model invoke error to unified error\n        The key is the error type thrown to the caller\n        The value is the error type thrown by the model,\n        which needs to be converted into a unified error type for the caller.\n\n        :return: Invoke error mapping\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: LLM Tool Parameter Generation with Python\nDESCRIPTION: This Python snippet shows how to utilize the LLM's output in combination with tool-calling code to automatically generate parameters that are needed for tool calls.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntool_instances = (\n    {tool.identity.name: tool for tool in params.tools} if params.tools else {}\n)\nfor tool_call_id, tool_call_name, tool_call_args in tool_calls:\n    tool_instance = tool_instances[tool_call_name]\n    self.session.tool.invoke(\n        provider_type=ToolProviderType.BUILT_IN,\n        provider=tool_instance.identity.provider,\n        tool_name=tool_instance.identity.name,\n        parameters={**tool_instance.runtime_parameters, **tool_call_args},\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Model UID Credential Schema\nDESCRIPTION: Defines the 'model_uid' as a text input credential for the Xinference vendor. It mandates users to enter the unique ID of the model. This credential is used to specifically identify and access the correct model instance within the Xinference environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/customizable-model.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n  - variable: model_uid\n    label:\n      zh_Hans: 模型 UID\n      en_US: Model uid\n    type: text-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入你的 Model UID\n      en_US: Enter the model uid\n```\n\n----------------------------------------\n\nTITLE: TTS Invoke Endpoint Definition Python\nDESCRIPTION: This snippet defines the `invoke` endpoint for TTS. It takes a `model_config` of type `TTSModelConfig` and the text to convert to speech (`content_text`). It returns a generator that yields bytes (mp3 audio byte stream).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self, model_config: TTSModelConfig, content_text: str\n) -> Generator[bytes, None, None]:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Resetting Encryption Key Pair with Source Code\nDESCRIPTION: This snippet shows how to reset the encryption key pair when starting from source code. First, navigate to the api directory. Then execute the `flask reset-encrypt-key-pair` command to reset the encryption key pair. This action is necessary if the `api/storage/privkeys` file is deleted.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/_book/zh_CN/learn-more/faq/install-faq.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nflask reset-encrypt-key-pair\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response: Prompt Too Long\nDESCRIPTION: This JSON presents an error indicating that either the query or the prefix prompt is too long. The suggested solutions involve reducing the prefix prompt length, shrinking the maximum token size, or switching to an LLM with a larger token limit.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/llms-use-faq.md#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"error\": \"Query or prefix prompt is too long, you can reduce the preix prompt, or shrink the max token, or switch to a llm with a larger token limit size\"\n}\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for Text Embedding in Python\nDESCRIPTION: This method gets the number of tokens for given texts. It takes the model name, credentials, and texts to embed as input. If the model doesn't provide a tokenizer, the `_get_num_tokens_by_gpt2` method can be used.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, texts: list[str]) -> int:\n        \"\"\"\n        Get number of tokens for given prompt messages\n\n        :param model: model name\n        :param credentials: model credentials\n        :param texts: texts to embed\n        :return:\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Reset Encryption Key Pair using Docker Compose\nDESCRIPTION: Resets the encryption key pair used for encrypting large model keys in a Dify deployment using Docker Compose. This command addresses \"File not found\" errors related to missing private keys. It is executed within the `docker-api-1` container.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_1\n\nLANGUAGE: docker\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-encrypt-key-pair\n```\n\n----------------------------------------\n\nTITLE: Invoking Workflow as Tool in Python\nDESCRIPTION: This snippet defines the endpoint for invoking a Workflow as Tool within a Dify plugin. It takes a provider, tool name, and parameters as input and yields ToolInvokeMessage objects.  `provider` is the tool's ID, and `tool_name` is required when creating the tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/tool.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef invoke_workflow_tool(\n    self, provider: str, tool_name: str, parameters: dict[str, Any]\n) -> Generator[ToolInvokeMessage, None, None]:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Define Tool Provider Identity in YAML\nDESCRIPTION: This YAML configuration defines the identity of a tool provider, including the author, name, labels for multiple languages, description, and icon. The `identity` field is mandatory and the icon file path is relative to the module's `_assets` folder.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/quick-tool-integration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nidentity: # ツールプロバイダーの基本情報\n  author: Dify # 著者\n  name: google # 名前、唯一無二で、他のプロバイダーと重複してはいけません\n  label: # ラベル、前端表示用\n    en_US: Google # 英語ラベル\n    zh_Hans: Google # 中国語ラベル\n    ja_JP: : Google # 日本語ラベル\n    pt_BR: : : Google # プルトガル語ラベル\n  description: # 説明、前端表示用\n    en_US: Google # 英語説明\n    zh_Hans: Google # 中国語説明\n    ja_JP: : Google # 日本語説明\n    pt_BR: : Google # プルトガル語説明\n  icon: icon.svg # アイコン、現在のモジュールの_assetsフォルダーに配置する必要があります\n```\n\n----------------------------------------\n\nTITLE: Network Process Check\nDESCRIPTION: This command is used to identify processes that are currently listening on port 80.  It combines `netstat` to list network connections, `grep` to filter for port 80, and flags to specify the output format. This helps in troubleshooting port conflicts.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnetstat -tunlp | grep 80\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies (Bash)\nDESCRIPTION: Installs the necessary dependencies for the Dify web application using pnpm.  First, pnpm is installed globally if it's not already available, then the project dependencies are installed using pnpm install.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nnpm i -g pnpm\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Start Dify with Docker Compose\nDESCRIPTION: Starts the Dify application using Docker Compose, bringing up all necessary containers in detached mode. This ensures the application is running in the background.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/faqs.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Resetting Password with Docker Compose\nDESCRIPTION: This snippet demonstrates how to reset the password for a Dify instance deployed using Docker Compose. It executes a flask command within the `docker-api-1` container to initiate the password reset process, prompting for the account email and new password.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/_book/zh_CN/learn-more/faq/install-faq.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-password\n```\n\n----------------------------------------\n\nTITLE: Installing API Dependencies with Poetry\nDESCRIPTION: Installs the dependencies for the API service using Poetry. It first activates the Python 3.12 environment, then installs the required packages.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_24\n\nLANGUAGE: Bash\nCODE:\n```\npoetry env use 3.12\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Backup Data Volumes\nDESCRIPTION: This snippet shows how to stop the docker containers and create a compressed archive of the volumes directory, backing up the data. It is a recommended step before performing a Dify upgrade.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/dify-premium.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\ntar -cvf volumes-$(date +%s).tgz volumes\n```\n\n----------------------------------------\n\nTITLE: Python: XinferenceProvider Class Implementation\nDESCRIPTION: This Python code provides a skeletal implementation of the `XinferenceProvider` class, inheriting from the `Provider` base class.  It includes an empty `validate_provider_credentials` method, which is necessary to avoid abstract class instantiation errors when the provider supports custom models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/new-provider.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass XinferenceProvider(Provider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Setting Notion Public Integration Environment Variables\nDESCRIPTION: These environment variables are required to configure Dify to use Notion's public integration. NOTION_INTEGRATION_TYPE should be set to 'パブリック', and NOTION_CLIENT_SECRET and NOTION_CLIENT_ID should be set to the client secret and ID obtained from Notion after upgrading to a public integration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/create-knowledge-and-upload-documents/import-content-data/sync-from-notion.md#_snippet_1\n\nLANGUAGE: env\nCODE:\n```\n**NOTION_INTEGRATION_TYPE**=パブリック\n\n**NOTION_CLIENT_SECRET**=you-client-secret\n\n**NOTION_CLIENT_ID**=you-client-id\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Tokens for Text Embedding in Python\nDESCRIPTION: This Python code defines the `get_num_tokens` method for pre-calculating the number of tokens for text embedding. It takes the model name, credentials, and a list of texts as input and returns the number of tokens.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/model/model-schema.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_num_tokens(self, model: str, credentials: dict, texts: list[str]) -> int:\n    \"\"\"\n    Get number of tokens for given prompt messages\n\n    :param model: model name\n    :param credentials: model credentials\n    :param texts: texts to embed\n    :return:\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Example Request Body for Weather Data Retrieval\nDESCRIPTION: Illustrates an example of the request body for retrieving weather data. It showcases the `app_id`, the `tool_variable` set to `weather_retrieve`, user inputs (e.g., location set to London), and the user query.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/external-data-tool.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n        \"point\": \"app.external_data_tool.query\",\n        \"params\": {\n            \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n            \"tool_variable\": \"weather_retrieve\",\n            \"inputs\": {\n                \"location\": \"London\"\n            },\n            \"query\": \"How's the weather today?\"\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Validating Provider Credentials in Python\nDESCRIPTION: This method validates the provider's credentials. It receives a dictionary containing the credentials and raises an exception if validation fails. The credentials' format is defined in the provider's YAML configuration file under `provider_credential_schema`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef validate_provider_credentials(self, credentials: dict) -> None:\n    \"\"\"\n    Validate provider credentials\n    You can choose any validate_credentials method of model type or implement validate method by yourself,\n    such as: get model list api\n\n    if validate failed, raise exception\n\n    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Example Error Traceback\nDESCRIPTION: This is an example of an error traceback that might appear in the logs when there are issues with large model key encryption.  It indicates a FileNotFoundError when attempting to load the private key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nERROR:root:Unknown Error in completion\nTraceback (most recent call last):\n  File \"/www/wwwroot/dify/dify/api/libs/rsa.py\", line 45, in decrypt\n    private_key = storage.load(filepath)\n  File \"/www/wwwroot/dify/dify/api/extensions/ext_storage.py\", line 65, in load\n    raise FileNotFoundError(\"File not found\")\nFileNotFoundError: File not found\n```\n\n----------------------------------------\n\nTITLE: TTS Invoke Endpoint (Python)\nDESCRIPTION: This code defines the signature for the `invoke` method that performs text-to-speech conversion. It takes a TTSModelConfig and the content text as input and returns a generator that yields byte streams (mp3 audio).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self, model_config: TTSModelConfig, content_text: str\n) -> Generator[bytes, None, None]:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Run DeepSeek Model in Ollama (Bash)\nDESCRIPTION: This command pulls and runs the DeepSeek R1 7B model within the Ollama environment. It downloads the model if it's not already present and starts the model server, making it available for use. It's a core step in making the LLM accessible to Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nollama run deepseek-r1:7b\n```\n\n----------------------------------------\n\nTITLE: Resetting Encryption Key Pair via Flask Command in Source Code\nDESCRIPTION: This command is used to reset the encryption key pair for Dify when running from source code. It executes a Flask command from the `api` directory to reset the keys used for encrypting sensitive data. This is necessary when the `api/storage/privkeys` file is lost or corrupted.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nflask reset-encrypt-key-pair\n```\n\n----------------------------------------\n\nTITLE: Migrate to Cloud Storage - Docker Compose\nDESCRIPTION: These commands are used to migrate files from local storage to cloud storage in a Docker Compose deployment.  The `docker exec` command executes the Flask migration commands within the `docker-api-1` container to upload private key files and local files to cloud storage.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask upload-private-key-file-to-cloud-storage\ndocker exec -it docker-api-1 flask upload-local-files-to-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: JSON Verification Code (Python)\nDESCRIPTION: This Python code snippet validates the format of a JSON string.  It attempts to parse the input JSON string and returns the parsed object within a dictionary. This code is used in a code node to verify the output of an LLM node. It requires the `json` library.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/error-handling/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef main(json_str: str) -> dict:\n    obj = json.loads(json_str)\n    return {'result': obj}\n```\n\n----------------------------------------\n\nTITLE: Resetting Encryption Key Pair with Docker Compose\nDESCRIPTION: This snippet shows how to reset the encryption key pair when deploying with Docker Compose.  This is necessary if the `api/storage/privkeys` file is deleted, which is used to encrypt large model keys. Loss of this file is irreversible, requiring a key pair reset.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/_book/zh_CN/learn-more/faq/install-faq.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-encrypt-key-pair\n```\n\n----------------------------------------\n\nTITLE: Converting User ID to Twitter URL using Python\nDESCRIPTION: This Python code snippet defines a function `main` that takes a Twitter user ID as a string and returns a dictionary containing the full Twitter URL. The function concatenates the base URL `https://twitter.com/` with the provided user ID.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/workshop/intermediate/twitter-chatflow.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef main(id: str) -> dict:\n    return {\n        \"url\": \"https://twitter.com/\"+id,\n    }\n```\n\n----------------------------------------\n\nTITLE: Starting the Worker Service (Linux/macOS)\nDESCRIPTION: This command starts the Celery worker service for Linux/macOS, which consumes asynchronous tasks such as dataset file importing and document updating. It specifies the application, concurrency, queues, and logging level.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run celery -A app.celery worker -P gevent -c 1 -Q dataset,generation,mail,ops_trace --loglevel INFO\n```\n\n----------------------------------------\n\nTITLE: Summarizing Content in Dify (Python)\nDESCRIPTION: This snippet shows how to use the Dify built-in tool to summarize long text. It takes a user ID and the content to be summarized as input, and returns the summarized text.  Dify uses the current workspace's default model for summarization.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/advanced-tool-integration.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    def summary(self, user_id: str, content: str) -> str:\n        \"\"\"\n            summary the content\n\n            :param user_id: the user id\n            :param content: the content\n            :return: the summary\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Define PromptMessageTool Class in Python\nDESCRIPTION: This code defines the `PromptMessageTool` class, which inherits from `BaseModel`. It includes attributes for `name` (tool name), `description` (tool description), and `parameters` (a dictionary representing the tool's parameters).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageTool(BaseModel):\n    \"\"\"\n    Model class for prompt message tool.\n    \"\"\"\n    name: str\n    description: str\n    parameters: dict\n```\n\n----------------------------------------\n\nTITLE: Restart Dify with Docker Compose\nDESCRIPTION: This snippet restarts Dify using Docker Compose with the updated environment variables from the `.env` file within the EC2 instance. It's essential after modifying environment variables for changes to take effect.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/dify-premium.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose down\nocker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Example External Data Tool Request (JSON)\nDESCRIPTION: Provides an example request body for the 'app.external_data_tool.query' extension point, used to query external weather information based on location.  Demonstrates the structure of a request to retreive weather data for a particular location.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/README.md#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"point\": \"app.external_data_tool.query\",\n    \"params\": {\n        \"app_id\": \"61248ab4-1125-45be-ae32-0ce91334d021\",\n        \"tool_variable\": \"weather_retrieve\",\n        \"inputs\": {\n            \"location\": \"London\"\n        },\n        \"query\": \"How's the weather today?\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding a Signature to the Plugin (bash)\nDESCRIPTION: This command adds a signature to a plugin using the specified private key. It takes the plugin file and the private key file as input and generates a new signed plugin file with 'signed' appended to the original filename. Requires dify CLI tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify signature sign your_plugin_project.difypkg -p your_key_pair.private.pem\n```\n\n----------------------------------------\n\nTITLE: Running Database Migrations\nDESCRIPTION: This command runs database migrations using Flask-Migrate to update the database schema to the latest version. It ensures that the database schema is up-to-date with the current version of the application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run flask db upgrade\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response from Stable Diffusion API\nDESCRIPTION: This JSON snippet shows the response from the Stable Diffusion API endpoint `/sdapi/v1/sd-models`. It contains information about the available models, including the `model_name` which is needed for configuring Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/stable-diffusion.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"title\": \"pastel-mix/pastelmix-better-vae-fp32.ckpt [943a810f75]\",\n        \"model_name\": \"pastel-mix_pastelmix-better-vae-fp32\",\n        \"hash\": \"943a810f75\",\n        \"sha256\": \"943a810f7538b32f9d81dc5adea3792c07219964c8a8734565931fcec90d762d\",\n        \"filename\": \"/home/takatost/stable-diffusion-webui/models/Stable-diffusion/pastel-mix/pastelmix-better-vae-fp32.ckpt\",\n        \"config\": null\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Base Class\nDESCRIPTION: Defines an abstract base class for all role message bodies, which includes the role, content (string or a list of PromptMessageContent objects), and name. It's designed for parameter declaration and can't be initialized directly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessage(ABC, BaseModel):\n    \"\"\"\n    Model class for prompt message.\n    \"\"\"\n    role: PromptMessageRole\n    content: Optional[str | list[PromptMessageContent]] = None  # Supports two types: string and content list. The content list is designed to meet the needs of multimodal inputs. For more details, see the PromptMessageContent explanation.\n    name: Optional[str] = None\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: This command installs essential packages needed for adding HTTPS support, handling certificates, downloading files, and managing software repositories. These packages are common prerequisites for installing Docker on Ubuntu.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install apt-transport-https ca-certificates curl software-properties-common\n```\n\n----------------------------------------\n\nTITLE: Nginx Mounting Error Message\nDESCRIPTION: This error message indicates that Docker Compose can't mount a local file (nginx.conf) to a location within the Docker container.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nError response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting \"/run/desktop/mnt/host/d/Documents/docker/nginx/nginx.conf\" to rootfs at \"/etc/nginx/nginx.conf\": mount /run/desktop/mnt/host/d/Documents/docker/nginx/nginx.conf:/etc/nginx/nginx.conf (via /proc/self/fd/9), flags: 0x5000: not a directory: unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type\n```\n\n----------------------------------------\n\nTITLE: Modifying Metadata Fields in Knowledge Base via Dify API\nDESCRIPTION: Modifies existing metadata fields within a dataset in the Dify knowledge base. Requires the dataset ID and metadata ID as path parameters, a valid API key in the Authorization header, and a JSON payload containing the updated metadata field properties.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request PATCH 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata/{metadata_id}' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer {api_key}' \\\n--data '{\n    \"name\":\"test\"\n}'\n```\n\n----------------------------------------\n\nTITLE: ToolPromptMessage Class Definition in Python\nDESCRIPTION: Defines the ToolPromptMessage class, inheriting from PromptMessage.  It sets the role to TOOL and includes a tool_call_id attribute, representing the ID of the tool call. The content stores the execution result of the tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nclass ToolPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for tool prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.TOOL\n    tool_call_id: str  # ツール呼び出しID。OpenAI tool callをサポートしない場合、ツール名を渡すこともできます。\n```\n\n----------------------------------------\n\nTITLE: Verify a Signed Dify Plugin (Bash)\nDESCRIPTION: This command verifies the signature of a Dify plugin using a public key. It requires the signed plugin file and the public key file as arguments.  If the signature is valid, the command will execute successfully without errors. If the signature is invalid or the plugin has been tampered with, the command will fail.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndify signature verify your_plugin_project.signed.difypkg -p your_key_pair.public.pem\n```\n\n----------------------------------------\n\nTITLE: Defining Server URL Credential Schema\nDESCRIPTION: Defines the 'server_url' as a text input credential for the Xinference vendor. It requires users to input the URL of their Xinference server. This credential is crucial for establishing a connection with the Xinference deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/customizable-model.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n  - variable: server_url\n    label:\n      zh_Hans: 服务器URL\n      en_US: Server url\n    type: text-input\n    required: true\n    placeholder:\n      zh_Hans: 在此输入Xinference的服务器地址，如 https://example.com/xxx\n      en_US: Enter the url of your Xinference, for example https://example.com/xxx\n```\n\n----------------------------------------\n\nTITLE: Creating Twitter URL in Python\nDESCRIPTION: This Python code defines a function `main` that takes a Twitter user ID as a string and returns a dictionary containing the complete Twitter URL. It concatenates the base URL `https://twitter.com/` with the provided user ID.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef main(id: str) -> dict:\n    return {\n        \"url\": \"https://twitter.com/\"+id,\n    }\n```\n\n----------------------------------------\n\nTITLE: Creating Variable Messages in Python\nDESCRIPTION: This Python code snippet illustrates how to create variable messages within the Dify tool plugin system. It takes a variable name (string) and variable value (Any) as input and returns a ToolInvokeMessage. Later set values overwrite previously set values.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/tool.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_variable_message(self, variable_name: str, variable_value: Any) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Create Public Keys Directory (Bash)\nDESCRIPTION: This command creates a directory to store public keys within the `plugin_daemon` volume, specifically under `/docker/volumes/plugin_daemon/public_keys`. This directory is used by Dify to locate the public keys needed for verifying plugin signatures during installation. This is part of enabling third-party signature verification.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmkdir docker/volumes/plugin_daemon/public_keys\n```\n\n----------------------------------------\n\nTITLE: Using Text Extraction Tool Node\nDESCRIPTION: This snippet demonstrates how to add a Text Extraction Tool node and use a 'single file' variable for document content extraction to prepare the extracted text for processing by an LLM. The 'start' node file variable will be used as an input variable for the 'Text Extraction Tool' node.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/file-upload.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Configure Vector Store in .env file\nDESCRIPTION: Configures the vector store to be used by Dify by modifying the `.env` file for source code deployments. This allows switching between different vector databases such as Weaviate, Qdrant, Milvus, and AnalyticDB. This example sets the vector store to Qdrant.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nVECTOR_STORE=qdrant\n```\n\n----------------------------------------\n\nTITLE: Deleting a Chunk in a Document via Dify API\nDESCRIPTION: Deletes a specific chunk (segment) from a document within a dataset in the Dify knowledge base. Requires the dataset ID, document ID, and segment ID as path parameters, along with a valid API key in the Authorization header.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments/{segment_id}' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Content Type Enum\nDESCRIPTION: Defines the `PromptMessageContentType` enumeration, specifying possible types for prompt message content: TEXT and IMAGE. This enumeration enables support for multimodal prompts, including both textual and visual information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContentType(Enum):\n    \"\"\"\n    Enum class for prompt message content type.\n    \"\"\"\n    TEXT = 'text'\n    IMAGE = 'image'\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Snippet - Code Editor\nDESCRIPTION: This example shows a JSON Schema snippet for defining a username field within the code editor, specifying its type, description, and requirement status. It highlights how individual fields can be configured within a larger JSON schema structure.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/workflow/node/llm.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"username\",\n  \"type\": \"string\",\n  \"description\": \"user's name\",\n  \"required\": true\n}\n```\n\n----------------------------------------\n\nTITLE: LLM Customizable Model Schema Interface in Python\nDESCRIPTION: Defines the interface for getting a customizable model schema. This method can be implemented to allow custom models to fetch model schema when the provider supports adding custom LLMs. The default return is null.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> Optional[AIModelEntity]:\n    \"\"\"\n    Get customizable model schema\n\n    :param model: model name\n    :param credentials: model credentials\n    :return: model schema\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Vector Database Migration - Flask\nDESCRIPTION: This command migrates data from one vector database to another. Requires Flask to be set up, and the VECTOR_STORE environment variable in .env to be correctly set to the target database. Can be run via docker exec or locally.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n`flask vdb-migrate # or docker exec -it docker-api-1 flask vdb-migrate`\n```\n\n----------------------------------------\n\nTITLE: LLM Result Chunk Class\nDESCRIPTION: Defines the `LLMResultChunk` class, representing a chunk in a streaming LLM result. It contains the model, prompt messages, system fingerprint, and the delta.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunk(BaseModel):\n    \"\"\"\n    Model class for llm result chunk.\n    \"\"\"\n    model: str  # 实际使用模型\n    prompt_messages: list[PromptMessage]  # prompt 消息列表\n    system_fingerprint: Optional[str] = None  # 请求指纹，可参考 OpenAI 该参数定义\n    delta: LLMResultChunkDelta  # 每个迭代存在变化的内容\n```\n\n----------------------------------------\n\nTITLE: Disable Plugin Signature Verification in Dify\nDESCRIPTION: This code snippet shows how to disable plugin signature verification in the Dify platform by setting the `FORCE_VERIFYING_SIGNATURE` environment variable to `false`. This allows installation of plugins not listed in the Dify Marketplace. Use with caution, and always test unverified plugins in a safe environment first.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/plugins.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nFORCE_VERIFYING_SIGNATURE=false\n```\n\n----------------------------------------\n\nTITLE: Configure Git User\nDESCRIPTION: Configures the Git user name and email address globally. This is necessary if Git is being used for the first time and helps identify the author of the commits. The user name and email address should be replaced with the actual user's information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n```\n\n----------------------------------------\n\nTITLE: Tailing Docker Logs for LocalAI\nDESCRIPTION: This command tails the logs of the Docker container named 'langchain-chroma-api-1'. It is used to monitor the startup process of the LocalAI service running within the container.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n$ docker logs -f langchain-chroma-api-1\n```\n\n----------------------------------------\n\nTITLE: Set Ollama Host Environment Variable (macOS) (Bash)\nDESCRIPTION: This command sets the OLLAMA_HOST environment variable on macOS using launchctl, allowing Docker containers to access the Ollama service. This is used when Ollama is installed as a macOS application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nlaunchctl setenv OLLAMA_HOST \"0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Backend Directory Structure\nDESCRIPTION: This snippet describes the directory structure of the Dify backend, which is written in Python using the Flask framework. It outlines the purpose of each directory, such as `controllers` for API routing, `models` for database schemas, and `tasks` for asynchronous job processing.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_62\n\nLANGUAGE: text\nCODE:\n```\n[api/]\n├── constants             // コードベース全体で使用される定数設定。\n├── controllers           // APIルート定義とリクエスト処理ロジック。\n├── core                  // コアアプリケーションオーケストレーション、モデル統合、ツール。\n├── docker                // Dockerおよびコンテナ化関連の設定。\n├── events                // イベント処理と処理\n├── extensions            // サードパーティフレームワーク/プラットフォームとの拡張機能。\n├── fields                // シリアライズ/マーシャリングのためのフィールド定義。\n├── libs                  // 再利用可能なライブラリとヘルパー。\n├── migrations            // データベース移行のためのスクリプト。\n├── models                // データベースモデルとスキーマ定義。\n├── services              // ビジネスロジックを指定。\n├── storage               // 秘密鍵保管。\n├── tasks                 // 非同期タスクとバックグラウンドジョブの処理。\n└── tests\n```\n\n----------------------------------------\n\nTITLE: Resetting Admin Password with Flask CLI\nDESCRIPTION: This command resets the administrator password using the Flask CLI within the `docker-api-1` container. It prompts for the email address and the new password. It assumes that Dify is deployed using Docker Compose and that the `docker-api-1` container is running.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/faqs.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-password\n```\n\n----------------------------------------\n\nTITLE: Restart Dify after Customization\nDESCRIPTION: This snippet demonstrates how to restart the Dify instance after making changes to the `.env` file by bringing down the Docker Compose setup and then bringing it back up. This ensures that the changes are applied.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/dify-premium.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose down\ndocker-compose -f docker-compose.yaml -f docker-compose.override.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Tool Calling Example (Function Calling)\nDESCRIPTION: This snippet showcases an example of how to call a tool or function, specifically with parameters `prompt_message.id`, `prompt_message.function.name`, and `args`.  It shows the structure of the function call with arguments within the context of Dify plugin development.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n                    prompt_message.id,\n                    prompt_message.function.name,\n                    args,\n                )\n            )\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Content Base Class Definition\nDESCRIPTION: This code snippet defines the base class for prompt message content. It specifies the 'type' and 'data' attributes. The data attribute stores the actual content, which could be text or image data.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContent(BaseModel):\n    \"\"\"\n    Model class for prompt message content.\n    \"\"\"\n    type: PromptMessageContentType\n    data: str  # コンテンツデータ\n```\n\n----------------------------------------\n\nTITLE: Initializing a new Bundle plugin project\nDESCRIPTION: This command initializes a new Dify Bundle plugin project in the current directory using the dify-plugin CLI tool.  It assumes the binary is located in the current directory. The user will be prompted for plugin name, author information, and description.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/bundle.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./dify-plugin-darwin-arm64 bundle init\n```\n\n----------------------------------------\n\nTITLE: Moderation Invoke Endpoint Definition Python\nDESCRIPTION: This snippet defines the `invoke` endpoint for moderation. It takes a `model_config` of type `ModerationModelConfig` and the text to check (`text`). It returns a boolean value indicating whether the text contains sensitive content.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(self, model_config: ModerationModelConfig, text: str) -> bool:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Running SearXNG with Docker\nDESCRIPTION: This command starts a SearXNG Docker container, mapping port 8081 on the host to port 8080 in the container and mounting the SearXNG configuration directory.  It assumes the current directory is the Dify root directory. Requires Docker to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/searxng.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd dify\ndocker run --rm -d -p 8081:8080 -v \"${PWD}/api/core/tools/provider/builtin/searxng/docker:/etc/searxng\" searxng/searxng\n```\n\n----------------------------------------\n\nTITLE: Adding Agent Strategies in manifest.yaml (YAML)\nDESCRIPTION: This snippet demonstrates how to add the `plugins.agent_strategies` field to the `manifest.yaml` file to include Agent strategies. It also defines the Agent provider's YAML file.  This allows Dify to recognize and use the implemented agent strategy.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/agent.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nversion: 0.0.2\ntype: plugin\nauthor: \"langgenius\"\nname: \"agent\"\nplugins:\n  agent_strategies:\n    - \"provider/agent.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Link Message in Dify (Python)\nDESCRIPTION: This snippet demonstrates how to create a link message within the Dify tool framework. It takes a link URL as input and returns a ToolInvokeMessage object. This function is used to return a URL to the user or LLM.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/advanced-tool-integration.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    def create_link_message(self, link: str, save_as: str = '') -> ToolInvokeMessage:\n        \"\"\"\n            create a link message\n\n            :param link: the url of the link\n            :return: the link message\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Deploying OpenLLM Model\nDESCRIPTION: This command deploys the specified model (facebook/opt-1.3b) using Docker and exposes it on port 3333. It uses the PyTorch backend.  The Docker image `ghcr.io/bentoml/openllm` is used to run the OpenLLM server.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/openllm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -it -p 3333:3000 ghcr.io/bentoml/openllm start facebook/opt-1.3b --backend pt\n```\n\n----------------------------------------\n\nTITLE: Pre-prompt example for English responses - Markdown\nDESCRIPTION: This is an example of a pre-prompt that can be inserted into the built-in prompt to instruct the LLM to respond in English. This modifies the built-in prompt to ensure responses are in the desired language.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/prompt-engineering/prompt-engineering-1/README.md#_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\nWhen answering the user:\n- If you don't know, just say that you don't know.\n- If you don't know or are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language English.\n```\n\n----------------------------------------\n\nTITLE: Customizing Squid Proxy Configuration\nDESCRIPTION: This snippet demonstrates customizing the Squid proxy configuration within `docker/volumes/ssrf_proxy/squid.conf` to restrict access to specific IP addresses. It defines an ACL to identify a restricted IP and then denies HTTP access to that IP.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/_book/zh_CN/learn-more/faq/install-faq.md#_snippet_8\n\nLANGUAGE: nginx\nCODE:\n```\nacl restricted_ip dst 192.168.101.19\nacl localnet src 192.168.101.0/24\n\nhttp_access deny restricted_ip\nhttp_access allow localnet\nhttp_access deny all\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository\nDESCRIPTION: This snippet demonstrates how to clone the Dify repository from GitHub and then start the services using Docker Compose. This is the recommended approach to resolve Nginx configuration mounting failures.  Prerequisites: Git, Docker, and Docker Compose.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/_book/zh_CN/learn-more/faq/install-faq.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\ncd dify/docker\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Restarting Dify Service (bash)\nDESCRIPTION: These commands restart the Dify service using Docker Compose to apply the environment variable configuration changes. It first brings the services down and then brings them up in detached mode. Requires docker and docker-compose.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ndocker compose down\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Adding Agent Strategies to Plugin Manifest (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to add Agent strategies to a plugin by including the `plugins.agent_strategies` field in the `manifest.yaml` file and defining the Agent provider. The example specifies the path to the agent configuration file (`provider/agent.yaml`).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/agent.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 0.0.2\ntype: plugin\nauthor: \"langgenius\"\nname: \"agent\"\nplugins:\n  agent_strategies:\n    - \"provider/agent.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Launching Dify with Docker Compose\nDESCRIPTION: These commands navigate to the Dify docker directory, copy the example environment file, and then launch Dify using Docker Compose in detached mode. This sets up the Dify application for local use.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/workshop/intermediate/twitter-chatflow.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd dify/docker\ncp .env.example .env\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Configure Agent Plugin for Memory with YAML\nDESCRIPTION: This snippet shows how to configure an Agent plugin to enable memory by adding the 'history-messages' feature to the plugin's YAML configuration file (strategies/agent.yaml). This allows the model to remember previous conversations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nidentity:\n  name: basic_agent  # Agent strategy name\n  author: novice     # Author\n  label:\n    en_US: BasicAgent  # English label\ndescription:\n  en_US: BasicAgent    # English description\nfeatures:\n  - history-messages   # Enable history messages feature\n...\n```\n\n----------------------------------------\n\nTITLE: Math Tutor Prompt Example\nDESCRIPTION: This prompt instructs the LLM to act as a math tutor, providing step-by-step solutions to math problems. It emphasizes the use of the explanation field for reasoning and the output field for equations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/how-to-use-json-schema-in-dify.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nYou are a helpful math tutor. You will be provided with a math problem,\nand your goal will be to output a step by step solution, along with a final answer.\nFor each step, just provide the output as an equation use the explanation field to detail the reasoning.\n```\n\n----------------------------------------\n\nTITLE: Chat Model Dialog Template USER Prompt - Dify\nDESCRIPTION: This is the USER prompt for a dialog-based chat application using a chat model in Dify. It takes the user's query as input. The 'Query' variable is meant to be replaced with the actual query from the user.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n{{Query}} //这里输入查询的变量\n```\n\n----------------------------------------\n\nTITLE: Downloading LLM and Embedding Models\nDESCRIPTION: This snippet downloads example LLM and Embedding models required for local deployment and inference with LocalAI. The models are stored in the `models` directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O models/bert\n$ wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j\n```\n\n----------------------------------------\n\nTITLE: Speech-to-Text Request Entry (Python)\nDESCRIPTION: Defines the entry point for requesting speech-to-text functionality using the Dify plugin session object.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.speech2text\n```\n\n----------------------------------------\n\nTITLE: Define PromptMessage Base Class in Python\nDESCRIPTION: This code defines the abstract base class `PromptMessage` using Pydantic's `BaseModel` and Python's `ABC`. It includes attributes for `role` (from PromptMessageRole), `content` (a string or list of PromptMessageContent objects), and an optional `name`. This base class serves as a template for different types of prompt messages (User, Assistant, System, Tool).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessage(ABC, BaseModel):\n    \"\"\"\n    Model class for prompt message.\n    \"\"\"\n    role: PromptMessageRole\n    content: Optional[str | list[PromptMessageContent]] = None  # Supports two types: string and content list. The content list is designed to meet the needs of multimodal inputs. For more details, see the PromptMessageContent explanation.\n    name: Optional[str] = None\n```\n\n----------------------------------------\n\nTITLE: Creating Link Messages in Python\nDESCRIPTION: This Python code snippet demonstrates the interface for creating link messages within the Dify tool plugin system. It accepts a link (URL) as input and returns a ToolInvokeMessage.  No external dependencies are explicitly required beyond the Dify plugin environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/tool.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef create_link_message(self, link: str) -> ToolInvokeMessage:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Configuring App ID and API Key in JavaScript\nDESCRIPTION: This snippet shows the configuration of the App ID and API Key in a JavaScript file. These values are essential for authenticating and connecting to the Dify application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-publishing/based-on-frontend-templates.md#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nexport const APP_ID = ''\nexport const API_KEY = ''\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack on Linux/MacOS\nDESCRIPTION: This shell script installs GPUStack as a service on systemd or launchd based systems. It downloads and executes the installation script from the GPUStack website.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | sh -s -\n```\n\n----------------------------------------\n\nTITLE: OpenAI Model Credential Schema (YAML)\nDESCRIPTION: This YAML snippet defines the model credential schema for OpenAI models, including a field for the fine-tuned model name and API key, organization ID and API base configurations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_credential_schema:\n  model: # ファインチューニングモデル名\n    label:\n      en_US: Model Name\n      zh_Hans: モデル名\n    placeholder:\n      en_US: Enter your model name\n      zh_Hans: モデル名を入力\n  credential_form_schemas:\n  - variable: openai_api_key\n    label:\n      en_US: API Key\n    type: secret-input\n    required: true\n    placeholder:\n      zh_Hans: APIキーを入力してください\n      en_US: Enter your API Key\n  - variable: openai_organization\n    label:\n        zh_Hans: 組織ID\n        en_US: Organization\n    type: text-input\n    required: false\n    placeholder:\n      zh_Hans: 組織IDを入力してください\n      en_US: Enter your Organization ID\n  - variable: openai_api_base\n    label:\n      zh_Hans: API Base\n      en_US: API Base\n    type: text-input\n    required: false\n    placeholder:\n      zh_Hans: API Baseを入力してください\n      en_US: Enter your API Base\n```\n\n----------------------------------------\n\nTITLE: Configure MCP SSE Plugin with Zapier MCP Server URL\nDESCRIPTION: This JSON configuration is used to authorize the MCP SSE plugin in Dify with the Zapier MCP Server URL. It specifies the server URL, headers, timeout, and SSE read timeout parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/best-practice/how-to-use-mcp-zapier.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"server_name\": {\n    \"url\": \"https://actions.zapier.com/mcp/*******/sse\",\n    \"headers\": {},\n    \"timeout\": 5,\n    \"sse_read_timeout\": 300\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Middleware (Bash)\nDESCRIPTION: These commands navigate to the docker directory, copy the middleware environment example file, and start the PostgreSQL, Redis, and Weaviate services using Docker Compose. It initializes the necessary infrastructure components for Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ncd docker\ncp middleware.env.example middleware.env\ndocker compose -f docker-compose.middleware.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Defining Model Name Credential Schema\nDESCRIPTION: Defines the 'model_name' as a text input credential for the Xinference vendor. It allows users to input the name of the model they want to use. This credential is required for identifying the specific model to be used.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/customizable-model.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n  - variable: model_name\n    type: text-input\n    label:\n      en_US: Model name\n      zh_Hans: 模型名称\n    required: true\n    placeholder:\n      zh_Hans: 填写模型名称\n      en_US: Input model name\n```\n\n----------------------------------------\n\nTITLE: Check Dify Plugin CLI Installation - Bash\nDESCRIPTION: This command verifies the installation of the Dify plugin CLI tool by printing its version number. It executes the downloaded Dify plugin CLI, which then outputs the installed version. This confirms that the CLI is correctly installed and executable.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/initialize-development-tools.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./dify-plugin-darwin-arm64 version\n```\n\n----------------------------------------\n\nTITLE: PromptMessageContentType Enum Definition in Python\nDESCRIPTION: Defines the PromptMessageContentType enumeration, specifying the possible content types for a prompt message: TEXT and IMAGE. This enumeration is used to distinguish between text-based and image-based content within a prompt message.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContentType(Enum):\n    \"\"\"\n    Enum class for prompt message content type.\n    \"\"\"\n    TEXT = 'text'\n    IMAGE = 'image'\n```\n\n----------------------------------------\n\nTITLE: Workflow MCP Agent Strategy Configuration JSON\nDESCRIPTION: JSON configuration for the MCP Agent Strategy plugin in Workflow, specifying the Zapier MCP Server URL. This allows the Workflow Agent to use the Zapier MCP service according to the Prompt instructions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/best-practice/how-to-use-mcp-zapier.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"server_name\": {\n    \"url\": \"https://actions.zapier.com/mcp/*******/sse\",\n    \"headers\": {},\n    \"timeout\": 5,\n    \"sse_read_timeout\": 300\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Text Prompt Message Content Class Definition\nDESCRIPTION: This code snippet defines the class for text-based prompt message content, inheriting from PromptMessageContent. It sets the type to TEXT and is used to structure text within a content list.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass TextPromptMessageContent(PromptMessageContent):\n    \"\"\"\n    テキストプロンプトメッセージのコンテンツを定義するモデルクラスです。\n    \"\"\"\n    type: PromptMessageContentType = PromptMessageContentType.TEXT\n```\n\n----------------------------------------\n\nTITLE: Extract Plugins - Bash\nDESCRIPTION: This command executes a Flask application within the docker-api container to extract plugin information. It requires Poetry to be installed within the container. The `workers` parameter specifies the number of parallel processes. Output is a `plugins.jsonl` file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/migration/migrate-to-v1.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask extract-plugins --workers=20\n```\n\n----------------------------------------\n\nTITLE: Rerank Invoke Endpoint Definition Python\nDESCRIPTION: This snippet defines the `invoke` endpoint for Rerank. It takes a `model_config` of type `RerankModelConfig`, a list of documents (`docs`), and a query (`query`). It returns a `RerankResult`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self, model_config: RerankModelConfig, docs: list[str], query: str\n) -> RerankResult:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Inspect Docker Containers for IP Addresses\nDESCRIPTION: This command retrieves the IP addresses of the running Docker containers. It's used to resolve 502 Bad Gateway errors by finding the correct IP addresses for the web and api services. Requires Docker.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n`docker ps -q | xargs -n 1 docker inspect --format '{{ .Name }}: {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'`\n```\n\n----------------------------------------\n\nTITLE: Adding a Marketplace plugin dependency to a Dify Bundle\nDESCRIPTION: This command adds a plugin dependency from the Dify Marketplace to the current bundle project. The --marketplace_pattern parameter specifies the plugin to include, using the format organization-name/plugin-name:version. The dot (.) indicates the current directory as the target bundle project.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/bundle.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify-plugin bundle append marketplace . --marketplace_pattern=langgenius/openai:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Initialization Page with IP and Port - Bash\nDESCRIPTION: This snippet shows how to access the Dify administrator initialization page when accessing through IP and Port. Replace `your_server_ip` with the actual IP address and ensure the port (8088 in this example) matches the configured port during installation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/bt-panel.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# If you choose to access through `IP+Port`\nhttp://your_server_ip:8088/install\n```\n\n----------------------------------------\n\nTITLE: Restarting Dify Services (Bash)\nDESCRIPTION: These commands are used to restart the Dify services after applying configuration changes, such as updating environment variables through a Docker Compose override file. It stops and then restarts the services in detached mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ndocker compose down\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Getting Chunks from a Document using Dify API\nDESCRIPTION: Retrieves the chunks (segments) associated with a specific document within a dataset in the Dify knowledge base.  It requires the dataset ID and document ID as path parameters and a valid API key in the Authorization header.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments' \\\n--header 'Authorization: Bearer {api_key}' \\\n--header 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Define SystemPromptMessage Class in Python\nDESCRIPTION: This code defines the `SystemPromptMessage` class, inheriting from `PromptMessage`. It sets the `role` attribute to `PromptMessageRole.SYSTEM`, signifying that this message contains system-level instructions or information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nclass SystemPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for system prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.SYSTEM\n```\n\n----------------------------------------\n\nTITLE: Finding Container IP Addresses\nDESCRIPTION: This command helps to find the IP addresses of the running Docker containers for Dify components (web and api). This is used for resolving 502 Bad Gateway errors by properly configuring Nginx.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/_book/zh_CN/learn-more/faq/install-faq.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker ps -q | xargs -n 1 docker inspect --format '{{ .Name }}: {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'\n```\n\n----------------------------------------\n\nTITLE: Install system dependencies on Ubuntu/Debian\nDESCRIPTION: Installs required system packages such as pkg-config, gcc, libseccomp-dev, git, and wget on Ubuntu/Debian-based Linux distributions. These dependencies are necessary for compiling and building DifySandbox.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/backend/sandbox/README.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get install pkg-config gcc libseccomp-dev git wget\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Tool Class Definition\nDESCRIPTION: This code snippet defines the PromptMessageTool class, representing a tool that can be used in a prompt. It contains the tool's name, description, and parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageTool(BaseModel):\n    \"\"\"\n    Model class for prompt message tool.\n    \"\"\"\n    name: str  # ツール名\n    description: str  # ツールの説明\n    parameters: dict  # ツールパラメータ（辞書形式）\n```\n\n----------------------------------------\n\nTITLE: Install FFmpeg on CentOS\nDESCRIPTION: Installs FFmpeg on a CentOS system. FFmpeg is required for OpenAI TTS to work properly due to audio stream segmentation.  This involves enabling the EPEL repository, updating yum packages, and installing ffmpeg and ffmpeg-devel.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum install epel-release\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo rpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-5.el7.nux.noarch.rpm\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum update\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum install ffmpeg ffmpeg-devel\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose\nDESCRIPTION: This command starts the Docker containers defined in the `docker-compose.yaml` file in detached mode (-d), meaning they will run in the background. It sets up and runs the SearXNG, Redis and Caddy services as configured.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Embedding Dify AI Customer Service using Script Tag\nDESCRIPTION: This snippet shows how to embed the Dify AI customer service application into a website by using the script tag method. The script code needs to be copied from Dify and pasted into the `<head>` or `<body>` tags of the website.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/create-an-ai-chatbot-with-business-data-in-minutes.md#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<script>/*Your script code here*/</script>\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Debugging Environment Variables\nDESCRIPTION: This snippet shows the environment variables needed for setting up remote debugging for a Dify plugin. It defines the installation method, remote host, port, and key required to connect to the Dify platform for debugging.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: Download LLM and Embedding Models\nDESCRIPTION: Downloads pre-trained LLM and embedding models from Hugging Face and gpt4all.io to be used with LocalAI. These models are small and platform-independent.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/localai.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O models/bert\n$ wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j\n```\n\n----------------------------------------\n\nTITLE: Cloning the Dify Documentation Repository\nDESCRIPTION: This command clones the Dify documentation repository from GitHub to your local machine, allowing you to make changes and contribute to the project. Replace `<your-github-account>` with your GitHub username. This is a prerequisite for adding new documentation files.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/community/docs-contribution.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/<your-github-account>/dify-docs.git\n```\n\n----------------------------------------\n\nTITLE: Text Embedding Result Class Definition\nDESCRIPTION: This code snippet defines the TextEmbeddingResult class, representing the result of a text embedding operation. It contains the model name, a list of embedding vectors, and usage information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/model/model-schema.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nclass TextEmbeddingResult(BaseModel):\n    \"\"\"\n    Model class for text embedding result.\n    \"\"\"\n    model: str  # 使用モデル\n    embeddings: list[list[float]]  # 埋め込みベクトルリスト（テキストに対応）\n    usage: EmbeddingUsage  # 使用情報\n```\n\n----------------------------------------\n\nTITLE: Delete Knowledge Base Metadata Field - Dify API - Bash\nDESCRIPTION: This snippet shows how to delete a metadata field from a Dify knowledge base using a DELETE request. It requires the dataset ID and metadata ID. The request includes a header for authorization (API key).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata/{metadata_id}' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: TextEmbeddingResult Class Definition in Python\nDESCRIPTION: Defines the TextEmbeddingResult class which encapsulates the result of text embedding model execution.  It includes the model used, list of embeddings, and usage information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nclass TextEmbeddingResult(BaseModel):\n    \"\"\"\n    Model class for text embedding result.\n    \"\"\"\n    model: str  # 実際に使用したモデル\n    embeddings: list[list[float]]  # テキストリストに対応するembeddingベクトルのリスト\n    usage: EmbeddingUsage  # 使用した情報\n```\n\n----------------------------------------\n\nTITLE: Signing a Plugin in Dify (Bash)\nDESCRIPTION: This command signs a Dify plugin file using a private key. The command takes the plugin file (`.difypkg`) and the private key (`.pem`) as input. A new signed plugin file is created with `signed` appended to its name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify signature sign your_plugin_project.difypkg -p your_key_pair.private.pem\n```\n\n----------------------------------------\n\nTITLE: Text Embedding Invoke Endpoint Definition Python\nDESCRIPTION: This snippet defines the `invoke` endpoint for text embedding. It takes a `model_config` of type `TextEmbeddingResult` and a list of texts (`texts`) to embed.  It returns a `TextEmbeddingResult`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self, model_config: TextEmbeddingResult, texts: list[str]\n) -> TextEmbeddingResult:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Enum Example in JSON Schema\nDESCRIPTION: This JSON schema snippet demonstrates how to use the `enum` keyword to restrict the possible values of a string field to a predefined set of options (red, green, blue). This is used in the LLM node to constrain output.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/llm.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"string\",\n  \"enum\": [\"red\", \"green\", \"blue\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Directory Structure for Custom Tool (Python)\nDESCRIPTION: This snippet shows the recommended directory structure for creating a custom external data tool in Dify. It initializes the necessary files for the 'Weather Search' tool within the `api/core/external_data_tool` directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/code-based-extension/external-data-tool.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.\\\n└── api\n    └── core\n        └── external_data_tool\n            └── weather_search\n                ├── __init__.py\n                ├── weather_search.py\n                └── schema.json\n```\n\n----------------------------------------\n\nTITLE: Starting Dify with Docker Compose\nDESCRIPTION: These commands navigate to the Docker directory within the Dify repository, copy the example environment file, and then start the Dify application using Docker Compose in detached mode. This sets up the Dify environment locally.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncd dify/docker\ncp .env.example .env\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Embedding Dify Chatbot iFrame Code\nDESCRIPTION: This code snippet demonstrates how to embed a Dify Chatbot into a website using an iframe. The `src` attribute specifies the URL of the chatbot. The `style` attribute sets the width, height, and minimum height of the iframe. The `frameborder` attribute removes the border around the iframe. The `allow` attribute enables microphone access.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/how-to-integrate-dify-chatbot-to-your-wix-website.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n<iframe src=\"https://udify.app/chatbot/1yS3gohroW1sKyLc\" style=\"width: 100%; height: 100%; min-height: 700px\" frameborder=\"0\" allow=\"microphone\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Workflow Configuration\nDESCRIPTION: This YAML configuration defines a GitHub Actions workflow that automates the process of packaging, pushing, and creating a pull request for Dify plugins when changes are pushed to the main branch. It involves setting up the environment, downloading necessary tools, extracting plugin information, packaging the plugin, pushing the changes to a forked repository, and creating a pull request to the main repository.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/plugin-auto-publish-pr.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n# .github/workflows/auto-pr.yml\nname: Auto Create PR on Main Push\n\non:\n  push:\n    branches: [ main ]  # Trigger on push to main\n\njobs:\n  create_pr: # Renamed job for clarity\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Print working directory # Kept for debugging\n        run: |\n          pwd\n          ls -la\n\n      - name: Download CLI tool\n        run: |\n          # Create bin directory in runner temp\n          mkdir -p $RUNNER_TEMP/bin\n          cd $RUNNER_TEMP/bin\n\n          # Download CLI tool\n          wget https://github.com/langgenius/dify-plugin-daemon/releases/download/0.0.6/dify-plugin-linux-amd64\n          chmod +x dify-plugin-linux-amd64\n\n          # Show download location and file\n          echo \"CLI tool location:\"\n          pwd\n          ls -la dify-plugin-linux-amd64\n\n      - name: Get basic info from manifest # Changed step name and content\n        id: get_basic_info\n        run: |\n          PLUGIN_NAME=$(grep \"^name:\" manifest.yaml | cut -d' ' -f2)\n          echo \"Plugin name: $PLUGIN_NAME\"\n          echo \"plugin_name=$PLUGIN_NAME\" >> $GITHUB_OUTPUT\n\n          VERSION=$(grep \"^version:\" manifest.yaml | cut -d' ' -f2)\n          echo \"Plugin version: $VERSION\"\n          echo \"version=$VERSION\" >> $GITHUB_OUTPUT\n\n          # If the author's name is not your github username, you can change the author here\n          AUTHOR=$(grep \"^author:\" manifest.yaml | cut -d' ' -f2)\n          echo \"Plugin author: $AUTHOR\"\n          echo \"author=$AUTHOR\" >> $GITHUB_OUTPUT\n\n      - name: Package Plugin\n        id: package\n        run: |\n          # Use the downloaded CLI tool to package\n          cd $GITHUB_WORKSPACE\n          # Use variables for package name\n          PACKAGE_NAME=\"${{ steps.get_basic_info.outputs.plugin_name }}-${{ steps.get_basic_info.outputs.version }}.difypkg\"\n          # Use CLI from runner temp\n          $RUNNER_TEMP/bin/dify-plugin-linux-amd64 plugin package . -o \"$PACKAGE_NAME\"\n\n          # Show packaging result\n          echo \"Package result:\"\n          ls -la \"$PACKAGE_NAME\"\n          echo \"package_name=$PACKAGE_NAME\" >> $GITHUB_OUTPUT\n\n          # Show full file path and directory structure (kept for debugging)\n          echo \"\\\\\\nFull file path:\"\n          pwd\n          echo \"\\\\\\nDirectory structure:\"\n          tree || ls -R\n\n      - name: Checkout target repo\n        uses: actions/checkout@v3\n        with:\n          # Use author variable for repository\n          repository: ${{steps.get_basic_info.outputs.author}}/dify-plugins\n          path: dify-plugins\n          token: ${{ secrets.PLUGIN_ACTION }}\n          fetch-depth: 1 # Fetch only the last commit to speed up checkout\n          persist-credentials: true # Persist credentials for subsequent git operations\n\n      - name: Prepare and create PR\n        run: |\n          # Debug info (kept)\n          echo \"Debug: Current directory $(pwd)\"\n          # Use variable for package name\n          PACKAGE_NAME=\"${{ steps.get_basic_info.outputs.plugin_name }}-${{ steps.get_basic_info.outputs.version }}.difypkg\"\n          echo \"Debug: Package name: $PACKAGE_NAME\"\n          ls -la\n\n          # Move the packaged file to the target directory using variables\n          mkdir -p dify-plugins/${{ steps.get_basic_info.outputs.author }}/${{ steps.get_basic_info.outputs.plugin_name }}\n          mv \"$PACKAGE_NAME\" dify-plugins/${{ steps.get_basic_info.outputs.author }}/${{ steps.get_basic_info.outputs.plugin_name }}/\n\n          # Enter the target repository directory\n          cd dify-plugins\n\n          # Configure git\n          git config user.name \"GitHub Actions\"\n          git config user.email \"actions@github.com\"\n\n          # Ensure we are on the latest main branch\n          git fetch origin main\n          git checkout main\n          git pull origin main\n\n          # Create and switch to a new branch using variables and new naming convention\n          BRANCH_NAME=\"bump-${{ steps.get_basic_info.outputs.plugin_name }}-plugin-${{ steps.get_basic_info.outputs.version }}\"\n          git checkout -b \"$BRANCH_NAME\"\n\n          # Add and commit changes (using git add .)\n          git add .\n          git status # for debugging\n          # Use variables in commit message\n          git commit -m \"bump ${{ steps.get_basic_info.outputs.plugin_name }} plugin to version ${{ steps.get_basic_info.outputs.version }}\"\n\n          # Push to remote (use force just in case the branch existed before from a failed run)\n          git push -u origin \"$BRANCH_NAME\" --force\n\n          # Confirm branch has been pushed and wait for sync (GitHub API might need a moment)\n          git branch -a\n          echo \"Waiting for branch to sync...\"\n          sleep 10  # Wait 10 seconds for branch sync\n\n      - name: Create PR via GitHub API\n        env:\n          GH_TOKEN: ${{ secrets.PLUGIN_ACTION }} # Use the provided token for authentication\n        run: |\n          gh pr create \\\\\n            --repo langgenius/dify-plugins \\\\\n            --head \"${{ steps.get_basic_info.outputs.author }}:${{ steps.get_basic_info.outputs.plugin_name }}-${{ steps.get_basic_info.outputs.version }}\" \\\\\n            --base main \\\\\n            --title \"bump ${{ steps.get_basic_info.outputs.plugin_name }} plugin to version ${{ steps.get_basic_info.outputs.version }}\" \\\\\n            --body \"bump ${{ steps.get_basic_info.outputs.plugin_name }} plugin package to version ${{ steps.get_basic_info.outputs.version }}\n\n            Changes:\n            - Updated plugin package file\" || echo \"PR already exists or creation skipped.\" # Handle cases where PR already exists\n\n      - name: Print environment info # Kept for debugging\n        run: |\n          echo \"GITHUB_WORKSPACE: $GITHUB_WORKSPACE\"\n          echo \"Current directory contents:\"\n          ls -R\n```\n\n----------------------------------------\n\nTITLE: Uploading Private Key File to Cloud Storage\nDESCRIPTION: These snippets demonstrate how to upload the private key file and local files to cloud storage.  This is needed to migrate from local storage to cloud storage.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/_book/zh_CN/learn-more/faq/install-faq.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nflask upload-private-key-file-to-cloud-storage\nflask upload-local-files-to-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack on Windows\nDESCRIPTION: This PowerShell command installs GPUStack on Windows. It downloads the installation script from the GPUStack website and executes it using Invoke-Expression. It requires running PowerShell as an administrator.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_2\n\nLANGUAGE: powershell\nCODE:\n```\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n```\n\n----------------------------------------\n\nTITLE: Example External Data Tool Response (JSON)\nDESCRIPTION: Shows an example API response for the external data tool query, containing weather information for London. The response contains the temperature, wind speed, and air quality.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/README.md#_snippet_6\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"result\": \"City: London\\nTemperature: 10°C\\nRealFeel®: 8°C\\nAir Quality: Poor\\nWind Direction: ENE\\nWind Speed: 8 km/h\\nWind Gusts: 14 km/h\\nPrecipitation: Light rain\"\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Docker Container IP Addresses\nDESCRIPTION: Displays the IP addresses of running Docker containers. Uses `docker ps` to list containers and `docker inspect` to retrieve their IP addresses.  Helpful for debugging network connectivity issues.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker ps -q | xargs -n 1 docker inspect --format '{{ .Name }}: {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'\n```\n\n----------------------------------------\n\nTITLE: Cloning the repository using Git\nDESCRIPTION: This command clones the dify-twilio-whatsapp repository from GitHub. This repository contains the necessary code for building the WhatsApp chatbot using Dify and Twilio. The repository includes the FastAPI application code and related configuration files.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/dify-on-whatsapp.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/somethingwentwell/dify-twilio-whatsapp\n```\n\n----------------------------------------\n\nTITLE: Starting the Worker Service (Windows)\nDESCRIPTION: This command starts the Celery worker service for Windows, which consumes asynchronous tasks. It uses the `solo` pool and disables gossip and mingle features.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run celery -A app.celery worker -P solo --without-gossip --without-mingle -Q dataset,generation,mail,ops_trace --loglevel INFO\n```\n\n----------------------------------------\n\nTITLE: Define PromptMessageTool Model\nDESCRIPTION: Defines the `PromptMessageTool` class, which represents a tool that can be used in a prompt message. It includes the tool's name, description, and parameters.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageTool(BaseModel):\n    \"\"\"\n    Model class for prompt message tool.\n    \"\"\"\n    name: str\n    description: str\n    parameters: dict\n```\n\n----------------------------------------\n\nTITLE: Loop Termination Condition Example\nDESCRIPTION: This example shows possible configurations for the loop termination condition.  It defines the condition that must be met to stop the loop. The examples include checking if a variable 'x' is less than 50 and checking an error rate.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/loop.md#_snippet_0\n\nLANGUAGE: N/A\nCODE:\n```\nx < 50\nerror_rate < 0.01\n```\n\n----------------------------------------\n\nTITLE: Generating Key Pair for Signing in Dify (Bash)\nDESCRIPTION: This command generates a new key pair (private and public keys) used for signing and verifying plugin signatures in Dify. The `-f` flag specifies the base filename for the generated keys.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndify signature generate -f your_key_pair\n```\n\n----------------------------------------\n\nTITLE: Install Python Dependencies with Poetry\nDESCRIPTION: Installs the Python dependencies for the Dify API service using Poetry. This command reads the `poetry.lock` and `pyproject.toml` files to determine the required dependencies. Requires Poetry to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_42\n\nLANGUAGE: Bash\nCODE:\n```\npoetry install\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository\nDESCRIPTION: This snippet shows how to clone the forked Dify repository from GitHub using the git command line tool. Replace `<github_username>` with your GitHub username to clone your forked repository.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone git@github.com:<github_username>/dify.git\n```\n\n----------------------------------------\n\nTITLE: Example API Response with Weather Data\nDESCRIPTION: Provides an example API response containing weather data for London, including temperature, RealFeel, air quality, wind direction, wind speed, wind gusts, and precipitation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/external-data-tool.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n        \"result\": \"City: London\\nTemperature: 10°C\\nRealFeel®: 8°C\\nAir Quality: Poor\\nWind Direction: ENE\\nWind Speed: 8 km/h\\nWind Gusts: 14 km/h\\nPrecipitation: Light rain\"\n    }\n```\n\n----------------------------------------\n\nTITLE: User Prompt Template for Chat Models (Conversational/Text Generation Apps)\nDESCRIPTION: This is the user prompt template used with chat models for conversational and text generation applications. It defines where the user's query is inserted into the prompt. The `{{Query}}` placeholder is where user input is placed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/prompt-engineering/prompt-template.md#_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n{{Query}} //Enter the Query variables here.\n```\n\n----------------------------------------\n\nTITLE: Configuring Git User in Bash\nDESCRIPTION: This snippet configures the global Git username and email. This is usually only needed the first time Git is used on a system and is essential for proper attribution of commits. It requires Git to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n```\n\n----------------------------------------\n\nTITLE: Migrating Vector Database (Flask Command)\nDESCRIPTION: Migrates vector data from one vector database to another.  Uses a flask command, either executed directly or within the docker-api container. Requires the VECTOR_STORE environment variable to be configured.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nflask vdb-migrate # or docker exec -it docker-api-1 flask vdb-migrate\n```\n\n----------------------------------------\n\nTITLE: Speech-to-Text Invoke Endpoint (Python)\nDESCRIPTION: This code snippet presents the invoke method definition for Speech-to-Text conversion. It takes a `Speech2TextModelConfig` and an audio file (mp3 encoded) and returns the transcribed text.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self, model_config: Speech2TextModelConfig, file: IO[bytes]\n) -> str:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Run Llama3.2 with Ollama\nDESCRIPTION: This command runs the Llama3.2 model using the Ollama framework. It assumes that Ollama is installed and configured correctly. Upon successful execution, Ollama starts an API service on the local port 11434.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/ollama.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama run llama3.2\n```\n\n----------------------------------------\n\nTITLE: Check for Tool Calls in LLM Result\nDESCRIPTION: Checks if a given LLM result chunk contains tool calls. This is done by inspecting the `tool_calls` attribute of the message within the chunk.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n    def check_tool_calls(self, llm_result_chunk: LLMResultChunk) -> bool:\n        \"\"\"\n        Check if there is any tool call in llm result chunk\n        \"\"\"\n        return bool(llm_result_chunk.delta.message.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Cloning the Dify Repository\nDESCRIPTION: Clones the Dify repository from GitHub. This provides the source code necessary for local deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_17\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\n----------------------------------------\n\nTITLE: Tool Provider YAML Configuration\nDESCRIPTION: This YAML file configures the tool provider with basic information such as author, name, labels, description, icon, and tags.  It's the foundation for providing necessary authorization information to the tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nidentity: # 工具供应商的基本信息\n  author: Your-name # 作者\n  name: google # 名称，唯一，不允许和其他供应商重名\n  label: # 标签，用于前端展示\n    en_US: Google # 英文标签\n    zh_Hans: Google # 中文标签\n  description: # 描述，用于前端展示\n    en_US: Google # 英文描述\n    zh_Hans: Google # 中文描述\n  icon: icon.svg # 工具图标，需要放置在 _assets 文件夹下\n  tags: # 标签，用于前端展示\n    - search\n```\n\n----------------------------------------\n\nTITLE: Modifying PostgreSQL pg_hba.conf for Network Access\nDESCRIPTION: This command modifies the `pg_hba.conf` file within the `docker-db-1` container to allow connections from a specific network. It adds a line to trust connections from the 172.19.0.0/16 network.  After modification, docker-compose needs to be restarted.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/faq.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-db-1 sh -c \"echo 'host all all 172.19.0.0/16 trust' >> /var/lib/postgresql/data/pgdata/pg_hba.conf\"\ndocker-compose restart\n```\n\n----------------------------------------\n\nTITLE: Content Moderation Input Response Direct Output Example\nDESCRIPTION: Demonstrates the response format when the `action` is set to `direct_output`, indicating that the input content violates the moderation rules.  The `preset_response` provides a predefined message to display.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/moderation.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"flagged\": true,\n    \"action\": \"direct_output\",\n    \"preset_response\": \"Your content violates our usage policy.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking Text-to-Speech Model in Dify (Python)\nDESCRIPTION: This method invokes a text-to-speech model to synthesize speech from a given text. It takes the text content as input and returns an audio stream representing the synthesized speech. The `streaming` parameter indicates whether the output should be streamed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, content_text: str, streaming: bool, user: Optional[str] = None):\n    \"\"\"\n    Invoke large language model\n\n    :param model: model name\n    :param credentials: model credentials\n    :param content_text: text content to be translated\n    :param streaming: output is streaming\n    :param user: unique user id\n    :return: translated audio file\n    \"\"\"        \n```\n\n----------------------------------------\n\nTITLE: Starting LocalAI with Docker Compose\nDESCRIPTION: This snippet uses Docker Compose to start LocalAI in detached mode, building the necessary images if required.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# start with docker-compose\n$ docker-compose up -d --build\n```\n\n----------------------------------------\n\nTITLE: Clone and Start Dify with Docker Compose\nDESCRIPTION: Clones the Dify repository from GitHub and starts the application using Docker Compose. This is a solution to Nginx configuration file mount failures.  It assumes Git and Docker Compose are installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd dify/docker\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: TextPromptMessageContent Class Definition in Python\nDESCRIPTION: Defines the TextPromptMessageContent class, which inherits from PromptMessageContent. It specifies the type as TEXT.  This is used when passing text as part of the content list.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass TextPromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for text prompt message content.\n    \"\"\"\n    type: PromptMessageContentType = PromptMessageContentType.TEXT\n```\n\n----------------------------------------\n\nTITLE: Generating and Setting a Random Secret Key (API)\nDESCRIPTION: Generates a random secret key and replaces the existing SECRET_KEY value in the .env file. This is important for security purposes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_23\n\nLANGUAGE: Bash\nCODE:\n```\nawk -v key=\"$(openssl rand -base64 42)\" '/^SECRET_KEY=/ {sub(/=.*/, \"=\" key)} 1' .env > temp_env && mv temp_env .env\n```\n\n----------------------------------------\n\nTITLE: Adding Docker's GPG Key\nDESCRIPTION: This command adds Docker's GPG key to the system's keyring, allowing the system to verify the authenticity of packages from the Docker repository. This ensures the packages are from a trusted source.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n```\n\n----------------------------------------\n\nTITLE: Passing Inputs to Dify Chatbot\nDESCRIPTION: This JavaScript snippet demonstrates how to pass inputs to the Dify chatbot using the `inputs` configuration option. It shows how to provide initial values for variables used in the chatbot's logic.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-publishing/embedding-in-websites.md#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nwindow.difyChatbotConfig = {\n    // ... 其他配置\n    inputs: {\n        name: 'apple',\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Get Document Embedding Status - Dify API (cURL)\nDESCRIPTION: This cURL command retrieves the embedding status (progress) of a document in the Dify API. It requires the dataset ID, batch ID, and API key for authentication and authorization.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{batch}/indexing-status' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Install Python Dependencies (pip)\nDESCRIPTION: This command installs the required Python dependencies for the plugin, including `werkzeug`, `flask`, and `dify-plugin`. These libraries are essential for creating the web server and defining the plugin endpoint.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npip install werkzeug\npip install flask\npip install dify-plugin\n```\n\n----------------------------------------\n\nTITLE: Starting Worker Service (Windows)\nDESCRIPTION: Starts the worker service for consuming asynchronous queue tasks on Windows. It uses Celery with the solo execution pool due to limitations with gevent on Windows.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run celery -A app.celery worker -P solo --without-gossip --without-mingle -Q dataset,generation,mail,ops_trace --loglevel INFO\n```\n\n----------------------------------------\n\nTITLE: Text Embedding Invoke Endpoint (Python)\nDESCRIPTION: This code defines the signature of the `invoke` method for generating text embeddings.  It accepts a `model_config` and a list of texts. It returns a TextEmbeddingResult.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self, model_config: TextEmbeddingResult, texts: list[str]\n) -> TextEmbeddingResult:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Accessing History Messages in Python\nDESCRIPTION: This Python snippet demonstrates how to access history messages within the Agent strategy's `_invoke` method. The `history_prompt_messages` attribute of the `params.model` object contains the conversation history.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass BasicAgentAgentStrategy(AgentStrategy):\n    def _invoke(self, parameters: dict[str, Any]) -> Generator[AgentInvokeMessage]:\n        params = BasicParams(**parameters)\n        print(f\"history_messages: {params.model.history_prompt_messages}\")\n        ...\n```\n\n----------------------------------------\n\nTITLE: Packaging the Dify Plugin\nDESCRIPTION: This command packages the Dify plugin into a `.difypkg` file, using the dify plugin packaging tool. Replace `./slack_bot` with the actual path to the plugin project directory. This is an optional step for distributing the plugin.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Replace ./slack_bot with your actual plugin project path.\n\ndify plugin package ./slack_bot\n```\n\n----------------------------------------\n\nTITLE: LLM Node Prompt Example (JSON format)\nDESCRIPTION: This is an example prompt for an LLM node that instructs the model to output code in JSON format, either correct or incorrect, based on user requirements.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nYou are a teaching assistant. According to the user's requirements, you only output a correct or incorrect sample code in json format.\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Example\nDESCRIPTION: This JSON schema example defines the structure for a rating and comment. It specifies that the 'comment' field should be a string and the 'rating' field should be a number. It is used to define the structured output of the LLM node.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/llm.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n \"comment\": \"This is great!\",\n \"rating\": 5\n}\n```\n\n----------------------------------------\n\nTITLE: Text Embedding Result Class\nDESCRIPTION: Defines the `TextEmbeddingResult` class, representing the result of a text embedding operation. It contains the model used, the list of embeddings, and usage information.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nclass TextEmbeddingResult(BaseModel):\n    \"\"\"\n    Model class for text embedding result.\n    \"\"\"\n    model: str  # 实际使用模型\n    embeddings: list[list[float]]  # embedding 向量列表，对应传入的 texts 列表\n    usage: EmbeddingUsage  # 使用信息\n```\n\n----------------------------------------\n\nTITLE: Chat Model Text Generation ASSISTANT Prompt - Dify\nDESCRIPTION: This ASSISTANT prompt is used for the text generation application with a chat model in Dify. As with the dialog application, it is an empty string, allowing the LLM to generate the full text based on the SYSTEM and USER prompts.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n\"\" \n```\n\n----------------------------------------\n\nTITLE: Initializing Dify Plugin Project (Renamed Binary - Bash)\nDESCRIPTION: Initializes a new Dify plugin project using the CLI tool after renaming the binary to `dify` and adding it to `/usr/local/bin`. This command assumes the binary is in the system's PATH.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: Getting a Value from Storage by Key (Python)\nDESCRIPTION: This snippet shows the `get` method, which retrieves a value from the persistent storage based on the provided key. It takes a key (string) as input and returns the corresponding value as bytes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/persistent-storage.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get(self, key: str) -> bytes:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Installing FFmpeg with Homebrew on Mac OS X\nDESCRIPTION: This command installs FFmpeg on macOS using Homebrew, a package manager.  It uses `brew` to install the `ffmpeg` package. Homebrew needs to be installed prior to running this command.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nbrew install ffmpeg\n```\n\n----------------------------------------\n\nTITLE: Setting executable permissions for dify-plugin-darwin-arm64 on macOS\nDESCRIPTION: This command grants execute permissions to the downloaded Dify plugin scaffolding tool binary on macOS, specifically for systems with M-series chips. This is necessary to run the tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/initialize-development-tools.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nchmod +x dify-plugin-darwin-arm64\n```\n\n----------------------------------------\n\nTITLE: Install FFmpeg on Mac OS X\nDESCRIPTION: Installs FFmpeg on a Mac OS X system using Homebrew. FFmpeg is required for OpenAI TTS to work properly due to audio stream segmentation. It assumes Homebrew is already installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n```\n\nLANGUAGE: bash\nCODE:\n```\nbrew install ffmpeg\n```\n\n----------------------------------------\n\nTITLE: Summary Invoke Endpoint (Python)\nDESCRIPTION: This code snippet shows the invoke method definition for text summarization. It accepts the text to be summarized and optional instructions for stylizing the summary, returning the summarized text.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self, text: str, instruction: str,\n) -> str:\n```\n\n----------------------------------------\n\nTITLE: Migrate data for plugins\nDESCRIPTION: This command migrates data to update provider names and ensure that the plugins function correctly within the new version. It addresses changes in the plugin structure and data format.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/migration/migrate-to-v1.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask migrate-data-for-plugin\n```\n\n----------------------------------------\n\nTITLE: Celery Broker URL (Redis Sentinel)\nDESCRIPTION: Specifies the URL for the Celery broker when using Redis Sentinel. It consists of the protocol (sentinel), credentials (username and password), host, port, and database number.  Multiple sentinel addresses are separated by semicolons.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/environments.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nsentinel://<sentinel_username>:<sentinel_password>@<sentinel_host>:<sentinel_port>/<redis_database>\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository with Git\nDESCRIPTION: This command clones the Dify repository from GitHub. It downloads the entire source code and history of the project to your local machine. This is the first step in deploying Dify locally.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\n----------------------------------------\n\nTITLE: Dify Frontend Directory Structure\nDESCRIPTION: This code snippet outlines the directory structure of the Dify frontend, built with Typescript and Next.js. It highlights key folders like app (layouts, pages, components), assets (static files), config (adjustable settings), and models (data models for API responses).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/community/contribution.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n[web/]\n├── app                   //layouts, pages, and components\n│   ├── (commonLayout)    //common layout used throughout the app\n│   ├── (shareLayout)     //layouts specifically shared across token-specific sessions \n│   ├── activate          //activate page\n│   ├── components        //shared by pages and layouts\n│   ├── install           //install page\n│   ├── signin            //signin page\n│   └── styles            //globally shared styles\n├── assets                // Static assets\n├── bin                   //scripts ran at build step\n├── config                //adjustable settings and options \n├── context               //shared contexts used by different portions of the app\n├── dictionaries          // Language-specific translate files \n├── docker                //container configurations\n├── hooks                 // Reusable hooks\n├── i18n                  // Internationalization configuration\n├── models                //describes data models & shapes of API responses\n├── public                //meta assets like favicon\n├── service               //specifies shapes of API actions\n├── test                  \n├── types                 //descriptions of function params and return values\n└── utils                 // Shared utility functions\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Main Page\nDESCRIPTION: These commands provide the URLs to access the Dify main page after the installation, either using a domain name or IP address and port number. It allows a user to access the Dify application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/bt-panel.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# 使用域名\nhttp://yourdomain/\n\n# 使用IP+端口\n\nhttp://your_server_ip:8088/\n```\n\n----------------------------------------\n\nTITLE: Applying Environment Variable Changes\nDESCRIPTION: These commands apply changes made to the environment variables after editing the `.env` file. It stops the existing containers and restarts them to use the new configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Tool Class\nDESCRIPTION: Defines the `PromptMessageTool` class, representing a tool available for the model to use. It includes the tool's `name`, `description`, and `parameters`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageTool(BaseModel):\n    \"\"\"\n    Model class for prompt message tool.\n    \"\"\"\n    name: str  # 工具名称\n    description: str  # 工具描述\n    parameters: dict  # 工具参数 dict\n```\n\n----------------------------------------\n\nTITLE: Verifying Ollama Installation using Version Check\nDESCRIPTION: This command is used to check the installed version of the Ollama client. It verifies that Ollama has been successfully installed and provides the version number as output.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama -v\n```\n\n----------------------------------------\n\nTITLE: API Response Structure (JSON)\nDESCRIPTION: Illustrates the basic structure of the API response, indicating that the content varies based on the specific module and extension point. The actual content is determined by the specific module's design.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/README.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    ...  // For the content returned by the API, see the specific module's design specifications for different extension points.\n}\n```\n\n----------------------------------------\n\nTITLE: Install GPUStack on Linux/MacOS (Bash)\nDESCRIPTION: This command downloads and executes a script to install GPUStack as a service on systemd or launchd based systems. It simplifies the installation process by automating the necessary steps.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/gpustack.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | sh -s -\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Installer with IP and Port - Bash\nDESCRIPTION: This code snippet shows how to access the Dify installer page when using IP address and port. Replace `your_server_ip` with the server's IP address and `8088` with the configured port.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/bt-panel.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# `IP+ポート`を介してアクセスする場合\nhttp://your_server_ip:8088/install\n```\n\n----------------------------------------\n\nTITLE: Backup Docker Volumes\nDESCRIPTION: These commands stop the Docker services and create a compressed archive of the Docker volumes directory. This is essential for backing up application data before an upgrade.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/dify-premium.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\ntar -cvf volumes-$(date +%s).tgz volumes\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository with Git\nDESCRIPTION: This command clones the Dify repository from GitHub using `git clone`. The `--branch` option specifies the version to clone.  Replace `0.15.3` with the desired version number.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git --branch 0.15.3\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables\nDESCRIPTION: These environment variables are used to configure the plugin for remote debugging with the Dify platform. The variables include the installation method, remote host, port, and key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: Define RerankDocument Model\nDESCRIPTION: Defines the `RerankDocument` class, which represents a reranked document. It includes the original index, the text, and the score.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nclass RerankDocument(BaseModel):\n    \"\"\"\n    Model class for rerank document.\n    \"\"\"\n    index: int  # original index\n    text: str\n    score: float\n```\n\n----------------------------------------\n\nTITLE: Dify SearXNG Base URL Configuration\nDESCRIPTION: This is the format for the Base URL that Dify requires to connect to the self-hosted SearXNG instance. The `<your-linux-vm-ip>` should be replaced with the actual public IP address of the Linux VM where SearXNG is running.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nhttp://<your-linux-vm-ip>:8081\n```\n\n----------------------------------------\n\nTITLE: RerankResult Class Definition in Python\nDESCRIPTION: Defines the `RerankResult` class as a Pydantic BaseModel representing the result of a reranking operation. It includes the model name and a list of reranked documents.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nclass RerankResult(BaseModel):\n    \"\"\"\n    Model class for rerank result.\n    \"\"\"\n    model: str  # 实际使用模型\n    docs: list[RerankDocument]  # 重排后的分段列表        \n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Debugging\nDESCRIPTION: This snippet shows how to configure the `.env` file for remote debugging of a Dify plugin. It includes setting the installation method, remote host, port, and key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: Configuring App Info in TypeScript\nDESCRIPTION: This snippet shows the configuration of the app info in TypeScript.  It includes the title, description, copyright, privacy policy and default language.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/application-publishing/based-on-frontend-templates.md#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nexport const APP_INFO: AppInfo = {\n  \"title\": 'Chat APP',\n  \"description\": '',\n  \"copyright\": '',\n  \"privacy_policy\": '',\n  \"default_language\": 'zh-Hans'\n}\n\nexport const isShowPrompt = true\nexport const promptTemplate = ''\n```\n\n----------------------------------------\n\nTITLE: Starting Stable Diffusion WebUI on Linux (Bash)\nDESCRIPTION: These commands navigate to the Stable Diffusion WebUI directory and start the WebUI with API and listen flags enabled. These flags allow external applications like Dify to interact with the WebUI.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/stable-diffusion.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd stable-diffusion-webui\n./webui.sh --api --listen\n```\n\n----------------------------------------\n\nTITLE: Accessing Parameter Extractor Node in Dify\nDESCRIPTION: This snippet shows the entry point for accessing the ParameterExtractor node within a Dify plugin session. The session object provides access to workflow nodes, including the parameter extractor.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.session.workflow_node.parameter_extractor\n```\n\n----------------------------------------\n\nTITLE: Starting Xinference Locally\nDESCRIPTION: This command starts the Xinference server locally. It will output the endpoint where the server is accessible. The default port is 9997, and access is limited to the local machine by default.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/models-integration/xinference.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ xinference-local\n```\n\n----------------------------------------\n\nTITLE: Backup docker-compose YAML file\nDESCRIPTION: This command backs up the existing docker-compose.yaml file to preserve the current configuration before upgrading. It uses the `date +%s` command to create a timestamped backup file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/migration/migrate-to-v1.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ncp docker-compose.yaml docker-compose.yaml.$(date +%s).bak\n```\n\n----------------------------------------\n\nTITLE: Run FastAPI App\nDESCRIPTION: This command starts the FastAPI application using Uvicorn. It specifies the main module, app instance, enables hot reloading, and binds the server to all available network interfaces.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nuvicorn main:app --reload --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Migrate Data for Plugins - Bash\nDESCRIPTION: This command executes a Flask application within the docker-api container to migrate the plugin data, updating the provider name. It requires Poetry to be installed within the container.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/migration/migrate-to-v1.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask migrate-data-for-plugin\n```\n\n----------------------------------------\n\nTITLE: Celery Broker URL (Redis Sentinel)\nDESCRIPTION: This code snippet illustrates the format of the Celery Broker URL when using Redis Sentinel for high availability. It includes the Sentinel username, password, host, port, and Redis database number. Configure these variables according to your Sentinel setup.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/environments.md#_snippet_1\n\nLANGUAGE: none\nCODE:\n```\nsentinel://<sentinel_username>:<sentinel_password>@<sentinel_host>:<sentinel_port>/<redis_database>\n```\n\n----------------------------------------\n\nTITLE: Initializing a new Dify Bundle project\nDESCRIPTION: This command initializes a new Dify Bundle plugin project using the dify-plugin CLI tool.  It assumes that the dify-plugin executable is in the current directory. This command sets up the basic directory structure and configuration files for the bundle plugin.  It requires the Dify plugin scaffolding tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/bundle.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./dify-plugin-darwin-arm64 bundle init\n```\n\n----------------------------------------\n\nTITLE: Text Completion Dialog Template - Dify\nDESCRIPTION: This template is for building a dialog application using a text completion model in Dify. It incorporates context, pre-prompt, and chat history. The template includes placeholders for context, pre-prompt, chat histories, and the user's query.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nUse the following context as your learned knowledge, inside <context></context> XML tags.\n\n<context>\n{{#context#}}\n</context>\n\nWhen answer to user:\n- If you don't know, just say that you don't know.\n- If you don't know when you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n\n{{pre_prompt}}\n\nHere is the chat histories between human and assistant, inside <histories></histories> XML tags.\n\n<histories>\n{{#histories#}}\n</histories>\n\n\nHuman: {{#query#}}\n\nAssistant: \n```\n\n----------------------------------------\n\nTITLE: Completing Agent Log Messages in Python\nDESCRIPTION: This code snippet demonstrates how to complete an agent log message, potentially changing its status and adding an error message. This function allows updating a log message, especially if the initial status was set to 'start' to indicate progress. It facilitates displaying the task execution status to users.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/agent.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef finish_log_message(\n    self,\n    log: AgentInvokeMessage,\n    status: AgentInvokeMessage.LogMessage.LogStatus = AgentInvokeMessage.LogMessage.LogStatus.SUCCESS,\n    error: Optional[str] = None,\n) -> AgentInvokeMessage\n```\n\n----------------------------------------\n\nTITLE: Start Web Service (Bash)\nDESCRIPTION: Starts the Dify web service using either npm, yarn, or pnpm. This command launches the application and makes it accessible through a web browser.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nnpm run start\n# or\nyarn start\n# or\npnpm start\n```\n\n----------------------------------------\n\nTITLE: Performing Database Migration (Bash)\nDESCRIPTION: This command runs the database migration script using Flask, updating the database schema to the latest version. This ensures that the database structure is compatible with the current version of the Dify API.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\npoetry run flask db upgrade\n```\n\n----------------------------------------\n\nTITLE: Cloning Stable Diffusion WebUI Repository\nDESCRIPTION: This command clones the Stable Diffusion WebUI repository from GitHub. It is a prerequisite for setting up Stable Diffusion WebUI locally. The command downloads the necessary files and directories to the specified location.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/stable-diffusion.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui\n```\n\n----------------------------------------\n\nTITLE: Installing FFmpeg on Ubuntu\nDESCRIPTION: Installs FFmpeg on Ubuntu for enabling audio processing functionalities, such as text-to-speech with OpenAI.  Uses apt-get package manager to update the package list and install ffmpeg.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install ffmpeg\n```\n\n----------------------------------------\n\nTITLE: Starting the Dify on WeChat Application\nDESCRIPTION: This command sequence navigates to the `dify-on-wechat` directory and then executes the `app.py` script using Python. This command initiates the Dify integration with WeChat or WeCom based on the configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\ncd dify-on-wechat\npython app.py\n```\n\n----------------------------------------\n\nTITLE: LLM Request Entry Point (Python)\nDESCRIPTION: This code shows the entry point for making a Language Model (LLM) request using the Dify plugin's session object. It's the starting point for interacting with LLM capabilities.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.llm\n```\n\n----------------------------------------\n\nTITLE: Migrate Data for Plugin (Flask)\nDESCRIPTION: This snippet runs a Flask command to migrate data for the installed plugins. This process updates the provider names to ensure compatibility and proper functionality of the plugins.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/migration/migrate-to-v1.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask migrate-data-for-plugin\n```\n\n----------------------------------------\n\nTITLE: Installing a Specific Python Version with pyenv\nDESCRIPTION: Installs a specific Python version (3.12) using pyenv. This command ensures the correct Python environment is available for the Dify API service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_19\n\nLANGUAGE: Bash\nCODE:\n```\npyenv install 3.12\n```\n\n----------------------------------------\n\nTITLE: Invoke Workflow as Tool in Dify Plugin (Python)\nDESCRIPTION: Defines the interface for invoking a Workflow as a Tool within a Dify plugin. The `invoke_workflow_tool` method allows plugins to use existing workflows as tools.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/tool.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef invoke_workflow_tool(\n    self, provider: str, tool_name: str, parameters: dict[str, Any]\n) -> Generator[ToolInvokeMessage, None, None]:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Copying Environment Variable Configuration File\nDESCRIPTION: This command copies the example environment variable configuration file (`.env.example`) to a new file named `.env`. This allows you to configure the environment variables for the API service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Rerank Result Class\nDESCRIPTION: Defines the `RerankResult` class, representing the result of a reranking operation. It contains the model used and the list of reranked documents.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nclass RerankResult(BaseModel):\n    \"\"\"\n    Model class for rerank result.\n    \"\"\"\n    model: str  # 实际使用模型\n    docs: list[RerankDocument]  # 重排后的分段列表\t\n```\n\n----------------------------------------\n\nTITLE: Find docker-api container ID\nDESCRIPTION: This command lists all running Docker containers to identify the `docker-api` container's ID. The ID is needed to execute commands within the container for plugin migration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/migration/migrate-to-v1.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker ps\n```\n\n----------------------------------------\n\nTITLE: Clone and Run Dify with Docker Compose\nDESCRIPTION: These commands clone the Dify repository from GitHub and starts the application using Docker Compose. This resolves nginx configuration file mounting errors.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n`git clone https://github.com/langgenius/dify.git`\n```\n\nLANGUAGE: shell\nCODE:\n```\n`cd dify/docker`\n```\n\nLANGUAGE: shell\nCODE:\n```\n`docker compose up -d`\n```\n\n----------------------------------------\n\nTITLE: Implementing validate_credentials method\nDESCRIPTION: This Python code snippet demonstrates the `validate_credentials` method. This is used to validate the model credentials. It accepts model name and credentials as input.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/predefined-model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef validate_credentials(self, model: str, credentials: dict) -> None:\n        \"\"\"\n        Validate model credentials\n\n        :param model: model name\n        :param credentials: model credentials\n        :return:\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Text Prompt Message Content Class\nDESCRIPTION: Defines the `TextPromptMessageContent` class, a subclass of `PromptMessageContent`, specifically for text-based content in prompts. It sets the content type to TEXT.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass TextPromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for text prompt message content.\n    \"\"\"\n    type: PromptMessageContentType = PromptMessageContentType.TEXT\n```\n\n----------------------------------------\n\nTITLE: FFmpeg Installation - MacOS\nDESCRIPTION: These commands install FFmpeg on macOS using Homebrew.  If Homebrew isn't installed, the first command installs it.  Then, ffmpeg is installed using `brew install ffmpeg`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n`/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"`\n```\n\nLANGUAGE: shell\nCODE:\n```\n`brew install ffmpeg`\n```\n\n----------------------------------------\n\nTITLE: Authorization Header Example\nDESCRIPTION: Illustrates how to include the API key in the Authorization header using the Bearer authentication scheme. The `API_KEY` placeholder should be replaced with the actual API key.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_22\n\nLANGUAGE: HTTP\nCODE:\n```\nAuthorization: Bearer {API_KEY}\n```\n\n----------------------------------------\n\nTITLE: Retrieval API Endpoint (POST) Example\nDESCRIPTION: Shows the basic structure of the POST endpoint URL for accessing the retrieval API.  Replace `<your-endpoint>` with the actual endpoint URL for your Dify instance.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_21\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST <your-endpoint>/retrieval\n```\n\n----------------------------------------\n\nTITLE: Initializing the Plugin Template (Bash)\nDESCRIPTION: This command initializes a plugin development template, prompting for plugin name, author, description, language (Python), and plugin type (Agent Strategy). It then configures plugin permissions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: Check Docker API Container ID\nDESCRIPTION: This command lists the running Docker containers and their IDs. The output includes the container ID needed for the next command which migrates the plugins.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/dify-premium.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker ps\n```\n\n----------------------------------------\n\nTITLE: Migrate files using Flask (Docker)\nDESCRIPTION: This command uploads private key files to cloud storage within a Docker container. It requires the `docker exec` command to run within the `docker-api-1` container.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask upload-private-key-file-to-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Rendering HTML Forms\nDESCRIPTION: This HTML snippet defines a form with various input fields, including text, password, textarea, date, time, datetime, select, and checkbox elements.  The form uses custom data attributes (`data-format`, `data-options`, `data-tip`, `data-size`, `data-variant`) to specify formatting, options, and tooltips.  The `data-format=\"json\"` attribute on the `<form>` tag indicates that form data should be formatted as JSON by default.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/template.md#_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<form data-format=\"json\"> // Default to text\n  <label for=\"username\">Username:</label>\n  <input type=\"text\" name=\"username\" />\n  <label for=\"password\">Password:</label>\n  <input type=\"password\" name=\"password\" />\n  <label for=\"content\">Content:</label>\n  <textarea name=\"content\"></textarea>\n  <label for=\"date\">Date:</label>\n  <input type=\"date\" name=\"date\" />\n  <label for=\"time\">Time:</label>\n  <input type=\"time\" name=\"time\" />\n  <label for=\"datetime\">Datetime:</label>\n  <input type=\"datetime\" name=\"datetime\" />\n  <label for=\"select\">Select:</label>\n  <input type=\"select\" name=\"select\" data-options='[\"hello\",\"world\"]'/>\n  <input type=\"checkbox\" name=\"check\" data-tip=\"By checking this means you agreed\"/>\n  <button data-size=\"small\" data-variant=\"primary\">Login</button>\n</form>\n```\n\n----------------------------------------\n\nTITLE: Starting the Worker Service (Windows)\nDESCRIPTION: Starts the Celery worker service on Windows. Uses the 'solo' pool and disables gossip and mingle features.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_28\n\nLANGUAGE: Bash\nCODE:\n```\ncelery -A app.celery worker -P solo --without-gossip --without-mingle -Q dataset,generation,mail,ops_trace --loglevel INFO\n```\n\n----------------------------------------\n\nTITLE: Define Retrieval Endpoint\nDESCRIPTION: This code snippet defines the endpoint used for retrieval requests to connect to an external knowledge base. It uses a POST request.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_23\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST <your-endpoint>/retrieval\n```\n\n----------------------------------------\n\nTITLE: Installing Python 3.12 with Pyenv\nDESCRIPTION: Installs Python 3.12 using Pyenv, a Python version management tool. This ensures the correct Python version is used for running the backend services.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\npyenv install 3.12\n```\n\n----------------------------------------\n\nTITLE: Cloning SearXNG Docker Repository\nDESCRIPTION: These commands clone the SearXNG Docker repository to the Linux VM and then navigate into the cloned directory.  It prepares the environment for setting up SearXNG.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/searxng.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/searxng/searxng-docker.git\ncd searxng-docker\n```\n\n----------------------------------------\n\nTITLE: Uploading Private Key File to Cloud Storage (Docker Compose)\nDESCRIPTION: Uploads the private key file to cloud storage when using Docker Compose. Requires cloud storage configuration (e.g., aliyun-oss) to be set up.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask upload-private-key-file-to-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Creating Public Keys Directory (Bash)\nDESCRIPTION: This command creates a directory within the `plugin_daemon` volume to store public keys used for third-party signature verification. This directory needs to be accessible by the plugin daemon container.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmkdir docker/volumes/plugin_daemon/public_keys\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Compose for Middleware\nDESCRIPTION: This bash command starts the Docker Compose service for the middleware component. It uses the `docker-compose.middleware.yaml` file to define and run the necessary containers in detached mode.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/code.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\ndocker-compose -f docker-compose.middleware.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Navigate to Web Directory (Bash)\nDESCRIPTION: Navigates to the web directory where the Dify web application's source code is located. This is a prerequisite for subsequent steps like installing dependencies and building the application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/local-source-code.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncd web\n```\n\n----------------------------------------\n\nTITLE: PromptMessageContentType Enum Definition in Python\nDESCRIPTION: Defines an enumeration `PromptMessageContentType` representing the different types of content a prompt message can have.  The content types include TEXT and IMAGE.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/model/model-schema.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContentType(Enum):\n    \"\"\"\n    Enum class for prompt message content type.\n    \"\"\"\n    TEXT = 'text'\n    IMAGE = 'image'\n```\n\n----------------------------------------\n\nTITLE: Install GPUStack on Windows\nDESCRIPTION: This PowerShell command installs GPUStack on Windows.  It downloads the installation script from the specified URL and executes it using Invoke-Expression.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_3\n\nLANGUAGE: powershell\nCODE:\n```\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n```\n\n----------------------------------------\n\nTITLE: Initializing a Bundle Project with Renamed Dify CLI\nDESCRIPTION: If the `dify-plugin-darwin-arm64` binary is renamed to `dify` and copied to `/usr/local/bin`, this command can be used to initialize a new Dify plugin bundle project.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/bundle.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify bundle init\n```\n\n----------------------------------------\n\nTITLE: Install GPUStack on Linux/MacOS\nDESCRIPTION: This shell script installs GPUStack as a service on systemd or launchd based systems. It downloads and executes an installation script from the provided URL.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | sh -s -\n```\n\n----------------------------------------\n\nTITLE: Text Prompt Message Content Class in Python\nDESCRIPTION: This code defines the Text Prompt Message Content class, inheriting from PromptMessageContent. It specifies that the content type is TEXT.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass TextPromptMessageContent(PromptMessageContent):\n    \"\"\"\n    Model class for text prompt message content.\n    \"\"\"\n    type: PromptMessageContentType = PromptMessageContentType.TEXT\n```\n\n----------------------------------------\n\nTITLE: Deleting Storage Key in Dify Plugin\nDESCRIPTION: This snippet defines the endpoint for deleting a key and its associated value from the plugin's persistent storage. It accepts a key (string) to identify the entry to be deleted.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/persistent-storage.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef delete(self, key: str) -> None:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Change Directory Bash\nDESCRIPTION: Navigates to the web directory, typically the location of the frontend code. This is a prerequisite for installing dependencies and starting the frontend service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_29\n\nLANGUAGE: Bash\nCODE:\n```\ncd web\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Initialization Page with Domain - Bash\nDESCRIPTION: This snippet shows how to access the Dify administrator initialization page when a domain name has been configured. Replace `yourdomain` with the actual domain name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/bt-panel.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# If you have set domain\nhttp://yourdomain/install\n```\n\n----------------------------------------\n\nTITLE: Creating a Text Message in Dify (Python)\nDESCRIPTION: This snippet demonstrates how to create a text message within the Dify tool framework. It takes a text string as input and returns a ToolInvokeMessage object.  This function returns a textual response to the user or LLM.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/advanced-tool-integration.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    def create_text_message(self, text: str, save_as: str = '') -> ToolInvokeMessage:\n        \"\"\"\n            create a text message\n\n            :param text: the text of the message\n            :return: the text message\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Question Classifier Node\nDESCRIPTION: This snippet shows how to access the QuestionClassifier node from the plugin session.  This allows a plugin to use Dify's built-in question classification capabilities.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nself.session.workflow_node.question_classifier\n```\n\n----------------------------------------\n\nTITLE: Migrate to Cloud Storage - Local Source Code\nDESCRIPTION: These commands are used to migrate files from local storage to cloud storage when using local source code deployment. They invoke Flask commands to upload private key files and local files to the configured cloud storage provider (e.g., Alibaba Cloud OSS).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nflask upload-private-key-file-to-cloud-storage\nflask upload-local-files-to-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Reset Admin Password using Flask command\nDESCRIPTION: Resets the password for the Dify admin account. This command is executed inside the `docker-api-1` container and prompts for the email address and new password.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/faqs.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-password\n```\n\n----------------------------------------\n\nTITLE: Deleting a Storage Key\nDESCRIPTION: This snippet demonstrates the `delete` method for removing a key-value pair. The key is a string indicating which pair should be removed from storage.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/persistent-storage.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef delete(self, key: str) -> None:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Content Base Class in Python\nDESCRIPTION: This snippet defines a base class `PromptMessageContent` for prompt message content with type and data attributes.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageContent(BaseModel):\n    \"\"\"\n    Model class for prompt message content.\n    \"\"\"\n    type: PromptMessageContentType\n    data: str\n```\n\n----------------------------------------\n\nTITLE: Ngrok Start Tunneling\nDESCRIPTION: This command starts Ngrok to create a public tunnel to a local port. Replace 'ポート番号' with the actual port number of the locally running API service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n$ ./ngrok http ポート番号\n```\n\n----------------------------------------\n\nTITLE: Launching Stable Diffusion WebUI on Windows\nDESCRIPTION: Launches the Stable Diffusion WebUI on a Windows machine, enabling the API and listening for incoming requests. Requires navigating to the cloned repository directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/stable-diffusion.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd stable-diffusion-webui\n./webui.bat --api --listen\n```\n\n----------------------------------------\n\nTITLE: Installing Web Dependencies\nDESCRIPTION: Installs the necessary dependencies for the web application using npm. This step is required before building and running the web frontend.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_13\n\nLANGUAGE: Bash\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Migrate files using Flask (Local)\nDESCRIPTION: This command uploads the private key files to the configured cloud storage. It is intended for use with local source code deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nflask upload-private-key-file-to-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Run Plugin (Python)\nDESCRIPTION: This command runs the plugin's main entry point, which starts the plugin and makes it available for debugging and testing within the Dify environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npython -m main\n```\n\n----------------------------------------\n\nTITLE: Chat Model Text Generation App Template ASSISTANT\nDESCRIPTION: This is the ASSISTANT prompt template for building text generation applications using chat models in Dify. It represents the expected output from the model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n\"\" \n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Main Page (Server)\nDESCRIPTION: This command shows the URL for accessing the main Dify page when deployed in a server environment. Replace `your_server_ip` with the actual server IP address.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# 服务器环境\nhttp://your_server_ip\n```\n\n----------------------------------------\n\nTITLE: Backup Docker Compose YAML (Optional)\nDESCRIPTION: This command creates a backup of the docker-compose.yaml file. The filename includes a timestamp to differentiate backups.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/dify-premium.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ncp docker-compose.yaml docker-compose.yaml.$(date +%s).bak\n```\n\n----------------------------------------\n\nTITLE: Installing system dependencies on Ubuntu/Debian\nDESCRIPTION: Installs necessary system components on Ubuntu/Debian Linux distributions to support DifySandbox. This includes package management tools, compilers, and development libraries.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/backend/sandbox/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get install pkg-config gcc libseccomp-dev git wget\n```\n\n----------------------------------------\n\nTITLE: Set Ollama Host Environment Variable on Linux\nDESCRIPTION: This shows how to set the `OLLAMA_HOST` environment variable on Linux using `systemctl` by editing the `ollama.service` file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_81\n\nLANGUAGE: INI\nCODE:\n```\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Admin Initialization Page (Local)\nDESCRIPTION: This URL is used to access the administrator initialization page on a local environment. This page is where you set up the initial administrator account.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/docker-compose.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhttp://localhost/install\n```\n\n----------------------------------------\n\nTITLE: Initializing Plugin Project (Bash)\nDESCRIPTION: This command initializes a new Dify plugin project. It prompts the user to fill in basic project information and select a template and permissions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: Prompt for UI Generation\nDESCRIPTION: This prompt instructs the AI to act as a UI generator, converting user input into UI structures.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/how-to-use-json-schema-in-dify.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nあなたはUIジェネレータAIです。ユーザー入力をUIに変換してください。\n```\n\n----------------------------------------\n\nTITLE: Reset Encryption Key Pair using Flask\nDESCRIPTION: Resets the encryption key pair used for encrypting large model keys in a Dify deployment running directly from source code. This command addresses \"File not found\" errors related to missing private keys. It's executed from the `api` directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nflask reset-encrypt-key-pair\n```\n\n----------------------------------------\n\nTITLE: Checking Docker Version\nDESCRIPTION: This command checks the installed version of Docker to verify that the installation was successful. It provides information about the Docker client and server versions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker --version\n```\n\n----------------------------------------\n\nTITLE: Installing Docker Compose\nDESCRIPTION: This command downloads and installs Docker Compose from the GitHub releases page. It downloads the appropriate version for the system's architecture, saves it to /usr/local/bin, and makes it executable.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsudo curl -L \"https://github.com/docker/compose/releases/download/2.32.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\n```\n\n----------------------------------------\n\nTITLE: Restart Dify Services with Docker Compose\nDESCRIPTION: Restarts the Dify services using Docker Compose. This is often required after modifying environment variables or other configuration files.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/faqs.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\ndocker compose up -d\n```\n\n----------------------------------------\n\nTITLE: Initializing a Dify Plugin Project (Alternative)\nDESCRIPTION: This command initializes a new Dify plugin project using the dify binary, assuming it has been renamed to 'dify' and copied to /usr/local/bin.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: Verifying Docker Installation\nDESCRIPTION: This command checks the installed Docker version, confirming that Docker has been successfully installed. It outputs the Docker version information to the console.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker --version\n```\n\n----------------------------------------\n\nTITLE: System Prompt Message Class in Python\nDESCRIPTION: This code defines the System Prompt Message class, inheriting from PromptMessage. It specifies the role as SYSTEM.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass SystemPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for system prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.SYSTEM\n```\n\n----------------------------------------\n\nTITLE: Cloning the Dify-Twilio-WhatsApp Repository\nDESCRIPTION: This command clones the dify-twilio-whatsapp repository from GitHub. This repository contains the necessary code for the WhatsApp chatbot.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-whatsapp.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/somethingwentwell/dify-twilio-whatsapp\n```\n\n----------------------------------------\n\nTITLE: Model Plugin Structure Example\nDESCRIPTION: This snippet illustrates the directory structure of a model plugin, showcasing the organization by model provider, model category, and specific model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n- モデルプロバイダー\n  - モデルカテゴリ\n    - 具体的なモデル\n```\n\n----------------------------------------\n\nTITLE: API Response Structure for External Data Tool Query\nDESCRIPTION: Defines the API response structure. The response contains a `result` field which is a string providing the query result from the external tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/external-data-tool.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"result\": string\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Web Service with NPM\nDESCRIPTION: Starts the web service using NPM. This command initiates the frontend application, making it accessible in a browser.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_15\n\nLANGUAGE: Bash\nCODE:\n```\nnpm run start\n```\n\n----------------------------------------\n\nTITLE: Plugin Packaging: dify plugin package\nDESCRIPTION: This command is used to package the Dify plugin into a `.difypkg` file. Replace `./google` with the actual path to the plugin project.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\n# Replace ./google with your actual plugin project path.\n\ndify plugin package ./google\n```\n\n----------------------------------------\n\nTITLE: Array to Text Conversion - Jinja2 Template Node\nDESCRIPTION: This Jinja2 template code snippet demonstrates how to convert an array of strings named `articleSections` to a single string with newline characters using a template node. The `join(\"/n\")` filter joins the elements with newline characters, effectively converting the array into a single text string for further processing in the Dify workflow.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_72\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ articleSections | join(\"/n\") }}\n```\n\n----------------------------------------\n\nTITLE: Copying Public Key (Bash)\nDESCRIPTION: This command copies the generated public key file into the `public_keys` directory within the `plugin_daemon` volume, making it available for signature verification by the Dify plugin daemon.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/signing-plugins-for-third-party-signature-verification.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncp your_key_pair.public.pem docker/volumes/plugin_daemon/public_keys\n```\n\n----------------------------------------\n\nTITLE: Copying Environment Configuration File\nDESCRIPTION: Copies the example environment configuration file (.env.example) to a new file named .env. This allows for customization of environment variables for the API service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Deploying API Extension (Bash)\nDESCRIPTION: These commands install the necessary dependencies and deploy the Cloudflare Worker using the `npm` package manager.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\nnpm run deploy\n```\n\n----------------------------------------\n\nTITLE: Finding Container IPs to Fix 502 Bad Gateway\nDESCRIPTION: This command retrieves the IP addresses of the Dify `docker-web-1` and `docker-api-1` containers. This information is crucial for updating the Nginx configuration to correctly forward requests to the appropriate services, resolving '502 Bad Gateway' errors caused by incorrect IP address configurations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndocker ps -q | xargs -n 1 docker inspect --format '{{ .Name }}: {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Containers with Compose V1\nDESCRIPTION: This command starts the Dify application's Docker containers using Docker Compose V1. The `-d` option runs the containers in detached mode (in the background).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Uvicorn startup command\nDESCRIPTION: This shell command starts the FastAPI application using Uvicorn, enabling hot-reloading and binding to all available network interfaces.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/README.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nuvicorn main:app --reload --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: NPM Install (Web)\nDESCRIPTION: Installs Node.js dependencies for web frontend using npm.  Assumes Node.js and NPM are pre-installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_46\n\nLANGUAGE: JavaScript\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Initialize Plugin Project (Renamed)\nDESCRIPTION: This command initializes a new Dify plugin project if the binary file has been renamed to `dify` and copied to `/usr/local/bin`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndify plugin init\n```\n\n----------------------------------------\n\nTITLE: Running in Development Mode\nDESCRIPTION: These commands are used to install dependencies and run the API extension in development mode for local testing.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Verifying Docker Installation\nDESCRIPTION: This command checks the installed version of Docker. It's used to confirm that Docker has been successfully installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/searxng.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker --version\n```\n\n----------------------------------------\n\nTITLE: Clone Dify Repository\nDESCRIPTION: Clones the Dify repository from GitHub. This downloads the entire source code to your local machine. Requires Git to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_34\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\n----------------------------------------\n\nTITLE: API Ping Check Request Body\nDESCRIPTION: This JSON snippet shows the request body Dify sends to check API availability.  The API should return `result=pong` when `point=ping` is received.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/README.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"point\": \"ping\"\n}\n```\n\n----------------------------------------\n\nTITLE: Initialize Dify Plugin Project (CLI)\nDESCRIPTION: This command initializes a new Dify plugin project using the Dify plugin scaffolding tool.  It assumes the binary is in the current directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/extension-plugin.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./dify-plugin-darwin-arm64 plugin init\n```\n\n----------------------------------------\n\nTITLE: Starting LocalAI with Docker Compose\nDESCRIPTION: This snippet uses Docker Compose to start LocalAI in detached mode, building the necessary Docker images.  It's a simple way to launch LocalAI with all its dependencies.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# start with docker-compose\n$ docker-compose up -d --build\n```\n\n----------------------------------------\n\nTITLE: Installing FFmpeg on CentOS\nDESCRIPTION: Installs FFmpeg on CentOS for enabling audio processing functionalities.  Requires enabling the EPEL and Nux Dextop repositories.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/faq/install-faq.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum install epel-release\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo rpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-5.el7.nux.noarch.rpm\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum update\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo yum install ffmpeg ffmpeg-devel\n```\n\n----------------------------------------\n\nTITLE: Cloning the Dify Repository with Git\nDESCRIPTION: This command clones the forked Dify repository from GitHub to your local machine. Replace <github_username> with your GitHub username.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_12\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone git@github.com:<github_username>/dify.git\n```\n\n----------------------------------------\n\nTITLE: Accessing Storage Interface in Plugin Session\nDESCRIPTION: This snippet shows how to access the storage interface within the plugin session. The storage interface provides methods to interact with the persistent storage.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/persistent-storage.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.session.storage\n```\n\n----------------------------------------\n\nTITLE: Checking Docker Compose Version\nDESCRIPTION: This command checks the installed version of Docker Compose to verify the installation was successful.  It prints the version number to the console.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose --version\n```\n\n----------------------------------------\n\nTITLE: Plugin Configuration Path (YAML)\nDESCRIPTION: This YAML code defines the location where plugins are located and configured by referencing to the plugin file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nplugins:\n  tools:\n    - \"google.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Plugin Directory Structure\nDESCRIPTION: This code snippet shows the required directory structure in the forked dify-plugins repository for the plugin.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/plugin-auto-publish-pr.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndify-plugins/\n└── your-author-name\n    └── plugin-name\n```\n\n----------------------------------------\n\nTITLE: Running the Plugin (Bash)\nDESCRIPTION: This command starts the plugin. It uses the `main` module to initialize and run the plugin server.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m main\n```\n\n----------------------------------------\n\nTITLE: Navigating to API Directory\nDESCRIPTION: This command changes the current directory to the `api` directory.  This is necessary to execute the API related commands.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\ncd api\n```\n\n----------------------------------------\n\nTITLE: Navigating to Web Directory\nDESCRIPTION: This command changes the current directory to the `web` directory. This is necessary to execute the web-related commands.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\ncd web\n```\n\n----------------------------------------\n\nTITLE: Building Frontend Docker Image\nDESCRIPTION: Builds the frontend Docker image from the source code.  It requires Docker to be installed and running. The command builds the image tagged as `dify-web`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/start-the-frontend-docker-container.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ncd web && docker build . -t dify-web\n```\n\n----------------------------------------\n\nTITLE: API Request Example (POST)\nDESCRIPTION: Shows the general structure for making a POST request to a Dify API extension endpoint.  It uses POST method to the specified endpoint.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nPOST {Your-API-Endpoint}\n```\n\n----------------------------------------\n\nTITLE: Creating Hierarchical Logs in Python\nDESCRIPTION: This snippet shows how to create a hierarchical log structure by specifying a parent log when creating a new log message. This allows for organizing logs into rounds or related operations, making it easier to track the flow of execution in multi-step agent tasks.  The `parent` parameter of `create_log_message` establishes the relationship.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfunction_call_round_log = self.create_log_message(\n    label=\"Function Call Round1 \",\n    data={},\n    metadata={},\n)\nyield function_call_round_log\n\nmodel_log = self.create_log_message(\n    label=f\"{params.model.model} Thought\",\n    data={},\n    metadata={\"start_at\": model_started_at, \"provider\": params.model.provider},\n    status=ToolInvokeMessage.LogMessage.LogStatus.START,\n    # add parent log\n    parent=function_call_round_log,\n)\nyield model_log\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Configuration\nDESCRIPTION: Shows the content of the .env.local file, used to configure the environment variables for the Dify web application, API prefix and other settings. This is necessary to configure the application's behavior and connection to the backend services.  It shows examples for development environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_31\n\nLANGUAGE: Text\nCODE:\n```\n# For production release, change this to PRODUCTION\nNEXT_PUBLIC_DEPLOY_ENV=DEVELOPMENT\n# The deployment edition, SELF_HOSTED\nNEXT_PUBLIC_EDITION=SELF_HOSTED\n# The base URL of console application, refers to the Console base URL of WEB service if console domain is\n# different from api or web app domain.\n# example: http://cloud.dify.ai/console/api\nNEXT_PUBLIC_API_PREFIX=http://localhost:5001/console/api\n# The URL for Web APP, refers to the Web App base URL of WEB service if web app domain is different from\n# console or api domain.\n# example: http://udify.app/api\nNEXT_PUBLIC_PUBLIC_API_PREFIX=http://localhost:5001/api\n\n# SENTRY\nNEXT_PUBLIC_SENTRY_DSN=\nNEXT_PUBLIC_SENTRY_ORG=\nNEXT_PUBLIC_SENTRY_PROJECT=\n```\n\n----------------------------------------\n\nTITLE: Interface Definition Example YAML\nDESCRIPTION: Defines the interface for an endpoint, specifying the path, HTTP method, and extra configuration information including the Python source file for implementation. The `path` follows the werkzeug interface standard. The `method` specifies the HTTP method for the endpoint. The `extra` object holds additional configuration, including the path to the Python source code that implements the interface.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/endpoint.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npath: \"/duck/<app_id>\"\nmethod: \"GET\"\nextra:\n  python:\n    source: \"endpoints/duck.py\"\n```\n\n----------------------------------------\n\nTITLE: Dify Frontend Directory Structure\nDESCRIPTION: This section describes the directory structure of Dify's frontend, built on Next.js in Typescript, using Tailwind CSS for styling and React-i18next for internationalization. It provides an overview of the different directories and their purposes, such as layouts, pages, components, static assets, and internationalization configurations.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_28\n\nLANGUAGE: Text\nCODE:\n```\n[web/]\n├── app                   // layouts, pages, and components\n│   ├── (commonLayout)    // common layout used throughout the app\n│   ├── (shareLayout)     // layouts specifically shared across token-specific sessions\n│   ├── activate          // activate page\n│   ├── components        // shared by pages and layouts\n│   ├── install           // install page\n│   ├── signin            // signin page\n│   └── styles            // globally shared styles\n├── assets                // Static assets\n├── bin                   // scripts ran at build step\n├── config                // adjustable settings and options\n├── context               // shared contexts used by different portions of the app\n├── dictionaries          // Language-specific translate files\n├── docker                // container configurations\n├── hooks                 // Reusable hooks\n├── i18n                  // Internationalization configuration\n├── models                // describes data models & shapes of API responses\n├── public                // meta assets like favicon\n├── service               // specifies shapes of API actions\n├── test\n├── types                 // descriptions of function params and return values\n└── utils                 // Shared utility functions\n```\n\n----------------------------------------\n\nTITLE: Navigating and Copying Configuration Files\nDESCRIPTION: These commands navigate to the docker directory within the cloned project and copy the example configuration files (.env.example and .bots.yaml.example) to .env and .bots.yaml respectively, which will then be modified to configure the service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-dingtalk.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd dify-on-dingtalk\n# 将配置文件示例拷贝到 docker 文件夹中\ncd docker\ncp ../.env.example .env\ncp ../.bots.yaml.example .bots.yaml\n# 分别修改.env 和.bots.yaml 文件，配置好所需参数。\n```\n\n----------------------------------------\n\nTITLE: Question Classifier Node Entry\nDESCRIPTION: This code snippet shows the entry point for accessing the Question Classifier node within a Dify plugin's session.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nself.session.workflow_node.question_classifier\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository\nDESCRIPTION: This command clones the Dify repository from GitHub. This is the first step to get the source code locally.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/local-source-code.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\n----------------------------------------\n\nTITLE: Accessing Logs of Cloudflare Workers (Bash)\nDESCRIPTION: This command allows you to access logs of Cloudflare Workers using the wrangler CLI.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nwrangler tail\n```\n\n----------------------------------------\n\nTITLE: Migration Completion Message - Bash\nDESCRIPTION: This is example output indicating the successful completion of the data migration process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/migration/migrate-to-v1.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nMigrate [tool_builtin_providers] data for plugin completed, total: 6\nMigrate data for plugin completed.\n```\n\n----------------------------------------\n\nTITLE: Configure .env File\nDESCRIPTION: Renames the .env.example file to .env to start configuring the environment variables for LocalAI.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/development/models-integration/localai.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ mv .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Install Plugins using Flask (Poetry)\nDESCRIPTION: This snippet executes a Flask command within the `docker-api` container to install plugins. It connects to a remote marketplace and installs necessary plugins to the Dify instance.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/migration/migrate-to-v1.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npoetry run flask install-plugins --workers=2\n```\n\n----------------------------------------\n\nTITLE: Setting Ollama Host Environment Variable (Mac) (Bash)\nDESCRIPTION: This command sets the OLLAMA_HOST environment variable on macOS using launchctl, which is necessary to make Ollama accessible from within a Docker container if running both Dify and Ollama within Docker. The variable is set to \"0.0.0.0\", allowing connections from any address. It requires restarting the Ollama application after setting the variable.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nlaunchctl setenv OLLAMA_HOST \"0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Speech-to-Text Invoke Entry Point Python\nDESCRIPTION: This snippet shows how to access the Speech-to-Text functionality in the Dify plugin session.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.speech2text\n```\n\n----------------------------------------\n\nTITLE: Rerank Invoke Entry Point Python\nDESCRIPTION: This snippet shows how to access the Rerank functionality in the Dify plugin session.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.rerank\n```\n\n----------------------------------------\n\nTITLE: Cloudflare Worker Logs\nDESCRIPTION: This command is used to view the logs of the Cloudflare worker, which is useful for debugging.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nwrangler tail\n```\n\n----------------------------------------\n\nTITLE: LLM Invoke Entry Point Python\nDESCRIPTION: This snippet shows how to access the LLM functionality in the Dify plugin session.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.llm\n```\n\n----------------------------------------\n\nTITLE: TTS Invoke Entry Point Python\nDESCRIPTION: This snippet shows how to access the TTS functionality in the Dify plugin session.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/model.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nself.session.model.tts\n```\n\n----------------------------------------\n\nTITLE: 画像メッセージを作成 (Python)\nDESCRIPTION: 画像のURLを渡して、Difyに画像メッセージを生成させるインターフェースです。Difyは自動的に画像をダウンロードしてユーザーに返します。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/advanced-tool-integration.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef create_image_message(self, image: str, save_as: str = '') -> ToolInvokeMessage:\n    \"\"\"\n        画像メッセージを作成\n\n        :param image: 画像のURL\n        :return: 画像メッセージ\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: LLM Node Prompt Example (JSON format)\nDESCRIPTION: This prompt instructs the LLM to output either a correct or incorrect sample code in JSON format, intended for use in a teaching assistant scenario within a Dify workflow.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nYou are a teaching assistant. According to the user's requirements, you only output a correct or incorrect sample code in json format.\n```\n\n----------------------------------------\n\nTITLE: Configure Git Username and Email - Bash\nDESCRIPTION: This snippet configures the Git username and email globally. This is only necessary if it's the user's first time using Git.  Replace \"Your Name\" and \"your.email@example.com\" with the user's actual name and email address.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n```\n\n----------------------------------------\n\nTITLE: Code Structure Overview\nDESCRIPTION: This snippet outlines the file structure of the project, providing context for understanding the organization of the codebase. It shows directories like `server`, `lib`, `test`, `build`, `internal` with subdirectories such as `controller`, `middleware`, `service`, `static`, `types`, and `core` including runner implementations for `nodejs` and `python`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_77\n\nLANGUAGE: text\nCODE:\n```\n[cmd/]\n├── server                // Server startup entry point\n├── lib                   // Shared library entry point\n└── test                  // Common test scripts\n[build/]                  // Build scripts for different architectures and platforms\n[internal/]               // Internal packages\n├── controller            // HTTP request handlers\n├── middleware            // Request processing middleware\n├── server                // Server setup and configuration\n├── service               // Controller services\n├── static                // Configuration files\n│   ├── nodejs_syscall    // Node.js system call whitelist\n│   └── python_syscall    // Python system call whitelist\n├── types                 // Entity definitions\n├── core                  // Core isolation and execution logic\n│   ├── lib               // Shared libraries\n│   ├── runner            // Code execution\n│   │   ├── nodejs        // Node.js executor\n|   |   └── python        // Python executor\n└── tests                 // CI/CD tests\n```\n\n----------------------------------------\n\nTITLE: Example Docker Logs\nDESCRIPTION: This is an example of the expected service logs indicating a successful startup. The logs show the number of bots being started and the establishment of a connection to the DingTalk API.  The absence of errors in the logs confirms that the service is running correctly.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-dingtalk.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n2024-08-20 18:16:42.019 | INFO     | __main__:run:33 - 待启动机器人数量：1, 预计使用最大线程数：1\n2024-08-20 18:16:42.019 | INFO     | __main__:run:37 - 启动第 1 个机器人：agent_chatbot\n2024-08-20 18:16:42.021 | INFO     | dingtalk_stream.stream:open_connection:131 - open connection, url=https://api.dingtalk.com/v1.0/gateway/connections/open\n2024-08-20 18:16:42.314 | INFO     | dingtalk_stream.stream:start:72 - endpoint is % s\n```\n\n----------------------------------------\n\nTITLE: Ngrok Tunneling\nDESCRIPTION: This shell command is for exposing the local service to public by creating a Ngrok tunnel.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/README.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n$ ./ngrok http 端口号\n```\n\n----------------------------------------\n\nTITLE: Reset Password using Docker Compose\nDESCRIPTION: Resets the administrator password for a Dify deployment using Docker Compose.  This command is executed within the `docker-api-1` container and calls the `flask reset-password` command. The user will be prompted for the account email and the new password.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/learn-more/faq/install-faq.md#_snippet_0\n\nLANGUAGE: docker\nCODE:\n```\ndocker exec -it docker-api-1 flask reset-password\n```\n\n----------------------------------------\n\nTITLE: Starting the API Service\nDESCRIPTION: Starts the API service using Flask. The service listens on all addresses (0.0.0.0) on port 5001 with debug mode enabled.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_26\n\nLANGUAGE: Bash\nCODE:\n```\nflask run --host 0.0.0.0 --port=5001 --debug\n```\n\n----------------------------------------\n\nTITLE: LLM Prompt for JSON Code Generation\nDESCRIPTION: This prompt instructs an LLM to generate either correct or incorrect JSON code samples based on user requirements. It sets the LLM's role as a teaching assistant focused solely on providing JSON code snippets.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_19\n\nLANGUAGE: text\nCODE:\n```\nYou are a teaching assistant. According to the user's requirements, you only output a correct or incorrect sample code in json format.\n```\n\n----------------------------------------\n\nTITLE: リンクメッセージを作成 (Python)\nDESCRIPTION: リンクのURLを渡して、Difyにリンクメッセージを生成させるインターフェースです。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/advanced-tool-integration.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef create_link_message(self, link: str, save_as: str = '') -> ToolInvokeMessage:\n    \"\"\"\n        リンクメッセージを作成\n\n        :param link: リンクのURL\n        :return: リンクメッセージ\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Import DSL File from URL - URL example\nDESCRIPTION: Shows an example of a URL that can be used to import a Dify DSL file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/creating-an-application.md#_snippet_0\n\nLANGUAGE: url\nCODE:\n```\nhttps://example.com/your_dsl.yml\n```\n\n----------------------------------------\n\nTITLE: Code Structure - Directory Listing\nDESCRIPTION: This code snippet illustrates the directory structure of the Dify project. It shows the organization of key components such as the server entry point, shared libraries, build scripts, internal packages (controller, middleware, service, static files, and core logic), and test directories. Understanding this structure is crucial for contributing to the project.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/backend/sandbox/contribution.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n[cmd/]\n├── server                // Enterpoint for starting the server.\n├── lib                   // Enterpoint for Shared libraries.\n└── test                  // Common test scripts.\n[build/]                  // build scripts for different architectures and platforms.\n[internal/]               // Internal packages.\n├── controller            // HTTP request handlers.\n├── middleware            // Middleware for request processing.\n├── server                // Server setup and configuration.\n├── service               // Provides service for controller.\n├── static                // Configuration files.\n│   ├── nodejs_syscall    // Whitelist for nodejs syscall.\n│   └── python_syscall    // Whitelist for python syscall.\n├── types                 // Entities\n├── core                  // Core logic for isolation and execution.\n│   ├── lib               // Shared libraries.\n│   ├── runner            // Code execution.\n│   │   ├── nodejs        // Nodejs runner.\n|   |   └── python        // Python runner.\n└── tests                 // Tests for CI/CD.\n```\n\n----------------------------------------\n\nTITLE: Accessing the Dify Web Interface\nDESCRIPTION: This command provides the URL to access the Dify web interface. The URL depends on whether it's a local or server environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/docker-compose.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Local environment\nhttp://localhost\n\n# Server environment\nhttp://your_server_ip\n```\n\n----------------------------------------\n\nTITLE: Anthropic Provider Class (Python)\nDESCRIPTION: This Python code defines the AnthropicProvider class, which inherits from the ModelProvider base class. It includes a method to validate the provider's credentials by validating model instance credentials.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nfrom dify_plugin.entities.model import ModelType\nfrom dify_plugin.errors.model import CredentialsValidateFailedError\nfrom dify_plugin import ModelProvider\n\nlogger = logging.getLogger(__name__)\n\n\nclass AnthropicProvider(ModelProvider):\n    def validate_provider_credentials(self, credentials: dict) -> None:\n        \"\"\"\n        Validate provider credentials\n\n        if validate failed, raise exception\n\n        :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.\n        \"\"\"\n        try:\n            model_instance = self.get_model_instance(ModelType.LLM)\n            model_instance.validate_credentials(model=\"claude-3-opus-20240229\", credentials=credentials)\n        except CredentialsValidateFailedError as ex:\n            raise ex\n        except Exception as ex:\n            logger.exception(f\"{self.get_provider_schema().provider} credentials validate failed\")\n            raise ex\n```\n\n----------------------------------------\n\nTITLE: Initialize Plugin Project\nDESCRIPTION: This command initializes a new Dify plugin project using the plugin scaffolding tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/model-plugin/create-model-providers.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./dify-plugin-darwin-arm64 plugin init\n```\n\n----------------------------------------\n\nTITLE: SystemPromptMessage Class Definition in Python\nDESCRIPTION: Defines the SystemPromptMessage class, inheriting from PromptMessage.  It sets the role to SYSTEM, representing a system-level instruction or message.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nclass SystemPromptMessage(PromptMessage):\n    \"\"\"\n    Model class for system prompt message.\n    \"\"\"\n    role: PromptMessageRole = PromptMessageRole.SYSTEM\n```\n\n----------------------------------------\n\nTITLE: Building the Web Application\nDESCRIPTION: Builds the web application using npm.  This command compiles the source code and prepares the application for deployment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_15\n\nLANGUAGE: Bash\nCODE:\n```\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: Getting Customizable Model Schema in Python\nDESCRIPTION: This method gets the customizable model schema. It takes the model name and credentials as input. It returns the model schema if the provider supports adding custom LLMs, otherwise it returns None.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/model-configuration/interfaces.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_customizable_model_schema(self, model: str, credentials: dict) -> Optional[AIModelEntity]:\n        \"\"\"\n        Get customizable model schema\n\n        :param model: model name\n        :param credentials: model credentials\n        :return: model schema\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Cloning Git Repository\nDESCRIPTION: This command clones the forked Dify repository from GitHub to the local machine.  Replace `<github_username>` with the actual GitHub username.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_59\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone git@github.com:<github_username>/dify.git\n```\n\n----------------------------------------\n\nTITLE: Tool Access in Dify Plugins\nDESCRIPTION: Provides access to tool invocation methods within a Dify plugin session.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/schema-definition/reverse-invocation-of-the-dify-service/tool.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nself.session.tool\n```\n\n----------------------------------------\n\nTITLE: Frontend Directory Structure\nDESCRIPTION: This snippet describes the directory structure of the Dify frontend, which uses Next.js and Tailwind CSS. It highlights key directories like `app` for layouts and components, `assets` for static files, and `i18n` for internationalization.  This helps developers understand the organization of the frontend codebase.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_63\n\nLANGUAGE: text\nCODE:\n```\n[web/]\n├── app                   // レイアウト、ページ、およびコンポーネント\n│   ├── (commonLayout)    // アプリ全体で使用される共通レイアウト\n│   ├── (shareLayout)     // トークン固有のセッション間で共有されるレイアウト\n│   ├── activate          // アクティベートページ\n│   ├── components        // ページとレイアウトで共有されるコンポーネント\n│   ├── install           // インストールページ\n│   ├── signin            // サインインページ\n│   └── styles            // グローバルに共有されるスタイル\n├── assets                // 静的アセット\n├── bin                   // ビルドステップで実行されるスクリプト\n├── config                // 調整可能な設定とオプション\n├── context               // アプリの異なる部分で使用される共有コンテキスト\n├── dictionaries          // 言語固有の翻訳ファイル\n├── docker                // コンテナ設定\n├── hooks                 // 再利用可能なフック\n├── i18n                  // 国際化設定\n├── models                // データモデルとAPIレスポンスの形状を記述\n├── public                // ファビコンなどのメタアセット\n├── service               // APIアクションの形状を指定\n├── test\n├── types                 // 関数パラメータと戻り値の記述\n└── utils                 // 共有ユーティリティ関数\n```\n\n----------------------------------------\n\nTITLE: Installing Python Environment (Bash)\nDESCRIPTION: Installs Python 3.12 using pyenv.  pyenv is recommended for quickly managing Python versions. This step is required to setup the API service.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\npyenv install 3.12\n```\n\n----------------------------------------\n\nTITLE: Starting Stable Diffusion WebUI on Linux\nDESCRIPTION: This command navigates to the Stable Diffusion WebUI directory and starts the WebUI with API and listen options enabled. The `--api` flag enables the API, allowing Dify to interact with Stable Diffusion, and `--listen` makes the WebUI accessible from other machines on the network.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/stable-diffusion.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd stable-diffusion-webui\n./webui.sh --api --listen\n```\n\n----------------------------------------\n\nTITLE: Chat Model Dialog Template SYSTEM Prompt - Dify\nDESCRIPTION: This is the SYSTEM prompt for creating a dialog-based application using a chat model within Dify. It utilizes context from an XML tag to inform the LLM's responses and instructs it to avoid mentioning the source and answer in the user's language. The pre_prompt variable is also included.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/extended-reading/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nUse the following context as your learned knowledge, inside <context></context> XML tags.\n\n<context>\n{{#context#}}\n</context>\n\nWhen answer to user:\n- If you don't know, just say that you don't know.\n- If you don't know when you are not sure, ask for clarification.\nAvoid mentioning that you obtained the information from the context.\nAnd answer according to the language of the user's question.\n{{pre_prompt}}\n```\n\n----------------------------------------\n\nTITLE: Generating and Setting Secret Key\nDESCRIPTION: Generates a random secret key using openssl and replaces the value of SECRET_KEY in the .env file. This key is used for securing the application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/local-source-code.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\nawk -v key=\"$(openssl rand -base64 42)\" '/^SECRET_KEY=/ {sub(/=.*/, \"=\" key)} 1' .env > temp_env && mv temp_env .env\n```\n\n----------------------------------------\n\nTITLE: Setting Dify App ID and API Key JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates how to set the Dify application ID and API key in a WebApp template. These values are crucial for connecting the frontend application to the Dify backend and accessing its AI functionalities. The snippet shows empty strings as placeholders that must be replaced with actual values obtained from Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/application-publishing/based-on-frontend-templates.md#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nexport const APP_ID = ''\nexport const API_KEY = ''\n```\n\n----------------------------------------\n\nTITLE: Accessing Question Classifier Node\nDESCRIPTION: This shows how to access the question classifier node within a Dify plugin session. It provides the entry point required to initiate requests to the question classification functionality.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nself.session.workflow_node.question_classifier\n```\n\n----------------------------------------\n\nTITLE: Defining EmbeddingUsage Model (Python)\nDESCRIPTION: Defines the `EmbeddingUsage` model, representing usage information for an embedding operation.  It inherits from `ModelUsage` and includes fields for token counts, unit prices, total price, currency and latency.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nclass EmbeddingUsage(ModelUsage):\n    \"\"\"\n    Model class for embedding usage.\n    \"\"\"\n    tokens: int  # Number of tokens used\n    total_tokens: int  # Total number of tokens used\n    unit_price: Decimal  # Unit price\n    price_unit: Decimal  # Price unit, i.e., the unit price based on how many tokens\n    total_price: Decimal  # Total cost\n    currency: str  # Currency unit\n    latency: float  # Request latency (s)\n```\n\n----------------------------------------\n\nTITLE: Run Dify on WeChat in Background on Server\nDESCRIPTION: Runs the Dify on WeChat application in the background on a server using nohup. The output, including the QR code for WeChat login, is redirected to nohup.out for monitoring. This allows the application to run even after the terminal session is closed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/dify-on-wechat.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd dify-on-wechat\nnohup python3 app.py & tail -f nohup.out   # 在后台运行程序并通过日志输出二维码\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx Ports in Docker Compose\nDESCRIPTION: This snippet shows how to configure the Nginx ports in the `.env` file for Docker Compose deployments. This is used to change the ports that Dify is accessible on. It assumes the user is using Docker Compose to run the application.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/faqs.md#_snippet_3\n\nLANGUAGE: docker\nCODE:\n```\nEXPOSE_NGINX_PORT=80\nEXPOSE_NGINX_SSL_PORT=443\n```\n\n----------------------------------------\n\nTITLE: Setting Ollama Host Environment Variable on MacOS\nDESCRIPTION: This command sets the `OLLAMA_HOST` environment variable to `0.0.0.0` on MacOS using `launchctl`. This allows Ollama to be accessible from other machines on the network. After setting the variable, the Ollama application needs to be restarted for the changes to take effect.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/ollama.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlaunchctl setenv OLLAMA_HOST \"0.0.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Starting Stable Diffusion WebUI on Windows\nDESCRIPTION: This command navigates to the Stable Diffusion WebUI directory and starts the WebUI with API and listen options enabled. The `--api` flag enables the API, allowing Dify to interact with Stable Diffusion, and `--listen` makes the WebUI accessible from other machines on the network.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/tool-configuration/stable-diffusion.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd stable-diffusion-webui\n./webui.bat --api --listen\n```\n\n----------------------------------------\n\nTITLE: Appending a GitHub Plugin Dependency\nDESCRIPTION: This command appends a plugin dependency from a GitHub repository to the bundle project. The `--repo_pattern` argument specifies the repository, release version, and asset filename in the format `organization_name/repository_name:release/asset_filename`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/quick-start/develop-plugins/bundle.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndify-plugin bundle append github . --repo_pattern=langgenius/openai:0.0.1/openai.difypkg\n```\n\n----------------------------------------\n\nTITLE: Upload Plugin Files\nDESCRIPTION: Pushes the plugin project to the GitHub repository. It renames the default branch to `main` and then pushes the local `main` branch to the remote `origin`. It is important that `manifest.yaml` author field should match with the GitHub ID.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit branch -M main\ngit push -u origin main\n```\n\n----------------------------------------\n\nTITLE: Clone and Setup Dify (Bash)\nDESCRIPTION: This set of commands clones the Dify repository, navigates to the Docker directory, copies the example environment file, and starts the Dify application using Docker Compose. It automates the deployment process and sets up the necessary environment for running Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\ncd dify/docker\ncp .env.example .env\ndocker compose up -d  # Docker Compose V1 を使用している場合は、`docker-compose up -d` を使用してください\n```\n\n----------------------------------------\n\nTITLE: Error Mapping - Python\nDESCRIPTION: This defines a mapping of model invocation errors to unified error types.  It allows Dify to handle different errors in a standardized manner. The keys are the error types thrown to the caller, and the values are the error types thrown by the model.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/model-plugin/customizable-model.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@property\ndef _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:\n    \"\"\"\n    Map model invocation errors to unified error types.\n    The key is the error type thrown to the caller.\n    The value is the error type thrown by the model, which needs to be mapped to a\n    unified Dify error for consistent handling.\n    \"\"\"\n    # return {\n    #   InvokeConnectionError: [requests.exceptions.ConnectionError],\n    #   ...\n    # }\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository and Starting with Docker Compose\nDESCRIPTION: These commands are used to clone the Dify GitHub repository, navigate to the Docker directory, copy the example environment file, and start the Dify application using Docker Compose. This sets up the Dify environment for further configuration and use.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/learn-more/use-cases/private-ai-ollama-deepseek-dify.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\ncd dify/docker\ncp .env.example .env\ndocker compose up -d # 如果版本是 Docker Compose V1，使用以下命令：docker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Clone LocalAI Repository\nDESCRIPTION: This snippet clones the LocalAI repository and navigates to the langchain-chroma example directory. This is the first step in setting up LocalAI for integration with Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/models-integration/localai.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/go-skynet/LocalAI\n$ cd LocalAI/examples/langchain-chroma\n```\n\n----------------------------------------\n\nTITLE: Backup docker-compose.yaml\nDESCRIPTION: This snippet demonstrates how to create a backup of the `docker-compose.yaml` file by copying it with a timestamped suffix in the docker directory. It is an optional step during the upgrade process.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/dify-premium.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\ncp docker-compose.yaml docker-compose.yaml.$(date +%s).bak\n```\n\n----------------------------------------\n\nTITLE: Updating APT Package Lists\nDESCRIPTION: This command updates the package lists for upgrades and new package installations using the apt package manager. It ensures you have the latest information about available packages and their dependencies.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/tool-configuration/searxng.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\n```\n\n----------------------------------------\n\nTITLE: Checking Docker Container Status\nDESCRIPTION: This command displays the status of all Docker containers defined in the Docker Compose file. It shows if the containers are running and their port mappings.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/docker-compose.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose ps\n```\n\n----------------------------------------\n\nTITLE: Adding a Marketplace plugin dependency to the Bundle\nDESCRIPTION: This command appends a Marketplace plugin dependency to the Bundle project. The `--marketplace_pattern` parameter specifies the plugin's identifier in the Marketplace, using the format `organization_name/plugin_name:version_number`. This command assumes that `dify-plugin` is in the current working directory.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/bundle.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndify-plugin bundle append marketplace . --marketplace_pattern=langgenius/openai:0.0.1\n```\n\n----------------------------------------\n\nTITLE: Backend Directory Structure Overview\nDESCRIPTION: This snippet provides an overview of the backend directory structure of the Dify project, written in Python using Flask. It shows the purpose of each directory, such as controllers, core, models, and tasks.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/community/contribution.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n[api/]\n├── constants             // Constant settings used throughout code base.\n├── controllers           // API route definitions and request handling logic.           \n├── core                  // Core application orchestration, model integrations, and tools.\n├── docker                // Docker & containerization related configurations.\n├── events                // Event handling and processing\n├── extensions            // Extensions with 3rd party frameworks/platforms.\n├── fields                // field definitions for serialization/marshalling.\n├── libs                  // Reusable libraries and helpers.\n├── migrations            // Scripts for database migration.\n├── models                // Database models & schema definitions.\n├── services              // Specifies business logic.\n├── storage               // Private key storage.      \n├── tasks                 // Handling of async tasks and background jobs.\n└── tests\n```\n\n----------------------------------------\n\nTITLE: Example npm run dev output (Bash)\nDESCRIPTION: This snippet shows the output of running `npm run dev`, showing that the worker has access to the TOKEN variable and is running on a local port.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ npm run dev\n> dev\n> wrangler dev src/index.ts\n\n ⛅️ wrangler 3.99.0\n-------------------\n\nYour worker has access to the following bindings:\n- Vars:\n  - TOKEN: \"ban****ool\"\n⎔ Starting local server...\n[wrangler:inf] Ready on http://localhost:58445\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Docker\nDESCRIPTION: This command installs the necessary packages required to enable Docker installation, including apt-transport-https, ca-certificates, curl, and software-properties-common. These packages provide the tools for secure communication and repository management.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/tools/tool-configuration/searxng.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install apt-transport-https ca-certificates curl software-properties-common\n```\n\n----------------------------------------\n\nTITLE: Direct Reply Node Markdown Example\nDESCRIPTION: This code snippet demonstrates how to use Markdown formatting in a direct reply node within Dify. It provides an example of text that can be used to direct users to the help documentation.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/workshop/intermediate/customer-service-bot.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n很抱歉，我不能回答你的问题。如果你需要更多帮助，请查看[帮助文档](https://docs.dify.ai)。\n```\n\n----------------------------------------\n\nTITLE: Tagging and Pushing a Release in Bash\nDESCRIPTION: This snippet creates a Git tag for a specific release and pushes it to the remote GitHub repository.  This allows for versioning and easier tracking of releases. It requires the repository to be set up and committed. It also relies on connectivity to the GitHub repository.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/publish-plugins/publish-plugin-on-personal-github-repo.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit tag -a v0.0.1 -m \"Release version 0.0.1\"\n\ngit push origin v0.0.1\n```\n\n----------------------------------------\n\nTITLE: Accessing Dify Web Interface with Domain - Bash\nDESCRIPTION: This code snippet shows how to access the Dify web interface when a domain name is configured. Replace `yourdomain` with the actual domain name.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/getting-started/install-self-hosted/bt-panel.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# ドメインを設定した場合\nhttp://yourdomain/\n```\n\n----------------------------------------\n\nTITLE: Grant Execution Permissions (macOS)\nDESCRIPTION: This command grants execution permissions to the downloaded Dify plugin scaffolding tool on macOS. It ensures that the tool can be executed from the terminal.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/initialize-development-tools.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nchmod +x dify-plugin-darwin-arm64\n```\n\n----------------------------------------\n\nTITLE: Example of Dev Output (Bash)\nDESCRIPTION: This is the output of running `npm run dev` showing the wrangler CLI, the worker having access to vars like TOKEN, and that the server is ready at a localhost address.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/cloudflare-workers.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ npm run dev\n> dev\n> wrangler dev src/index.ts\n\n ⛅️ wrangler 3.99.0\n-------------------\n\nYour worker has access to the following bindings:\n- Vars:\n  - TOKEN: \"ban****ool\"\n⎔ Starting local server...\n[wrangler:inf] Ready on http://localhost:58445\n```\n\n----------------------------------------\n\nTITLE: Enabling EPEL Repository on CentOS\nDESCRIPTION: This command enables the EPEL (Extra Packages for Enterprise Linux) repository on CentOS. It uses `yum` to install the `epel-release` package, providing access to a wider range of software packages. Root or sudo privileges are required.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nsudo yum install epel-release\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Docs Repository\nDESCRIPTION: This snippet demonstrates how to clone the Dify documentation repository from GitHub to a local machine. Replace `<your-github-account>` with your actual GitHub username. This allows users to make changes locally before submitting a pull request.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/community/docs-contribution.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/<your-github-account>/dify-docs.git\n```\n\n----------------------------------------\n\nTITLE: Frontend Directory Structure Overview\nDESCRIPTION: This snippet shows the frontend directory structure of the Dify project, built on Next.js in Typescript with Tailwind CSS for styling. It describes the purpose of each directory, including app, assets, config, and models.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/community/contribution.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n[web/]\n├── app                   // layouts, pages, and components\n│   ├── (commonLayout)    // common layout used throughout the app\n│   ├── (shareLayout)     // layouts specifically shared across token-specific sessions \n│   ├── activate          // activate page\n│   ├── components        // shared by pages and layouts\n│   ├── install           // install page\n│   ├── signin            // signin page\n│   └── styles            // globally shared styles\n├── assets                // Static assets\n├── bin                   // scripts ran at build step\n├── config                // adjustable settings and options \n├── context               // shared contexts used by different portions of the app\n├── dictionaries          // Language-specific translate files \n├── docker                // container configurations\n├── hooks                 // Reusable hooks\n├── i18n                  // Internationalization configuration\n├── models                // describes data models & shapes of API responses\n├── public                // meta assets like favicon\n├── service               // specifies shapes of API actions\n├── test                  \n├── types                 // descriptions of function params and return values\n└── utils                 // Shared utility functions\n```\n\n----------------------------------------\n\nTITLE: Question Classifier Node Endpoint\nDESCRIPTION: This defines the endpoint signature for invoking the QuestionClassifier node. It specifies the required parameters like the list of `ClassConfig`, the `ModelConfig`, the `query` string, and the optional `instruction` string. The final classification result is expected to be in `NodeResponse.outputs['class_name']`.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef invoke(\n    self,\n    classes: list[ClassConfig],\n    model: ModelConfig,\n    query: str,\n    instruction: str = \"\",\n) -> NodeResponse:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository with Git\nDESCRIPTION: This command clones the Dify source code from GitHub, specifically targeting version 0.15.3. It's a prerequisite for deploying Dify using Docker Compose.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/docker-compose.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Assuming current latest version is 0.15.3\ngit clone https://github.com/langgenius/dify.git --branch 0.15.3\n```\n\n----------------------------------------\n\nTITLE: Dify Application Example Structure\nDESCRIPTION: This snippet outlines the recommended structure for sharing innovative Dify application examples. It provides a clear and organized way to present the project overview, system design, prerequisites, Dify implementation steps, and FAQs, enabling community members to understand and replicate the application easily.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/community/docs-contribution.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n1. プロジェクト概要\n   - アプリケーションの活用シーンと解決課題\n   - コア機能と特徴\n   - デモンストレーションと成果\n\n2. システム設計・処理フロー\n\n3. 事前準備（必要な場合）\n   - 必要リソース一覧\n   - ツールと依存関係\n\n4. Dify実装手順（参考）\n   - アプリケーション作成と基本設定\n   - ワークフロー構築の詳細\n   - 主要ノード設定解説\n\n5. FAQ\n```\n\n----------------------------------------\n\nTITLE: Packaging a Plugin\nDESCRIPTION: This command packages the plugin into a `.difypkg` file.  The command takes the plugin project path as an argument and creates the package in the current directory.  Replace `./slack_bot` with your actual plugin project path.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/best-practice/develop-a-slack-bot-plugin.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Replace ./slack_bot with your actual plugin project path.\n\ndify plugin package ./slack_bot\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository\nDESCRIPTION: This command clones the forked Dify repository from your GitHub account to your local machine. Replace `<github_username>` with your actual GitHub username. This step is a prerequisite for setting up the development environment.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_26\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone git@github.com:<github_username>/dify.git\n```\n\n----------------------------------------\n\nTITLE: LLMノードのプロンプト例（JSONコード生成）\nDESCRIPTION: LLMノードで使用するプロンプトの例を示します。このプロンプトは、ユーザーの要求に応じて正しい形式または間違った形式のJSONコードを生成するようにLLMに指示します。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/error-handling/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nYou are a teaching assistant. According to the user's requirements, you only output a correct or incorrect sample code in json format.\n```\n\n----------------------------------------\n\nTITLE: Convert Array to Text with Template Node\nDESCRIPTION: This Jinja template code snippet demonstrates how to convert an array to text by joining its elements using a newline character. The `articleSections` variable, assumed to be a list, is piped to the `join` filter with the argument `'/n'`. This creates a string where each element of the array is separated by a newline character, useful for converting array outputs from iteration nodes to text.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_74\n\nLANGUAGE: Jinja\nCODE:\n```\n{{ articleSections | join(\"/n\") }}\n```\n\n----------------------------------------\n\nTITLE: Define Endpoint for Retrieval API\nDESCRIPTION: Defines the endpoint to connect to the external knowledge base maintained by the developer. This endpoint will be used to send POST requests to retrieve information from the knowledge base. The endpoint typically includes the base URL followed by '/retrieval'.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_22\n\nLANGUAGE: none\nCODE:\n```\nPOST <your-endpoint>/retrieval\n```\n\n----------------------------------------\n\nTITLE: Error Response Example\nDESCRIPTION: This snippet demonstrates an example of an error response in JSON format. It includes a 'code' indicating the type of error, a 'message' providing a user-friendly explanation, and a 'status' code representing the HTTP status code.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\n{\n  \"code\": \"no_file_uploaded\",\n  \"message\": \"Please upload your file.\",\n  \"status\": 400\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Knowledge Base using Dify API (curl)\nDESCRIPTION: This snippet shows how to delete a knowledge base (dataset) using the Dify API. It requires the dataset ID and API key for authorization. A successful deletion returns a 204 No Content response.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Install GPUStack on Linux/MacOS with Shell\nDESCRIPTION: This script installs GPUStack as a service on systemd or launchd based systems. It uses curl to download and execute the installation script.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | sh -s -\n```\n\n----------------------------------------\n\nTITLE: Plugin Initialization Output (Bash)\nDESCRIPTION: This is an example of the output from the `dify plugin init` command, showing the interactive prompts and choices presented to the user during plugin initialization. The user is asked to provide plugin metadata and configure permissions.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/quick-start/develop-plugins/agent-strategy-plugin.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n➜ ./dify-plugin-darwin-arm64 plugin init                                                                                                                                 ─╯\nEdit profile of the plugin\nPlugin name (press Enter to next step): # Enter the plugin name\nAuthor (press Enter to next step): # Enter the plugin author\nDescription (press Enter to next step): # Enter the plugin description\n---\nSelect the language you want to use for plugin development, and press Enter to continue,\nBTW, you need Python 3.12+ to develop the Plugin if you choose Python.\n-> python # Select Python environment\n  go (not supported yet)\n---\nBased on the ability you want to extend, we have divided the Plugin into four types: Tool, Model, Extension, and Agent Strategy.\n\n- Tool: It's a tool provider, but not only limited to tools, you can implement an endpoint there, for example, you need both Sending Message and Receiving Message if you are\n- Model: Just a model provider, extending others is not allowed.\n- Extension: Other times, you may only need a simple http service to extend the functionalities, Extension is the right choice for you.\n- Agent Strategy: Implement your own logics here, just by focusing on Agent itself\n\nWhat's more, we have provided the template for you, you can choose one of them below:\n  tool\n-> agent-strategy # Select Agent strategy template\n  llm\n  text-embedding\n---\nConfigure the permissions of the plugin, use up and down to navigate, tab to select, after selection, press enter to finish\nBackwards Invocation:\nTools:\n    Enabled: [✔]  You can invoke tools inside Dify if it's enabled # Enabled by default\nModels:\n    Enabled: [✔]  You can invoke models inside Dify if it's enabled # Enabled by default\n    LLM: [✔]  You can invoke LLM models inside Dify if it's enabled # Enabled by default\n  → Text Embedding: [✘]  You can invoke text embedding models inside Dify if it's enabled\n    Rerank: [✘]  You can invoke rerank models inside Dify if it's enabled\n    TTS: [✘]  You can invoke TTS models inside Dify if it's enabled\n    Speech2Text: [✘]  You can invoke speech2text models inside Dify if it's enabled\n    Moderation: [✘]  You can invoke moderation models inside Dify if it's enabled\nApps:\n    Enabled: [✘]  Ability to invoke apps like BasicChat/ChatFlow/Agent/Workflow etc.\nResources:\nStorage:\n    Enabled: [✘]  Persistence storage for the plugin\n    Size: N/A  The maximum size of the storage\nEndpoints:\n    Enabled: [✘]  Ability to register endpoints\n```\n\n----------------------------------------\n\nTITLE: Dify Frontend Directory Structure\nDESCRIPTION: This snippet shows the directory structure of the Dify frontend, built with Next.js and Typescript. It highlights key directories such as app, assets, config, and utils, offering insight into the frontend's organization and components.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_29\n\nLANGUAGE: plaintext\nCODE:\n```\n[web/]\n├── app                   // layouts, pages, and components\n│   ├── (commonLayout)    // common layout used throughout the app\n│   ├── (shareLayout)     // layouts specifically shared across token-specific sessions\n│   ├── activate          // activate page\n│   ├── components        // shared by pages and layouts\n│   ├── install           // install page\n│   ├── signin            // signin page\n│   └── styles            // globally shared styles\n├── assets                // Static assets\n├── bin                   // scripts ran at build step\n├── config                // adjustable settings and options\n├── context               // shared contexts used by different portions of the app\n├── dictionaries          // Language-specific translate files\n├── docker                // container configurations\n├── hooks                 // Reusable hooks\n├── i18n                  // Internationalization configuration\n├── models                // describes data models & shapes of API responses\n├── public                // meta assets like favicon\n├── service               // specifies shapes of API actions\n├── test\n├── types                 // descriptions of function params and return values\n└── utils                 // Shared utility functions\n```\n\n----------------------------------------\n\nTITLE: Cloning Dify Repository with Git\nDESCRIPTION: This command clones the Dify repository from GitHub, allowing you to work with the source code locally. It's the first step to deploy Dify locally. Ensure you have Git installed on your system.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_16\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone https://github.com/langgenius/dify.git\n```\n\n----------------------------------------\n\nTITLE: Extract Parameter Example\nDESCRIPTION: This example demonstrates how to use the ParameterExtractor node to extract a person's name from a conversation using a Dify plugin.  It shows how to configure the ParameterConfig and ModelConfig, and how to invoke the node with a query and instruction.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/plugins/schema-definition/reverse-invocation-of-the-dify-service/node.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Generator\nfrom dify_plugin.entities.tool import ToolInvokeMessage\nfrom dify_plugin import Tool\nfrom dify_plugin.entities.workflow_node import ModelConfig, ParameterConfig\n\nclass ParameterExtractorTool(Tool):\n    def _invoke(\n        self, tool_parameters: dict\n    ) -> Generator[ToolInvokeMessage, None, None]:\n        response = self.session.workflow_node.parameter_extractor.invoke(\n            parameters=[\n                ParameterConfig(\n                    name=\"name\",\n                    description=\"name of the person\",\n                    required=True,\n                    type=\"string\",\n                )\n            ],\n            model=ModelConfig(\n                provider=\"langgenius/openai/openai\",\n                name=\"gpt-4o-mini\",\n                completion_params={},\n            ),\n            query=\"My name is John Doe\",\n            instruction=\"Extract the name of the person\",\n        )\n        yield self.create_text_message(response.outputs[\"name\"])\n```\n\n----------------------------------------\n\nTITLE: Example Docker Compose PS Output\nDESCRIPTION: This is example output from the `docker compose ps` command, displaying the status, ports, and other details of the running Docker containers.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\n[+] Running 11/11\n ✔ Network docker_ssrf_proxy_network  Created                                                                 0.1s \n ✔ Network docker_default             Created                                                                 0.0s \n ✔ Container docker-redis-1           Started                                                                 2.4s \n ✔ Container docker-ssrf_proxy-1      Started                                                                 2.8s \n ✔ Container docker-sandbox-1         Started                                                                 2.7s \n ✔ Container docker-web-1             Started                                                                 2.7s \n ✔ Container docker-weaviate-1        Started                                                                 2.4s \n ✔ Container docker-db-1              Started                                                                 2.7s \n ✔ Container docker-api-1             Started                                                                 6.5s \n ✔ Container docker-worker-1          Started                                                                 6.4s \n ✔ Container docker-nginx-1           Started                                                                 7.1s\n```\n\nLANGUAGE: Shell\nCODE:\n```\nNAME                  IMAGE                              COMMAND                   SERVICE      CREATED              STATUS                        PORTS\ndocker-api-1          langgenius/dify-api:0.6.13         \"/bin/bash /entrypoi...\"   api          About a minute ago   Up About a minute             5001/tcp\ndocker-db-1           postgres:15-alpine                 \"docker-entrypoint.s...\"   db           About a minute ago   Up About a minute (healthy)   5432/tcp\ndocker-nginx-1        nginx:latest                       \"sh -c 'cp /docker-e...\"   nginx        About a minute ago   Up About a minute             0.0.0.0:80->80/tcp, :::80->80/tcp, 0.0.0.0:443->443/tcp, :::443->443/tcp\ndocker-redis-1        redis:6-alpine                     \"docker-entrypoint.s...\"   redis        About a minute ago   Up About a minute (healthy)   6379/tcp\ndocker-sandbox-1      langgenius/dify-sandbox:0.2.1      \"/main\"                   sandbox      About a minute ago   Up About a minute             \ndocker-ssrf_proxy-1   ubuntu/squid:latest                \"sh -c 'cp /docker-e...\"   ssrf_proxy   About a minute ago   Up About a minute             3128/tcp\ndocker-weaviate-1     semitechnologies/weaviate:1.19.0   \"/bin/weaviate --hos...\"   weaviate     About a minute ago   Up About a minute             \ndocker-web-1          langgenius/dify-web:0.6.13         \"/bin/sh ./entrypoin...\"   web          About a minute ago   Up About a minute             3000/tcp\ndocker-worker-1       langgenius/dify-api:0.6.13         \"/bin/bash /entrypoi...\"   worker       About a minute ago   Up About a minute             5001/tcp\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response: Invalid Request (Temperature)\nDESCRIPTION: This snippet shows an error response indicating an invalid request due to the temperature parameter being out of range for the Anthropic model.  It suggests configuring the parameter within the allowed range (0..1).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/application-orchestrate/llms-use-faq.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"Anthropic: Error code: 400 - f'error': f'type': \\\"invalid request error, 'message': 'temperature: range: -1 or 0..1)\\\"\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Start Xinference Locally\nDESCRIPTION: This command starts the Xinference service locally, making it accessible for model inference and embedding. By default, it starts a worker with the endpoint `http://127.0.0.1:9997`.  You can modify the host and port using the `--help` option.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n$ xinference-local\n2023-08-20 19:21:05,265 xinference   10148 INFO     Xinference successfully started. Endpoint: http://127.0.0.1:9997\n2023-08-20 19:21:05,266 xinference.core.supervisor 10148 INFO     Worker 127.0.0.1:37822 has been added successfully\n2023-08-20 19:21:05,267 xinference.deploy.worker 10148 INFO     Xinference worker successfully started.\n```\n\n----------------------------------------\n\nTITLE: Shell: Directory Structure for Anthropic Tests\nDESCRIPTION: This shell code illustrates the recommended directory structure for writing test code for the Anthropic provider. It shows the location of the provider test file (`test_provider.py`) and the model type test file (`test_llm.py`) within the `tests/anthropic` module.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/new-provider.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n.\n├── __init__.py\n├── anthropic\n│   ├── __init__.py\n│   ├── test_llm.py       # LLM Test\n│   └── test_provider.py  # Provider Test\n```\n\n----------------------------------------\n\nTITLE: API Ping Check Response\nDESCRIPTION: This JSON snippet shows the expected response when the API receives a ping request. The API should return result=pong.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/extension/api-based-extension/README.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"result\": \"pong\"\n}\n```\n\n----------------------------------------\n\nTITLE: Access Docker Host from Container\nDESCRIPTION: This shows how to access the Docker host from within a container by replacing `localhost` with `host.docker.internal` in the Ollama service URL.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_80\n\nLANGUAGE: HTTP\nCODE:\n```\nhttp://host.docker.internal:11434\n```\n\n----------------------------------------\n\nTITLE: Tailing Docker Logs for Build Completion\nDESCRIPTION: This command tails the logs of the specified Docker container, 'langchain-chroma-api-1'. This is useful for monitoring the progress and ensuring the successful startup of the LocalAI service deployed via Docker.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_13\n\nLANGUAGE: docker\nCODE:\n```\n$ docker logs -f langchain-chroma-api-1\n```\n\n----------------------------------------\n\nTITLE: JSON Error Response: Rate Limit Exceeded\nDESCRIPTION: This JSON snippet shows an error message indicating that the rate limit for the 'default-gpt-3.5-turbo' model has been reached. It advises to try again later and suggests adding a payment method to increase the rate limit.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/llms-use-faq.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"error\": \"Rate limit reached for default-gpt-3.5-turboin organization org-wDrZCxxxxxxxxxissoZb on requestsper min。 Limit: 3 / min. Please try again in 20s. Contact us through our help center   at help.openai.com   if you continue to haveissues. Please add a payment method toyour account to increase your rate limit.Visit https://platform.openai.com/account/billingto add a payment method.\"\n}\n```\n\n----------------------------------------\n\nTITLE: NPM Install Dependencies\nDESCRIPTION: Installs the required Node.js dependencies for the web frontend.  This command must be executed within the web directory. Requires Node.js and NPM to be installed.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_30\n\nLANGUAGE: JavaScript\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Define LLMResultChunk Class in Python\nDESCRIPTION: This code defines the `LLMResultChunk` class, which inherits from `BaseModel`. It represents each iteration entity in streaming returns. It contains attributes for `model`, `prompt_messages`, `system_fingerprint`, and `delta` (an LLMResultChunkDelta object).\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/model-configuration/interfaces.md#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nclass LLMResultChunk(BaseModel):\n    \"\"\"\n    Model class for llm result chunk.\n    \"\"\"\n    model: str  # Actual used modele\n    prompt_messages: list[PromptMessage]  # prompt messages\n    system_fingerprint: Optional[str] = None  # request fingerprint, refer to OpenAI definition\n    delta: LLMResultChunkDelta\n```\n\n----------------------------------------\n\nTITLE: Dify Version Check (Bash)\nDESCRIPTION: Dify 開発ツールがインストールされているかを確認するために、ターミナルで `dify version` コマンドを実行し、バージョン情報を表示します。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/plugins/publish-plugins/package-plugin-file-and-publish.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndify version\n```\n\n----------------------------------------\n\nTITLE: NPM Build (Web)\nDESCRIPTION: Builds the web application using NPM. This bundles and optimizes the code for production deployment. Requires NPM and associated dependencies.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_48\n\nLANGUAGE: JavaScript\nCODE:\n```\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: Tokenizer Load Error Message\nDESCRIPTION: This error message indicates that the tokenizer 'gpt2' could not be loaded. This usually means there's a problem with the gpt2 model or its configuration.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/learn-more/faq/install-faq.md#_snippet_13\n\nLANGUAGE: text\nCODE:\n```\nCan't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.\n```\n\n----------------------------------------\n\nTITLE: Installing FFmpeg on Ubuntu\nDESCRIPTION: These commands are used to install FFmpeg on Ubuntu systems. First it updates the package list using `apt-get update`, then it installs FFmpeg using `apt-get install ffmpeg`. Root or sudo privileges are required.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get update\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install ffmpeg\n```\n\n----------------------------------------\n\nTITLE: Prompt Message Tool Class in Python\nDESCRIPTION: This code defines the Prompt Message Tool class. It includes the name, description, and parameters of a tool.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/model-configuration/interfaces.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nclass PromptMessageTool(BaseModel):\n    \"\"\"\n    Model class for prompt message tool.\n    \"\"\"\n    name: str\n    description: str\n    parameters: dict\n```\n\n----------------------------------------\n\nTITLE: Updating Yum Package on CentOS\nDESCRIPTION: This command updates the yum package manager and all installed packages on CentOS. It is essential to update the package list after adding a new repository. Root or sudo privileges are required.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/getting-started/install-self-hosted/install-faq.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nsudo yum update\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Configuration (Bash)\nDESCRIPTION: This example shows the configuration of environment variables for remote plugin debugging. `INSTALL_METHOD` is set to `remote`, and `REMOTE_INSTALL_HOST`, `REMOTE_INSTALL_PORT`, and `REMOTE_INSTALL_KEY` are set to appropriate values.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/plugins/quick-start/develop-plugins/tool-plugin.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nINSTALL_METHOD=remote\nREMOTE_INSTALL_HOST=remote\nREMOTE_INSTALL_PORT=5003\nREMOTE_INSTALL_KEY=****-****-****-****-****\n```\n\n----------------------------------------\n\nTITLE: Start Celery Worker Service\nDESCRIPTION: Starts the Celery worker service. This service processes asynchronous tasks such as knowledge base updates. The command specifies the Celery app, concurrency, queues, and log level.  Different commands are provided for Linux/MacOS vs Windows.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/lang-check/commit-79f79-0214.txt#_snippet_45\n\nLANGUAGE: Bash\nCODE:\n```\ncelery -A app.celery worker -P gevent -c 1 -Q dataset,generation,mail,ops_trace --loglevel INFO\n```\n\nLANGUAGE: Bash\nCODE:\n```\ncelery -A app.celery worker -P solo --without-gossip --without-mingle -Q dataset,generation,mail,ops_trace --loglevel INFO\n```\n\n----------------------------------------\n\nTITLE: Install GPUStack on Windows with PowerShell\nDESCRIPTION: This command installs GPUStack on Windows using PowerShell. It downloads the installation script from the provided URL and executes it. Ensure to run PowerShell as administrator.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_4\n\nLANGUAGE: PowerShell\nCODE:\n```\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n```\n\n----------------------------------------\n\nTITLE: Text-to-Speech Model Invocation Interface in Python\nDESCRIPTION: This snippet defines the `_invoke` method for a text-to-speech model. It accepts a model name, credentials, text content, a streaming flag, and an optional user ID. It returns the translated audio file.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndef _invoke(self, model: str, credentials: dict, content_text: str, streaming: bool, user: Optional[str] = None):\n      \"\"\"\n      Invoke large language model\n\n      :param model: model name\n      :param credentials: model credentials\n      :param content_text: text content to be translated\n      :param streaming: output is streaming\n      :param user: unique user id\n      :return: translated audio file\n      \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Verify Go installation\nDESCRIPTION: Verifies that Go is installed and displays the installed version. This command is used to confirm that the Go installation was successful.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/development/backend/sandbox/README.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\ngo version  # Displays the installed Go version\n```\n\n----------------------------------------\n\nTITLE: Renaming .env.example to .env\nDESCRIPTION: This snippet renames the `.env.example` file to `.env`, allowing the configuration of environment variables for LocalAI.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ mv .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Vectorizer Tool Invoke (Python)\nDESCRIPTION: Vectorizer.AIを使用して、DallE3が生成したPNG画像をベクター画像に変換する例です。変数プールから画像を取得し、ベクター化されたSVG画像を返します。\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/tools/advanced-tool-integration.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom core.tools.tool.builtin_tool import BuiltinTool\nfrom core.tools.entities.tool_entities import ToolInvokeMessage, ToolParameter\nfrom core.tools.errors import ToolProviderCredentialValidationError\n\nfrom typing import Any, Dict, List, Union\nfrom httpx import post\nfrom base64 import b64decode\n\nclass VectorizerTool(BuiltinTool):\n    def _invoke(self, user_id: str, tool_Parameters: Dict[str, Any]) \\\n        -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:\n        \"\"\"\n            ツールを呼び出す\n        \"\"\"\n        api_key_name = self.runtime.credentials.get('api_key_name', None)\n        api_key_value = self.runtime.credentials.get('api_key_value', None)\n\n        if not api_key_name or not api_key_value:\n            raise ToolProviderCredentialValidationError('APIキー名と値を入力してください')\n\n        # 画像IDを取得、画像IDの定義はget_runtime_parametersで確認できます。\n        image_id = tool_Parameters.get('image_id', '')\n        if not image_id:\n            return self.create_text_message('画像IDを入力してください')\n\n        # 変数プールから以前のDallEが生成した画像を取得\n        image_binary = self.get_variable_file(self.VARIABLE_KEY.IMAGE)\n        if not image_binary:\n            return self.create_text_message('画像が見つかりません。まずユーザーに画像を生成するよう依頼してください。')\n\n        # ベクター画像を生成\n        response = post(\n            'https://vectorizer.ai/api/v1/vectorize',\n            files={ 'image': image_binary },\n            data={ 'mode': 'test' },\n            auth=(api_key_name, api_key_value), \n            timeout=30\n        )\n\n        if response.status_code != 200:\n            raise Exception(response.text)\n        \n        return [\n            self.create_text_message('ベクター化されたSVGが画像として保存されました。'),\n            self.create_blob_message(blob=response.content,\n                                    meta={'mime_type': 'image/svg+xml'})\n        ]\n    \n    def get_runtime_parameters(self) -> List[ToolParameter]:\n        \"\"\"\n        実行時パラメータをオーバーライド\n        \"\"\"\n        # ここでは、ツールパラメータリストをオーバーライドし、image_idを定義し、そのオプションリストを現在の変数プール内のすべての画像に設定しました。ここでの設定はyamlの設定と一致しています。\n        return [\n            ToolParameter.get_simple_instance(\n                name='image_id',\n                llm_description=f'ベクター化する画像ID。画像IDは{[i.name for i in self.list_default_image_variables()]}の中から指定してください。',\n                type=ToolParameter.ToolParameterType.SELECT,\n                required=True,\n                options=[i.name for i in self.list_default_image_variables()]\n            )\n        ]\n    \n    def is_tool_available(self) -> bool:\n        # 変数プールに画像がある場合のみ、LLMがこのツールを使用する必要があります。\n        return len(self.list_default_image_variables()) > 0\n```\n\n----------------------------------------\n\nTITLE: Installing Go 1.21\nDESCRIPTION: Installs the Go programming language environment version 1.20.6 on a Debian-based system, a prerequisite for DifySandbox. This enables the execution of Go-based components within the sandbox.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/development/backend/sandbox/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install -y golang-1.20.6\n```\n\n----------------------------------------\n\nTITLE: Enabling/Disabling Built-in Metadata Field with cURL\nDESCRIPTION: This cURL command enables or disables a built-in metadata field in the Dify knowledge base. It requires the dataset ID and the action (enable or disable).  It modifies the enabled/disabled status of the built-in metadata field. It returns a 200 success status if successful.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-dataset-via-api.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncurl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/metadata/built-in/{action}' \\\n--header 'Authorization: Bearer {api_key}'\n```\n\n----------------------------------------\n\nTITLE: Cloning a Git Repository\nDESCRIPTION: This command clones a forked repository from GitHub using the Git protocol. Replace `<github_username>` with your actual GitHub username. This is the first step in setting up a local development environment for contributing to Dify.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_61\n\nLANGUAGE: git\nCODE:\n```\ngit clone git@github.com:<github_username>/dify.git\n```\n\n----------------------------------------\n\nTITLE: Unauthorized File Access in Python\nDESCRIPTION: This Python code attempts to read the `/etc/passwd` file, which is a security risk because it contains user account information. The Cloudflare WAF automatically blocks this code to prevent unauthorized access to sensitive system data.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/workflow/node/code.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef main() -> dict:\n    return {\n        \"result\": open(\"/etc/passwd\").read(),\n    }\n```\n\n----------------------------------------\n\nTITLE: Defining Google Tool Provider Icon SVG\nDESCRIPTION: SVG content for the google tool provider icon.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/guides/tools/quick-tool-integration.md#_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"25\" viewBox=\"0 0 24 25\" fill=\"none\">\n  <path d=\"M22.501 12.7332C22.501 11.8699 22.4296 11.2399 22.2748 10.5865H12.2153V14.4832H18.12C18.001 15.4515 17.3582 16.9099 15.9296 17.8898L15.9096 18.0203L19.0902 20.435L19.3106 20.4565C21.3343 18.6249 22.501 15.9298 22.501 12.7332Z\" fill=\"#4285F4\"/>\n  <path d=\"M12.214 23C15.1068 23 17.5353 22.0666 19.3092 20.4567L15.9282 17.8899C15.0235 18.5083 13.8092 18.9399 12.214 18.9399C9.38069 18.9399 6.97596 17.1083 6.11874 14.5766L5.99309 14.5871L2.68583 17.0954L2.64258 17.2132C4.40446 20.6433 8.0235 23 12.214 23Z\" fill=\"#34A853\"/>\n  <path d=\"M6.12046 14.5766C5.89428 13.9233 5.76337 13.2233 5.76337 12.5C5.76337 11.7766 5.89428 11.0766 6.10856 10.4233L6.10257 10.2841L2.75386 7.7355L2.64429 7.78658C1.91814 9.20993 1.50146 10.8083 1.50146 12.5C1.50146 14.1916 1.91814 15.7899 2.64429 17.2132L6.12046 14.5766Z\" fill=\"#FBBC05\"/>\n  <path d=\"M12.2141 6.05997C14.2259 6.05997 15.583 6.91163 16.3569 7.62335L19.3807 4.73C17.5236 3.03834 15.1069 2 12.2141 2C8.02353 2 4.40447 4.35665 2.64258 7.78662L6.10686 10.4233C6.97598 7.89166 9.38073 6.05997 12.2141 6.05997Z\" fill=\"#EB4335\"/>\n</svg>\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Example for Importing (Duplicate)\nDESCRIPTION: This code snippet shows a JSON schema example that can be imported to create a schema with comment and rating fields. The example includes a 'comment' field of type string and a 'rating' field of type number. This snippet illustrates how to quickly define a simple schema structure. It is a duplicate from earlier in the document.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/guides/workflow/node/llm.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n \"comment\": \"This is great!\",\n \"rating\": 5\n}\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Docker Directory\nDESCRIPTION: This command changes the current directory to the `docker` directory within the Dify source code. This is a prerequisite for running Docker Compose commands.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/getting-started/install-self-hosted/docker-compose.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd dify/docker\n```\n\n----------------------------------------\n\nTITLE: Reading System File (Blocked by WAF)\nDESCRIPTION: This Python code snippet attempts to read the `/etc/passwd` file, which is a critical system file. This action is considered a security risk and will be automatically blocked by Cloudflare WAF.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/en/llms.txt#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\ndef main() -> dict:\n    return {\n        \"result\": open(\"/etc/passwd\").read(),\n    }\n```\n\n----------------------------------------\n\nTITLE: Potentially Dangerous File Access in Python\nDESCRIPTION: This Python code snippet demonstrates a potentially dangerous operation: attempting to read the `/etc/passwd` file. This file contains sensitive user account information and should not be accessed directly. The code is provided as an example of code that will be blocked by Cloudflare WAF for security reasons.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/zh_CN/llms.txt#_snippet_70\n\nLANGUAGE: Python\nCODE:\n```\ndef main() -> dict:\n    return {\n        \"result\": open(\"/etc/passwd\").read(),\n    }\n```\n\n----------------------------------------\n\nTITLE: Attempting to Access Sensitive File\nDESCRIPTION: This code snippet shows a potentially dangerous operation: attempting to read the contents of the `/etc/passwd` file. This is flagged as dangerous due to unauthorized file access and sensitive information disclosure.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/llms.txt#_snippet_71\n\nLANGUAGE: Python\nCODE:\n```\ndef main() -> dict:\n    return {\n        \"result\": open(\"/etc/passwd\").read(),\n    }\n```\n\n----------------------------------------\n\nTITLE: API Response Example\nDESCRIPTION: This example shows the expected JSON response from the API endpoint after processing the request for weather information. It includes a \"result\" field containing a formatted string with weather details.\nSOURCE: https://github.com/langgenius/dify-docs/blob/main/jp/guides/extension/api-based-extension/README.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"result\": \"City: London\\nTemperature: 10°C\\nRealFeel®: 8°C\\nAir Quality: Poor\\nWind Direction: ENE\\nWind Speed: 8 km/h\\nWind Gusts: 14 km/h\\nPrecipitation: Light rain\"\n}\n```"
  }
]
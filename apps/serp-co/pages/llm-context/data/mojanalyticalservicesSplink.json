[
  {
    "owner": "moj-analytical-services",
    "repo": "splink",
    "content": "TITLE: Estimating Parameters with EM Algorithm Blocking on Name Fields\nDESCRIPTION: Uses the Expectation Maximisation algorithm to estimate parameters by blocking on first_name and surname fields. This approach generates pairwise record comparisons with exact matches on these fields to estimate parameters for other comparison columns.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntraining_blocking_rule = block_on(\"first_name\", \"surname\")\ntraining_session_fname_sname = (\n    linker.training.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Splink Settings with Comparison Functions\nDESCRIPTION: Sets up the Splink configuration with dedupe-only link type, blocking rules for prediction generation, and various comparison functions for different fields including name, date of birth, city, and email.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/pairwise_labels.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    comparisons=[\n        cl.NameComparison(\"first_name\"),\n        cl.NameComparison(\"surname\"),\n        cl.DateOfBirthComparison(\n            \"dob\",\n            input_is_string=True,\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        cl.EmailComparison(\"email\"),\n    ],\n    retain_intermediate_calculation_columns=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Splink Model with SettingsCreator\nDESCRIPTION: This code snippet demonstrates how to create a basic Splink model using the SettingsCreator object, defining the link type as deduplication, setting blocking rules based on first_name or a combination of surname and dob, and configuring comparisons for various fields including name, date, city, and email fields.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/settings.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_template_library as ctl\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\", \"dob\"),\n    ],\n    comparisons=[\n        ctl.NameComparison(\"first_name\"),\n        ctl.NameComparison(\"surname\"),\n        ctl.DateComparison(\n            \"dob\",\n            input_is_string=True,\n            datetime_metrics=[\"month\", \"year\"],\n            datetime_thresholds=[\n                1,\n                1,\n            ],\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        ctl.EmailComparison(\"email\"),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Splink Deduplication Model\nDESCRIPTION: Complete example demonstrating how to set up a Splink deduplication model using the DuckDB backend. The code imports necessary modules, creates settings with various comparison types, estimates model parameters, generates predictions, and clusters results.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/getting_started.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndb_api = DuckDBAPI()\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.NameComparison(\"first_name\"),\n        cl.JaroAtThresholds(\"surname\"),\n        cl.DateOfBirthComparison(\n            \"dob\",\n            input_is_string=True,\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        cl.EmailComparison(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\", \"dob\"),\n        block_on(\"surname\"),\n    ]\n)\n\nlinker = Linker(df, settings, db_api)\n\nlinker.training.estimate_probability_two_random_records_match(\n    [block_on(\"first_name\", \"surname\")],\n    recall=0.7,\n)\n\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    block_on(\"first_name\", \"surname\")\n)\n\nlinker.training.estimate_parameters_using_expectation_maximisation(block_on(\"email\"))\n\npairwise_predictions = linker.inference.predict(threshold_match_weight=-5)\n\nclusters = linker.clustering.cluster_pairwise_predictions_at_threshold(\n    pairwise_predictions, 0.95\n)\n\ndf_clusters = clusters.as_pandas_dataframe(limit=5)\n```\n\n----------------------------------------\n\nTITLE: Configuring Splink Settings for Person Record Linkage\nDESCRIPTION: Creates a Splink configuration with blocking rules and comparison functions specifically designed for person data. Includes comparisons for names, date of birth, postcodes, birth places, and occupations with appropriate string matching methods.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/quick_and_dirty_persons.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import block_on, SettingsCreator\nimport splink.comparison_library as cl\n\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"full_name\"),\n        block_on(\"substr(full_name,1,6)\", \"dob\", \"birth_place\"),\n        block_on(\"dob\", \"birth_place\"),\n        block_on(\"postcode_fake\"),\n    ],\n    comparisons=[\n        cl.ForenameSurnameComparison(\n            \"first_name\",\n            \"surname\",\n            forename_surname_concat_col_name=\"first_and_surname\",\n        ),\n        cl.DateOfBirthComparison(\n            \"dob\",\n            input_is_string=True,\n        ),\n        cl.LevenshteinAtThresholds(\"postcode_fake\", 2),\n        cl.JaroWinklerAtThresholds(\"birth_place\", 0.9).configure(\n            term_frequency_adjustments=True\n        ),\n        cl.ExactMatch(\"occupation\").configure(term_frequency_adjustments=True),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Training Splink Model for Transaction Linking\nDESCRIPTION: These code snippets train the Splink model by estimating parameters using random sampling and expectation maximisation. It includes steps for estimating u parameters and running EM algorithm on different blocking rules.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/transactions.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n```\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_parameters_using_expectation_maximisation(block_on(\"memo\"))\n```\n\nLANGUAGE: python\nCODE:\n```\nsession = linker.training.estimate_parameters_using_expectation_maximisation(block_on(\"amount\"))\n```\n\n----------------------------------------\n\nTITLE: Estimating U Probabilities with Random Sampling\nDESCRIPTION: Estimates the u probabilities (probability of agreement for non-matches) using random sampling with a maximum of 1 million pairs.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/pairwise_labels.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n```\n\n----------------------------------------\n\nTITLE: Complete Splink Data Linkage Model Implementation\nDESCRIPTION: This code demonstrates a complete Splink model implementation, from defining settings through training and generating clusters. It includes initializing the database API, loading sample data, configuring the model settings, training the model using expectation maximization with different blocking rules, and generating cluster results.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/settings.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_template_library as ctl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndb_api = DuckDBAPI()\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    comparisons=[\n        ctl.NameComparison(\"first_name\"),\n        ctl.NameComparison(\"surname\"),\n        ctl.DateComparison(\n            \"dob\",\n            input_is_string=True,\n            datetime_metrics=[\"month\", \"year\"],\n            datetime_thresholds=[\n                1,\n                1,\n            ],\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        ctl.EmailComparison(\"email\"),\n    ],\n)\n\nlinker = Linker(df, settings, db_api=db_api)\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on(\"first_name\", \"surname\")\nlinker.training.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.training.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\npairwise_predictions = linker.inference.predict()\n\nclusters = linker.clustering.cluster_pairwise_predictions_at_threshold(pairwise_predictions, 0.95)\nclusters.as_pandas_dataframe(limit=5)\n```\n\n----------------------------------------\n\nTITLE: Configuring Splink Linkage Settings\nDESCRIPTION: Creates linkage configuration using Splink's SettingsCreator with various comparison methods for fields like names, dates, city, and email. Sets up blocking rules for prediction and enables retention of intermediate calculations.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/spark/deduplicate_1k_synthetic.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nfrom splink import Linker, SettingsCreator, SparkAPI, block_on\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.NameComparison(\"first_name\"),\n        cl.NameComparison(\"surname\"),\n        cl.LevenshteinAtThresholds(\n            \"dob\"\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        cl.EmailComparison(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        \"l.surname = r.surname\",  # alternatively, you can write BRs in their SQL form\n    ],\n    retain_intermediate_calculation_columns=True,\n    em_convergence=0.01,\n)\n```\n\n----------------------------------------\n\nTITLE: Using Splink for Probabilistic Record Linkage in Python\nDESCRIPTION: This code demonstrates a complete Splink workflow for deduplicating records. It includes setting up comparisons, training the model using expectation maximization, making predictions on pairs of records, and clustering to generate unique IDs.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/README.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_template_library as ctl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndb_api = DuckDBAPI()\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.JaroWinklerAtThresholds(\"first_name\", [0.9, 0.7]),\n        cl.JaroAtThresholds(\"surname\", [0.9, 0.7]),\n        ctl.DateComparison(\n            \"dob\",\n            input_is_string=True,\n            datetime_metrics=[\"year\", \"month\"],\n            datetime_thresholds=[1, 1],\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        ctl.EmailComparison(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ]\n)\n\nlinker = Linker(df, settings, db_api)\n\nlinker.training.estimate_probability_two_random_records_match(\n    [block_on(\"first_name\", \"surname\")],\n    recall=0.7,\n)\n\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    block_on(\"first_name\", \"surname\")\n)\n\nlinker.training.estimate_parameters_using_expectation_maximisation(block_on(\"dob\"))\n\npairwise_predictions = linker.inference.predict(threshold_match_weight=-10)\n\nclusters = linker.clustering.cluster_pairwise_predictions_at_threshold(\n    pairwise_predictions, 0.95\n)\n\ndf_clusters = clusters.as_pandas_dataframe(limit=5)\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink Linker and Estimating Match Probability\nDESCRIPTION: Creates a Linker object with DuckDB as the backend database and defines deterministic rules for training. Estimates the probability of two random records matching based on these rules with a recall target of 0.6.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/quick_and_dirty_persons.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import Linker, DuckDBAPI\n\n\nlinker = Linker(df, settings, db_api=DuckDBAPI(), set_up_basic_logging=False)\ndeterministic_rules = [\n    \"l.full_name = r.full_name\",\n    \"l.postcode_fake = r.postcode_fake and l.dob = r.dob\",\n]\n\nlinker.training.estimate_probability_two_random_records_match(\n    deterministic_rules, recall=0.6\n)\n```\n\n----------------------------------------\n\nTITLE: Estimating M Probabilities from Pairwise Labels\nDESCRIPTION: Registers the pairwise labels table with the database and uses it to estimate the m probabilities (probability of agreement for matches).\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/pairwise_labels.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Register the pairwise labels table with the database, and then use it to estimate the m values\nlabels_df = linker.table_management.register_labels_table(pairwise_labels, overwrite=True)\nlinker.training.estimate_m_from_pairwise_labels(labels_df)\n\n\n# If the labels table already existing in the dataset you could run\n# linker.training.estimate_m_from_pairwise_labels(\"labels_tablename_here\")\n```\n\n----------------------------------------\n\nTITLE: Designing Blocking Rules for Transaction Linking\nDESCRIPTION: This snippet defines blocking rules for linking transactions, considering differences in transaction dates and amounts. It also includes a chart to analyze the cumulative comparisons resulting from these blocking rules.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/transactions.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import DuckDBAPI, block_on\nfrom splink.blocking_analysis import (\n    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,\n)\n\n# Design blocking rules that allow for differences in transaction date and amounts\nblocking_rule_date_1 = \"\"\"\n    strftime(l.transaction_date, '%Y%m') = strftime(r.transaction_date, '%Y%m')\n    and substr(l.memo, 1,3) = substr(r.memo,1,3)\n    and l.amount/r.amount > 0.7   and l.amount/r.amount < 1.3\n\"\"\"\n\n# Offset by half a month to ensure we capture case when the dates are e.g. 31st Jan and 1st Feb\nblocking_rule_date_2 = \"\"\"\n    strftime(l.transaction_date+15, '%Y%m') = strftime(r.transaction_date, '%Y%m')\n    and substr(l.memo, 1,3) = substr(r.memo,1,3)\n    and l.amount/r.amount > 0.7   and l.amount/r.amount < 1.3\n\"\"\"\n\nblocking_rule_memo = block_on(\"substr(memo,1,9)\")\n\nblocking_rule_amount_1 = \"\"\"\nround(l.amount/2,0)*2 = round(r.amount/2,0)*2 and yearweek(r.transaction_date) = yearweek(l.transaction_date)\n\"\"\"\n\nblocking_rule_amount_2 = \"\"\"\nround(l.amount/2,0)*2 = round((r.amount+1)/2,0)*2 and yearweek(r.transaction_date) = yearweek(l.transaction_date + 4)\n\"\"\"\n\nblocking_rule_cheat = block_on(\"unique_id\")\n\n\nbrs = [\n    blocking_rule_date_1,\n    blocking_rule_date_2,\n    blocking_rule_memo,\n    blocking_rule_amount_1,\n    blocking_rule_amount_2,\n    blocking_rule_cheat,\n]\n\n\ndb_api = DuckDBAPI()\n\ncumulative_comparisons_to_be_scored_from_blocking_rules_chart(\n    table_or_tables=[df_origin, df_destination],\n    blocking_rules=brs,\n    db_api=db_api,\n    link_type=\"link_only\"\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Predictions\nDESCRIPTION: Uses the trained model to generate predictions with a threshold match probability of 0.2.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/05_Predicting_results.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf_predictions = linker.inference.predict(threshold_match_probability=0.2)\ndf_predictions.as_pandas_dataframe(limit=5)\n```\n\n----------------------------------------\n\nTITLE: Profiling Column Value Distributions in Splink\nDESCRIPTION: Analyzes the distribution of values across columns to understand cardinality and skew. This helps identify which columns will be most useful for linkage based on their uniqueness and distribution patterns.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/02_Exploratory_analysis.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.exploratory import profile_columns\n\nprofile_columns(df, db_api=DuckDBAPI(), top_n=10, bottom_n=5)\n```\n\n----------------------------------------\n\nTITLE: Stricter Training Block Implementation in Splink\nDESCRIPTION: Shows how to implement a stricter blocking rule that blocks on both 'first_name' and 'surname' columns to reduce the number of record pairs considered during training while maintaining a good mix of matches and non-matches.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/blocking/model_training.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.duckdb.blocking_rule_library import block_on\nblocking_rule = block_on([\"first_name\", \"surname\"])\nlinker.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Splink Linker with Custom Settings\nDESCRIPTION: Sets up a Splink Linker with custom comparison settings for matching records between datasets. The configuration includes token-based name comparison with frequency weighting, postcode comparison, and exact address number matching, along with multiple blocking rules.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on\nimport splink.comparison_library as cl\ncon = duckdb.connect(\":default:\")\ndb_api = DuckDBAPI(connection=con)\n\n\nsettings = SettingsCreator(\n    link_type=\"link_only\",\n    unique_id_column_name=\"unique_id\",\n    probability_two_random_records_match=1/1e6,\n    comparisons=[\n        {\n            \"output_column_name\": \"name_tokens_with_freq\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": '\"name_tokens_with_freq_l\" IS NULL OR \"name_tokens_with_freq_r\" IS NULL',\n                    \"label_for_charts\": \"name_tokens_with_freq is NULL\",\n                    \"is_null_level\": True,\n                },\n                {\n                    \"sql_condition\": f\"\"\"\n                    {calculate_tf_product_array_sql(\"name_tokens_with_freq\")} < 1e-12\n                    \"\"\",\n                    \"label_for_charts\": \"Array product is less than 1e-10\",\n                },\n                {\n                    \"sql_condition\": f\"\"\"\n                    {calculate_tf_product_array_sql(\"name_tokens_with_freq\")} < 1e-10\n                    \"\"\",\n                    \"label_for_charts\": \"Array product is less than 1e-10\",\n                },\n                {\n                    \"sql_condition\": f\"\"\"\n                    {calculate_tf_product_array_sql(\"name_tokens_with_freq\")} < 1e-8\n                    \"\"\",\n                    \"label_for_charts\": \"Array product is less than 1e-8\",\n                },\n                {\n                    \"sql_condition\": f\"\"\"\n                    {calculate_tf_product_array_sql(\"name_tokens_with_freq\")} < 1e-6\n                    \"\"\",\n                    \"label_for_charts\": \"Array product is less than 1e-6\",\n                },\n                {\n                    \"sql_condition\": f\"\"\"\n                    {calculate_tf_product_array_sql(\"name_tokens_with_freq\")} < 1e-4\n                    \"\"\",\n                    \"label_for_charts\": \"Array product is less than 1e-4\",\n                },\n                {\"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\"},\n            ],\n            \"comparison_description\": \"ExactMatch\",\n        },\n        cl.PostcodeComparison(\"postcode\"),\n        cl.ExactMatch(\"first_num_in_address\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"postcode\"),\n        block_on(\"rarest_tokens\", \"substr(postcode,1,3)\"),\n        \"l.rarest_tokens[1] = r.rarest_tokens[2] and substr(l.company_name,1,3) = substr(r.company_name,1,3)\",\n        \"l.rarest_tokens[2] = r.rarest_tokens[1] and substr(l.company_name,1,3) = substr(r.company_name,1,3)\",\n        block_on(\"company_name\"),\n    ],\n    additional_columns_to_retain=[\"company_name\"],\n    retain_intermediate_calculation_columns=True,\n    retain_matching_columns=True,\n)\n\nlinker = Linker([df_stockport, df_all_companies], settings, db_api)\n```\n\n----------------------------------------\n\nTITLE: Complete Splink Comparison Viewer Dashboard Implementation\nDESCRIPTION: Comprehensive example demonstrating how to create a Splink Linker object, define comparison functions, train the model using EM, generate predictions, and visualize the results with the comparison_viewer_dashboard.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/comparison_viewer_dashboard.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_template_library as ctl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.JaroWinklerAtThresholds(\"first_name\", [0.9, 0.7]),\n        cl.JaroAtThresholds(\"surname\", [0.9, 0.7]),\n        ctl.DateComparison(\n            \"dob\",\n            input_is_string=True,\n            datetime_metrics=[\"year\", \"month\"],\n            datetime_thresholds=[1, 1],\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        ctl.EmailComparison(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"substr(first_name,1,1)\"),\n        block_on(\"substr(surname, 1,1)\"),\n    ],\n    retain_intermediate_calculation_columns=True,\n    retain_matching_columns=True,\n)\n\nlinker = Linker(df, settings, DuckDBAPI())\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on(\"first_name\", \"surname\")\n\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\ndf_predictions = linker.inference.predict(threshold_match_probability=0.2)\n\nlinker.visualisations.comparison_viewer_dashboard(\n    df_predictions, \"img/scv.html\", overwrite=True\n)\n\n# You can view the scv.html file in your browser, or inline in a notebook as follows\nfrom IPython.display import IFrame\nIFrame(\n    src=\"./img/scv.html\", width=\"100%\", height=1200\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Transaction Linking Model\nDESCRIPTION: These snippets evaluate the Splink model's performance by analyzing prediction errors. It generates waterfall charts for false positives and false negatives based on the ground truth labels.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/transactions.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npred_errors = linker.evaluation.prediction_errors_from_labels_column(\n    \"ground_truth\", include_false_positives=True, include_false_negatives=False\n)\nlinker.visualisations.waterfall_chart(pred_errors.as_record_dict(limit=5))\n```\n\nLANGUAGE: python\nCODE:\n```\npred_errors = linker.evaluation.prediction_errors_from_labels_column(\n    \"ground_truth\", include_false_positives=False, include_false_negatives=True\n)\nlinker.visualisations.waterfall_chart(pred_errors.as_record_dict(limit=5))\n```\n\n----------------------------------------\n\nTITLE: Configuring Splink Linker with Link-Only Settings in Python\nDESCRIPTION: Sets up the Splink linker with link-only configuration, specifying blocking rules and comparison methods for different field types. This configuration uses specialized comparisons for names, dates of birth, cities, and emails.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/link_only.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on\n\nsettings = SettingsCreator(\n    link_type=\"link_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    comparisons=[\n        cl.NameComparison(\n            \"first_name\",\n        ),\n        cl.NameComparison(\"surname\"),\n        cl.DateOfBirthComparison(\n            \"dob\",\n            input_is_string=True,\n            invalid_dates_as_null=True,\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        cl.EmailComparison(\"email\"),\n    ],\n)\n\nlinker = Linker(\n    [df_l, df_r],\n    settings,\n    db_api=DuckDBAPI(),\n    input_table_aliases=[\"df_left\", \"df_right\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Predicting Record Links with Probability Threshold in Python\nDESCRIPTION: Performs record linkage prediction using the trained model, with a high threshold of 0.9 to ensure high precision in the matches. This generates the final set of linked records between the two datasets.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/link_only.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresults = linker.inference.predict(threshold_match_probability=0.9)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Match Weights in Splink\nDESCRIPTION: Generates a visualization of the final estimated match weights from the trained model, helping to understand the relative importance of different comparison fields in determining matches.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.match_weights_chart()\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Comparison for First Name Matching in Splink\nDESCRIPTION: This code snippet demonstrates how to create a custom comparison in Splink that combines multiple string comparison methods for matching first names. It includes exact matching, Jaro-Winkler similarity, Levenshtein distance, and Dmetaphone phonetic matching. The comparison is set up with multiple levels to handle different scenarios.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_level_library as cll\nfirst_name_comparison = cl.CustomComparison(\n    output_column_name=\"first_name\",\n    comparison_levels=[\n        cll.NullLevel(\"first_name\"),\n        cll.ExactMatchLevel(\"first_name\"),\n        cll.JaroWinklerLevel(\"first_name\", 0.9),\n        cll.LevenshteinLevel(\"first_name\", 0.8),\n        cll.ArrayIntersectLevel(\"first_name_dm\", 1),\n        cll.ElseLevel()\n    ]\n)\n\nprint(first_name_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Implementing Double Metaphone Phonetic Transformation for Names\nDESCRIPTION: Shows how to create a function that applies the Double Metaphone phonetic algorithm to a name column. This transformation helps match names that sound similar but are spelled differently.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/data_preparation/feature_engineering.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport phonetics\n\nfrom splink import splink_datasets\ndf = splink_datasets.fake_1000\n\n# Define a function to apply the dmetaphone phonetic algorithm to each name in the column\ndef dmetaphone_name(name):\n    if name is None:\n        pass\n    else:\n        return phonetics.dmetaphone(name)\n```\n\n----------------------------------------\n\nTITLE: Cleaning and Deduplicating Data with Splink and DuckDB\nDESCRIPTION: This example demonstrates a complete workflow for deduplicating data using Splink with DuckDB. It includes data loading, cleaning steps, model configuration with comparison functions, training the model using EM algorithm, and generating predictions and clusters.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/llms/prompting_llms.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nfrom splink import Linker, SettingsCreator, DuckDBAPI, block_on\nimport splink.comparison_library as cl\n\n# Load and clean data in DuckDB\ncon = duckdb.connect()\ncon.execute(\"\"\"\n    CREATE TABLE raw_data AS SELECT * FROM read_csv('your_data.csv', HEADER=TRUE)\n\"\"\")\n\n# Example cleaning steps: lowercasing fields, removing punctuation from full_name, standardizing dob format\ncon.execute(\"\"\"\n    CREATE TABLE cleaned AS\n    SELECT\n        regexp_replace(lower(full_name), '[^a-z ]', '', 'g') as full_name,\n        regexp_replace(dob, '[^0-9-]', '', 'g') as dob,\n        lower(occupation) as occupation,\n        regexp_replace(lower(zip), '[^a-z0-9 ]', '', 'g') as zip\n    FROM raw_data\n\"\"\")\n\ndf = con.table(\"cleaned\")\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"full_name\"),\n        block_on(\"dob\"),\n        block_on(\"zip\"),\n    ],\n    comparisons=[\n        cl.NameComparison(\"full_name\"),\n        cl.DateOfBirthComparison(\"dob\", input_is_string=True),\n        cl.ExactMatch(\"occupation\").configure(term_frequency_adjustments=True),\n        cl.LevenshteinAtThresholds(\"zip\", 2),\n    ],\n    retain_intermediate_calculation_columns=True,\n)\n\ndb_api = DuckDBAPI()\nlinker = Linker(df, settings, db_api=db_api)\n\ndeterministic_rules = [\n    \"l.full_name = r.full_name and l.dob = r.dob\",\n    \"l.zip = r.zip and levenshtein(l.full_name, r.full_name) < 2\"\n]\n\nlinker.training.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\nlinker.training.estimate_parameters_using_expectation_maximisation(block_on(\"full_name\"))\nlinker.training.estimate_parameters_using_expectation_maximisation(block_on(\"dob\"))\n\npredictions = linker.inference.predict(threshold_match_probability=0.9)\nclusters = linker.clustering.cluster_pairwise_predictions_at_threshold(predictions, threshold_match_probability=0.95)\n```\n\n----------------------------------------\n\nTITLE: Configuring Linker Settings and Comparisons\nDESCRIPTION: Creates a Splink Linker instance with comparison configurations for various fields. Each comparison is configured with specific methods appropriate for the data type, with term frequency adjustments disabled for name fields.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/demo_settings/model_create_h50k.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\nfrom splink import Linker, SettingsCreator\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=blocking_rules,\n    comparisons=[\n        cl.NameComparison(\"first_name\").configure(term_frequency_adjustments=False),\n        cl.NameComparison(\"surname\").configure(term_frequency_adjustments=False),\n        cl.DateOfBirthComparison(\"dob\", input_is_string=True),\n        cl.PostcodeComparison(\"postcode_fake\"),\n        cl.ExactMatch(\"birth_place\").configure(term_frequency_adjustments=False),\n        cl.ExactMatch(\"occupation\").configure(term_frequency_adjustments=False),\n    ],\n    retain_intermediate_calculation_columns=True,\n)\n\nlinker = Linker(df, settings, db_api=db_api)\n```\n\n----------------------------------------\n\nTITLE: Direct Estimation of m-values from Label Column in Splink\nDESCRIPTION: Demonstrates how to estimate 'm' parameters directly from a labeled column such as a social security number in the input dataset. This approach is useful when you have partially or fully populated unique identifiers.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlinker.estimate_m_from_label_column(\"social_security_number\")\n```\n\n----------------------------------------\n\nTITLE: Estimating Parameters using Expectation Maximisation in Splink\nDESCRIPTION: Estimates parameters using the Expectation Maximisation algorithm in Splink, blocking on different columns and estimating without term frequencies.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/accuracy_analysis_from_labels_column.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsession_dob = linker.training.estimate_parameters_using_expectation_maximisation(\n    block_on(\"dob\"), estimate_without_term_frequencies=True\n)\nsession_email = linker.training.estimate_parameters_using_expectation_maximisation(\n    block_on(\"email\"), estimate_without_term_frequencies=True\n)\nsession_dob = linker.training.estimate_parameters_using_expectation_maximisation(\n    block_on(\"first_name\", \"surname\"), estimate_without_term_frequencies=True\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Unlinkable Records\nDESCRIPTION: Generates visualization to identify records with data quality issues that prevent reliable matching.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/07_Evaluation.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlinker.evaluation.unlinkables_chart()\n```\n\n----------------------------------------\n\nTITLE: Comparing Parameter Estimates Across Training Sessions\nDESCRIPTION: Generates a visualization comparing parameter estimates from different EM training sessions, allowing users to assess consistency or differences in estimates across training approaches.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.parameter_estimate_comparisons_chart()\n```\n\n----------------------------------------\n\nTITLE: Blocking on Array Columns with Splink in Python\nDESCRIPTION: This example illustrates how to use block_on to create pairwise comparisons for records where any elements in the array columns match. It sets up a dataset with array columns and uses the Linker class for deduplication.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/cookbook.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on\n\n\ndata = [\n    {\"unique_id\": 1, \"first_name\": \"John\", \"postcode\": [\"A\", \"B\"]},\n    {\"unique_id\": 2, \"first_name\": \"John\", \"postcode\": [\"B\"]},\n    {\"unique_id\": 3, \"first_name\": \"John\", \"postcode\": [\"C\"]},\n\n]\n\ndf = pd.DataFrame(data)\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"postcode\", arrays_to_explode=[\"postcode\"]),\n    ],\n    comparisons=[\n        cl.ArrayIntersectAtSizes(\"postcode\", [2, 1]),\n        cl.ExactMatch(\"first_name\"),\n    ]\n)\n\n\nlinker = Linker(df, settings, DuckDBAPI(), set_up_basic_logging=False)\n\nlinker.inference.predict().as_pandas_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Training M Probabilities using Expectation Maximisation in Splink with Python\nDESCRIPTION: This code demonstrates how to estimate m probabilities using Expectation Maximisation with different blocking strategies. The first EM pass blocks on date of birth, while the second blocks on first_name and surname, allowing estimation of all parameters through a round-robin approach.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/training/training_rationale.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_parameters_using_expectation_maximisation(block_on(\"dob\"))\nlinker.training.estimate_parameters_using_expectation_maximisation(block_on(\"first_name\", \"surname\"))\n```\n\n----------------------------------------\n\nTITLE: Secondary EM Parameter Estimation with Date of Birth Blocking\nDESCRIPTION: Demonstrates a second pass of parameter estimation using date of birth as a blocking rule. This complementary approach allows estimating parameters for the first_name and surname fields that couldn't be estimated in the first pass.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntraining_blocking_rule = block_on(\"dob\")\ntraining_session_dob = linker.training.estimate_parameters_using_expectation_maximisation(\n    training_blocking_rule\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Nested Linkage for Company-Person Data in Splink\nDESCRIPTION: This example shows a two-phase record linkage process: first deduplicating companies, then deduplicating persons within each resolved company cluster. The approach uses DuckDB with Splink to create a hierarchical linkage pipeline where person-level matching only occurs within company clusters identified in the first phase.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/cookbook.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nimport pandas as pd\nimport os\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on\nfrom splink.clustering import cluster_pairwise_predictions_at_threshold\n\n# Example data with companies and persons\ncompany_person_records_list = [\n    {\n        \"unique_id\": 1001,\n        \"client_id\": \"GGN1\",\n        \"company_name\": \"Green Garden Nurseries Ltd\",\n        \"postcode\": \"NR1 1AB\",\n        \"person_firstname\": \"John\",\n        \"person_surname\": \"Smith\",\n    },\n    {\n        \"unique_id\": 1002,\n        \"client_id\": \"GGN1\",\n        \"company_name\": \"Green Gardens Ltd\",\n        \"postcode\": \"NR1 1AB\",\n        \"person_firstname\": \"Sarah\",\n        \"person_surname\": \"Jones\",\n    },\n    {\n        \"unique_id\": 1003,\n        \"client_id\": \"GGN2\",\n        \"company_name\": \"Green Garden Nurseries Ltd\",\n        \"postcode\": \"NR1 1AB\",\n        \"person_firstname\": \"John\",\n        \"person_surname\": \"Smith\",\n    },\n    {\n        \"unique_id\": 3001,\n        \"client_id\": \"GW1\",\n        \"company_name\": \"Garden World\",\n        \"postcode\": \"LS2 3EF\",\n        \"person_firstname\": \"Emma\",\n        \"person_surname\": \"Wilson\",\n    },\n    {\n        \"unique_id\": 3002,\n        \"client_id\": \"GW1\",\n        \"company_name\": \"Garden World UK\",\n        \"postcode\": \"LS2 3EF\",\n        \"person_firstname\": \"Emma\",\n        \"person_surname\": \"Wilson\",\n    },\n    {\n        \"unique_id\": 3003,\n        \"client_id\": \"GW2\",\n        \"company_name\": \"Garden World\",\n        \"postcode\": \"LS2 3EF\",\n        \"person_firstname\": \"Emma\",\n        \"person_surname\": \"Wilson\",\n    },\n    {\n        \"unique_id\": 3004,\n        \"client_id\": \"GW2\",\n        \"company_name\": \"Garden World\",\n        \"postcode\": \"LS2 3EF\",\n        \"person_firstname\": \"James\",\n        \"person_surname\": \"Taylor\",\n    },\n]\ncompany_person_records = pd.DataFrame(company_person_records_list)\ncompany_person_records\nprint(\"========== NESTED COMPANY-PERSON LINKAGE EXAMPLE ==========\")\nprint(\"This example demonstrates a two-phase linkage process:\")\nprint(\"1. First, link and cluster to find duplicate companies (client_id)\")\nprint(\"2. Then, deduplicate persons ONLY within each company cluster\")\n\n# Initialize database\nif os.path.exists(\"nested_linkage.ddb\"):\n    os.remove(\"nested_linkage.ddb\")\ncon = duckdb.connect(\"nested_linkage.ddb\")\n\n# Load data into DuckDB\ncon.execute(\n    \"CREATE OR REPLACE TABLE company_person_records AS \"\n    \"SELECT * FROM company_person_records\"\n)\n\n\nprint(\"\\n--- PHASE 1: COMPANY LINKAGE ---\")\nprint(\"Company records to be linked:\")\ncon.table(\"company_person_records\").show()\n\n# STEP 1: Find duplicate client_ids\n\n\n# Configure company linkage\n# We match on person name because if we have duplicate client_ids,\n# it's likely that they may share the same contact\n# Note though, at this stage the entity is client not a person\ncompany_settings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    unique_id_column_name=\"unique_id\",\n    probability_two_random_records_match=0.001,\n    comparisons=[\n        cl.ExactMatch(\"client_id\"),\n        cl.JaroWinklerAtThresholds(\"person_firstname\"),\n        cl.JaroWinklerAtThresholds(\"person_surname\"),\n        cl.JaroWinklerAtThresholds(\"company_name\"),\n        cl.ExactMatch(\"postcode\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"postcode\"),\n        block_on(\"company_name\"),\n    ],\n    retain_matching_columns=True,\n)\n\ndb_api = DuckDBAPI(connection=con)\ncompany_linker = Linker(\"company_person_records\", company_settings, db_api)\ncompany_predictions = company_linker.inference.predict(threshold_match_probability=0.5)\n\nprint(\"\\nCompany pairwise matches:\")\ncompany_predictions.as_duckdbpyrelation().show()\n\n# Cluster companies\ncompany_nodes = con.sql(\"SELECT DISTINCT client_id FROM company_person_records\")\ncompany_edges = con.sql(f\"\"\"\n    SELECT\n        client_id_l as n_1,\n        client_id_r as n_2,\n        match_probability\n    FROM {company_predictions.physical_name}\n\"\"\")\n\n# Perform company clustering\ncompany_clusters = cluster_pairwise_predictions_at_threshold(\n    company_nodes,\n    company_edges,\n    node_id_column_name=\"client_id\",\n    edge_id_column_name_left=\"n_1\",\n    edge_id_column_name_right=\"n_2\",\n    db_api=db_api,\n    threshold_match_probability=0.5,\n)\n\n# Add company cluster IDs to original records\ncompany_clusters_ddb = company_clusters.as_duckdbpyrelation()\ncon.register(\"company_clusters_ddb\", company_clusters_ddb)\n\n\nsql = \"\"\"\nCREATE TABLE records_with_company_cluster AS\nSELECT cr.*,\n       cc.cluster_id as company_cluster_id\nFROM company_person_records cr\nLEFT JOIN company_clusters_ddb cc\nON cr.client_id = cc.client_id\n\"\"\"\ncon.execute(sql)\nprint(\"Records with company cluster:\")\ncon.table(\"records_with_company_cluster\").show()\n\n# Not needed, just to see what's happening\nprint(\"\\nCompany clustering results:\")\ncon.sql(\"\"\"\nSELECT\n    company_cluster_id,\n    array_agg(DISTINCT client_id) as client_ids,\n    array_agg(DISTINCT company_name) as company_names\nFROM records_with_company_cluster\nGROUP BY company_cluster_id\n\"\"\").show()\n\nprint(\"\\n--- PHASE 2: PERSON LINKAGE WITHIN COMPANIES ---\")\nprint(\"Now linking persons, but only within their company clusters\")\n\n# STEP 2: Link persons within company clusters\n# Create a new connection to isolate this step\ncon2 = duckdb.connect()\ncon2.sql(\"attach 'nested_linkage.ddb' as linkage_db\")\ncon2.execute(\n    \"create table records_with_company_cluster as select * from linkage_db.records_with_company_cluster\"\n)\ndb_api2 = DuckDBAPI(connection=con2)\n\n# Configure person linkage within company clusters\n# Simple linking model just distinguishes between people within a client_id\n# There shouldn't be many so this model can be straightforward\nperson_settings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    probability_two_random_records_match=0.01,\n    comparisons=[\n        cl.JaroWinklerAtThresholds(\"person_firstname\"),\n        cl.JaroWinklerAtThresholds(\"person_surname\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        # Critical: Block on company_cluster_id to only compare within company\n        block_on(\"company_cluster_id\"),\n    ],\n    retain_matching_columns=True,\n)\n\n# Link persons within company clusters\nperson_linker = Linker(\"records_with_company_cluster\", person_settings, db_api2)\nperson_predictions = person_linker.inference.predict(threshold_match_probability=0.5)\n\nprint(\"\\nPerson pairwise matches (within company clusters):\")\nperson_predictions.as_duckdbpyrelation().show(max_width=1000)\n\nperson_clusters = person_linker.clustering.cluster_pairwise_predictions_at_threshold(\n    person_predictions, threshold_match_probability=0.5\n)\n\nperson_clusters.as_duckdbpyrelation().sort(\"cluster_id\").show(max_width=1000)\n```\n\n----------------------------------------\n\nTITLE: Extending PostcodeComparison with Geospatial Distance in Splink\nDESCRIPTION: Creates an enhanced postcode comparison that includes geospatial distance thresholds using latitude and longitude columns. This allows for matching based on geographical proximity.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npc_comparison = cl.PostcodeComparison(\"postcode\", lat_col=\"lat\", long_col=\"long\", km_thresholds=[1,10,50])\nprint(pc_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Using Double Metaphone Columns in Name Comparison with Splink\nDESCRIPTION: This example demonstrates how to create a name comparison in Splink that utilizes a pre-generated double metaphone column ('first_name_dm') along with standard string comparisons. The comparison includes exact matching, Jaro-Winkler similarity at different thresholds, and an array intersection check on the double metaphone values.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/phonetic.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\nfirst_name_comparison = cl.NameComparison(\n                        \"first_name\",\n                        dmeta_col_name= \"first_name_dm\")\nprint(first_name_comparison.human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Generating Waterfall Chart\nDESCRIPTION: Creates a waterfall chart to visualize how Splink computed the final matchweight for specific record comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/06_Visualising_predictions.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrecords_to_view = df_predictions.as_record_dict(limit=5)\nlinker.visualisations.waterfall_chart(records_to_view, filter_nulls=False)\n```\n\n----------------------------------------\n\nTITLE: Generating Match Weights Chart with Splink\nDESCRIPTION: Complete example showing how to create a Linker object with various comparison types, train it using expectation maximization on different blocking rules, and generate a match_weights_chart visualization.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/match_weights_chart.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.JaroWinklerAtThresholds(\"first_name\", [0.9, 0.7]),\n        cl.JaroAtThresholds(\"surname\", [0.9, 0.7]),\n        cl.DateOfBirthComparison(\n            \"dob\",\n            input_is_string=True,\n            datetime_metrics=[\"year\", \"month\"],\n            datetime_thresholds=[1, 1],\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        cl.EmailComparison(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n)\n\nlinker = Linker(df, settings, DuckDBAPI())\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on(\"first_name\", \"surname\")\n\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nchart = linker.visualisations.match_weights_chart()\nchart\n```\n\n----------------------------------------\n\nTITLE: Enhanced Postcode Comparison with Geographic Distance in Splink\nDESCRIPTION: Demonstrates creating an advanced postcode comparison that includes geographic distance thresholds. This comparison evaluates if two postcodes are within specified distances (1km and 10km) of each other.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/data_preparation/feature_engineering.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npc_comparison = cl.PostcodeComparison(\n    \"postcode\", lat_col=\"lat\", long_col=\"long\", km_thresholds=[1, 10]\n).get_comparison(\"duckdb\")\nprint(pc_comparison.human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Specifying Settings with Custom Comparisons\nDESCRIPTION: Shows how to incorporate custom comparisons into the SettingsCreator object, which is used to configure a Splink model with specified blocking rules and comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import SettingsCreator, block_on\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    comparisons=[\n        custom_name_comparison,\n        cl.LevenshteinAtThresholds(\"dob\", [1, 2]),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing ForenameSurnameComparison in Splink\nDESCRIPTION: Creates a combined comparison for forename and surname fields, which helps address correlation between names and potential swapping of values. This comparison better models real-world name variations.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\nfull_name_comparison = cl.ForenameSurnameComparison(\"forename\", \"surname\")\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Splink Linkage Model\nDESCRIPTION: Loads a fake dataset and pre-trained linkage model settings from a JSON file hosted on GitHub. Initializes a Splink Linker object with DuckDB as the backend.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/real_time_record_linkage.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport urllib.request\nimport json\nfrom pathlib import Path\nfrom splink import Linker, DuckDBAPI, block_on, SettingsCreator, splink_datasets\n\ndf = splink_datasets.fake_1000\n\nurl = \"https://raw.githubusercontent.com/moj-analytical-services/splink_demos/master/demo_settings/real_time_settings.json\"\n\nwith urllib.request.urlopen(url) as u:\n    settings = json.loads(u.read().decode())\n\n\nlinker = Linker(df, settings, db_api=DuckDBAPI())\n```\n\n----------------------------------------\n\nTITLE: Creating an M/U Parameters Chart with Splink\nDESCRIPTION: This code demonstrates how to create an m_u_parameters_chart to visualize the m and u probability values from a trained Splink model. It includes setting up comparison functions, training the model using expectation maximization, and generating the visualization.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/m_u_parameters_chart.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.JaroWinklerAtThresholds(\"first_name\", [0.9, 0.7]),\n        cl.JaroAtThresholds(\"surname\", [0.9, 0.7]),\n        cl.DateOfBirthComparison(\n            \"dob\",\n            input_is_string=True,\n            datetime_metrics=[\"year\", \"month\"],\n            datetime_thresholds=[1, 1],\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        cl.EmailComparison(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n)\n\nlinker = Linker(df, settings, DuckDBAPI())\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on(\"first_name\", \"surname\")\n\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nchart = linker.visualisations.m_u_parameters_chart()\nchart\n```\n\n----------------------------------------\n\nTITLE: Implementing Splink Waterfall Chart with Full Configuration\nDESCRIPTION: Complete example showing how to create a Splink linker object, configure settings, train the model, and generate a waterfall chart. Includes comparison configurations for names, dates, city, and email fields with term frequency adjustments.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/waterfall_chart.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_template_library as ctl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        ctl.NameComparison(\"first_name\").configure(term_frequency_adjustments=True),\n        ctl.NameComparison(\"surname\"),\n        ctl.DateComparison(\n            \"dob\",\n            input_is_string=True,\n            datetime_metrics=[\"year\", \"month\"],\n            datetime_thresholds=[1, 1],\n        ),\n        cl.ExactMatch(\"city\"),\n        ctl.EmailComparison(\"email\", include_username_fuzzy_level=False),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    retain_intermediate_calculation_columns=True,\n    retain_matching_columns=True,\n)\n\nlinker = Linker(df, settings, DuckDBAPI())\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on(\"first_name\", \"surname\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\ndf_predictions = linker.inference.predict(threshold_match_probability=0.2)\nrecords_to_view = df_predictions.as_record_dict(limit=5)\n\nchart = linker.visualisations.waterfall_chart(records_to_view, filter_nulls=False)\nchart\n```\n\n----------------------------------------\n\nTITLE: Generating Splink Waterfall Chart for Comparison Analysis in Python\nDESCRIPTION: Creates a waterfall chart to visualize how individual model parameters contribute to the final match probabilities for each comparison.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/bias_eval.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrecords_to_plot = comparisons.to_dict(orient=\"records\")\nlinker.visualisations.waterfall_chart(records_to_plot)\n```\n\n----------------------------------------\n\nTITLE: Training Splink Model with Expectation-Maximization\nDESCRIPTION: Runs two separate Expectation-Maximization training sessions using different blocking rules - one based on matching first name and surname, and another based on matching date of birth.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/spark/deduplicate_1k_synthetic.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntraining_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\ntraining_session_fname_sname = (\n    linker.training.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n)\n\ntraining_blocking_rule = \"l.dob = r.dob\"\ntraining_session_dob = linker.training.estimate_parameters_using_expectation_maximisation(\n    training_blocking_rule\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Splink Settings Configuration\nDESCRIPTION: Configures Splink settings with blocking rules and comparison definitions for deduplication\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nfrom splink import block_on\n\nfrom splink import Linker, SettingsCreator, splink_datasets\n\ndf = splink_datasets.historical_50k\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\", \"surname\"),\n        block_on(\"surname\", \"dob\"),\n    ],\n    comparisons=[\n        cl.ExactMatch(\"first_name\").configure(term_frequency_adjustments=True),\n        cl.LevenshteinAtThresholds(\"surname\", [1, 3]),\n        cl.LevenshteinAtThresholds(\"dob\", [1, 2]),\n        cl.LevenshteinAtThresholds(\"postcode_fake\", [1, 2]),\n        cl.ExactMatch(\"birth_place\").configure(term_frequency_adjustments=True),\n        cl.ExactMatch(\"occupation\").configure(term_frequency_adjustments=True),\n    ],\n    retain_intermediate_calculation_columns=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Threshold Selection Analysis\nDESCRIPTION: Creates interactive dashboard showing key accuracy statistics including F1 score.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/07_Evaluation.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlinker.evaluation.accuracy_analysis_from_labels_table(\n    labels_table, output_type=\"threshold_selection\", add_metrics=[\"f1\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Generating ROC Curve\nDESCRIPTION: Creates ROC curve visualization to show trade-off between false positives and negatives at different thresholds.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/07_Evaluation.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlinker.evaluation.accuracy_analysis_from_labels_table(labels_table, output_type=\"roc\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Multi-Column Comparison in Splink\nDESCRIPTION: Example of creating a custom comparison in Splink that combines different comparison levels, including cross-column comparisons. This demonstrates how to check for matches between different columns (first_name and surname), along with standard comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/comparison_playground.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_level_library as cll\n\nfirst_name_comparison = cl.CustomComparison(\n    comparison_levels=[\n        cll.NullLevel(\"first_name\"),\n        cll.ExactMatchLevel(\"first_name\"),\n        {\n            \"sql_condition\": \"first_name_l = surname_r\",\n            \"label_for_charts\": \"Match on reversed cols: first_name and surname\",\n        },\n        cll.JaroWinklerLevel(\"first_name\", 0.8),\n        cll.ElseLevel(),\n    ]\n)\n# Need to be able to pass values in as a dict {\"first_name_l\": \"Robin\", \"first_name_r\": \"Robyn\", \"surname_l\": \"Linacre\", \"surname_r\": \"Linacre\"}\nvalues = {\n    \"first_name_l\": \"Robin\",\n    \"first_name_r\": \"Linacre\",\n    \"surname_l\": \"Linacre\",\n    \"surname_r\": \"Robin\",\n}\ndisplay(Markdown(report_comparison_levels(first_name_comparison, values, \"first_name\")))\n```\n\n----------------------------------------\n\nTITLE: Basic Column Blocking in Python using Splink\nDESCRIPTION: Demonstrates how to create a basic blocking rule that matches on first name and surname columns using Splink's block_on function.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/03_Blocking.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import block_on\nblock_on(\"first_name\", \"surname\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing String Comparator Scores with comparator_score_chart\nDESCRIPTION: Uses the comparator_score_chart function to visualize how different string comparison metrics perform across various string variations, showing which comparators work best for different types of string differences.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsa.comparator_score_chart(data, \"string1\", \"string2\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom DuckDB UDF for String Similarity in Splink Comparison\nDESCRIPTION: This example demonstrates how to create a custom User-Defined Function (UDF) in DuckDB to compute string similarity using Python's difflib. The function is then used within a Splink comparison level to perform fuzzy matching of email addresses with a similarity threshold of 0.8.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/cookbook.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport difflib\n\nimport duckdb\n\nimport splink.comparison_level_library as cll\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\n\ndef custom_partial_ratio(s1, s2):\n    \"\"\"Custom function to compute partial ratio similarity between two strings.\"\"\"\n    s1, s2 = str(s1), str(s2)\n    matcher = difflib.SequenceMatcher(None, s1, s2)\n    return matcher.ratio()\n\n\ndf = splink_datasets.fake_1000\n\ncon = duckdb.connect()\ncon.create_function(\n    \"custom_partial_ratio\",\n    custom_partial_ratio,\n    [duckdb.typing.VARCHAR, duckdb.typing.VARCHAR],\n    duckdb.typing.DOUBLE,\n)\ndb_api = DuckDBAPI(connection=con)\n\n\nfuzzy_email_comparison = {\n    \"output_column_name\": \"email_fuzzy\",\n    \"comparison_levels\": [\n        cll.NullLevel(\"email\"),\n        cll.ExactMatchLevel(\"email\"),\n        {\n            \"sql_condition\": \"custom_partial_ratio(email_l, email_r) > 0.8\",\n            \"label_for_charts\": \"Fuzzy match ( 0.8)\",\n        },\n        cll.ElseLevel(),\n    ],\n}\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.ExactMatch(\"first_name\"),\n        cl.ExactMatch(\"surname\"),\n        cl.ExactMatch(\"dob\"),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        fuzzy_email_comparison,\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    max_iterations=2,\n)\n\nlinker = Linker(df, settings, db_api)\n\nlinker.training.estimate_probability_two_random_records_match(\n    [block_on(\"first_name\", \"surname\")], recall=0.7\n)\n\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e5)\n\nlinker.training.estimate_parameters_using_expectation_maximisation(block_on(\"dob\"))\n\npairwise_predictions = linker.inference.predict(threshold_match_weight=-10)\n```\n\n----------------------------------------\n\nTITLE: Estimating Match Probability Using Deterministic Rules in Python\nDESCRIPTION: Defines deterministic matching rules and estimates the probability of two random records matching. These rules combine exact matching with fuzzy matching using Levenshtein distance for fields like names and dates of birth.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/link_only.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\ndeterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\",\n    block_on(\"email\"),\n]\n\n\nlinker.training.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\n```\n\n----------------------------------------\n\nTITLE: Performing Accuracy Analysis from Labels Column in Splink\nDESCRIPTION: Performs accuracy analysis using the 'cluster' column as ground truth in Splink. This snippet demonstrates table output, ROC curve generation, and threshold selection.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/accuracy_analysis_from_labels_column.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlinker.evaluation.accuracy_analysis_from_labels_column(\n    \"cluster\", output_type=\"table\"\n).as_pandas_dataframe(limit=5)\n```\n\nLANGUAGE: python\nCODE:\n```\nlinker.evaluation.accuracy_analysis_from_labels_column(\"cluster\", output_type=\"roc\")\n```\n\nLANGUAGE: python\nCODE:\n```\nlinker.evaluation.accuracy_analysis_from_labels_column(\n    \"cluster\",\n    output_type=\"threshold_selection\",\n    threshold_match_probability=0.5,\n    add_metrics=[\"f1\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Predicting Matches in Transaction Data\nDESCRIPTION: This snippet uses the trained Splink model to predict matches in the transaction data, with a specified threshold for match probability.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/transactions.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf_predict = linker.inference.predict(threshold_match_probability=0.001)\n```\n\n----------------------------------------\n\nTITLE: Creating ColumnExpression Objects with Various Transformations in Python\nDESCRIPTION: Examples of creating different ColumnExpression objects with transformations such as lowercase conversion, string casting, substring extraction, date parsing, and string concatenation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/column_expression.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import ColumnExpression\n\nemail_lowercase = ColumnExpression(\"email\").lower()\ndob_as_string = ColumnExpression(\"dob\").cast_to_string()\nsurname_initial_lowercase = ColumnExpression(\"surname\").substr(1, 1).lower()\nentry_date = ColumnExpression(\"entry_date_str\").try_parse_date(date_format=\"YYYY-MM-DD\")\nfull_name_lowercase = ColumnExpression(\"first_name || ' ' || surname\").lower()\n```\n\n----------------------------------------\n\nTITLE: Estimating Parameters Using Expectation Maximisation\nDESCRIPTION: This snippet shows how to estimate parameters using the Expectation Maximisation algorithm with different blocking rules. It demonstrates the process of training the Splink model with various data subsets.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/parameter_estimate_comparisons_chart.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nblocking_rule_for_training = block_on(\"first_name\", \"surname\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nblocking_rule_for_training = block_on(\"email\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Results with Cluster Studio Dashboard (Python)\nDESCRIPTION: This code generates a Cluster Studio Dashboard for visualizing the deterministic linkage results and displays it in an IFrame within the notebook.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/deterministic_dedupe.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.cluster_studio_dashboard(\n    df_predict,\n    clusters,\n    \"dashboards/50k_deterministic_cluster.html\",\n    sampling_method=\"by_cluster_size\",\n    overwrite=True,\n)\n\nfrom IPython.display import IFrame\n\nIFrame(src=\"./dashboards/50k_deterministic_cluster.html\", width=\"100%\", height=1200)\n```\n\n----------------------------------------\n\nTITLE: Creating an ExactMatch Comparison with Term-Frequency Adjustments using ComparisonLevelLibrary\nDESCRIPTION: Demonstrates building an exact match comparison with term-frequency adjustments by composing comparison levels from the ComparisonLevelLibrary.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport splink.duckdb.comparison_level_library as cll\n\nfirst_name_comparison = cl.CustomComparison(\n    output_column_name=\"first_name\",\n    comparison_description=\"Exact match vs. anything else\",\n    comparison_levels=[\n        cll.NullLevel(\"first_name\"),\n        cll.ExactMatchLevel(\"first_name\").configure(tf_adjustment_column=\"first_name\"),\n        cll.ElseLevel(),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkAPI for Optimal Performance in Python\nDESCRIPTION: This snippet demonstrates how to configure the SparkAPI for optimal performance in a Splink job. It sets the Spark parallelism, configures the break lineage method, and specifies the number of partitions for repartitioning.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/performance/optimising_spark.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import SparkAPI\n\nspark.conf.set(\"spark.default.parallelism\", \"50\")\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n\ndb_api = SparkAPI(\n    spark_session=spark,\n    break_lineage_method=\"parquet\",\n    num_partitions_on_repartition=80,\n)\n```\n\n----------------------------------------\n\nTITLE: Complex Blocking Rules for Large Datasets\nDESCRIPTION: Demonstrates a comprehensive blocking strategy for large datasets using multiple tight blocking rules. This approach combines various field combinations to maintain efficiency while ensuring good coverage of potential matches.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/blocking/blocking_rules.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nSettingsCreator(\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\", \"surname\", \"substr(postcode,1,3)\"),\n        block_on(\"surname\", \"dob\"),\n        block_on(\"first_name\", \"dob\"),\n        block_on(\"dob\", \"postcode\")\n        block_on(\"first_name\", \"postcode\")\n        block_on(\"surname\", \"postcode\")\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Viewing Splink Record Linkage Results as DataFrame in Python\nDESCRIPTION: Converts the linkage results to a pandas DataFrame and displays the first 5 records. This allows for inspection of the matching results and the associated match probabilities.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/link_only.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresults.as_pandas_dataframe(limit=5)\n```\n\n----------------------------------------\n\nTITLE: Creating ForenameSurnameComparison with Full Name in Splink\nDESCRIPTION: This code demonstrates how to create a ForenameSurnameComparison object that includes the 'full_name' column for term frequency adjustments. It uses the Splink comparison library and outputs the comparison as a dictionary.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/data_preparation/feature_engineering.md#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncomparison = cl.ForenameSurnameComparison(\n    \"first_name\", \"surname\", forename_surname_concat_col_name=\"full_name\"\n)\ncomparison.get_comparison(\"duckdb\").as_dict()\n```\n\n----------------------------------------\n\nTITLE: Initializing Linker and Estimating Match Probability\nDESCRIPTION: Creates a Linker object with the configured settings and estimates the probability that two random records match using deterministic rules with a specified recall.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/pairwise_labels.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlinker = Linker(df, settings, db_api=DuckDBAPI(), set_up_basic_logging=False)\ndeterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\",\n    \"l.email = r.email\",\n]\n\nlinker.training.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\n```\n\n----------------------------------------\n\nTITLE: Setting up and Training a DuckDBLinker with Comparison Rules\nDESCRIPTION: Demonstrates the complete workflow for initializing a DuckDBLinker, configuring comparison settings, and training the model using expectation maximization. The example uses fake data and various comparison types including name, date, and email comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/template.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.duckdb.linker import DuckDBLinker\nimport splink.duckdb.comparison_library as cl\nimport splink.duckdb.comparison_template_library as ctl\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.datasets import splink_datasets\nimport logging, sys\nlogging.disable(sys.maxsize)\n\ndf = splink_datasets.fake_1000\n\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        ctl.name_comparison(\"first_name\"),\n        ctl.name_comparison(\"surname\"),\n        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n        cl.exact_match(\"city\", term_frequency_adjustments=True),\n        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n\nlinker.training.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.training.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Comparison Using ComparisonLevels\nDESCRIPTION: Demonstrates composing a custom comparison for 'first_name' with multiple levels including null matching, exact matching, and soundex matching. Shows how to use the ComparisonLevel library for finer control.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.comparison_library import CustomComparison\nimport splink.comparison_level_library as cll\n\ncustom_name_comparison = CustomComparison(\n    output_column_name=\"first_name\",\n    comparison_levels=[\n        cll.NullLevel(\"first_name\"),\n        cll.ExactMatchLevel(\"first_name\").configure(tf_adjustment_column=\"first_name\"),\n        cll.ExactMatchLevel(\"soundex_first_name\").configure(\n            tf_adjustment_column=\"soundex_first_name\"\n        ),\n        cll.ElseLevel(),\n    ],\n)\n\nprint(custom_name_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Configuring Complete Splink Settings\nDESCRIPTION: Sets up the complete linkage model configuration including multiple comparison types, blocking rules, and model parameters.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import Linker, SettingsCreator, block_on, DuckDBAPI\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.NameComparison(\"first_name\"),\n        cl.NameComparison(\"surname\"),\n        cl.LevenshteinAtThresholds(\"dob\", 1),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        cl.EmailComparison(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\", \"city\"),\n        block_on(\"surname\"),\n\n    ],\n    retain_intermediate_calculation_columns=True,\n)\n\nlinker = Linker(df, settings, db_api=DuckDBAPI())\n```\n\n----------------------------------------\n\nTITLE: Loading Business Data from External Sources using DuckDB\nDESCRIPTION: Loads Companies House and Stockport business rates data from parquet files hosted on GitHub, then displays a sample of each dataset to understand their structure.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nimport altair as alt\n\ndf_companies_house = duckdb.read_parquet(\"https://github.com/RobinL/company_matching_example/raw/refs/heads/main/companies_house.parquet\")\ndf_companies_house.sort(\"company_name\").show(max_rows=5)\n\ndf_stockport_business_rates = duckdb.read_parquet(\"https://github.com/RobinL/company_matching_example/raw/refs/heads/main/stockport_business_rates.parquet\")\ndf_stockport_business_rates.sort(\"company_name\").show(max_rows=5)\n```\n\n----------------------------------------\n\nTITLE: Training Splink Model and Visualizing Match Weights\nDESCRIPTION: Trains the Splink model by estimating u values (probability of attribute agreement for non-matching records) using random sampling, then visualizes the match weights chart to assess the model's configuration.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Probably not worth training m values given how few columns we have;\n# by not training them we'll be using the defaults\n\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e7)\nlinker.visualisations.match_weights_chart()\n```\n\n----------------------------------------\n\nTITLE: Defining Comparison Levels in SQL for Splink\nDESCRIPTION: This SQL snippet demonstrates how to define comparison levels for a 'first_name' field in Splink. It includes conditions for null values, exact matches, and a catch-all else condition. Each level specifies a SQL condition and a label for charting.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/settings_dict_guide.md#2025-04-16_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n[\n    {\n        \"sql_condition\": \"first_name_l IS NULL OR first_name_r IS NULL\",\n        \"label_for_charts\": \"null\",\n        \"null_level\": True\n    },\n    {\n        \"sql_condition\": \"first_name_l = first_name_r\",\n        \"label_for_charts\": \"exact_match\",\n        \"tf_adjustment_column\": \"first_name\"\n    },\n    {\n        \"sql_condition\": \"ELSE\",\n        \"label_for_charts\": \"else\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Splink Settings for Transaction Linking\nDESCRIPTION: This code sets up the Splink model settings, including comparison levels for amount and transaction date, and defines blocking rules for generating predictions. It uses custom comparison levels to handle the specific characteristics of transaction data.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/transactions.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Full settings for linking model\nimport splink.comparison_level_library as cll\nimport splink.comparison_library as cl\n\ncomparison_amount = {\n    \"output_column_name\": \"amount\",\n    \"comparison_levels\": [\n        cll.NullLevel(\"amount\"),\n        cll.ExactMatchLevel(\"amount\"),\n        cll.PercentageDifferenceLevel(\"amount\", 0.01),\n        cll.PercentageDifferenceLevel(\"amount\", 0.03),\n        cll.PercentageDifferenceLevel(\"amount\", 0.1),\n        cll.PercentageDifferenceLevel(\"amount\", 0.3),\n        cll.ElseLevel(),\n    ],\n    \"comparison_description\": \"Amount percentage difference\",\n}\n\n# The date distance is one sided becaause transactions should only arrive after they've left\n# As a result, the comparison_template_library date difference functions are not appropriate\nwithin_n_days_template = \"transaction_date_r - transaction_date_l <= {n} and transaction_date_r >= transaction_date_l\"\n\ncomparison_date = {\n    \"output_column_name\": \"transaction_date\",\n    \"comparison_levels\": [\n        cll.NullLevel(\"transaction_date\"),\n        {\n            \"sql_condition\": within_n_days_template.format(n=1),\n            \"label_for_charts\": \"1 day\",\n        },\n        {\n            \"sql_condition\": within_n_days_template.format(n=4),\n            \"label_for_charts\": \"<=4 days\",\n        },\n        {\n            \"sql_condition\": within_n_days_template.format(n=10),\n            \"label_for_charts\": \"<=10 days\",\n        },\n        {\n            \"sql_condition\": within_n_days_template.format(n=30),\n            \"label_for_charts\": \"<=30 days\",\n        },\n        cll.ElseLevel(),\n    ],\n    \"comparison_description\": \"Transaction date days apart\",\n}\n\n\nsettings = SettingsCreator(\n    link_type=\"link_only\",\n    probability_two_random_records_match=1 / len(df_origin),\n    blocking_rules_to_generate_predictions=[\n        blocking_rule_date_1,\n        blocking_rule_date_2,\n        blocking_rule_memo,\n        blocking_rule_amount_1,\n        blocking_rule_amount_2,\n        blocking_rule_cheat,\n    ],\n    comparisons=[\n        comparison_amount,\n        cl.LevenshteinAtThresholds(\"memo\", [2, 6, 10]),\n        comparison_date,\n    ],\n    retain_intermediate_calculation_columns=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Comparison Viewer Dashboard\nDESCRIPTION: Generates an interactive dashboard showing example predictions across different match scores.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/06_Visualising_predictions.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.comparison_viewer_dashboard(df_predictions, \"scv.html\", overwrite=True)\n\n# You can view the scv.html file in your browser, or inline in a notbook as follows\nfrom IPython.display import IFrame\n\nIFrame(src=\"./scv.html\", width=\"100%\", height=1200)\n```\n\n----------------------------------------\n\nTITLE: Complete Splink Workflow with Cluster Studio Dashboard Generation\nDESCRIPTION: A comprehensive example showing the entire Splink workflow from data loading to dashboard generation. This includes creating settings, instantiating a Linker, training the model using EM algorithm, generating predictions, clustering, and finally visualizing the results with cluster_studio_dashboard.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/cluster_studio_dashboard.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_template_library as ctl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.JaroWinklerAtThresholds(\"first_name\", [0.9, 0.7]),\n        cl.JaroAtThresholds(\"surname\", [0.9, 0.7]),\n        ctl.DateComparison(\n            \"dob\",\n            input_is_string=True,\n            datetime_metrics=[\"year\", \"month\"],\n            datetime_thresholds=[1, 1],\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        ctl.EmailComparison(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"substr(first_name,1,1)\"),\n        block_on(\"substr(surname, 1,1)\"),\n    ],\n    retain_intermediate_calculation_columns=True,\n    retain_matching_columns=True,\n)\n\nlinker = Linker(df, settings, DuckDBAPI())\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on(\"first_name\", \"surname\")\n\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\ndf_predictions = linker.inference.predict(threshold_match_probability=0.2)\ndf_clusters = linker.clustering.cluster_pairwise_predictions_at_threshold(\n    df_predictions, threshold_match_probability=0.5\n)\n\nlinker.visualisations.cluster_studio_dashboard(\n    df_predictions, df_clusters, \"img/cluster_studio.html\",\n    sampling_method=\"by_cluster_size\", overwrite=True\n)\n\n# You can view the scv.html file in your browser, or inline in a notebook as follows\nfrom IPython.display import IFrame\nIFrame(src=\"./img/cluster_studio.html\", width=\"100%\", height=1200)\n```\n\n----------------------------------------\n\nTITLE: Multiple Column Term-Frequency Adjustments for Ethnicity Data in Python\nDESCRIPTION: This example demonstrates how to apply term-frequency adjustments to multiple columns in the context of ethnicity data. It shows how to handle both exact matches on the full ethnicity code and matches on just the ethnic group.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/term-frequency.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nethnicity_comparison = cl.CustomComparison(\n    output_column_name=\"ethnicity\",\n    comparison_description=\"Self-defined ethnicity\",\n    comparison_levels=[\n        cll.NullLevel(\"ethnicity\"),\n        cll.ExactMatchLevel(\"ethnicity\").configure(tf_adjustment_column=\"ethnicity\"),\n        cll.ExactMatchLevel(\"ethnic_group\").configure(tf_adjustment_column=\"ethnic_group\"),\n        cll.else_level(),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Using ColumnExpression in Comparisons and Blocking Rules in Splink\nDESCRIPTION: Demonstrates how to use ColumnExpression objects in various Splink components including blocking rules, library comparisons, and custom comparison levels for record linkage.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/column_expression.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import block_on\nimport splink.comparison_library as cl\nimport splink.comparison_level_library as cll\n\nfull_name_lower_br = block_on([full_name_lowercase])\n\nemail_comparison = cl.DamerauLevenshteinAtThresholds(email_lowercase, distance_threshold_or_thresholds=[1, 3])\nentry_date_comparison = cl.AbsoluteTimeDifferenceAtThresholds(\n    entry_date,\n    input_is_string=False,\n    metrics=[\"day\", \"day\"],\n    thresholds=[1, 10],\n)\nname_comparison = cl.CustomComparison(\n    comparison_levels=[\n        cll.NullLevel(full_name_lowercase),\n        cll.ExactMatch(full_name_lowercase),\n        cll.ExactMatch(\"surname\")\n        cll.ExactMatch(\"first_name\"),\n        cll.ExactMatch(surname_initial_lowercase),\n        cll.ElseLevel()\n    ],\n    output_column_name=\"name\",\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Splink Results to External Files in Python\nDESCRIPTION: This example shows how to save Splink prediction results to external files in either parquet or CSV format. Parquet is recommended for its typing, compression, and support for nested data.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/querying_splink_results.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf_predict = linker.inference.predict()\ndf_predict.to_parquet(\"splink_predictions.parquet\", overwrite=True)\n# or alternatively:\ndf_predict.to_csv(\"splink_predictions.csv\", overwrite=True)\n```\n\n----------------------------------------\n\nTITLE: Creating Interactive Interface for Finding Matching Records\nDESCRIPTION: Builds an interactive interface using ipywidgets for finding matches to a record in the existing dataset. Users can modify field values and see matching results update in real-time, displayed in a waterfall chart.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/real_time_record_linkage.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@widgets.interact(\n    first_name=\"Robert\",\n    surname=\"Alan\",\n    dob=\"1971-05-24\",\n    city=\"London\",\n    email=\"robert255@smith.net\",\n)\ndef interactive_link(first_name, surname, dob, city, email):\n    record = {\n        \"unique_id\": 123987,\n        \"first_name\": first_name,\n        \"surname\": surname,\n        \"dob\": dob,\n        \"city\": city,\n        \"email\": email,\n        \"group\": 0,\n    }\n\n    for key in record.keys():\n        if type(record[key]) == str:\n            if record[key].strip() == \"\":\n                record[key] = None\n\n    df_inc = linker.inference.find_matches_to_new_records(\n        [record], blocking_rules=[f\"(true)\"]\n    ).as_pandas_dataframe()\n    df_inc = df_inc.sort_values(\"match_weight\", ascending=False)\n    recs = df_inc.to_dict(orient=\"records\")\n\n    display(linker.visualisations.waterfall_chart(recs, filter_nulls=False))\n```\n\n----------------------------------------\n\nTITLE: Visualizing Comparator Performance with Thresholds\nDESCRIPTION: Shows how to use the comparator_score_threshold_chart function to visualize which string pairs would be considered matches at specific distance and similarity thresholds.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsa.comparator_score_threshold_chart(\n    data, \"string1\", \"string2\", distance_threshold=2, similarity_threshold=0.8\n)\n```\n\n----------------------------------------\n\nTITLE: Blocking Rule Analysis Function\nDESCRIPTION: Shows how to analyze the number of comparisons generated by a blocking rule before implementation. This helps prevent performance issues from rules that generate too many comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/blocking/blocking_rules.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.blocking_analysis import count_comparisons_from_blocking_rule\n\nbr = block_on(\"substr(first_name, 1,1)\", \"surname\")\n\ncount_comparisons_from_blocking_rule(\n        table_or_tables=df,\n        blocking_rule=br,\n        link_type=\"dedupe_only\",\n        db_api=db_api,\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Synthetic Data for Bias Analysis in Python\nDESCRIPTION: This code creates a synthetic dataset with partial changes in surnames and postcodes to analyze how different types of record changes affect match probabilities. It generates test cases for partial surname changes (double-barrel), full postcode changes, and combinations of both.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/bias_eval.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsynthetic_comparison_partial_raw = [\n    {\"unique_id\": 2, \"person_id\": 1, \"first_name\": \"sarah\", \"surname\": \"brown-doyle\", \"dob\": \"1862-07-11\", \"birth_place\": \"london\", \"postcode_fake\": \"tf3 2ng\", \"gender\": \"female\", \"occupation\": \"politician\"},\n    {\"unique_id\": 3, \"person_id\": 1, \"first_name\": \"sarah\", \"surname\": \"doyle\", \"dob\": \"1862-07-11\", \"birth_place\": \"london\", \"postcode_fake\": \"ba13 2ng\", \"gender\": \"female\", \"occupation\": \"politician\"},\n    {\"unique_id\": 4, \"person_id\": 1, \"first_name\": \"sarah\", \"surname\": \"brown-doyle\", \"dob\": \"1862-07-11\", \"birth_place\": \"london\", \"postcode_fake\": \"ba13 2ng\", \"gender\": \"female\", \"occupation\": \"politician\"}\n]\n\nsynthetic_comparison_partial_df = pd.DataFrame(synthetic_comparison_partial_raw)\n```\n\n----------------------------------------\n\nTITLE: Comparing Two Specific Records with Splink\nDESCRIPTION: Demonstrates how to compare two specific records using the compare_two_records function. The example computes term frequency tables for each field and then calculates the match weight between two sample records.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/real_time_record_linkage.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrecord_1 = {\n    \"unique_id\": 1,\n    \"first_name\": \"Lucas\",\n    \"surname\": \"Smith\",\n    \"dob\": \"1984-01-02\",\n    \"city\": \"London\",\n    \"email\": \"lucas.smith@hotmail.com\",\n}\n\nrecord_2 = {\n    \"unique_id\": 2,\n    \"first_name\": \"Lucas\",\n    \"surname\": \"Smith\",\n    \"dob\": \"1983-02-12\",\n    \"city\": \"Machester\",\n    \"email\": \"lucas.smith@hotmail.com\",\n}\n\nlinker._settings_obj._retain_intermediate_calculation_columns = True\n\n\n# To `compare_two_records` the linker needs to compute term frequency tables\n# If you have precomputed tables, you can linker.register_term_frequency_lookup()\nlinker.table_management.compute_tf_table(\"first_name\")\nlinker.table_management.compute_tf_table(\"surname\")\nlinker.table_management.compute_tf_table(\"dob\")\nlinker.table_management.compute_tf_table(\"city\")\nlinker.table_management.compute_tf_table(\"email\")\n\n\ndf_two = linker.inference.compare_two_records(record_1, record_2)\ndf_two.as_pandas_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Trained Splink Model\nDESCRIPTION: Runs the inference step to predict matching pairs with a high threshold of 0.9 match probability to ensure high-confidence matches.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/spark/deduplicate_1k_synthetic.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresults = linker.inference.predict(threshold_match_probability=0.9)\n```\n\n----------------------------------------\n\nTITLE: Manually Defining a Comparison as a Dictionary\nDESCRIPTION: Demonstrates the most flexible approach of defining a comparison by manually creating a dictionary that follows the Splink JSON schema, providing full control over all aspects of the comparison.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncomparison_first_name = {\n    \"output_column_name\": \"first_name\",\n    \"comparison_levels\": [\n        {\n            \"sql_condition\": \"first_name_l IS NULL OR first_name_r IS NULL\",\n            \"label_for_charts\": \"Null\",\n            \"is_null_level\": True,\n        },\n        {\n            \"sql_condition\": \"first_name_l = first_name_r\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"first_name\",\n            \"tf_adjustment_weight\": 1.0,\n            \"tf_minimum_u_value\": 0.001,\n        },\n        {\n            \"sql_condition\": \"dmeta_first_name_l = dmeta_first_name_r\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"dmeta_first_name\",\n            \"tf_adjustment_weight\": 1.0,\n        },\n        {\n            \"sql_condition\": \"jaro_winkler_sim(first_name_l, first_name_r) > 0.8\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"first_name\",\n            \"tf_adjustment_weight\": 0.5,\n            \"tf_minimum_u_value\": 0.001,\n        },\n        {\"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\"},\n    ],\n}\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    comparisons=[\n        comparison_first_name,\n        cl.LevenshteinAtThresholds(\"dob\", [1, 2]),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Full Splink Graph Metrics Computation Example in Python\nDESCRIPTION: This comprehensive example demonstrates the entire process of computing graph metrics using Splink. It includes setting up a dedupe model, training the model, making predictions, clustering, and finally computing and displaying graph metrics.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/clusters/how_to_compute_metrics.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndf = splink_datasets.historical_50k\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.ExactMatch(\n            \"first_name\",\n        ).configure(term_frequency_adjustments=True),\n        cl.JaroWinklerAtThresholds(\"surname\", score_threshold_or_thresholds=[0.9, 0.8]),\n        cl.LevenshteinAtThresholds(\n            \"postcode_fake\", distance_threshold_or_thresholds=[1, 2]\n        ),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"postcode_fake\", \"first_name\"),\n        block_on(\"first_name\", \"surname\"),\n        block_on(\"dob\", \"substr(postcode_fake,1,2)\"),\n        block_on(\"postcode_fake\", \"substr(dob,1,3)\"),\n        block_on(\"postcode_fake\", \"substr(dob,4,5)\"),\n    ],\n    retain_intermediate_calculation_columns=True,\n)\n\ndb_api = DuckDBAPI()\nlinker = Linker(df, settings, db_api)\n\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    block_on(\"first_name\", \"surname\")\n)\n\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    block_on(\"dob\", \"substr(postcode_fake, 1,3)\")\n)\n\npairwise_predictions = linker.inference.predict()\nclusters = linker.clustering.cluster_pairwise_predictions_at_threshold(\n    pairwise_predictions, 0.95\n)\n\ngraph_metrics = linker.clustering.compute_graph_metrics(pairwise_predictions, clusters)\n\ndf_clusters = graph_metrics.clusters.as_pandas_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Training Name Parameters\nDESCRIPTION: Estimates parameters using EM algorithm for name-based blocking\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nblocking_rule = block_on(\"first_name\", \"surname\")\ntraining_session_names = (\n    linker.training.estimate_parameters_using_expectation_maximisation(blocking_rule)\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Synthetic Dataset for Exploratory Analysis\nDESCRIPTION: Loads a synthetic dataset with 1,000 rows containing duplicates and removes the 'cluster' column which represents ground truth. The dataset is displayed to show its structure.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/02_Exploratory_analysis.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import  splink_datasets\n\ndf = splink_datasets.fake_1000\ndf = df.drop(columns=[\"cluster\"])\ndf.head(5)\n```\n\n----------------------------------------\n\nTITLE: Finding Matches to New Records with Splink\nDESCRIPTION: Demonstrates how to find matches to a new record in the existing dataset using the find_matches_to_new_records function. The results are sorted by match weight to show the most likely matches first.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/real_time_record_linkage.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrecord = {\n    \"unique_id\": 123987,\n    \"first_name\": \"Robert\",\n    \"surname\": \"Alan\",\n    \"dob\": \"1971-05-24\",\n    \"city\": \"London\",\n    \"email\": \"robert255@smith.net\",\n}\n\n\ndf_inc = linker.inference.find_matches_to_new_records(\n    [record], blocking_rules=[]\n).as_pandas_dataframe()\ndf_inc.sort_values(\"match_weight\", ascending=False)\n```\n\n----------------------------------------\n\nTITLE: Extracting House Numbers from Addresses for Matching in DuckDB\nDESCRIPTION: Extracts house numbers from address strings using regular expressions to create a simplified address representation for matching, combining the first numeric part of the address with the postcode.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Finally we're going to extract the 'house number' from the address, which, combined with the postcode will be used for matching\n\nsql = \"\"\"\nCREATE OR REPLACE TABLE data_for_matching AS\nselect\n    unique_id,\n    source_dataset,\n    company_name,\n    company_number,\n    COALESCE(\n        REGEXP_EXTRACT(address_concat, '(\\\\d+[A-Z]?)'),\n        REGEXP_EXTRACT(address_concat, '(\\\\S+)(?=\\\\s+HOUSE)')\n    ) AS first_num_in_address,\n    postcode,\n    name_tokens_with_freq,\n    rarest_tokens\nfrom input_data_with_tokens_and_rarest_tokens\n\n\"\"\"\nduckdb.execute(sql)\nduckdb.table(\"data_for_matching\").show(max_rows=10, max_width=400)\n```\n\n----------------------------------------\n\nTITLE: Parameter Estimation Using Expectation Maximization\nDESCRIPTION: Applies the Expectation-Maximization algorithm with a specified blocking rule to further refine parameter estimates.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/pairwise_labels.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntraining_blocking_rule = block_on(\"first_name\")\nlinker.training.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n```\n\n----------------------------------------\n\nTITLE: Defining Token Frequency Product Function for Similarity in Python\nDESCRIPTION: Creates a SQL function that calculates similarity between two business names by finding the product of frequencies for matching tokens, providing a weighted measure of name similarity that considers token rarity.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_tf_product_array_sql(token_rel_freq_array_name):\n\n    return f\"\"\"\n    list_intersect({token_rel_freq_array_name}_l, {token_rel_freq_array_name}_r)\n        .list_transform(x -> x.rel_freq::float)\n        .list_concat([1.0::FLOAT]) -- in case there are no matches\n        .list_reduce((p, q) -> p * q)\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating an ExactMatch Comparison with Term-Frequency Adjustments using ComparisonLibrary\nDESCRIPTION: Shows how to create an ExactMatch comparison with term-frequency adjustments enabled using the ComparisonLibrary approach.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\nfirst_name_comparison = cl.ExactMatch(\"first_name\").configure(\n    term_frequency_adjustments=True\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Splink Model and Production Data in Python\nDESCRIPTION: Initializes a Splink Linker object with production data and a pre-trained model for bias analysis in data linking.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/bias_eval.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import DuckDBAPI\nfrom splink import Linker, SettingsCreator\nfrom splink import splink_datasets\n\ndb_api = DuckDBAPI()\n\nproduction_df = splink_datasets.historical_50k\n\nlinker = Linker(production_df, settings='../../demo_settings/model_h50k.json', db_api=db_api)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Postcode Comparison with Distance Levels in Splink\nDESCRIPTION: Shows how to build a custom postcode comparison with explicit distance thresholds. This example creates comparison levels for exact matches and three distance thresholds (1km, 10km, and 50km).\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/data_preparation/feature_engineering.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_level_library as cll\nimport splink.comparison_library as cl\n\ncustom_postcode_comparison = cl.CustomComparison(\n    output_column_name=\"postcode\",\n    comparison_description=\"Postcode\",\n    comparison_levels=[\n        cll.NullLevel(\"postcode\"),\n        cll.ExactMatchLevel(\"postcode\"),\n        cll.DistanceInKMLevel(\"lat\", \"long\", 1),\n        cll.DistanceInKMLevel(\"lat\", \"long\", 10),\n        cll.DistanceInKMLevel(\"lat\", \"long\", 50),\n        cll.ElseLevel(),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Multiple Blocking Rules Analysis\nDESCRIPTION: Shows how to analyze the cumulative effect of multiple blocking rules using Splink's visualization tools.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/03_Blocking.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.blocking_analysis import (\n    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,\n)\n\nblocking_rules_for_analysis = [\n    block_on(\"substr(first_name, 1,1)\", \"surname\"),\n    block_on(\"surname\"),\n    block_on(\"email\"),\n    block_on(\"city\", \"first_name\"),\n    \"l.first_name = r.first_name and levenshtein(l.surname, r.surname) < 2\",\n]\n\n\ncumulative_comparisons_to_be_scored_from_blocking_rules_chart(\n    table_or_tables=df,\n    blocking_rules=blocking_rules_for_analysis,\n    db_api=db_api,\n    link_type=\"dedupe_only\",\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Deterministic Linkage and Displaying Results (Python)\nDESCRIPTION: This snippet performs the deterministic linkage using the configured Splink Linker and displays the first few rows of the results.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/deterministic_dedupe.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf_predict = linker.inference.deterministic_link()\ndf_predict.as_pandas_dataframe().head()\n```\n\n----------------------------------------\n\nTITLE: Running Record Comparison and Visualizing Match Probabilities in Python\nDESCRIPTION: This code demonstrates how to compare records using a linking model and visualize the match probabilities. The function compare_records processes the synthetic data through the linker, and the results are displayed with cell highlighting based on probability values.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/bias_eval.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncomparisons_partial = compare_records(synthetic_base_raw, synthetic_comparison_partial_raw, linker)\n\ncomparisons_partial[columns_of_interest].style.map(highlight_cells, subset=['match_probability'])\n```\n\n----------------------------------------\n\nTITLE: Analyzing Prediction Errors in Splink\nDESCRIPTION: Analyzes prediction errors by comparing to the 'cluster' ground truth column in Splink. It includes both false positives and false negatives, and demonstrates how to create a waterfall chart of the results.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/accuracy_analysis_from_labels_column.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlinker.evaluation.prediction_errors_from_labels_column(\n    \"cluster\", include_false_negatives=True, include_false_positives=True\n).as_pandas_dataframe(limit=5)\n```\n\nLANGUAGE: python\nCODE:\n```\nrecords = linker.evaluation.prediction_errors_from_labels_column(\n    \"cluster\", include_false_negatives=True, include_false_positives=True\n).as_record_dict(limit=5)\n\nlinker.visualisations.waterfall_chart(records)\n```\n\n----------------------------------------\n\nTITLE: Applying Term-Frequency Adjustments using Comparison Level Library in Python\nDESCRIPTION: This code demonstrates how to implement term-frequency adjustments using Splink's comparison_level_library. It shows a custom name comparison with specific TF adjustment columns for different comparison levels.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/term-frequency.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_level_library as cll\n\nname_comparison = cl.CustomComparison(\n    output_column_name=\"name\",\n    comparison_description=\"Full name\",\n    comparison_levels=[\n        cll.NullLevel(\"full_name\"),\n        cll.ExactMatchLevel(\"full_name\").configure(tf_adjustment_column=\"full_name\"),\n        cll.ColumnsReversedLevel(\"first_name\", \"surname\").configure(\n            tf_adjustment_column=\"surname\"\n        ),\n        cll.else_level(),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Cluster Analysis\nDESCRIPTION: Clusters the pairwise predictions using a threshold match probability of 0.5.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/05_Predicting_results.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclusters = linker.clustering.cluster_pairwise_predictions_at_threshold(\n    df_predictions, threshold_match_probability=0.5\n)\nclusters.as_pandas_dataframe(limit=10)\n```\n\n----------------------------------------\n\nTITLE: Querying Splink Results with SQL in Python\nDESCRIPTION: This example shows how to query Splink's predict results using SQL to find the best match for each input record in a link-only job. The query partitions results by unique_id_l and selects only the top match by match_weight for each record.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/querying_splink_results.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# linker is a Linker with link_type set to \"link_only\"\ndf_predict = linker.predict(threshold_match_probability=0.75)\n\nsql = f\"\"\"\nwith ranked as\n(\nselect *,\nrow_number() OVER (\n    PARTITION BY unique_id_l order by match_weight desc\n    ) as row_number\nfrom {df_predict.physical_name}\n)\n\nselect *\nfrom ranked\nwhere row_number = 1\n\"\"\"\n\ndf_query_result = linker.misc.query_sql(sql)  # pandas dataframe\n```\n\n----------------------------------------\n\nTITLE: Customizing NameComparison with Jaro-Winkler Thresholds in Splink\nDESCRIPTION: Creates a customized name comparison for surname with specified Jaro-Winkler thresholds and Double Metaphone column for phonetic matching. This allows for tuning the fuzzy matching behavior.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsurname_comparison = cl.NameComparison(\n    \"surname\",\n    jaro_winkler_thresholds=[0.95, 0.9],\n    dmeta_col_name=\"surname_dmeta\",\n)\nprint(surname_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Configuring Splink for Deterministic Linkage (Python)\nDESCRIPTION: This code sets up the Splink Linker object with deterministic blocking rules for deduplication. It defines the settings and initializes the linker with the dataset.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/deterministic_dedupe.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import Linker, SettingsCreator\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\", \"surname\", \"dob\"),\n        block_on(\"surname\", \"dob\", \"postcode_fake\"),\n        block_on(\"first_name\", \"dob\", \"occupation\"),\n    ],\n    retain_intermediate_calculation_columns=True,\n)\n\nlinker = Linker(df, settings, db_api=db_api)\n```\n\n----------------------------------------\n\nTITLE: Extracting Postcode Area with Regex in Splink (Python)\nDESCRIPTION: This snippet demonstrates how to use a regular expression to extract the postcode area (first 1-2 letters) from a postcode column for comparison in Splink. It uses ColumnExpression and ExactMatchLevel from the comparison_level_library.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/regular_expressions.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_level_library as cll\nfrom splink import ColumnExpression\n\npc_ce = ColumnExpression(\"postcode\").regex_extract(\"^[A-Z]{1,2}\")\nprint(cll.ExactMatchLevel(pc_ce).get_comparison_level(\"duckdb\").sql_condition)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Data Completeness for Record Linkage in Python\nDESCRIPTION: Creates a completeness chart to visualize the completeness of key fields in both datasets. This helps to identify potential data quality issues that might affect the linking process.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/link_only.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.exploratory import completeness_chart\n\ncompleteness_chart(\n    [df_l, df_r],\n    cols=[\"first_name\", \"surname\", \"dob\", \"city\", \"email\"],\n    db_api=DuckDBAPI(),\n    table_names_for_chart=[\"df_left\", \"df_right\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing DateOfBirthComparison in Splink\nDESCRIPTION: Creates a comparison for date of birth fields with the option to specify if the input is a string. This comparison handles date of birth matching with appropriate logic for this data type.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\ndate_of_birth_comparison = cl.DateOfBirthComparison(\n    \"date_of_birth\",\n    input_is_string=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Estimating U Probabilities\nDESCRIPTION: Estimates u probabilities using random sampling method with a maximum of 1 million pairs.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n```\n\n----------------------------------------\n\nTITLE: Creating Levenshtein Comparison using Comparison Library in Python\nDESCRIPTION: Implements a Levenshtein distance comparison for the 'email' column with thresholds at 2 and 4 using Splink's high-level comparison library. This is the most concise implementation approach.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\nemail_comparison = cl.LevenshteinAtThresholds(\"email\", [2, 4])\n```\n\n----------------------------------------\n\nTITLE: Largest Blocks Analysis\nDESCRIPTION: Demonstrates how to identify the largest blocking groups using Splink's n_largest_blocks function.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/03_Blocking.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.blocking_analysis import n_largest_blocks\n\nresult = n_largest_blocks(    table_or_tables=df,\n    blocking_rule= block_on(\"city\", \"first_name\"),\n    link_type=\"dedupe_only\",\n    db_api=db_api,\n    n_largest=3\n    )\n\nresult.as_pandas_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Data Preparation for Link-Only Record Linkage in Python\nDESCRIPTION: Loads a fake dataset and splits it into two separate dataframes (df_l and df_r) that will be linked. This splitting creates two datasets that can be used to demonstrate the link-only functionality.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/link_only.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import splink_datasets\n\ndf = splink_datasets.fake_1000\n\n# Split a simple dataset into two, separate datasets which can be linked together.\ndf_l = df.sample(frac=0.5)\ndf_r = df.drop(df_l.index)\n\ndf_l.head(2)\n```\n\n----------------------------------------\n\nTITLE: Creating a SplinkDataFrame from Database Table in Python\nDESCRIPTION: This example demonstrates how to create a SplinkDataFrame from an existing table in a DuckDB database. It sets up a connection, creates a sample table, and then registers it with the Splink linker to create a SplinkDataFrame.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/querying_splink_results.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport duckdb\n\nfrom splink import Linker, SettingsCreator, DuckDBAPI\nfrom splink.datasets import splink_datasets\n\ncon = duckdb.connect()\ndf_numbers = pd.DataFrame({\"id\": [1, 2, 3], \"number\": [\"one\", \"two\", \"three\"]})\ncon.sql(\"CREATE TABLE number_table AS SELECT * FROM df_numbers\")\n\ndb_api = DuckDBAPI(connection=con)\ndf = splink_datasets.fake_1000\n\nlinker = Linker(df, settings=SettingsCreator(link_type=\"dedupe_only\"), db_api=db_api)\nsplink_df = linker.table_management.register_table(\"number_table\", \"a_templated_name\")\nsplink_df.as_pandas_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Implementing Splink Unlinkables Chart with Data Processing\nDESCRIPTION: Complete example showing how to create a Splink linker object, train it using EM algorithm, and generate an unlinkables chart. Uses fake data and multiple comparison methods including Jaro-Winkler, exact matching, and date comparison.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/unlinkables_chart.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_template_library as ctl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndb_api = DuckDBAPI()\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.JaroWinklerAtThresholds(\"first_name\", [0.9, 0.7]),\n        cl.JaroAtThresholds(\"surname\", [0.9, 0.7]),\n        ctl.DateComparison(\n            \"dob\",\n            input_is_string=True,\n            datetime_metrics=[\"year\", \"month\"],\n            datetime_thresholds=[1, 1],\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        ctl.EmailComparison(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n)\n\nlinker = Linker(df, settings, db_api)\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on(\"first_name\", \"surname\")\n\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nchart = linker.evaluation.unlinkables_chart()\nchart\n```\n\n----------------------------------------\n\nTITLE: Importing Splink and Loading Transaction Data\nDESCRIPTION: This snippet imports necessary Splink modules and loads the origin and destination transaction datasets. It displays the first two rows of each dataset.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/transactions.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndf_origin = splink_datasets.transactions_origin\ndf_destination = splink_datasets.transactions_destination\n\ndisplay(df_origin.head(2))\ndisplay(df_destination.head(2))\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink Linker\nDESCRIPTION: Creates a Linker instance with configured settings and database API\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\n\nfrom splink import Linker, SettingsCreator\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\", \"surname\"),\n        block_on(\"surname\", \"dob\"),\n    ],\n    comparisons=[\n        cl.ExactMatch(\"first_name\").configure(term_frequency_adjustments=True),\n        cl.LevenshteinAtThresholds(\"surname\", [1, 3]),\n        cl.LevenshteinAtThresholds(\"dob\", [1, 2]),\n        cl.LevenshteinAtThresholds(\"postcode_fake\", [1, 2]),\n        cl.ExactMatch(\"birth_place\").configure(term_frequency_adjustments=True),\n        cl.ExactMatch(\"occupation\").configure(term_frequency_adjustments=True),\n    ],\n    retain_intermediate_calculation_columns=True,\n)\n\nlinker = Linker(df, settings, db_api=db_api)\n```\n\n----------------------------------------\n\nTITLE: Creating Interactive Comparison Playground in Splink\nDESCRIPTION: Comprehensive function that creates an interactive widget interface for testing Splink comparison functions. It allows users to select from various comparison methods, input test values, and see detailed results including match levels and SQL conditions.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/comparison_playground.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nfrom splink.internals.testing import comparison_vector_value, is_in_level\nfrom splink import DuckDBAPI\nimport ipywidgets as widgets\nfrom IPython.display import Markdown\n\n\ndef report_comparison_levels(comparison, values_dict, column_name):\n    db_api = DuckDBAPI()\n    levels = comparison.create_comparison_levels()\n    comparison_dict = comparison.get_comparison(\"duckdb\").as_dict()\n\n    table_rows = []\n    table_rows.append(\"| Match | Level | Description | SQL Condition |\")\n    table_rows.append(\"|-------|-------|-------------|---------------|\")\n\n    total_levels = len(levels)\n    matched_level = None\n    for i, level in enumerate(levels):\n        level_number = total_levels - i - 1\n        label = level.create_label_for_charts()\n        sql_condition = comparison_dict['comparison_levels'][i]['sql_condition']\n\n        is_match = is_in_level(level, values_dict, db_api)\n\n        if is_match and matched_level is None:\n            matched_level = level_number\n            match_indicator = \"\"\n            row = f\"| {match_indicator} | **{level_number}** | **{label}** | `{sql_condition}` |\"\n        else:\n            match_indicator = \"\"\n            row = f\"| {match_indicator} | {level_number} | {label} | `{sql_condition}` |\"\n\n        table_rows.append(row)\n\n    markdown_output = \"\\n\".join(table_rows)\n    return markdown_output\n\ndef create_comparison_playground(column_name):\n\n    comparison_types = [\n        'ExactMatch', 'LevenshteinAtThresholds', 'JaroAtThresholds',\n        'JaroWinklerAtThresholds', 'DamerauLevenshteinAtThresholds',\n        'JaccardAtThresholds',\n        'AbsoluteDateDifferenceAtThresholds',\n        'ArrayIntersectAtSizes', 'DateOfBirthComparison',\n         'EmailComparison',\n        'NameComparison', 'PostcodeComparison'\n    ]\n\n    default_values = {\n        'ExactMatch': ('john', 'jon'),\n        'LevenshteinAtThresholds': ('smith', 'smyth'),\n        'JaroAtThresholds': ('martha', 'matha'),\n        'JaroWinklerAtThresholds': ('williams', 'willaims'),\n        'DamerauLevenshteinAtThresholds': ('receive', 'recieve'),\n        'CosineSimilarityAtThresholds': ('data science', 'science data'),\n        'JaccardAtThresholds': ('0123456789', '012345678'),\n        'AbsoluteDateDifferenceAtThresholds': ('2023-01-01', '2023-01-15'),\n        'ArrayIntersectAtSizes': ('apple,banana,cherry', 'banana,cherry,date'),\n        'DateOfBirthComparison': ('1990-05-15', '1990-05-16'),\n        'EmailComparison': ('john.doe@example.com', 'john.doe@other.com'),\n        'NameComparison': ('Elizabeth', 'Elisabeth'),\n        'PostcodeComparison': ('SW1A 1AA', 'SW1A 1AB')\n    }\n\n    db_api = DuckDBAPI()\n\n    docstrings = {}\n    for comp_type in comparison_types:\n        class_obj = getattr(cl, comp_type)\n        init_doc = getattr(class_obj.__init__, '__doc__', None)\n        docstrings[comp_type] = init_doc if init_doc else class_obj.__doc__\n\n\n    def get_comparison(comp_type):\n        if comp_type in ['DateOfBirthComparison']:\n\n            return getattr(cl, comp_type)(column_name, input_is_string=True)\n        if comp_type == 'AbsoluteDateDifferenceAtThresholds':\n            return getattr(cl, comp_type)(column_name, input_is_string=True, metrics=[\"day\", \"month\"], thresholds=[1, 1])\n        elif comp_type in ['EmailComparison', 'ForenameSurnameComparison', 'NameComparison', 'PostcodeComparison', 'ArrayIntersectAtSizes']:\n            return getattr(cl, comp_type)(column_name)\n        else:\n            return getattr(cl, comp_type)(column_name)\n\n    def run_comparison(change):\n        left_value = left_input.value if left_input.value != \"\" else None\n        right_value = right_input.value if right_input.value != \"\" else None\n        comparison = get_comparison(comparison_select.value)\n\n        if comparison_select.value == 'ArrayIntersectAtSizes':\n            left_value = left_value.split(',') if left_value else None\n            right_value = right_value.split(',') if right_value else None\n\n        values_dict = {f\"{column_name}_l\": left_value, f\"{column_name}_r\": right_value}\n\n        output.clear_output()\n        markdown_output = report_comparison_levels(comparison, values_dict, column_name)\n        with output:\n            display(Markdown(\"### Comparison levels:\"))\n            display(Markdown(markdown_output))\n\n            docstring = docstrings.get(comparison_select.value, \"No docstring available\")\n            processed_docstring = \"\\n\".join(line.strip() for line in docstring.split(\"\\n\"))\n            display(Markdown(\"### Comparison Function Docstring:\"))\n            display(Markdown(processed_docstring))\n\n\n        # Store the markdown output for later use\n        playground.markdown_output = markdown_output\n\n\n\n    def on_comparison_change(change):\n        new_value = change['new']\n        left_value, right_value = default_values.get(new_value, ('', ''))\n\n        # Temporarily unobserve the input widgets\n        left_input.unobserve(run_comparison, names='value')\n        right_input.unobserve(run_comparison, names='value')\n\n        # Update the values\n        left_input.value = left_value\n        right_input.value = right_value\n\n        # Re-observe the input widgets\n        left_input.observe(run_comparison, names='value')\n        right_input.observe(run_comparison, names='value')\n\n        # Run the comparison once after updating both inputs\n        run_comparison(None)\n\n\n    comparison_select = widgets.Dropdown(\n        options=comparison_types,\n        value='ExactMatch',\n        description='Comparison:',\n    )\n    left_input = widgets.Text(description=f\"{column_name} Left:\", value=default_values['ExactMatch'][0])\n    right_input = widgets.Text(description=f\"{column_name} Right:\", value=default_values['ExactMatch'][1])\n    output = widgets.Output()\n\n    comparison_select.observe(on_comparison_change, names='value')\n    for widget in (comparison_select, left_input, right_input):\n        widget.observe(run_comparison, names='value')\n\n    # Call run_comparison immediately to compute initial output\n    playground = widgets.VBox([comparison_select, left_input, right_input, output])\n\n    run_comparison(None)\n\n    return playground\n\nplayground = create_comparison_playground(\"column\")\ndisplay(playground)\n```\n\n----------------------------------------\n\nTITLE: Running Predictions and Ranking Matches\nDESCRIPTION: Runs the prediction process with a threshold, then uses SQL to rank matches and find the best matching company from the Companies House data for each Stockport business. Results are ordered by match probability.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Get some results\n\ndf_predictions = linker.inference.predict(threshold_match_weight=-5)\n\n\n\n# Since the companies house data is a canonical list, here we're taking the\n# best matching company for each Stockport business\nsql = f\"\"\"\nWITH ranked_matches AS (\n    SELECT *,\n           ROW_NUMBER() OVER (PARTITION BY unique_id_l ORDER BY match_weight DESC) as rank\n    FROM {df_predictions.physical_name}\n),\nbest_match as (\n    select * from ranked_matches\n    where rank = 1\n    order by match_weight desc\n),\nmatched_stockport_ids as (\n    select distinct unique_id_l\n    from best_match\n)\nselect * from best_match\n\norder by match_probability desc\n\"\"\"\n\nranked_matches = con.sql(sql)\nranked_matches.show(max_rows=10, max_width=400)\n```\n\n----------------------------------------\n\nTITLE: Computing Token Frequencies for Business Names in DuckDB\nDESCRIPTION: Calculates the relative frequency of each token across all business names in the dataset, which will be used to weight the importance of token matches in the similarity calculation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Compute token relative frequencies\nsql = \"\"\"\nCREATE OR REPLACE TABLE token_frequencies AS\nSELECT\n    name_token as token,\n    count(*)::float/(select count(*) from unnested) as rel_freq\nFROM unnested\nGROUP BY token\nORDER BY rel_freq DESC\n\n\"\"\"\n\n# Execute and show results\ntoken_frequencies = duckdb.execute(sql)\nduckdb.table(\"token_frequencies\").show(max_rows=10, max_width=100000)\n```\n\n----------------------------------------\n\nTITLE: Creating Structured Token Lists with Frequencies in DuckDB\nDESCRIPTION: Creates a structured representation of each company name as a list of tokens with their corresponding frequency weights, allowing for more sophisticated comparison of business names during matching.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Reconstruct the company name as a list of tokens with their relative frequency\n# This creates a new column called name_tokens_with_freq\n# Example: 101 CANVAS PRINTS LTD  becomes\n# [\n#  {'token': 101, 'rel_freq': 3.19e-05},\n#  {'token': CANVAS, 'rel_freq': 2.20e-05},\n#  {'token': PRINTS, 'rel_freq': 3.16e-05},\n#  {'token': LTD, 'rel_freq': 0.142}\n# ]\nsql = \"\"\"\nCREATE OR REPLACE TABLE input_data_with_tokens AS\nWITH\ntokens_with_freq AS (\n    SELECT\n        m.unique_id,\n        -- Create a list of structs containing each token and its frequency\n        list_transform(\n            list_zip(\n                array_agg(u.name_token ORDER BY u.token_position_in_name),\n                array_agg(COALESCE(tf.rel_freq, 0.0) ORDER BY u.token_position_in_name)\n            ),\n            x -> struct_pack(token := x[1], rel_freq := x[2])\n        ) as name_tokens_with_freq\n    FROM all_input_data m\n    JOIN unnested u ON m.unique_id = u.unique_id\n    LEFT JOIN token_frequencies tf ON u.name_token = tf.token\n    GROUP BY m.unique_id\n)\nSELECT\n    m.*,\n    t.name_tokens_with_freq\nFROM all_input_data m\nLEFT JOIN tokens_with_freq t ON m.unique_id = t.unique_id\norder by m.unique_id\n\"\"\"\n\n# Execute and show results\nduckdb.execute(sql)\nduckdb.table(\"input_data_with_tokens\").show(max_rows=10, max_width=200)\n```\n\n----------------------------------------\n\nTITLE: Initializing PostgreSQL Connection for Splink in Python\nDESCRIPTION: Sets up a SQLAlchemy engine to connect to PostgreSQL and initializes a PostgresLinker for Splink. Demonstrates two methods of passing data: using an existing table name or a pandas DataFrame.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/backends/postgres.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sqlalchemy import create_engine\n\nfrom splink.postgres.linker import PostgresLinker\nimport splink.postgres.comparison_library as cl\n\n# create a sqlalchemy engine to manage connecting to the database\nengine = create_engine(\"postgresql+psycopg2://USER:PASSWORD@HOST:PORT/DB_NAME\")\n\nsettings = SettingsCreator(\n    link_type= \"dedupe_only\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ndbapi = PostgresAPI(engine=engine)\nlinker = Linker(\n    \"my_data_table,\n    settings_dict,\n    db_api=db_api,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\n# create pandas frame from csv\ndf = pd.read_csv(\"./my_data_table.csv\")\n\ndbapi = PostgresAPI(engine=engine)\nlinker = Linker(\n    df,\n    settings_dict,\n    db_api=db_api,\n)\n```\n\n----------------------------------------\n\nTITLE: Estimating U Parameters in Splink using Random Sampling\nDESCRIPTION: Estimates the u parameters (probability of agreement given non-match) using random sampling in Splink.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/accuracy_analysis_from_labels_column.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6, seed=5)\n```\n\n----------------------------------------\n\nTITLE: Comparing Array Columns with Splink in Python\nDESCRIPTION: This snippet demonstrates how to use ArrayIntersectAtSizes to assess the similarity of columns containing arrays in Splink. It sets up a simple dataset with array columns and uses the Linker class to perform deduplication.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/cookbook.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on\n\n\ndata = [\n    {\"unique_id\": 1, \"first_name\": \"John\", \"postcode\": [\"A\", \"B\"]},\n    {\"unique_id\": 2, \"first_name\": \"John\", \"postcode\": [\"B\"]},\n    {\"unique_id\": 3, \"first_name\": \"John\", \"postcode\": [\"A\"]},\n    {\"unique_id\": 4, \"first_name\": \"John\", \"postcode\": [\"A\", \"B\"]},\n    {\"unique_id\": 5, \"first_name\": \"John\", \"postcode\": [\"C\"]},\n]\n\ndf = pd.DataFrame(data)\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n    ],\n    comparisons=[\n        cl.ArrayIntersectAtSizes(\"postcode\", [2, 1]),\n        cl.ExactMatch(\"first_name\"),\n    ]\n)\n\n\nlinker = Linker(df, settings, DuckDBAPI(), set_up_basic_logging=False)\n\nlinker.inference.predict().as_pandas_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Estimating U Parameters Using Random Sampling in Python\nDESCRIPTION: Estimates the U parameters (probability of agreement given records don't match) using random sampling. This is a key step in configuring the probabilistic matching model.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/link_only.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6, seed=1)\n```\n\n----------------------------------------\n\nTITLE: Estimating Match Probabilities for Record Pairs\nDESCRIPTION: Estimates the probability of two random records matching using specific matching conditions. The function targets a recall of 0.6 across three different match criteria.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/demo_settings/model_create_h50k.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_probability_two_random_records_match(\n    [\n        \"l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob\",\n        \"substr(l.first_name,1,2) = substr(r.first_name,1,2) and l.surname = r.surname and substr(l.postcode_fake,1,2) = substr(r.postcode_fake,1,2)\",\n        \"l.dob = r.dob and l.postcode_fake = r.postcode_fake\",\n    ],\n    recall=0.6,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Splink Linker and Estimating Match Probability Using Deterministic Rules\nDESCRIPTION: Initializes the Splink Linker with a Spark backend and estimates the probability of two random records matching using deterministic matching rules, targeting a recall of 0.6.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/spark/deduplicate_1k_synthetic.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlinker = Linker(df, settings, db_api=SparkAPI(spark_session=spark))\ndeterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\",\n    \"l.email = r.email\",\n]\n\nlinker.training.estimate_probability_two_random_records_match(deterministic_rules, recall=0.6)\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink Linker and Estimating Match Probability in Python\nDESCRIPTION: Initializes the Splink Linker with the dataset and settings, and estimates the probability of two random records matching using deterministic rules.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/accuracy_analysis_from_labels_column.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndb_api = DuckDBAPI()\nlinker = Linker(df, settings, db_api=db_api)\ndeterministic_rules = [\n    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\",\n    \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\",\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\",\n    \"l.email = r.email\",\n]\n\nlinker.training.estimate_probability_two_random_records_match(\n    deterministic_rules, recall=0.7\n)\n```\n\n----------------------------------------\n\nTITLE: SQL Equivalent of Basic Blocking Rule\nDESCRIPTION: Shows the SQL representation of a blocking rule that performs an inner join on matching first name and surname fields.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/03_Blocking.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT ...\nFROM input_tables as l\nINNER JOIN input_tables as r\nON l.first_name = r.first_name AND l.surname = r.surname\n```\n\n----------------------------------------\n\nTITLE: Displaying Linkage Results\nDESCRIPTION: Converts the linkage results to a Pandas DataFrame and displays the first 5 rows to review the matched records and their match probabilities.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/quick_and_dirty_persons.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresults.as_pandas_dataframe(limit=5)\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink for Both Linking and Deduplication\nDESCRIPTION: Code to set up Splink for both linking and deduplication (link_and_dedupe) across multiple input tables. This performs both operations in a single process.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/link_type.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import SettingsCreator\n\nsettings = SettingsCreator(\n    link_type= \"link_and_dedupe\",\n)\n\nlinker = Linker(\n    [df_1, df_2, df_n],\n    settings,\n    db_api=dbapi,\n    input_table_aliases=[\"name1\", \"name2\", \"name3\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing False Positives\nDESCRIPTION: Generates visualization of false positive prediction errors using waterfall chart.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/07_Evaluation.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsplink_df = linker.evaluation.prediction_errors_from_labels_table(\n    labels_table, include_false_negatives=False, include_false_positives=True, threshold_match_probability=0.01\n)\nfalse_postives = splink_df.as_record_dict(limit=5)\nlinker.visualisations.waterfall_chart(false_postives)\n```\n\n----------------------------------------\n\nTITLE: Creating NameComparison with Double Metaphone in Splink\nDESCRIPTION: This code creates a NameComparison object for the 'first_name' column, incorporating the Double Metaphone encoded column 'first_name_dm'. It uses the Splink comparison template library for DuckDB.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/data_preparation/feature_engineering.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport splink.duckdb.comparison_template_library as ctl\n\ncomparison = cl.NameComparison(\"first_name\", dmeta_col_name=\"first_name_dm\").get_comparison(\"duckdb\")\ncomparison.human_readable_description\n```\n\n----------------------------------------\n\nTITLE: Creating Term Frequency Adjustment Chart with Splink in Python\nDESCRIPTION: This code demonstrates how to create a Term Frequency Adjustment Chart using the Splink library. It includes setting up the linker with various comparisons, estimating parameters, and generating the chart for the 'first_name' field.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/tf_adjustment_chart.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_template_library as ctl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.JaroWinklerAtThresholds(\"first_name\", [0.9, 0.7]).configure(\n            term_frequency_adjustments=True\n        ),\n        cl.JaroAtThresholds(\"surname\", [0.9, 0.7]),\n        ctl.DateComparison(\n            \"dob\",\n            input_is_string=True,\n            datetime_metrics=[\"year\", \"month\"],\n            datetime_thresholds=[1, 1],\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        ctl.EmailComparison(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n)\n\nlinker = Linker(df, settings, DuckDBAPI())\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on(\"first_name\", \"surname\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nchart = linker.visualisations.tf_adjustment_chart(\n    \"first_name\", vals_to_include=[\"Robert\", \"Grace\"]\n)\nchart\n```\n\n----------------------------------------\n\nTITLE: Visualizing m and u Parameters in Splink\nDESCRIPTION: Creates a chart displaying the estimated m and u parameters, which represent the probability distributions for match and non-match comparisons across different comparison levels.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.m_u_parameters_chart()\n```\n\n----------------------------------------\n\nTITLE: Generating Clusters from Deterministic Linkage Results (Python)\nDESCRIPTION: This code generates clusters from the deterministic linkage results using a threshold match probability of 1.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/deterministic_dedupe.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclusters = linker.clustering.cluster_pairwise_predictions_at_threshold(\n    df_predict\n)\n```\n\n----------------------------------------\n\nTITLE: Combining Datasets into a Single Table with DuckDB\nDESCRIPTION: Creates a unified table by concatenating both datasets vertically, adding a source identifier column and a unique ID for each record to facilitate later processing and identification.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Start by creating a table with the data vertically concatenated\nsql = \"\"\"\ncreate or replace table all_input_data as\nwith concat_data as (\n    select *, 'stockport' as source_dataset\n\nfrom df_stockport_business_rates\nunion all\nselect *, 'z_all_companies' as source_dataset\nfrom df_companies_house\n)\nselect ROW_NUMBER() OVER () as unique_id, *\nfrom concat_data\n\"\"\"\n\nduckdb.execute(sql)\n\nduckdb.table(\"all_input_data\").show(max_rows=10, max_width=100000)\n```\n\n----------------------------------------\n\nTITLE: Basic Blocking Rule Implementation in Splink\nDESCRIPTION: Demonstrates setting up a simple blocking rule in Splink that blocks on first name and surname fields. This configuration will only generate comparisons between records where both first name and surname match exactly.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/blocking/blocking_rules.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nSettingsCreator(\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\", \"surname\")\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a PostcodeComparison for Address Matching\nDESCRIPTION: Creates a specialized PostcodeComparison for the 'postcode' field and displays its human-readable description with the DuckDB backend.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npc_comparison = cl.PostcodeComparison(\"postcode\")\nprint(pc_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Configuring Blocking Rules with Salting in Splink with DuckDB\nDESCRIPTION: Example of configuring blocking rules with salting partitions in Splink to increase parallelism. This snippet demonstrates setting up three blocking rules (first_name, dob, and surname) each with 2 salting partitions to increase the overall parallelism of the operation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/performance/optimising_duckdb.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsettings =  {\n    ...\n    \"blocking_rules_to_generate_predictions\" ; [\n        block_on([\"first_name\"], salting_partitions=2),\n        block_on([\"dob\"], salting_partitions=2),\n        block_on([\"surname\"], salting_partitions=2),\n    ]\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Term-Frequency Adjustments using Comparison Libraries in Python\nDESCRIPTION: This snippet shows how to enable term-frequency adjustments in Splink using the ComparisonLibrary and ComparisonTemplateLibrary functions. It demonstrates applying TF adjustments to sex, name, and email comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/term-frequency.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_template_library as ctl\n\nsex_comparison = cl.ExactMatch(\"sex\").configure(term_frequency_adjustments=True)\n\nname_comparison = cl.JaroWinklerAtThresholds(\n    \"name\",\n    score_threshold_or_thresholds=[0.9, 0.8],\n).configure(term_frequency_adjustments=True)\n\nemail_comparison = ctl.EmailComparison(\"email\").configure(\n    term_frequency_adjustments=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Matching Results in Spark\nDESCRIPTION: Displays the resulting matches as a Spark DataFrame, showing pairs of records that are likely to belong to the same entity.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/spark/deduplicate_1k_synthetic.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nspark_df = results.as_spark_dataframe().show()\n```\n\n----------------------------------------\n\nTITLE: Extracting Rare Tokens for Blocking in DuckDB\nDESCRIPTION: Identifies the rarest tokens in each company name that have a frequency less than 1%, which will be used for blocking strategies to reduce the number of pairwise comparisons during matching.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Derive rarest tokens - these will be used for blocking\n# This would be simpler if Duckdb list sort supported a lambda function\n# but it doesn't, so we have to unnest the list and sort manually\n# https://duckdb.org/docs/sql/functions/list.html#sorting-lists\nsql = \"\"\"\nwith\ntokens_unnested as (\n    SELECT\n        unique_id,\n        unnest(name_tokens_with_freq) as token_info\n    FROM input_data_with_tokens\n),\nrare_tokens as (\n    SELECT\n        unique_id,\n        array_agg(token_info.token ORDER BY token_info.rel_freq ASC)[:2] as rarest_tokens\n    FROM tokens_unnested\n    WHERE token_info.rel_freq < 0.01\n    GROUP BY unique_id\n)\nselect m.*, rarest_tokens\nfrom input_data_with_tokens as m\nleft join rare_tokens on m.unique_id = rare_tokens.unique_id\norder by m.unique_id\n\"\"\"\ninput_data_with_tokens_and_rarest_tokens = duckdb.sql(sql)\ninput_data_with_tokens_and_rarest_tokens.show(max_rows=10, max_width=1000)\n```\n\n----------------------------------------\n\nTITLE: SQL Blocking Rules for Splink Predictions\nDESCRIPTION: This SQL snippet shows examples of blocking rules used in Splink to generate predictions. It demonstrates how to block on first name and surname, as well as date of birth, using SQL conditions with table aliases 'l' and 'r'.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/settings_dict_guide.md#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n[\"l.first_name = r.first_name AND l.surname = r.surname\", \"l.dob = r.dob\"]\n```\n\n----------------------------------------\n\nTITLE: Analyzing Comparison Details with Waterfall Chart\nDESCRIPTION: Creates a comparison table of non-exact matches and displays a waterfall chart to visualize how different comparison attributes contribute to the overall match probability for a specific record pair.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nsql = \"\"\"\nCREATE OR REPLACE TABLE comparison AS\nselect * from ranked_matches\nwhere company_name_l != company_name_r\norder by random()\nlimit 1\n\"\"\"\nduckdb.execute(sql)\n\n\nsql = \"\"\"\nSELECT match_probability, COLUMNS('^(t[^f_].*|b[^f_].*|[^tb].*_l)$') AS '\\\\1'\nFROM comparison\nUNION ALL\nSELECT match_probability, COLUMNS('^(t[^f_].*|b[^f_].*|[^tb].*_r)$') AS '\\\\1'\nFROM comparison;\n\"\"\"\nduckdb.sql(sql).show(max_rows=10, max_width=200)\n\nrecs = duckdb.table(\"comparison\").df().to_dict(orient=\"records\")\nlinker.visualisations.waterfall_chart(recs)\n```\n\n----------------------------------------\n\nTITLE: Implementing Full Name Comparison with Component-Based Term-Frequency Adjustments in Python\nDESCRIPTION: This code shows a more complex example of term-frequency adjustments applied to a full name comparison. It handles matches on full name, first name, and surname separately, with different adjustment columns for each level.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/term-frequency.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nname_comparison = cl.CustomComparison(\n    output_column_name=\"name\",\n    comparison_description=\"Full name\",\n    comparison_levels=[\n        cll.NullLevel(\"full_name\"),\n        cll.ExactMatchLevel(\"full_name\").configure(tf_adjustment_column=\"full_name\"),\n        cll.ExactMatchLevel(\"first_name\").configure(tf_adjustment_column=\"first_name\"),\n        cll.ExactMatchLevel(\"surname\").configure(tf_adjustment_column=\"surname\"),\n        cll.else_level(),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Data for Bias Analysis in Python\nDESCRIPTION: Creates synthetic datasets to test the hypothesis of gender bias in data linking due to surname and address changes. The data includes multiple records for the same person with variations in surname and postcode.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/bias_eval.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nsynthetic_base_raw = [\n    {\"unique_id\": 1, \"person_id\": 1, \"first_name\": \"sarah\", \"surname\": \"brown\", \"dob\": \"1862-07-11\", \"birth_place\": \"london\", \"postcode_fake\": \"ba12 0ay\", \"gender\": \"female\", \"occupation\": \"politician\"}\n]\n\nsynthetic_comparison_raw = [\n    {\"unique_id\": 2, \"person_id\": 1, \"first_name\": \"sarah\", \"surname\": \"brown\", \"dob\": \"1862-07-11\", \"birth_place\": \"london\", \"postcode_fake\": \"ba12 0ay\", \"gender\": \"female\", \"occupation\": \"politician\"},\n    {\"unique_id\": 3, \"person_id\": 1, \"first_name\": \"sarah\", \"surname\": \"doyle\", \"dob\": \"1862-07-11\", \"birth_place\": \"london\", \"postcode_fake\": \"ba12 0ay\", \"gender\": \"female\", \"occupation\": \"politician\"},\n    {\"unique_id\": 4, \"person_id\": 1, \"first_name\": \"sarah\", \"surname\": \"brown\", \"dob\": \"1862-07-11\", \"birth_place\": \"london\", \"postcode_fake\": \"tf3 2ng\", \"gender\": \"female\", \"occupation\": \"politician\"},\n    {\"unique_id\": 5, \"person_id\": 1, \"first_name\": \"sarah\", \"surname\": \"doyle\", \"dob\": \"1862-07-11\", \"birth_place\": \"london\", \"postcode_fake\": \"tf3 2ng\", \"gender\": \"female\", \"occupation\": \"politician\"},\n    {\"unique_id\": 6, \"person_id\": 2, \"first_name\": \"jane\", \"surname\": \"brown\", \"dob\": \"1860-01-01\", \"birth_place\": \"london\", \"postcode_fake\": \"ba12 0ay\", \"gender\": \"female\", \"occupation\": \"artist\"}\n]\n\nsynthetic_base_df = pd.DataFrame(synthetic_base_raw)\nsynthetic_comparison_df = pd.DataFrame(synthetic_comparison_raw)\n```\n\n----------------------------------------\n\nTITLE: Loading Model Settings and Making Predictions\nDESCRIPTION: Loads pre-saved model settings from a JSON file and generates predictions using the Splink Linker.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/06_Visualising_predictions.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport urllib\n\nurl = \"https://raw.githubusercontent.com/moj-analytical-services/splink/847e32508b1a9cdd7bcd2ca6c0a74e547fb69865/docs/demos/demo_settings/saved_model_from_demo.json\"\n\nwith urllib.request.urlopen(url) as u:\n    settings = json.loads(u.read().decode())\n\n\nlinker = Linker(df, settings, db_api=DuckDBAPI())\ndf_predictions = linker.inference.predict(threshold_match_probability=0.2)\n```\n\n----------------------------------------\n\nTITLE: Generating Match Weights Chart\nDESCRIPTION: Visualizes match weights distribution\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.match_weights_chart()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Splink Linker for Deduplication in Python\nDESCRIPTION: This code sets up a Splink Linker for deduplication using a fake dataset. It defines comparison rules, blocking rules, and initializes the Linker with these settings. The snippet also includes steps for estimating model parameters and preparing for threshold selection.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/threshold_selection_tool_from_labels_table.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_template_library as ctl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\nfrom splink.datasets import splink_dataset_labels\n\ndb_api = DuckDBAPI()\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.JaroWinklerAtThresholds(\"first_name\", [0.9, 0.7]),\n        cl.JaroAtThresholds(\"surname\", [0.9, 0.7]),\n        ctl.DateComparison(\n            \"dob\",\n            input_is_string=True,\n            datetime_metrics=[\"year\", \"month\"],\n            datetime_thresholds=[1, 1],\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        ctl.EmailComparison(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"substr(first_name,1,1)\"),\n        block_on(\"substr(surname, 1,1)\"),\n    ],\n)\n\nlinker = Linker(df, settings, db_api)\n\nlinker.training.estimate_probability_two_random_records_match(\n    [block_on(\"first_name\", \"surname\")], recall=0.7\n)\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nblocking_rule_for_training = block_on(\"first_name\", \"surname\")\n\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\nblocking_rule_for_training = block_on(\"dob\")\nlinker.training.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n\ndf_labels = splink_dataset_labels.fake_1000_labels\nlabels_table = linker.table_management.register_labels_table(df_labels)\n\nchart = linker.evaluation.accuracy_analysis_from_labels_table(\n    labels_table, output_type=\"threshold_selection\", add_metrics=[\"f1\"]\n)\nchart\n```\n\n----------------------------------------\n\nTITLE: Visualizing Record Linkage with Waterfall Chart\nDESCRIPTION: Generates a waterfall chart visualization of linkage results, showing the contribution of each comparison to the overall match weight for the first two records in the dataset.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/real_time_record_linkage.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.waterfall_chart(\n    linker.inference.predict().as_record_dict(limit=2)\n)\n```\n\n----------------------------------------\n\nTITLE: Complete Match Weight Formula with Prior and Features\nDESCRIPTION: This expanded formula shows the complete calculation of match weight, incorporating the prior probability and the sum of individual feature weights.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/theory/fellegi_sunter.md#2025-04-16_snippet_3\n\nLANGUAGE: latex\nCODE:\n```\n\\begin{equation}\n\\begin{aligned}\n    M_\\textsf{obs} &= \\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right) + \\sum_{i}^\\textsf{features}\\log_2(\\frac{m_i}{u_i}) \\\\[10pt]\n    &= \\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right) + \\log_2\\left(\\prod_i^\\textsf{features}\\frac{m_i}{u_i}\\right)\n\\end{aligned}\n\\end{equation}\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink Linker with DuckDB Backend in Python\nDESCRIPTION: This code snippet demonstrates how to initialize a Splink Linker object using the DuckDB backend. It imports the necessary classes and creates a Linker instance with a dataframe, settings, and the DuckDBAPI.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/backends/backends.md#2025-04-16_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom splink import Linker, DuckDBAPI\n\nlinker = Linker(df, settings, db_api=DuckDBAPI(...))\n```\n\n----------------------------------------\n\nTITLE: Parameter Estimation Using Expectation Maximisation in Python\nDESCRIPTION: Runs Expectation Maximisation (EM) algorithm to estimate model parameters for different blocking rules. Each session focuses on a different blocking field (dob, email, first_name) to ensure comprehensive parameter estimation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/link_only.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsession_dob = linker.training.estimate_parameters_using_expectation_maximisation(block_on(\"dob\"))\nsession_email = linker.training.estimate_parameters_using_expectation_maximisation(\n    block_on(\"email\")\n)\nsession_first_name = linker.training.estimate_parameters_using_expectation_maximisation(\n    block_on(\"first_name\")\n)\n```\n\n----------------------------------------\n\nTITLE: Estimating U Probabilities in Splink using Random Sampling in Python\nDESCRIPTION: This code shows how to train u probabilities using random sampling in Splink. The max_pairs parameter controls the maximum number of record pairs to sample for estimation, with a value of 10 million used in this example.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/training/training_rationale.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e7)\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink Linker with Spark Backend in Python\nDESCRIPTION: This code snippet shows how to initialize a Splink Linker object using the Spark backend. It imports the required classes and creates a Linker instance with a dataframe, settings, and the SparkAPI.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/backends/backends.md#2025-04-16_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom splink import Linker, SparkAPI\n\nlinker = Linker(df, settings, db_api=SparkAPI(...))\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Session with Splink Custom Similarity Functions\nDESCRIPTION: Creates and configures a Spark session with appropriate memory settings and parallelism for Splink. Adds the custom similarity functions jar that comes bundled with Splink for enhanced matching capabilities.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/spark/deduplicate_1k_synthetic.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nfrom splink.backends.spark import similarity_jar_location\n\nconf = SparkConf()\n# This parallelism setting is only suitable for a small toy example\nconf.set(\"spark.driver.memory\", \"12g\")\nconf.set(\"spark.default.parallelism\", \"8\")\nconf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n\n\n# Add custom similarity functions, which are bundled with Splink\n# documented here: https://github.com/moj-analytical-services/splink_scalaudfs\npath = similarity_jar_location()\nconf.set(\"spark.jars\", path)\n\nsc = SparkContext.getOrCreate(conf=conf)\n\nspark = SparkSession(sc)\nspark.sparkContext.setCheckpointDir(\"./tmp_checkpoints\")\n```\n\n----------------------------------------\n\nTITLE: Column Profile Analysis\nDESCRIPTION: Demonstrates how to analyze column value distributions using Splink's profile_columns function.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/03_Blocking.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.exploratory import profile_columns\n\nprofile_columns(df, column_expressions=[\"city || left(first_name,1)\"], db_api=db_api)\n```\n\n----------------------------------------\n\nTITLE: Basic Column Profiling with Splink\nDESCRIPTION: Demonstrates basic usage of profile_columns to analyze distributions in a subset of columns from a historical dataset. Uses DuckDBAPI and limits results to top/bottom 5 values.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/profile_columns.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import splink_datasets, DuckDBAPI\nfrom splink.exploratory import profile_columns\n\ndf = splink_datasets.historical_50k\ndf = df[[\"unique_id\", \"full_name\", \"dob\", \"birth_place\"]]\nchart = profile_columns(df, db_api=DuckDBAPI(), top_n=5, bottom_n=5)\nchart\n```\n\n----------------------------------------\n\nTITLE: Tokenizing Company Names with SQL in DuckDB\nDESCRIPTION: Splits company names into individual tokens using regular expressions and creates a table with each token and its position within the original company name to preserve order information.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Tokenize company names, and explode tokens across rows\n# The tokenisation approach here is simplistic, more advanced strategies could be used\n# in a more fully-fledged application\n\n\nsql = \"\"\"\nCREATE OR REPLACE TABLE unnested AS\nSELECT\n    unique_id,\n    unnest(regexp_split_to_array(upper(trim(company_name)), '\\\\s+')) as name_token,\n    generate_subscripts(regexp_split_to_array(upper(trim(company_name)), '\\\\s+'), 1) as token_position_in_name\nFROM all_input_data\n\"\"\"\n\nunnested = duckdb.execute(sql)\nduckdb.table(\"unnested\").show(max_rows=10, max_width=100000)\n```\n\n----------------------------------------\n\nTITLE: Accessing EM Training Session Convergence Charts in Splink\nDESCRIPTION: This snippet shows how to create a training session using expectation maximisation in Splink and then access the interactive history chart to visualize convergence of match weights during training. It demonstrates blocking on first_name and surname fields.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/model.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntraining_session = linker.training.estimate_parameters_using_expectation_maximisation(\n    block_on(\"first_name\", \"surname\")\n)\ntraining_session.match_weights_interactive_history_chart()\n```\n\n----------------------------------------\n\nTITLE: Adding Chart Method to Splink Linker Class\nDESCRIPTION: Implementation of a chart generation method in the Splink Linker class that processes and formats data for visualization.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom .charts import my_new_chart\n\n...\n\nclass Linker:\n\n    ...\n\n    def my_new_chart(self):\n        \n        # Take linker object and extract complete settings dict\n        records = self._settings_obj._parameters_as_detailed_records\n\n        cols_to_keep = [\n            \"comparison_name\",\n            \"sql_condition\",\n            \"label_for_charts\",\n            \"m_probability\",\n            \"u_probability\",\n            \"bayes_factor\",\n            \"log2_bayes_factor\",\n            \"comparison_vector_value\"\n        ]\n\n        # Keep useful information for a match weights chart\n        records = [{k: r[k] for k in cols_to_keep}\n                   for r in records \n                   if r[\"comparison_vector_value\"] != -1 and r[\"comparison_sort_order\"] != -1]\n\n        return my_new_chart(records)\n```\n\n----------------------------------------\n\nTITLE: Advanced Substring Blocking in Python\nDESCRIPTION: Shows how to create a more complex blocking rule using substring matching on first name initial and full surname.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/03_Blocking.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import block_on\nblock_on(\"substr(first_name, 1, 2)\", \"surname\")\n```\n\n----------------------------------------\n\nTITLE: Adding Geographical Coordinates to Postcodes with DuckDB\nDESCRIPTION: Shows how to enrich a dataset with latitude and longitude coordinates by joining with the ONS Postcode Directory. This enables distance-based postcode comparisons in Splink.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/data_preparation/feature_engineering.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\n\nfrom splink import splink_datasets\n\ndf = splink_datasets.historical_50k\n\ndf_with_pc = \"\"\"\nWITH postcode_lookup AS (\n    SELECT\n        pcd AS postcode,\n        lat,\n        long\n    FROM\n        read_csv_auto('./path/to/ONSPD_FEB_2023_UK.csv')\n)\nSELECT\n    df.*,\n    postcode_lookup.lat,\n    postcode_lookup.long\nFROM\n    df\nLEFT JOIN\n    postcode_lookup\nON\n    upper(df.postcode_fake) = postcode_lookup.postcode\n\"\"\"\n\ndf_with_postcode = duckdb.sql(df_with_pc)\n```\n\n----------------------------------------\n\nTITLE: Configuring Splink Settings in Python\nDESCRIPTION: Sets up the Splink linker settings including link type, blocking rules, and comparisons. It uses various comparison methods from the Splink comparison library.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/accuracy_analysis_from_labels_column.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import SettingsCreator, Linker, block_on, DuckDBAPI\n\nimport splink.comparison_library as cl\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n        block_on(\"dob\"),\n        block_on(\"email\"),\n    ],\n    comparisons=[\n        cl.ForenameSurnameComparison(\"first_name\", \"surname\"),\n        cl.DateOfBirthComparison(\n            \"dob\",\n            input_is_string=True,\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        cl.EmailComparison(\"email\"),\n    ],\n    retain_intermediate_calculation_columns=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Highlighting Match Probabilities in Python\nDESCRIPTION: Defines a function to highlight cells based on match probability thresholds for visualizing linkage results.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/bias_eval.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef highlight_cells(val):\n    if val >= 0.999:\n        color = '#c4f5bf'  # High threshold, green\n    elif val >= 0.99:\n        color = '#faf9c0'  # Medium threshold, yellow\n    elif val >= 0.95:\n        color = '#f5e1bf'  # Low threshold, orange\n    else:\n        color = '#f5c8bf'  # Below threshold, red\n    return f'background-color: {color}'\n```\n\n----------------------------------------\n\nTITLE: Implementing Salting Blocking Rules with Splink and Apache Spark in Python\nDESCRIPTION: This code snippet demonstrates how to set up a Splink Linker with salting blocking rules using Apache Spark. It includes Spark configuration, Splink settings with salted blocking rules, and the initialization of the Linker object.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/performance/salting.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom pyspark.context import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nimport splink.comparison_library as cl\nfrom splink import Linker, SparkAPI, splink_datasets\n\nconf = SparkConf()\nconf.set(\"spark.driver.memory\", \"12g\")\nconf.set(\"spark.sql.shuffle.partitions\", \"8\")\nconf.set(\"spark.default.parallelism\", \"8\")\n\nsc = SparkContext.getOrCreate(conf=conf)\nspark = SparkSession(sc)\nspark.sparkContext.setCheckpointDir(\"./tmp_checkpoints\")\n\nsettings = {\n    \"probability_two_random_records_match\": 0.01,\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n        \"l.dob = r.dob\",\n        {\"blocking_rule\": \"l.first_name = r.first_name\", \"salting_partitions\": 4},\n    ],\n    \"comparisons\": [\n        cl.LevenshteinAtThresholds(\"first_name\", 2),\n        cl.ExactMatch(\"surname\"),\n        cl.ExactMatch(\"dob\"),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        cl.ExactMatch(\"email\"),\n    ],\n    \"retain_matching_columns\": True,\n    \"retain_intermediate_calculation_columns\": True,\n    \"additional_columns_to_retain\": [\"cluster\"],\n    \"max_iterations\": 1,\n    \"em_convergence\": 0.01,\n}\n\n\ndf = splink_datasets.fake_1000\n\nspark_api = SparkAPI(spark_session=spark)\nlinker = Linker(df, settings, db_api=spark_api)\nlogging.getLogger(\"splink\").setLevel(5)\n\nlinker.inference.deterministic_link()\n```\n\n----------------------------------------\n\nTITLE: Creating Levenshtein Comparison using Comparison Level Library in Python\nDESCRIPTION: Creates a custom email comparison with explicit comparison levels, including null handling, Levenshtein distance thresholds at 2 and 4, and a catch-all else level. This approach offers more granular control over comparison behavior.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_level_library as cll\n\nemail_comparison = cl.CustomComparison(\n    output_column_name=\"email\",\n    comparison_description=\"Exact match vs. Email within levenshtein thresholds 2, 4 vs. anything else\",\n    comparison_levels=[\n        cll.NullLevel(\"email\"),\n        cll.LevenshteinLevel(\"email\", distance_threshold=2),\n        cll.LevenshteinLevel(\"email\", distance_threshold=4),\n        cll.ElseLevel(),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Fixing m or u Probabilities During Training in Splink\nDESCRIPTION: This example shows how to fix m or u probabilities during training in Splink. It uses a CustomComparison with fixed probabilities for the ExactMatchLevel of the first_name comparison.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/cookbook.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_level_library as cll\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\n\ndb_api = DuckDBAPI()\n\nfirst_name_comparison = cl.CustomComparison(\n    comparison_levels=[\n        cll.NullLevel(\"first_name\"),\n        cll.ExactMatchLevel(\"first_name\").configure(\n            m_probability=0.9999,\n            fix_m_probability=True,\n            u_probability=0.7,\n            fix_u_probability=True,\n        ),\n        cll.ElseLevel(),\n    ]\n)\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        first_name_comparison,\n        cl.ExactMatch(\"surname\"),\n        cl.ExactMatch(\"dob\"),\n        cl.ExactMatch(\"city\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"dob\"),\n    ],\n    additional_columns_to_retain=[\"cluster\"],\n)\n\ndf = splink_datasets.fake_1000\nlinker = Linker(df, settings, db_api, set_up_basic_logging=False)\n\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\nlinker.training.estimate_parameters_using_expectation_maximisation(block_on(\"dob\"))\n\nlinker.visualisations.m_u_parameters_chart()\n```\n\n----------------------------------------\n\nTITLE: Generating Truth Table\nDESCRIPTION: Creates detailed table of accuracy metrics used for ROC and precision-recall curves.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/07_Evaluation.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nroc_table = linker.evaluation.accuracy_analysis_from_labels_table(\n    labels_table, output_type=\"table\"\n)\nroc_table.as_pandas_dataframe(limit=5)\n```\n\n----------------------------------------\n\nTITLE: Estimating U Parameters with Random Sampling\nDESCRIPTION: Estimates the u parameters (probability of agreement given non-match) using random sampling with a maximum of 2 million pairs to improve model accuracy.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/quick_and_dirty_persons.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_u_using_random_sampling(max_pairs=2e6)\n```\n\n----------------------------------------\n\nTITLE: Chart Generation Function Reference\nDESCRIPTION: Reference to the Python function that combines the chart template with prepared data to generate the final visualization.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/understanding_and_editing_charts.md#2025-04-16_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nwaterfall_chart\n```\n\n----------------------------------------\n\nTITLE: Basic Training Block Implementation in Splink\nDESCRIPTION: Demonstrates how to implement a simple blocking rule for model training that blocks on the 'first_name' column, creating a set of record pairs for the EM algorithm to use in parameter estimation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/blocking/model_training.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.duckdb.blocking_rule_library import block_on\n\nblocking_rule_for_training = block_on(\"first_name\")\nlinker.estimate_parameters_using_expectation_maximisation(\n    blocking_rule_for_training\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Data Completeness with Splink\nDESCRIPTION: Creates a chart to visualize the completeness (absence of null values) of each column in the dataset. This helps identify which columns might be less useful for linking due to missing values.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/02_Exploratory_analysis.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.exploratory import completeness_chart\nfrom splink import DuckDBAPI\ndb_api = DuckDBAPI()\ncompleteness_chart(df, db_api=db_api)\n```\n\n----------------------------------------\n\nTITLE: Getting Raw Phonetic Matching Data with phonetic_transform_df\nDESCRIPTION: Demonstrates how to get the underlying data frame of phonetic transformations instead of the visualization, providing the raw transformation results for all phonetic algorithms.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsa.phonetic_transform_df(data, \"string1\", \"string2\")\n```\n\n----------------------------------------\n\nTITLE: Multiple Blocking Rules Implementation\nDESCRIPTION: Shows how to implement multiple blocking rules in Splink, combining name-based and postcode-based blocking. This approach provides better coverage for catching potential matches even when some fields contain errors.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/blocking/blocking_rules.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nSettingsCreator(\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\", \"surname\"),\n        block_on(\"postcode\")\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Synthetic Dataset into Spark DataFrame\nDESCRIPTION: Loads a sample synthetic dataset with 1000 records from Splink's built-in datasets and converts it to a Spark DataFrame.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/spark/deduplicate_1k_synthetic.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import splink_datasets\n\npandas_df = splink_datasets.fake_1000\n\ndf = spark.createDataFrame(pandas_df)\n```\n\n----------------------------------------\n\nTITLE: Complex SQL-Based Blocking Rule\nDESCRIPTION: Example of a more complex blocking rule using SQL-style fuzzy matching with Levenshtein distance. Note that while possible, such rules may not execute efficiently.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/blocking/blocking_rules.md#2025-04-16_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nl.first_name and r.first_name and levenshtein(l.surname, r.surname) < 3\n```\n\n----------------------------------------\n\nTITLE: Analyzing Blocking Rules for Deterministic Linkage (Python)\nDESCRIPTION: This snippet demonstrates how to analyze the number of record comparisons generated by deterministic blocking rules using Splink's blocking analysis functionality.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/deterministic_dedupe.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import DuckDBAPI, block_on\nfrom splink.blocking_analysis import (\n    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,\n)\n\ndb_api = DuckDBAPI()\ncumulative_comparisons_to_be_scored_from_blocking_rules_chart(\n    table_or_tables=df,\n    blocking_rules=[\n        block_on(\"first_name\", \"surname\", \"dob\"),\n        block_on(\"surname\", \"dob\", \"postcode_fake\"),\n        block_on(\"first_name\", \"dob\", \"occupation\"),\n    ],\n    db_api=db_api,\n    link_type=\"dedupe_only\",\n)\n```\n\n----------------------------------------\n\nTITLE: Clustering Predictions\nDESCRIPTION: Clusters pairwise predictions using probability threshold\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclusters = linker.clustering.cluster_pairwise_predictions_at_threshold(\n    df_predict, threshold_match_probability=0.95\n)\n```\n\n----------------------------------------\n\nTITLE: Building Cluster Studio Dashboard\nDESCRIPTION: Creates an interactive dashboard for visualizing clustering results of predictions, useful for identifying potential false positives and negatives.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/06_Visualising_predictions.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf_clusters = linker.clustering.cluster_pairwise_predictions_at_threshold(\n    df_predictions, threshold_match_probability=0.5\n)\n\nlinker.visualisations.cluster_studio_dashboard(\n    df_predictions,\n    df_clusters,\n    \"cluster_studio.html\",\n    sampling_method=\"by_cluster_size\",\n    overwrite=True,\n)\n\n# You can view the scv.html file in your browser, or inline in a notbook as follows\nfrom IPython.display import IFrame\n\nIFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1000)\n```\n\n----------------------------------------\n\nTITLE: Getting Raw Comparator Score Data with comparator_score_df\nDESCRIPTION: Demonstrates how to get the underlying data frame of comparator scores instead of the visualization, providing the raw numerical values for all comparators across the test data.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsa.comparator_score_df(data, \"string1\", \"string2\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Levenshtein City Comparison\nDESCRIPTION: Creates a comparison object for city names using Levenshtein distance with a threshold of 2.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\ncity_comparison = cl.LevenshteinAtThresholds(\"city\", 2)\nprint(city_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Defining Deterministic Linkage Rules in Python\nDESCRIPTION: This code snippet demonstrates the logic for deterministic linkage rules. It checks for matches based on name, date of birth, and postcode combinations.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/theory/probabilistic_vs_deterministic.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nIF Name matches AND DOB matches (Rule 1)\nTHEN records are a match\n\nELSE\n\nIF Forename matches AND DOB matches AND Postcode match (Rule 2)\nTHEN records are a match\n\nELSE\n\nrecords do not match\n```\n\n----------------------------------------\n\nTITLE: Creating Levenshtein Comparison using Settings Dictionary in Python\nDESCRIPTION: Defines a Levenshtein comparison for email matching using a raw Python dictionary structure. This approach provides the most explicit control over the comparison configuration and is directly compatible with Splink's settings dictionary.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nemail_comparison = {\n    'output_column_name': 'email',\n    'comparison_levels': [{'sql_condition': '\"email_l\" IS NULL OR \"email_r\" IS NULL',\n    'label_for_charts': 'Null',\n    'is_null_level': True},\n    {\n        'sql_condition': '\"email_l\" = \"email_r\"',\n        'label_for_charts': 'Exact match'\n    },\n    {\n        'sql_condition': 'levenshtein(\"email_l\", \"email_r\") <= 2',\n        'label_for_charts': 'Levenshtein <= 2'\n    },\n    {\n        'sql_condition': 'levenshtein(\"email_l\", \"email_r\") <= 4',\n        'label_for_charts': 'Levenshtein <= 4'\n    },\n    {\n        'sql_condition': 'ELSE', \n        'label_for_charts': 'All other comparisons'\n    }],\n    'comparison_description': 'Exact match vs. Email within levenshtein thresholds 2, 4 vs. anything else'}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Comparison for Postcode Area in Splink (Python)\nDESCRIPTION: This code configures a custom comparison for postcode areas in Splink. It uses CustomComparison with NullLevel, ExactMatchLevel, and ElseLevel to define comparison levels for postcodes based on their area codes.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/regular_expressions.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.comparison_library import CustomComparison\n\ncc = CustomComparison(\n    output_column_name=\"postcode\",\n    comparison_levels=[\n        cll.NullLevel(\"postcode\"),\n        cll.ExactMatchLevel(pc_ce),\n        cll.ElseLevel()\n    ]\n\n)\nprint(cc.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Generating Predictions\nDESCRIPTION: Performs prediction and converts results to pandas DataFrame\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf_predict = linker.inference.predict()\ndf_e = df_predict.as_pandas_dataframe(limit=5)\ndf_e\n```\n\n----------------------------------------\n\nTITLE: Accessing Graph Metrics Results in Python\nDESCRIPTION: This code demonstrates how to access the individual Splink dataframes containing node, edge, and cluster metrics after calling compute_graph_metrics(). It converts the results to pandas dataframes for further analysis.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/clusters/how_to_compute_metrics.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ngraph_metrics = linker.clustering.compute_graph_metrics(\n    pairwise_predictions, clusters\n)\n\ndf_edges = graph_metrics.edges.as_pandas_dataframe()\ndf_nodes = graph_metrics.nodes.as_pandas_dataframe()\ndf_clusters = graph_metrics.clusters.as_pandas_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Creating Blocking Rules with Splink's Blocking Rule Library\nDESCRIPTION: Shows how to use Splink's Blocking Rule Library to create blocking rules for date of birth fields. This simplifies the process of defining blocking rules for record linkage.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2023-07-27-feature_update.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport splink.duckdb.blocking_rule_library as brl\n\nbrl.exact_match_rule(\"date_of_birth\")\n```\n\n----------------------------------------\n\nTITLE: Estimating Match Probabilities\nDESCRIPTION: Estimates probability of random record matches using blocking rules\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_probability_two_random_records_match(\n    [\n        block_on(\"first_name\", \"surname\", \"dob\"),\n        block_on(\"substr(first_name,1,2)\", \"surname\", \"substr(postcode_fake, 1,2)\"),\n        block_on(\"dob\", \"postcode_fake\"),\n    ],\n    recall=0.6,\n)\n```\n\n----------------------------------------\n\nTITLE: Estimating Probability Two Random Records Match in Splink using Python\nDESCRIPTION: This code demonstrates how to estimate the probability_two_random_records_match parameter in Splink using deterministic matching rules. It specifies rules for blocking on exact matches of first_name, surname, and dob, or email, with an estimated recall of 70%.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/training/training_rationale.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndeterministic_rules = [\n    block_on(\"first_name\", \"surname\", \"dob\"),\n    block_on(\"email\")\"\n]\n\nlinker.training.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\n```\n\n----------------------------------------\n\nTITLE: Setting Spark Parallelism and Creating SparkSession in Python\nDESCRIPTION: This snippet demonstrates how to set Spark parallelism by configuring spark.default.parallelism and spark.sql.shuffle.partitions. It then creates a SparkContext and SparkSession with these configurations.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/performance/optimising_spark.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.context import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf()\n\nconf.set(\"spark.default.parallelism\", \"50\")\nconf.set(\"spark.sql.shuffle.partitions\", \"50\")\n\nsc = SparkContext.getOrCreate(conf=conf)\nspark = SparkSession(sc)\n```\n\n----------------------------------------\n\nTITLE: Using ExactMatch Comparison with DuckDB Backend\nDESCRIPTION: Creates an ExactMatch comparison for the 'first_name' field and displays the human-readable description of the generated comparison using the DuckDB backend.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\nfirst_name_comparison = cl.ExactMatch(\"first_name\")\nprint(first_name_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Creating Full Name Column for Splink Comparison\nDESCRIPTION: This snippet shows how to create a 'full_name' column by concatenating 'first_name' and 'surname'. This is useful for more accurate term frequency calculations in Splink comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/data_preparation/feature_engineering.md#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfrom splink import splink_datasets\n\ndf = splink_datasets.fake_1000\n\ndf['full_name'] = df['first_name'] + ' ' + df['surname']\n\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink Linker with PostgreSQL Backend in Python\nDESCRIPTION: This code snippet shows how to initialize a Splink Linker object using the PostgreSQL backend. It imports the necessary classes and creates a Linker instance with a dataframe, settings, and the PostgresAPI.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/backends/backends.md#2025-04-16_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom splink import Linker, PostgresAPI\n\nlinker = Linker(df, settings, db_api=PostgresAPI(...))\n```\n\n----------------------------------------\n\nTITLE: Creating Term-Frequency Adjustments with Detailed Dictionary Specification in Python\nDESCRIPTION: This snippet shows how to provide a detailed dictionary specification for term-frequency adjustments in Splink. It includes advanced options such as tf_adjustment_weight and tf_minimum_u_value for fine-tuning the adjustment behavior.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/term-frequency.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncomparison_first_name = {\n    \"output_column_name\": \"first_name\",\n    \"comparison_description\": \"First name jaro dmeta\",\n    \"comparison_levels\": [\n        {\n            \"sql_condition\": \"first_name_l IS NULL OR first_name_r IS NULL\",\n            \"label_for_charts\": \"Null\",\n            \"is_null_level\": True,\n        },\n        {\n            \"sql_condition\": \"first_name_l = first_name_r\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"first_name\",\n            \"tf_adjustment_weight\": 1.0,\n            \"tf_minimum_u_value\": 0.001,\n        },\n        {\n            \"sql_condition\": \"jaro_winkler_sim(first_name_l, first_name_r) > 0.8\",\n            \"label_for_charts\": \"Exact match\",\n            \"tf_adjustment_column\": \"first_name\",\n            \"tf_adjustment_weight\": 0.5,\n            \"tf_minimum_u_value\": 0.001,\n        },\n        {\"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\"},\n    ],\n}\n```\n\n----------------------------------------\n\nTITLE: Training Model Parameters on Date of Birth\nDESCRIPTION: Runs another Expectation-Maximization algorithm to estimate model parameters, this time using records that share the same date of birth. Term frequency adjustments are also disabled for this training session.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/demo_settings/model_create_h50k.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntraining_blocking_rule = block_on(\"dob\")\ntraining_session_dob = (\n    linker.training.estimate_parameters_using_expectation_maximisation(\n        training_blocking_rule, estimate_without_term_frequencies=True\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Trained Model to JSON\nDESCRIPTION: Saves the fully trained Splink model to a JSON file named 'model_h50k.json', allowing for later reuse without retraining. The overwrite parameter is set to true to replace any existing file.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/demo_settings/model_create_h50k.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlinker.misc.save_model_to_json(\"model_h50k.json\", overwrite=True)\n```\n\n----------------------------------------\n\nTITLE: Generating a Completeness Chart in Splink with DuckDB\nDESCRIPTION: This code snippet demonstrates how to create a completeness chart to analyze the proportion of populated values across columns in multiple datasets. It uses a fake dataset that is split into two parts and then visualizes completeness using the DuckDB API.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/completeness_chart.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import splink_datasets, DuckDBAPI\nfrom splink.exploratory import completeness_chart\n\ndf = splink_datasets.fake_1000\n\n# Split a simple dataset into two, separate datasets which can be linked together.\ndf_l = df.sample(frac=0.5)\ndf_r = df.drop(df_l.index)\n\n\nchart = completeness_chart([df_l, df_r], db_api=DuckDBAPI())\nchart\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Splink Model from JSON\nDESCRIPTION: Python code demonstrating how to initialize a Linker object with a pre-trained model loaded from a JSON file. The code shows using the settings parameter to specify the model file path.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/settings.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlinker = Linker(\n    new_df,\n    settings=\"./path/to/model.json\",\n    db_api=db_api\n)\n```\n\n----------------------------------------\n\nTITLE: Blocking Rule Implementation Using block_on - Python\nDESCRIPTION: Demonstrates the new simplified blocking rule syntax using block_on function to create blocks based on multiple columns like first_name and surname.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2023-12-06-feature_update.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.duckdb.blocking_rule_library import block_on\nblock_on([\"first_name\", \"surname\"])\n```\n\n----------------------------------------\n\nTITLE: Spark SQL DataFrame Creation Example\nDESCRIPTION: Demonstrates the lazy evaluation nature of Spark transformations where DataFrame creation does not immediately compute results.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/spark_pipelining_and_caching.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.sql(sql)\n```\n\n----------------------------------------\n\nTITLE: Using Regular Expressions in Splink Comparisons for Postcodes\nDESCRIPTION: Shows how to use the regex_extract parameter in Splink comparisons to compare only specific parts of strings, such as the outcode portion of UK postcodes.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2023-07-27-feature_update.md#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport splink.duckdb.duckdb_comparison_library as cl\n\npc_comparison = cl.exact_match(\"postcode\", regex_extract=\"^[A-Z]{1,2}\")\n```\n\n----------------------------------------\n\nTITLE: Testing Metaphone Phonetic Transformation with phonetics Package\nDESCRIPTION: This example shows how to use the Metaphone algorithm from the phonetics package to compare two similar-sounding names. The example demonstrates that 'Smith' and 'Smyth' produce identical Metaphone codes (SM0), confirming their phonetic similarity.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/phonetic.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport phonetics\nprint(phonetics.metaphone(\"Smith\"), phonetics.metaphone(\"Smyth\"))\n```\n\n----------------------------------------\n\nTITLE: Detecting Unlinkable Records in Splink\nDESCRIPTION: Generates a chart to identify and analyze records that don't contain enough information to be reliably linked. Unlinkable records are identified by comparing records to themselves and checking if they meet the match threshold.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nlinker.evaluation.unlinkables_chart()\n```\n\n----------------------------------------\n\nTITLE: Applying Double Metaphone Encoding to Name Columns in Python\nDESCRIPTION: This snippet demonstrates how to apply the Double Metaphone phonetic algorithm to 'first_name' and 'surname' columns in a DataFrame. It uses the apply method to create new columns with the encoded values.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/data_preparation/feature_engineering.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Apply the function to the \"first_name\" and surname columns using the apply method\ndf['first_name_dm'] = df['first_name'].apply(dmetaphone_name)\ndf['surname_dm'] = df['surname'].apply(dmetaphone_name)\n\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Blocking Rule Comparison Count Analysis\nDESCRIPTION: Demonstrates how to count comparisons generated by a specific blocking rule using Splink's analysis tools.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/03_Blocking.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.blocking_analysis import count_comparisons_from_blocking_rule\n\ndb_api = DuckDBAPI()\n\nbr = block_on(\"substr(first_name, 1,1)\", \"surname\")\n\ncounts = count_comparisons_from_blocking_rule(\n    table_or_tables=df,\n    blocking_rule=br,\n    link_type=\"dedupe_only\",\n    db_api=db_api,\n)\n\ncounts\n```\n\n----------------------------------------\n\nTITLE: Analyzing False Negatives\nDESCRIPTION: Generates visualization of false negative prediction errors using waterfall chart.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/07_Evaluation.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsplink_df = linker.evaluation.prediction_errors_from_labels_table(\n    labels_table, include_false_negatives=True, include_false_positives=False\n)\nfalse_negatives = splink_df.as_record_dict(limit=5)\nlinker.visualisations.waterfall_chart(false_negatives)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Different Ordering of Blocking Rules\nDESCRIPTION: This example shows how changing the order of blocking rules affects the number of comparisons. It demonstrates reordering surname and first_name blocking rules to see the difference in comparison counts.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/cumulative_comparisons_to_be_scored_from_blocking_rules_chart.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nblocking_rules_for_analysis = [\n    block_on(\"surname\"),\n    block_on(\"first_name\"),\n    block_on(\"email\"),\n]\n\ncumulative_comparisons_to_be_scored_from_blocking_rules_chart(\n    table_or_tables=df,\n    blocking_rules=blocking_rules_for_analysis,\n    db_api=db_api,\n    link_type=\"dedupe_only\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Rows for Parameter Estimation in Splink\nDESCRIPTION: Example of how to set the target_rows parameter when calling estimate_u_using_random_sampling() to balance processing time with accuracy during model development versus final runs.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/08_building_your_own_model.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nestimate_u_using_random_sampling(target_rows=1e6)  # For iteration/development\nestimate_u_using_random_sampling(target_rows=1e8)  # For final full-dataset run\n```\n\n----------------------------------------\n\nTITLE: Visualizing Match Weights for Transaction Linking\nDESCRIPTION: This code generates a chart to visualize the match weights of different comparison levels in the Splink model for transaction linking.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/transactions.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.match_weights_chart()\n```\n\n----------------------------------------\n\nTITLE: Using ErrorLogger for Multiple Exception Handling in Python\nDESCRIPTION: Demonstrates how to use the ErrorLogger class to collect and raise multiple exceptions simultaneously, providing better error feedback to users.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/settings_validation/extending_settings_validator.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.exceptions import ErrorLogger\n\n# Create an error logger instance\ne = ErrorLogger()\n\n# Log your errors\ne.append(SyntaxError(\"The syntax is wrong\"))\ne.append(NameError(\"Invalid name entered\"))\n\n# Raise your errors\ne.raise_and_log_all_errors()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Parameter Estimate Comparisons\nDESCRIPTION: Generates a chart comparing the different parameter estimates from various estimation methods used in the model.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/pairwise_labels.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.parameter_estimate_comparisons_chart()\n```\n\n----------------------------------------\n\nTITLE: Testing Double Metaphone Phonetic Transformation with phonetics Package\nDESCRIPTION: This code snippet demonstrates the Double Metaphone algorithm from the phonetics package, which returns two phonetic codes for each name: a primary and an alternative pronunciation. The example shows that 'Smith' and 'Smyth' return identical Double Metaphone codes ('SM0', 'XMT').\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/phonetic.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport phonetics\nprint(phonetics.dmetaphone(\"Smith\"), phonetics.dmetaphone(\"Smyth\"))\n```\n\n----------------------------------------\n\nTITLE: Counting Comparisons from a Blocking Rule in Splink\nDESCRIPTION: Demonstrates how to check the number of record pairs that would be generated by a specific blocking rule before running a potentially expensive training session, helping to optimize blocking rule selection.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/blocking/model_training.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.duckdb.blocking_rule_library import block_on\nblocking_rule = block_on([\"first_name\", \"surname\"])\nlinker.count_num_comparisons_from_blocking_rule(blocking_rule)\n```\n\n----------------------------------------\n\nTITLE: Using Levenshtein Distance with DuckDB in Python\nDESCRIPTION: Demonstrates how to calculate the Levenshtein distance between two strings using DuckDB in Python. The Levenshtein distance measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into another.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/comparators.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nduckdb.sql(\"SELECT levenshtein('CAKE', 'ACKE')\").df().iloc[0,0]\n```\n\n----------------------------------------\n\nTITLE: Reducing Comparisons with More Restrictive Blocking Rules\nDESCRIPTION: This example demonstrates how to optimize blocking rules by adding additional conditions to reduce the number of comparisons, combining first_name with dob to make the blocking rule more restrictive.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/cumulative_comparisons_to_be_scored_from_blocking_rules_chart.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nblocking_rules_for_analysis = [\n    block_on(\"first_name\", \"dob\"),\n    block_on(\"surname\"),\n    block_on(\"email\"),\n]\n\n\ncumulative_comparisons_to_be_scored_from_blocking_rules_chart(\n    table_or_tables=df,\n    blocking_rules=blocking_rules_for_analysis,\n    db_api=db_api,\n    link_type=\"dedupe_only\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating an ExactMatch Comparison with Term-Frequency Adjustments using a Dictionary\nDESCRIPTION: Shows how to define an exact match comparison with term-frequency adjustments by directly creating a dictionary that follows the Splink JSON schema.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfirst_name_comparison = {\n    'output_column_name': 'first_name',\n    'comparison_levels': [\n        {\n            'sql_condition': '\"first_name_l\" IS NULL OR \"first_name_r\" IS NULL',\n            'label_for_charts': 'Null',\n            'is_null_level': True\n        },\n        {\n            'sql_condition': '\"first_name_l\" = \"first_name_r\"',\n            'label_for_charts': 'Exact match',\n            'tf_adjustment_column': 'first_name',\n            'tf_adjustment_weight': 1.0\n        },\n        {\n            'sql_condition': 'ELSE', \n            'label_for_charts': 'All other comparisons'\n        }],\n    'comparison_description': 'Exact match vs. anything else'\n}\n```\n\n----------------------------------------\n\nTITLE: Profiling Data Columns\nDESCRIPTION: Analyzes column profiles using Splink's exploratory functions\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.exploratory import profile_columns\n\nprofile_columns(df, db_api, column_expressions=[\"first_name\", \"substr(surname,1,2)\"])\n```\n\n----------------------------------------\n\nTITLE: Performing Record Comparisons with Splink in Python\nDESCRIPTION: Defines a function to compare base records against comparison records using the Splink linker, and generates comparisons for the synthetic data.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/bias_eval.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef compare_records(base_records, comparison_records, linker):\n    results = []\n    for record_1 in base_records:\n        results.extend(\n            linker.inference.compare_two_records(record_1, record_2).as_pandas_dataframe()\n            for record_2 in comparison_records\n        )\n    all_comparisons_df = pd.concat(results, ignore_index=True)\n    return all_comparisons_df\n\ncomparisons = compare_records(synthetic_base_raw, synthetic_comparison_raw, linker)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Match Quality with Histogram using Altair\nDESCRIPTION: Creates a histogram using Altair to visualize the distribution of match weights, including unmatched records. This helps assess the overall quality of the matching process and identify potential issues.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport altair as alt\n# Plot histogram of match weights\n\n# This list does not include any stockport records where no comparison pairs were made\n# because blocking rules did not find any potential matches\n\n# Add in unmatched records\nsql = \"\"\"\nWITH matched_stockport_ids as (\n    select distinct unique_id_l\n    from ranked_matches\n)\nselect\n    -10 as match_weight,\n    0.0 as match_probability,\nfrom data_for_matching t\nwhere t.source_dataset = 'stockport'\nand t.unique_id not in (select unique_id_l from matched_stockport_ids)\n\"\"\"\n\nunmatched_records = duckdb.sql(sql)\nunmatched_records.show(max_rows=10, max_width=400)\n\n\nsql = \"\"\"\nselect match_probability, match_weight from ranked_matches\nunion all\nselect match_probability, match_weight from unmatched_records\norder by match_probability desc\n\"\"\"\n\nall_records = con.sql(sql)\n\n# Create the histogram\nchart = (\n    alt.Chart(all_records.df())\n    .mark_bar()\n    .encode(\n        alt.X(\"match_weight:Q\", bin=alt.Bin(maxbins=50), title=\"Match Weight\"),\n        alt.Y(\"count():Q\", title=\"Count\"),\n    )\n    .properties(title=\"Distribution of Match Probabilities\", width=600, height=400)\n)\n\n# Save the chart\nchart\n```\n\n----------------------------------------\n\nTITLE: Saving a Trained Splink Model to JSON\nDESCRIPTION: Saves the trained model including all estimated parameters to a JSON file for later use. This allows the model to be reused without having to retrain it each time.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nsettings = linker.misc.save_model_to_json(\n    \"../demo_settings/saved_model_from_demo.json\", overwrite=True\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink Linker with Athena Backend in Python\nDESCRIPTION: This code snippet illustrates how to initialize a Splink Linker object using the Athena backend. It imports the necessary classes and creates a Linker instance with a dataframe, settings, and the AthenaAPI.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/backends/backends.md#2025-04-16_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom splink import Linker, AthenaAPI\n\nlinker = Linker(df, settings, db_api=AthenaAPI(...))\n```\n\n----------------------------------------\n\nTITLE: Implementing Email Comparison\nDESCRIPTION: Creates a specialized comparison object for email addresses using Splink's built-in email comparison function.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nemail_comparison = cl.EmailComparison(\"email\")\nprint(email_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Registering Levenshtein Distance UDF in SQLite\nDESCRIPTION: Example of registering a Levenshtein distance UDF in SQLite using Python. This code imports the distance function from rapidfuzz, creates an in-memory SQLite connection, and registers the Levenshtein function for use in SQL queries.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/udfs.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom rapidfuzz.distance.Levenshtein import distance\nconn = sqlite3.connect(\":memory:\")\nconn.create_function(\"levenshtein\", 2, distance)\n```\n\n----------------------------------------\n\nTITLE: Viewing ForenameSurnameComparison Structure in Splink\nDESCRIPTION: Displays the human-readable description of the combined forename and surname comparison structure for the DuckDB backend, showing how it handles both name fields together.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(full_name_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Column Blocking in Splink\nDESCRIPTION: Example showing how to implement a simple equality-based blocking rule using Splink's block_on function.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/blocking/performance.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import block_on\nblock_on(\"first_name\")\n```\n\n----------------------------------------\n\nTITLE: Initializing PostcodeComparison in Splink\nDESCRIPTION: Creates a basic comparison for postcode fields, which includes appropriate string matching for postal codes.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\npc_comparison = cl.PostcodeComparison(\"postcode\")\n```\n\n----------------------------------------\n\nTITLE: Estimating U Parameters\nDESCRIPTION: Estimates U parameters using random sampling method\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_u_using_random_sampling(max_pairs=5e6)\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink Linker for Transaction Data\nDESCRIPTION: This snippet creates a Splink Linker object using the previously defined settings and the origin and destination transaction datasets. It sets up the environment for performing the linking process.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/transactions.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlinker = Linker(\n    [df_origin, df_destination],\n    settings,\n    input_table_aliases=[\"__ori\", \"_dest\"],\n    db_api=db_api,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Blocking Rules\nDESCRIPTION: Sets up blocking rules to reduce the number of record comparisons. Each rule defines criteria for grouping records that might potentially match, using various combinations of partial and full field matches.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/demo_settings/model_create_h50k.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import DuckDBAPI, block_on\nfrom splink.blocking_analysis import (\n    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,\n)\n\nblocking_rules = [\n    block_on(\"substr(first_name,1,3)\", \"substr(surname,1,4)\"),\n    block_on(\"surname\", \"dob\"),\n    block_on(\"first_name\", \"dob\"),\n    block_on(\"postcode_fake\", \"first_name\"),\n    block_on(\"postcode_fake\", \"surname\"),\n    block_on(\"dob\", \"birth_place\"),\n    block_on(\"substr(postcode_fake,1,3)\", \"dob\"),\n    block_on(\"substr(postcode_fake,1,3)\", \"first_name\"),\n    block_on(\"substr(postcode_fake,1,3)\", \"surname\"),\n    block_on(\"substr(first_name,1,2)\", \"substr(surname,1,2)\", \"substr(dob,1,4)\"),\n]\n```\n\n----------------------------------------\n\nTITLE: Viewing the Dictionary Representation of a Comparison\nDESCRIPTION: Demonstrates how to view the underlying dictionary representation of a comparison that conforms to the JSON specification of a Splink model.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfirst_name_comparison.get_comparison(\"duckdb\").as_dict()\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Linker with In-built Datasets in Python\nDESCRIPTION: Complete example showing how to import a dataset and use it to initialize a Splink Linker for deduplication. The example sets up exact match comparisons for first_name and surname fields.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/datasets.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink splink_datasets, Linker, DuckDBAPI, SettingsCreator\n\ndf = splink_datasets.fake_1000\nlinker = Linker(\n    df,\n    SettingsCreator(\n        link_type=\"dedupe_only\",\n        comparisons=[\n            cl.exact_match(\"first_name\"),\n            cl.exact_match(\"surname\"),\n        ],\n    ),\n    db_api=DuckDBAPI()\n)\n```\n\n----------------------------------------\n\nTITLE: Using DuckDB without Pandas in Splink\nDESCRIPTION: This snippet demonstrates how to read data directly using DuckDB and obtain results in native DuckDB DuckDBPyRelation format. It creates a temporary parquet file, reads it with DuckDB, and uses Splink for deduplication.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/cookbook.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nimport tempfile\nimport os\n\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\n# Create a parquet file on disk to demontrate native DuckDB parquet reading\ndf = splink_datasets.fake_1000\ntemp_file = tempfile.NamedTemporaryFile(delete=True, suffix=\".parquet\")\ntemp_file_path = temp_file.name\ndf.to_parquet(temp_file_path)\n\n# Example would start here if you already had a parquet file\nduckdb_df = duckdb.read_parquet(temp_file_path)\n\ndb_api = DuckDBAPI(\":default:\")\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.NameComparison(\"first_name\"),\n        cl.JaroAtThresholds(\"surname\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\", \"dob\"),\n        block_on(\"surname\"),\n    ],\n)\n\nlinker = Linker(df, settings, db_api, set_up_basic_logging=False)\n\nresult = linker.inference.predict().as_duckdbpyrelation()\n\n# Since result is a DuckDBPyRelation, we can use all the usual DuckDB API\n# functions on it.\n\n# For example, we can use the `sort` function to sort the results,\n# or could use result.to_parquet() to write to a parquet file.\nresult.sort(\"match_weight\")\n```\n\n----------------------------------------\n\nTITLE: Extracting First Letter of Name for Comparison in Splink (Python)\nDESCRIPTION: This snippet shows how to use the substr function to extract the first letter of a name for comparison in Splink. It uses ColumnExpression to select the first character of the 'first_name' column and ExactMatchLevel for comparison.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/regular_expressions.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_level_library as cll\nfrom splink import ColumnExpression\n\ninitial = ColumnExpression(\"first_name\").substr(1,1)\nprint(cll.ExactMatchLevel(initial).get_comparison_level(\"duckdb\").sql_condition)\n```\n\n----------------------------------------\n\nTITLE: Calculating Jaccard Similarity in Python\nDESCRIPTION: This function calculates the Jaccard similarity between two strings. It converts the strings into sets of characters, computes the intersection and union of these sets, and returns the ratio of their lengths. The function is demonstrated with an example comparing 'DUCK' and 'LUCK'.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/comparators.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef jaccard_similarity(str1, str2):\n        set1 = set(str1)\n        set2 = set(str2)\n        return len(set1 & set2) / len(set1 | set2)\n\njaccard_similarity(\"DUCK\", \"LUCK\")\n```\n\n----------------------------------------\n\nTITLE: Example of Saved Splink Model JSON Structure\nDESCRIPTION: This JSON snippet represents the structure of a saved Splink model. It includes various model settings, blocking rules, and detailed comparison configurations for different fields like first name, surname, date of birth, city, and email.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/settings.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"link_type\": \"dedupe_only\",\n    \"probability_two_random_records_match\": 0.0008208208208208208,\n    \"retain_matching_columns\": true,\n    \"retain_intermediate_calculation_columns\": false,\n    \"additional_columns_to_retain\": [],\n    \"sql_dialect\": \"duckdb\",\n    \"linker_uid\": \"29phy7op\",\n    \"em_convergence\": 0.0001,\n    \"max_iterations\": 25,\n    \"bayes_factor_column_prefix\": \"bf_\",\n    \"term_frequency_adjustment_column_prefix\": \"tf_\",\n    \"comparison_vector_value_column_prefix\": \"gamma_\",\n    \"unique_id_column_name\": \"unique_id\",\n    \"source_dataset_column_name\": \"source_dataset\",\n    \"blocking_rules_to_generate_predictions\": [\n        {\n            \"blocking_rule\": \"l.\\\"first_name\\\" = r.\\\"first_name\\\"\",\n            \"sql_dialect\": \"duckdb\"\n        },\n        {\n            \"blocking_rule\": \"l.\\\"surname\\\" = r.\\\"surname\\\"\",\n            \"sql_dialect\": \"duckdb\"\n        }\n    ],\n    \"comparisons\": [\n        {\n            \"output_column_name\": \"first_name\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"first_name_l\\\" IS NULL OR \\\"first_name_r\\\" IS NULL\",\n                    \"label_for_charts\": \"first_name is NULL\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"first_name_l\\\" = \\\"first_name_r\\\"\",\n                    \"label_for_charts\": \"Exact match on first_name\",\n                    \"m_probability\": 0.48854806009621365,\n                    \"u_probability\": 0.0056770619302010565\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"first_name_l\\\", \\\"first_name_r\\\") >= 0.9\",\n                    \"label_for_charts\": \"Jaro-Winkler distance of first_name >= 0.9\",\n                    \"m_probability\": 0.1903763096120358,\n                    \"u_probability\": 0.003424501164330396\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"first_name_l\\\", \\\"first_name_r\\\") >= 0.8\",\n                    \"label_for_charts\": \"Jaro-Winkler distance of first_name >= 0.8\",\n                    \"m_probability\": 0.08609678978546921,\n                    \"u_probability\": 0.006620702251038765\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\",\n                    \"m_probability\": 0.23497884050628137,\n                    \"u_probability\": 0.9842777346544298\n                }\n            ],\n            \"comparison_description\": \"jaro_winkler at thresholds 0.9, 0.8 vs. anything else\"\n        },\n        {\n            \"output_column_name\": \"surname\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"surname_l\\\" IS NULL OR \\\"surname_r\\\" IS NULL\",\n                    \"label_for_charts\": \"surname is NULL\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"surname_l\\\" = \\\"surname_r\\\"\",\n                    \"label_for_charts\": \"Exact match on surname\",\n                    \"m_probability\": 0.43210610613512185,\n                    \"u_probability\": 0.004322481469643699\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"surname_l\\\", \\\"surname_r\\\") >= 0.9\",\n                    \"label_for_charts\": \"Jaro-Winkler distance of surname >= 0.9\",\n                    \"m_probability\": 0.2514700606335103,\n                    \"u_probability\": 0.002907020988387136\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"surname_l\\\", \\\"surname_r\\\") >= 0.8\",\n                    \"label_for_charts\": \"Jaro-Winkler distance of surname >= 0.8\",\n                    \"m_probability\": 0.0757748206402343,\n                    \"u_probability\": 0.0033636211436311888\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\",\n                    \"m_probability\": 0.2406490125911336,\n                    \"u_probability\": 0.989406876398338\n                }\n            ],\n            \"comparison_description\": \"jaro_winkler at thresholds 0.9, 0.8 vs. anything else\"\n        },\n        {\n            \"output_column_name\": \"dob\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"dob_l\\\" IS NULL OR \\\"dob_r\\\" IS NULL\",\n                    \"label_for_charts\": \"dob is NULL\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"dob_l\\\" = \\\"dob_r\\\"\",\n                    \"label_for_charts\": \"Exact match on dob\",\n                    \"m_probability\": 0.39025358731716286,\n                    \"u_probability\": 0.0016036280808555408\n                },\n                {\n                    \"sql_condition\": \"damerau_levenshtein(\\\"dob_l\\\", \\\"dob_r\\\") <= 1\",\n                    \"label_for_charts\": \"Damerau-Levenshtein distance of dob <= 1\",\n                    \"m_probability\": 0.1489444378965258,\n                    \"u_probability\": 0.0016546990388445707\n                },\n                {\n                    \"sql_condition\": \"ABS(EPOCH(try_strptime(\\\"dob_l\\\", '%Y-%m-%d')) - EPOCH(try_strptime(\\\"dob_r\\\", '%Y-%m-%d'))) <= 2629800.0\",\n                    \"label_for_charts\": \"Abs difference of 'transformed dob <= 1 month'\",\n                    \"m_probability\": 0.08866691175438302,\n                    \"u_probability\": 0.002594404665842722\n                },\n                {\n                    \"sql_condition\": \"ABS(EPOCH(try_strptime(\\\"dob_l\\\", '%Y-%m-%d')) - EPOCH(try_strptime(\\\"dob_r\\\", '%Y-%m-%d'))) <= 31557600.0\",\n                    \"label_for_charts\": \"Abs difference of 'transformed dob <= 1 year'\",\n                    \"m_probability\": 0.10518866178811104,\n                    \"u_probability\": 0.030622146410222362\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\",\n                    \"m_probability\": 0.26694640124381713,\n                    \"u_probability\": 0.9635251218042348\n                }\n            ],\n            \"comparison_description\": \"Exact match vs. Damerau-Levenshtein distance <= 1 vs. month difference <= 1 vs. year difference <= 1 vs. anything else\"\n        },\n        {\n            \"output_column_name\": \"city\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"city_l\\\" IS NULL OR \\\"city_r\\\" IS NULL\",\n                    \"label_for_charts\": \"city is NULL\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"city_l\\\" = \\\"city_r\\\"\",\n                    \"label_for_charts\": \"Exact match on city\",\n                    \"m_probability\": 0.561103053663773,\n                    \"u_probability\": 0.052019405886043986,\n                    \"tf_adjustment_column\": \"city\",\n                    \"tf_adjustment_weight\": 1.0\n                },\n                {\n                    \"sql_condition\": \"ELSE\",\n                    \"label_for_charts\": \"All other comparisons\",\n                    \"m_probability\": 0.438896946336227,\n                    \"u_probability\": 0.947980594113956\n                }\n            ],\n            \"comparison_description\": \"Exact match 'city' vs. anything else\"\n        },\n        {\n            \"output_column_name\": \"email\",\n            \"comparison_levels\": [\n                {\n                    \"sql_condition\": \"\\\"email_l\\\" IS NULL OR \\\"email_r\\\" IS NULL\",\n                    \"label_for_charts\": \"email is NULL\",\n                    \"is_null_level\": true\n                },\n                {\n                    \"sql_condition\": \"\\\"email_l\\\" = \\\"email_r\\\"\",\n                    \"label_for_charts\": \"Exact match on email\",\n                    \"m_probability\": 0.5521904988218763,\n                    \"u_probability\": 0.0023577568563241916\n                },\n                {\n                    \"sql_condition\": \"NULLIF(regexp_extract(\\\"email_l\\\", '^[^@]+', 0), '') = NULLIF(regexp_extract(\\\"email_r\\\", '^[^@]+', 0), '')\",\n                    \"label_for_charts\": \"Exact match on transformed email\",\n                    \"m_probability\": 0.22046667643566936,\n                    \"u_probability\": 0.0010970118706508391\n                },\n                {\n                    \"sql_condition\": \"jaro_winkler_similarity(\\\"email_l\\\", \\\"email_r\\\") >= 0.88\",\n                    \"label_for_charts\": \"Jaro-Winkler distance of email >= 0.88\",\n                    \"m_probability\": 0.21374764835824084,\n                    \"u_probability\": 0.0007367990176013098\n                }\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink Environment\nDESCRIPTION: Sets up the Splink environment by importing required libraries and loading sample dataset.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/07_Evaluation.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Rerun our predictions to we're ready to view the charts\nimport pandas as pd\n\nfrom splink import DuckDBAPI, Linker, splink_datasets\n\npd.options.display.max_columns = 1000\n\ndb_api = DuckDBAPI()\ndf = splink_datasets.fake_1000\n```\n\n----------------------------------------\n\nTITLE: Visualizing Waterfall Chart\nDESCRIPTION: Creates waterfall chart visualization for selected records\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nrecords_to_plot = df_e.to_dict(orient=\"records\")\nlinker.visualisations.waterfall_chart(records_to_plot, filter_nulls=False)\n```\n\n----------------------------------------\n\nTITLE: Accessing Splink Comparison Viewer\nDESCRIPTION: Reference to using Splink's comparison viewer to examine prediction results and identify areas for model improvement.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/08_building_your_own_model.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.parameter_estimate_comparisons_chart\n```\n\n----------------------------------------\n\nTITLE: Complete Worked Example with Splink Datasets\nDESCRIPTION: This comprehensive example shows the complete workflow for analyzing blocking rules, importing necessary functions from Splink, creating a database connection, loading the fake dataset, defining blocking rules, and generating the chart.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/cumulative_comparisons_to_be_scored_from_blocking_rules_chart.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import DuckDBAPI, block_on, splink_datasets\nfrom splink.blocking_analysis import (\n    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,\n)\n\ndb_api = DuckDBAPI()\n\ndf = splink_datasets.fake_1000\n\nblocking_rules_for_analysis = [\n    block_on(\"first_name\"),\n    block_on(\"surname\"),\n    block_on(\"email\"),\n]\n\nchart = cumulative_comparisons_to_be_scored_from_blocking_rules_chart(\n    table_or_tables=df,\n    blocking_rules=blocking_rules_for_analysis,\n    db_api=db_api,\n    link_type=\"dedupe_only\",\n)\nchart\n```\n\n----------------------------------------\n\nTITLE: Creating Interactive Record Comparison Interface with ipywidgets\nDESCRIPTION: Builds an interactive interface using ipywidgets that allows users to modify record field values in real-time and see how the match weight changes. The interface displays two sets of input fields side by side with a waterfall chart showing the match results.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/real_time_record_linkage.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n\nfields = [\"unique_id\", \"first_name\", \"surname\", \"dob\", \"email\", \"city\"]\n\nleft_text_boxes = []\nright_text_boxes = []\n\ninputs_to_interactive_output = {}\n\nfor f in fields:\n    wl = widgets.Text(description=f, value=str(record_1[f]))\n    left_text_boxes.append(wl)\n    inputs_to_interactive_output[f\"{f}_l\"] = wl\n    wr = widgets.Text(description=f, value=str(record_2[f]))\n    right_text_boxes.append(wr)\n    inputs_to_interactive_output[f\"{f}_r\"] = wr\n\nb1 = widgets.VBox(left_text_boxes)\nb2 = widgets.VBox(right_text_boxes)\nui = widgets.HBox([b1, b2])\n\n\ndef myfn(**kwargs):\n    my_args = dict(kwargs)\n\n    record_left = {}\n    record_right = {}\n\n    for key, value in my_args.items():\n        if value == \"\":\n            value = None\n        if key.endswith(\"_l\"):\n            record_left[key[:-2]] = value\n        elif key.endswith(\"_r\"):\n            record_right[key[:-2]] = value\n\n    # Assuming 'linker' is defined earlier in your code\n    linker._settings_obj._retain_intermediate_calculation_columns = True\n\n    df_two = linker.inference.compare_two_records(record_left, record_right)\n\n    recs = df_two.as_pandas_dataframe().to_dict(orient=\"records\")\n\n    display(linker.visualisations.waterfall_chart(recs, filter_nulls=False))\n\n\nout = widgets.interactive_output(myfn, inputs_to_interactive_output)\n\ndisplay(ui, out)\n```\n\n----------------------------------------\n\nTITLE: Generating Parameter Estimate Comparisons Chart\nDESCRIPTION: This snippet demonstrates how to generate the parameter estimate comparisons chart using the trained Splink model. It calls the parameter_estimate_comparisons_chart method on the linker's visualisations object.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/parameter_estimate_comparisons_chart.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nchart = linker.visualisations.parameter_estimate_comparisons_chart()\nchart\n```\n\n----------------------------------------\n\nTITLE: Creating Layered Altair Chart with Text Labels for Splink\nDESCRIPTION: This code creates a more advanced Altair chart by layering a bar chart with text labels. It defines a base chart and then adds bar and text layers, resulting in a comprehensive visualization of Splink comparison levels.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create base chart with shared data and encodings (mark type not specified)\nbase = alt.Chart(df).encode(\n    y=alt.Y(\"cl_id\",\n            sort=\"-x\",\n            title=\"Comparison level\"\n            ),\n    x=alt.X(\"log2_bayes_factor\",\n            title=\"Comparison level match weight = log2(m/u)\",\n            scale=alt.Scale(domain=[-10, 10])\n            ),\n    tooltip=[\n        \"comparison_name\",\n        \"label_for_charts\",\n        \"sql_condition\",\n        \"m_probability\",\n        \"u_probability\",\n        \"bayes_factor\",\n        \"log2_bayes_factor\"\n    ]\n)\n\n# Build bar chart from base (color legend made redundant by text labels)\nbar = base.mark_bar().encode(\n    color=alt.Color(\"comparison_name\", legend=None)\n)\n\n# Build text layer from base\ntext = base.mark_text(dx=0, align=\"right\").encode(\n    text=\"comparison_name\"\n)\n\n# Final layered chart\nchart = bar + text\n```\n\n----------------------------------------\n\nTITLE: Visualizing Match Weights Distribution\nDESCRIPTION: Generates a match weights chart that visualizes the distribution of match weights in the linked dataset, which helps in understanding the model's performance and setting appropriate thresholds.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/real_time_record_linkage.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.match_weights_chart()\n```\n\n----------------------------------------\n\nTITLE: Configuring Probability Settings in Splink JSON\nDESCRIPTION: JSON configuration showing comparison level settings with m_probability and u_probability values for name matching. This example shows probability settings for exact first name matching.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/settings.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"sql_condition\": \"\\\"first_name_l\\\" = \\\"first_name_r\\\"\",\n    \"label_for_charts\": \"Exact match on first_name\",\n    \"m_probability\": 0.48854806009621365,\n    \"u_probability\": 0.0056770619302010565\n}\n```\n\n----------------------------------------\n\nTITLE: Viewing EmailComparison Structure in Splink\nDESCRIPTION: Displays the human-readable description of the email comparison structure for the DuckDB backend, showing how email addresses are compared.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nprint(email_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Initializing DuckDB API\nDESCRIPTION: Creates a DuckDB API instance that will be used as the database backend for Splink operations.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/demo_settings/model_create_h50k.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import DuckDBAPI\ndb_api = DuckDBAPI()\n```\n\n----------------------------------------\n\nTITLE: Modifying Settings After Loading from Serialized JSON Model in Splink\nDESCRIPTION: This snippet demonstrates how to modify settings after loading them from a serialized JSON model in Splink. It creates a model, saves it to JSON, then loads and modifies the settings before creating a new Linker instance.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/cookbook.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\n# setup to create a model\n\ndb_api = DuckDBAPI()\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.LevenshteinAtThresholds(\"first_name\"),\n        cl.LevenshteinAtThresholds(\"surname\"),\n\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\", \"dob\"),\n        block_on(\"surname\"),\n    ]\n)\n\nlinker = Linker(df, settings, db_api)\n\n\nlinker.misc.save_model_to_json(\"mod.json\", overwrite=True)\n\nnew_settings = SettingsCreator.from_path_or_dict(\"mod.json\")\n\nnew_settings.retain_intermediate_calculation_columns = True\nnew_settings.blocking_rules_to_generate_predictions = [\"1=1\"]\nnew_settings.additional_columns_to_retain = [\"cluster\"]\n\nlinker = Linker(df, new_settings, DuckDBAPI())\n\nlinker.inference.predict().as_duckdbpyrelation().show()\n```\n\n----------------------------------------\n\nTITLE: Using Pre-Built Name Comparisons from Splink's Comparison Template Library\nDESCRIPTION: Demonstrates how to use Splink's out-of-the-box comparison templates for common data fields. This example shows importing and using a pre-configured comparison for first names.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2023-07-27-feature_update.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.duckdb.comparison_template_library as ctl\n\nfirst_name_comparison = ctl.name_comparison(\"first_name\")\n```\n\n----------------------------------------\n\nTITLE: Estimating U Parameters Using Random Sampling\nDESCRIPTION: Estimates the u parameters (probability of agreement for non-matching records) using random sampling of up to 500,000 record pairs.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/spark/deduplicate_1k_synthetic.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_u_using_random_sampling(max_pairs=5e5)\n```\n\n----------------------------------------\n\nTITLE: Configuring TF Adjustment Weight for Fuzzy Matches in Splink\nDESCRIPTION: This code snippet demonstrates how to configure term frequency adjustments for fuzzy matches in Splink. It specifies a Jaro-Winkler similarity threshold of 0.8 for first names, enables TF adjustment on the first_name column, and sets a weight of 0.5 to reduce the impact of the adjustment by half.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/term-frequency.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n{\n  \"sql_condition\": \"jaro_winkler_sim(first_name_l, first_name_r) > 0.8\",\n  \"label_for_charts\": \"Exact match\",\n  \"tf_adjustment_column\": \"first_name\",\n  \"tf_adjustment_weight\": 0.5\n}\n```\n\n----------------------------------------\n\nTITLE: Using Damerau-Levenshtein Distance with DuckDB in Python\nDESCRIPTION: Shows how to calculate the Damerau-Levenshtein distance between two strings using DuckDB in Python. This algorithm extends Levenshtein by also considering transpositions of adjacent characters as a single edit operation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/comparators.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nduckdb.sql(\"SELECT damerau_levenshtein('CAKE', 'ACKE')\").df().iloc[0,0]\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Altair Chart for Splink Data\nDESCRIPTION: This snippet creates a simple bar chart using Altair, displaying comparison levels and their log2 Bayes factors. It includes data preparation and basic chart encoding.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport altair as alt\n\ndf = pd.DataFrame(records)\n\n# Need a unique name for each comparison level - easier to create in pandas than altair\ndf[\"cl_id\"] = df[\"comparison_name\"] + \"_\" + \\\n    df[\"comparison_vector_value\"].astype(\"str\")\n\n# Simple start - bar chart with x, y and color encodings\nalt.Chart(df).mark_bar().encode(\n    y=\"cl_id\",\n    x=\"log2_bayes_factor\",\n    color=\"comparison_name\"\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Trained Splink Model to JSON in Python\nDESCRIPTION: This snippet shows how to use the save_model_to_json function from the Splink library to save a trained model to a JSON file. The function takes a filename as an argument.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/settings.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlinker.misc.save_model_to_json(\"model.json\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Comparison Levels and Visualization in Splink (JSON)\nDESCRIPTION: This JSON snippet defines comparison levels for different fields (city and email) in a data matching project. It includes match probabilities, Bayes factors, and SQL conditions for each comparison level. The configuration also specifies visualization settings using Vega-Lite for displaying match weights and other metrics.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"bayes_factor\": 0.42188504195296994,\n  \"cl_id\": \"city_0\",\n  \"comparison_name\": \"city\",\n  \"comparison_vector_value\": 0,\n  \"label_for_charts\": \"All other comparisons\",\n  \"log2_bayes_factor\": -1.2450781575619725,\n  \"m_probability\": 0.3986191065720299,\n  \"sql_condition\": \"ELSE\",\n  \"u_probability\": 0.9448524288198547\n},\n{\n  \"bayes_factor\": 269.6074384240141,\n  \"cl_id\": \"email_2\",\n  \"comparison_name\": \"email\",\n  \"comparison_vector_value\": 2,\n  \"label_for_charts\": \"Exact match\",\n  \"log2_bayes_factor\": 8.07471649055784,\n  \"m_probability\": 0.5914840252879943,\n  \"sql_condition\": \"\\\"email_l\\\" = \\\"email_r\\\"\",\n  \"u_probability\": 0.0021938713143283602\n},\n{\n  \"bayes_factor\": 222.9721189153553,\n  \"cl_id\": \"email_1\",\n  \"comparison_name\": \"email\",\n  \"comparison_vector_value\": 1,\n  \"label_for_charts\": \"Levenshtein <= 2\",\n  \"log2_bayes_factor\": 7.800719512398763,\n  \"m_probability\": 0.3019669634613132,\n  \"sql_condition\": \"levenshtein(\\\"email_l\\\", \\\"email_r\\\") <= 2\",\n  \"u_probability\": 0.0013542812658830492\n},\n{\n  \"bayes_factor\": 0.10692840956298139,\n  \"cl_id\": \"email_0\",\n  \"comparison_name\": \"email\",\n  \"comparison_vector_value\": 0,\n  \"label_for_charts\": \"All other comparisons\",\n  \"log2_bayes_factor\": -3.225282884575804,\n  \"m_probability\": 0.10654901125069259,\n  \"sql_condition\": \"ELSE\",\n  \"u_probability\": 0.9964518474197885\n}\n],\n\"layer\": [\n{\n  \"encoding\": {\n    \"color\": {\n      \"field\": \"comparison_name\",\n      \"legend\": null,\n      \"type\": \"nominal\"\n    },\n    \"tooltip\": [\n      {\n        \"field\": \"comparison_name\",\n        \"type\": \"nominal\"\n      },\n      {\n        \"field\": \"label_for_charts\",\n        \"type\": \"nominal\"\n      },\n      {\n        \"field\": \"sql_condition\",\n        \"type\": \"nominal\"\n      },\n      {\n        \"field\": \"m_probability\",\n        \"type\": \"quantitative\"\n      },\n      {\n        \"field\": \"u_probability\",\n        \"type\": \"quantitative\"\n      },\n      {\n        \"field\": \"bayes_factor\",\n        \"type\": \"quantitative\"\n      },\n      {\n        \"field\": \"log2_bayes_factor\",\n        \"type\": \"quantitative\"\n      }\n    ],\n    \"x\": {\n      \"field\": \"log2_bayes_factor\",\n      \"scale\": {\n        \"domain\": [\n          -10,\n          10\n        ]\n      },\n      \"title\": \"Comparison level match weight = log2(m/u)\",\n      \"type\": \"quantitative\"\n    },\n    \"y\": {\n      \"field\": \"cl_id\",\n      \"sort\": \"-x\",\n      \"title\": \"Comparison level\",\n      \"type\": \"nominal\"\n    }\n  },\n  \"mark\": {\n    \"type\": \"bar\"\n  }\n},\n{\n  \"encoding\": {\n    \"text\": {\n      \"field\": \"comparison_name\",\n      \"type\": \"nominal\"\n    },\n    \"tooltip\": [\n      {\n        \"field\": \"comparison_name\",\n        \"type\": \"nominal\"\n      },\n      {\n        \"field\": \"label_for_charts\",\n        \"type\": \"nominal\"\n      },\n      {\n        \"field\": \"sql_condition\",\n        \"type\": \"nominal\"\n      },\n      {\n        \"field\": \"m_probability\",\n        \"type\": \"quantitative\"\n      },\n      {\n        \"field\": \"u_probability\",\n        \"type\": \"quantitative\"\n      },\n      {\n        \"field\": \"bayes_factor\",\n        \"type\": \"quantitative\"\n      },\n      {\n        \"field\": \"log2_bayes_factor\",\n        \"type\": \"quantitative\"\n      }\n    ],\n    \"x\": {\n      \"field\": \"log2_bayes_factor\",\n      \"scale\": {\n        \"domain\": [\n          -10,\n          10\n        ]\n      },\n      \"title\": \"Comparison level match weight = log2(m/u)\",\n      \"type\": \"quantitative\"\n    },\n    \"y\": {\n      \"field\": \"cl_id\",\n      \"sort\": \"-x\",\n      \"title\": \"Comparison level\",\n      \"type\": \"nominal\"\n    }\n  },\n  \"mark\": {\n    \"align\": \"right\",\n    \"dx\": 0,\n    \"type\": \"text\"\n  }\n}\n]\n```\n\n----------------------------------------\n\nTITLE: Loading Sample Dataset\nDESCRIPTION: Imports and loads the historical_50k sample dataset that comes with Splink for demonstration purposes.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/demo_settings/model_create_h50k.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import splink_datasets\n\ndf = splink_datasets.historical_50k\n```\n\n----------------------------------------\n\nTITLE: Configuring Prediction Output in Splink\nDESCRIPTION: How to configure the Splink predict() method to optimize memory usage by controlling intermediate calculation columns.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/08_building_your_own_model.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# During development\npredict(retain_intermediate_calculation_columns=True, retain_intermediate_calculation_columns_for_prediction=True)\n\n# For production/final runs\npredict(retain_intermediate_calculation_columns=False, retain_intermediate_calculation_columns_for_prediction=False)\n```\n\n----------------------------------------\n\nTITLE: Converting Match Weight to Match Probability\nDESCRIPTION: This formula shows how to convert a match weight to a match probability, providing a more intuitive measure of similarity between records.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/theory/fellegi_sunter.md#2025-04-16_snippet_5\n\nLANGUAGE: latex\nCODE:\n```\nPr(\\textsf{Match | Observation}) = \\frac{2^{M_\\textsf{obs}}}{1+2^{M_\\textsf{obs}}}\n```\n\n----------------------------------------\n\nTITLE: Using Temporary On-Disk Database with DuckDB in Splink\nDESCRIPTION: Example of creating a Splink linker with a temporary on-disk database to handle memory limitations. This approach allows DuckDB to spill to disk when RAM is insufficient.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/performance/optimising_duckdb.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlinker = Linker(\n    df, settings, DuckDBAPI(connection=\":temporary:\")\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Splink Charts using Altair 5\nDESCRIPTION: Demonstrates how to save Splink visualization charts using Altair 5's built-in save functionality, which supports multiple file formats and scaling options.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2023-07-27-feature_update.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nc.save(\"chart.png\", scale_factor=2)\n```\n\n----------------------------------------\n\nTITLE: Using Jaro-Winkler Similarity with DuckDB in Python\nDESCRIPTION: Shows how to calculate the Jaro-Winkler similarity between two strings using DuckDB in Python. This algorithm extends Jaro similarity by giving more weight to matches at the beginning of the strings, making it particularly useful for comparing names.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/comparators.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nduckdb.sql(\"SELECT jaro_winkler_similarity('MARTHA', 'MARHTA')\").df().iloc[0,0]\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink Linker and Training Model\nDESCRIPTION: This snippet sets up the Splink environment, creates a Linker object with specific settings, and trains the model using random sampling and expectation maximisation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n      cl.NameComparison(\"first_name\"),\n        cl.NameComparison(\"surname\"),\n        cl.DateOfBirthComparison(\"dob\", input_is_string=True),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        cl.LevenshteinAtThresholds(\"email\", 2),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\", \"dob\"),\n        block_on(\"surname\"),\n    ]\n)\n\nlinker = Linker(df, settings,DuckDBAPI())\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\nfor rule in [block_on(\"first_name\"), block_on(\"dob\")]:\n    linker.training.estimate_parameters_using_expectation_maximisation(rule)\n```\n\n----------------------------------------\n\nTITLE: Customizing ForenameSurnameComparison with Concatenated Column in Splink\nDESCRIPTION: Creates a customized forename-surname comparison with a specified concatenated column for term frequency adjustments. This allows for better handling of full name term frequencies.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfull_name_comparison = cl.ForenameSurnameComparison(\"forename\", \"surname\", forename_surname_concat_col_name=\"first_and_last_name\")\n```\n\n----------------------------------------\n\nTITLE: Training DOB Parameters\nDESCRIPTION: Estimates parameters using EM algorithm for DOB-based blocking\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nblocking_rule = block_on(\"dob\")\ntraining_session_dob = (\n    linker.training.estimate_parameters_using_expectation_maximisation(blocking_rule)\n)\n```\n\n----------------------------------------\n\nTITLE: Splink Dataset Initialization\nDESCRIPTION: Initializes Splink with a test dataset for demonstration purposes.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/03_Blocking.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import DuckDBAPI, block_on, splink_datasets\n\ndf = splink_datasets.fake_1000\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink Linker with SQLite Backend in Python\nDESCRIPTION: This code snippet demonstrates how to initialize a Splink Linker object using the SQLite backend. It imports the required classes and creates a Linker instance with a dataframe, settings, and the SQLiteAPI.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/backends/backends.md#2025-04-16_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom splink import Linker, SQLiteAPI\n\nlinker = Linker(df, settings, db_api=SQLiteAPI(...))\n```\n\n----------------------------------------\n\nTITLE: Importing and Using Splink In-built Datasets in Python\nDESCRIPTION: Basic example showing how to import and use a fake dataset from splink_datasets. The fake_1000 dataset is loaded into a DataFrame variable which can be used for testing.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/datasets.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import splink_datasets\n\ndf = splink_datasets.fake_1000\n```\n\n----------------------------------------\n\nTITLE: Training Model Parameters on Name Fields\nDESCRIPTION: Runs Expectation-Maximization algorithm to estimate model parameters using records that share the same first name and surname. The term frequency adjustments are disabled for this training session.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/demo_settings/model_create_h50k.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntraining_blocking_rule = block_on(\"first_name\", \"surname\")\ntraining_session_names = (\n    linker.training.estimate_parameters_using_expectation_maximisation(\n        training_blocking_rule, estimate_without_term_frequencies=True\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Using PostcodeComparison Template in Splink with DuckDB\nDESCRIPTION: Demonstrates how to use Splink's pre-built PostcodeComparison template to generate comparison levels for postcodes. This creates a comparison with levels for exact matching on full postcode, sector, district, and area.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/data_preparation/feature_engineering.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\npc_comparison = ctl.PostcodeComparison(\"postcode\").get_comparison(\"duckdb\")\nprint(pc_comparison.human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Configuring Splink Logging with Basic Logging Disabled\nDESCRIPTION: Demonstrates how to set a custom logging level for Splink after initialization when basic logging is disabled. This must be done after the linker is initialized to prevent the default INFO setting from overriding your custom level.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/debug_modes.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nlinker = Linker(df, settings, db_api, set_up_basic_logging=False)\n\n# This must come AFTER the linker is intialised, because the logging level\n# will be set to INFO\nlogging.getLogger(\"splink\").setLevel(logging.DEBUG)\n```\n\n----------------------------------------\n\nTITLE: Displaying Highlighted Comparison Results in Python\nDESCRIPTION: Displays the comparison results with highlighted cells based on match probability thresholds to visualize linkage performance.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/bias_eval.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncolumns_of_interest = ['match_weight', 'match_probability', 'unique_id_l', 'unique_id_r', 'first_name_l', 'first_name_r', 'surname_l','surname_r', 'dob_l', 'dob_r','postcode_fake_l', 'postcode_fake_r','birth_place_l', 'birth_place_r', 'occupation_l', 'occupation_r']\ncomparisons[columns_of_interest].style.map(highlight_cells, subset=['match_probability'])\n```\n\n----------------------------------------\n\nTITLE: Analyzing Unlinkable Records\nDESCRIPTION: Generates chart for unlinkable records analysis\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlinker.evaluation.unlinkables_chart()\n```\n\n----------------------------------------\n\nTITLE: Computing Match Weight Formula from Bayes Factor\nDESCRIPTION: This formula shows how match weights are derived from the , m, and u parameters, where  is the prior probability of a match, and m/u represents the Bayes factor K.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/theory/fellegi_sunter.md#2025-04-16_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n\\begin{equation}\n\\begin{aligned}\n    M &= \\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right) + \\log_2 K \\\\[10pt]\n    &= \\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right) + \\log_2 m - \\log_2 u\n\\end{aligned}\n\\end{equation}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Splink Comparison Function Performance with Altair\nDESCRIPTION: This snippet creates bar charts using Altair to visualize the performance of different comparison functions in Splink for both DuckDB and Spark backends. It defines a reusable chart creation function and generates separate charts for each backend.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/performance/performance_of_comparison_functions.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport altair as alt\n\n\n\n# Create base chart function to avoid code duplication\ndef create_runtime_chart(data, backend_name):\n    return alt.Chart(data).mark_bar().encode(\n        y=alt.Y('comparison_type:N',\n                sort=alt.EncodingSortField(field='median', order='ascending'),\n                title='Comparison Type'),\n        x=alt.X('median:Q',\n                title='Median Runtime (seconds)'),\n        tooltip=[\n            alt.Tooltip('comparison_type', title='Comparison'),\n            alt.Tooltip('median', title='Runtime (s)', format='.3f'),\n            alt.Tooltip('multiple_of_exact_match', title='Times slower than exact match', format='.1f'),\n            alt.Tooltip('rounds', title='Rounds'),\n        ]\n    ).properties(\n        title=f'{backend_name} Comparison Runtimes',\n        height=400\n    )\n\n# Create charts for both backends\nduckdb_df = df[df['backend'] == 'duckdb'].sort_values('median')\nspark_df = df[df['backend'] == 'spark'].sort_values('median')\n\nduckdb_chart = create_runtime_chart(duckdb_df, 'DuckDB')\nspark_chart = create_runtime_chart(spark_df, 'Spark')\n```\n\n----------------------------------------\n\nTITLE: Managing Splink Dataset Cache in Python\nDESCRIPTION: Example showing how to use splink_dataset_utils to manage the cached datasets. The example demonstrates checking which datasets are already downloaded and clearing specific datasets from the cache.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/datasets.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.datasets import splink_dataset_utils\n\nsplink_dataset_utils.show_downloaded_data()\nsplink_dataset_utils.clear_cache(['fake_1000'])\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink Environment\nDESCRIPTION: Sets up the Splink environment by importing required libraries and loading sample dataset.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/06_Visualising_predictions.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Rerun our predictions to we're ready to view the charts\nfrom splink import Linker, DuckDBAPI, splink_datasets\n\nimport pandas as pd\n\npd.options.display.max_columns = 1000\n\ndb_api = DuckDBAPI()\ndf = splink_datasets.fake_1000\n```\n\n----------------------------------------\n\nTITLE: Loading Built-in Datasets in Splink\nDESCRIPTION: Demonstrates how to use Splink's built-in datasets module to access example datasets, which simplifies following tutorials and examples without worrying about file paths.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2023-07-27-feature_update.md#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.datasets import splink_datasets\n\ndf = splink_datasets.fake_1000\n```\n\n----------------------------------------\n\nTITLE: Bayes Factor for Multiple Features\nDESCRIPTION: This formula demonstrates how the Bayes factors for different comparisons are multiplied together, assuming independence between column comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/theory/fellegi_sunter.md#2025-04-16_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\nK_\\textsf{features} = K_\\textsf{forename} \\cdot K_\\textsf{surname} \\cdot K_\\textsf{dob} \\cdot K_\\textsf{city} \\cdot K_\\textsf{email}\n```\n\n----------------------------------------\n\nTITLE: Creating Cluster Dashboard\nDESCRIPTION: Generates interactive dashboard for cluster analysis\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.cluster_studio_dashboard(\n    df_predict,\n    clusters,\n    \"dashboards/50k_cluster.html\",\n    sampling_method=\"by_cluster_size\",\n    overwrite=True,\n)\n\nfrom IPython.display import IFrame\n\nIFrame(src=\"./dashboards/50k_cluster.html\", width=\"100%\", height=1200)\n```\n\n----------------------------------------\n\nTITLE: Disabling UDFs in SQLite Backend for Splink in Python\nDESCRIPTION: This code snippet demonstrates how to disable the use of user-defined functions (UDFs) when creating a Splink linker with the SQLite backend. This can be useful if you don't want to install the RapidFuzz package or prefer not to use Python-based fuzzy matching functions.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/backends/backends.md#2025-04-16_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nSQLiteAPI(register_udfs=False)\n```\n\n----------------------------------------\n\nTITLE: Creating Test Data for Phonetic Matching Analysis\nDESCRIPTION: Creates a test dataset with various variations of the name \"Stephen\" to evaluate how different phonetic algorithms handle various types of string differences.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndata = [\n    {\"string1\": \"Stephen\", \"string2\": \"Stephen\", \"error_type\": \"None\"},\n    {\"string1\": \"Stephen\", \"string2\": \"Steven\", \"error_type\": \"Spelling Variation\"},\n    {\"string1\": \"Stephen\", \"string2\": \"Stephan\", \"error_type\": \"Spelling Variation/Similar Name\"},\n    {\"string1\": \"Stephen\", \"string2\": \"Steve\", \"error_type\": \"Nickname/Alias\"},\n    {\"string1\": \"Stephen\", \"string2\": \"Stehpen\", \"error_type\": \"Transposition\"},\n    {\"string1\": \"Stephen\", \"string2\": \"tSephen\", \"error_type\": \"Transposition\"},\n    {\"string1\": \"Stephen\", \"string2\": \"Stephne\", \"error_type\": \"Transposition\"},\n    {\"string1\": \"Stephen\", \"string2\": \"Stphen\", \"error_type\": \"Deletion\"},\n    {\"string1\": \"Stephen\", \"string2\": \"Stepheb\", \"error_type\": \"Replacement\"},\n    {\"string1\": \"Stephen\", \"string2\": \"Stephanie\", \"error_type\": \"Different Name\"},\n    {\"string1\": \"Stephen\", \"string2\": \"Richard\", \"error_type\": \"Different Name\"},\n]\n\n\ndf = pd.DataFrame(data)\ndf\n```\n\n----------------------------------------\n\nTITLE: Using Splink's Record Comparison Function\nDESCRIPTION: Reference to the Splink API method for comparing individual record pairs, useful for generating waterfall charts for specific comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/08_building_your_own_model.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlinker.inference.compare_two_records\n```\n\n----------------------------------------\n\nTITLE: Manually Altering m and u Probabilities Post-Training in Splink\nDESCRIPTION: This snippet demonstrates how to manually alter m and u probabilities after training in Splink. It's not officially supported but can be useful for ad-hoc alterations to trained models.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/cookbook.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_level_library as cll\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\nfrom splink.datasets import splink_dataset_labels\n\nlabels = splink_dataset_labels.fake_1000_labels\n\ndb_api = DuckDBAPI()\n\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.ExactMatch(\"first_name\"),\n        cl.ExactMatch(\"surname\"),\n        cl.ExactMatch(\"dob\"),\n        cl.ExactMatch(\"city\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"dob\"),\n    ],\n)\ndf = splink_datasets.fake_1000\nlinker = Linker(df, settings, db_api, set_up_basic_logging=False)\n\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\nlinker.training.estimate_parameters_using_expectation_maximisation(block_on(\"dob\"))\n\n\nsurname_comparison = linker._settings_obj._get_comparison_by_output_column_name(\n    \"surname\"\n)\nelse_comparison_level = (\n    surname_comparison._get_comparison_level_by_comparison_vector_value(0)\n)\nelse_comparison_level._m_probability = 0.1\n\n\nlinker.visualisations.m_u_parameters_chart()\n```\n\n----------------------------------------\n\nTITLE: Pseudocode Implementation of First Name Comparison in Splink\nDESCRIPTION: This pseudocode snippet demonstrates how Splink's comparison levels are implemented as a sequence of conditional checks, with each level being evaluated in order. The code shows how first name comparisons are categorized into different similarity levels.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/comparisons_and_comparison_levels.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nif first_name_l_ == first_name_r:\n    return \"Assign to category: Exact match\"\nelif JaroWinklerSimilarity(first_name_l_, first_name_r) > 0.95:\n    return \"Assign to category: JaroWinklerSimilarity > 0.95\"\nelif JaroWinklerSimilarity(first_name_l_, first_name_r) > 0.8:\n    return \"Assign to category: JaroWinklerSimilarity > 0.8\"\nelse:\n    return \"Assign to category: All other\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink for Deduplication\nDESCRIPTION: Code to set up Splink for deduplication (dedupe_only) with a single input table. This process finds links within a dataset.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/link_type.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import SettingsCreator\n\nsettings = SettingsCreator(\n    link_type= \"dedupe_only\",\n)\n\nlinker = Linker(df, settings, db_api=dbapi, )\n```\n\n----------------------------------------\n\nTITLE: Loading and Filtering Pairwise Labels Dataset\nDESCRIPTION: Loads a sample dataset of labeled record pairs and filters to keep only those marked as true matches based on a clerical match score of 1.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/pairwise_labels.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.datasets import splink_dataset_labels\n\npairwise_labels = splink_dataset_labels.fake_1000_labels\n\n# Choose labels indicating a match\npairwise_labels = pairwise_labels[pairwise_labels[\"clerical_match_score\"] == 1]\npairwise_labels\n```\n\n----------------------------------------\n\nTITLE: Date Substring Analysis\nDESCRIPTION: Analyzes the distribution of month and day values in dates by extracting a substring from the dob field to identify specific patterns.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/profile_columns.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprofile_columns(df, column_expressions=\"substr(dob, 6, 10)\", db_api=DuckDBAPI())\n```\n\n----------------------------------------\n\nTITLE: Viewing NameComparison Structure in Splink\nDESCRIPTION: Displays the human-readable description of the name comparison structure for the DuckDB backend. This helps understand how the name comparison function works.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(first_name_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Initializing NameComparison in Splink\nDESCRIPTION: Creates a basic comparison for an individual name field like first name. This comparison includes string similarity metrics appropriate for name matching.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\nfirst_name_comparison = cl.NameComparison(\"first_name\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Splink Installation in Google Colab\nDESCRIPTION: Optional installation commands for running Splink and PySpark in Google Colab environment.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/spark/deduplicate_1k_synthetic.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment and run this cell if you're running in Google Colab.\n# !pip install splink\n# !pip install pyspark\n```\n\n----------------------------------------\n\nTITLE: Viewing DateOfBirthComparison Structure in Splink\nDESCRIPTION: Displays the human-readable description of the date of birth comparison structure for the DuckDB backend. This helps understand how the comparison function works.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(date_of_birth_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Loading Sample Data for Record Linkage\nDESCRIPTION: Loads a fake dataset with 1000 records that will be used for the record linkage demonstration and displays the first two rows.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/pairwise_labels.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import splink_datasets\n\ndf = splink_datasets.fake_1000\ndf.head(2)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Comparison Results for Transactions\nDESCRIPTION: This code generates a comparison viewer dashboard for the predicted matches in the transaction data and displays it in an IFrame.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/transactions.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.comparison_viewer_dashboard(\n    df_predict, \"dashboards/comparison_viewer_transactions.html\", overwrite=True\n)\nfrom IPython.display import IFrame\n\nIFrame(\n    src=\"./dashboards/comparison_viewer_transactions.html\", width=\"100%\", height=1200\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Beta Labelling Tool in Splink\nDESCRIPTION: This example shows how to generate the beta labelling tool in Splink. It sets up a dataset, trains a model, and then creates a labelling tool for a specific record.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/cookbook.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndb_api = DuckDBAPI()\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.ExactMatch(\"first_name\"),\n        cl.ExactMatch(\"surname\"),\n        cl.ExactMatch(\"dob\"),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        cl.ExactMatch(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n    max_iterations=2,\n)\n\nlinker = Linker(df, settings, db_api, set_up_basic_logging=False)\n\nlinker.training.estimate_probability_two_random_records_match(\n    [block_on(\"first_name\", \"surname\")], recall=0.7\n)\n\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n\nlinker.training.estimate_parameters_using_expectation_maximisation(block_on(\"dob\"))\n\npairwise_predictions = linker.inference.predict(threshold_match_weight=-10)\n\nfirst_unique_id = df.iloc[0].unique_id\nlinker.evaluation.labelling_tool_for_specific_record(unique_id=first_unique_id, overwrite=True)\n```\n\n----------------------------------------\n\nTITLE: Converting Splink Results to Various DataFrame Formats\nDESCRIPTION: Examples of how to convert Splink results to different dataframe formats depending on the backend being used.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/08_building_your_own_model.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsplink_dataframe.as_pandas_dataframe()  # Convert to pandas (not recommended for large datasets)\nsplink_dataframe.as_spark_dataframe  # Convert to Spark dataframe\nsplinkdataframe.as_duckdbpyrelation  # Convert to DuckDB relation\n```\n\n----------------------------------------\n\nTITLE: Calling compute_graph_metrics() Method in Python\nDESCRIPTION: This snippet shows how to call the compute_graph_metrics() method on a Splink linker object. It takes df_predict and df_clustered as parameters, along with an optional threshold_match_probability.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/clusters/how_to_compute_metrics.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nlinker.compute_graph_metrics(df_predict, df_clustered, threshold_match_probability=0.95)\n```\n\n----------------------------------------\n\nTITLE: Estimating U Parameters with Random Sampling\nDESCRIPTION: Estimates the U parameters (probability of agreement for unmatched records) using random sampling with a maximum of 5 million pairs.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/demo_settings/model_create_h50k.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlinker.training.estimate_u_using_random_sampling(max_pairs=5e6)\n```\n\n----------------------------------------\n\nTITLE: Converting ForenameSurnameComparison to Dictionary in Splink\nDESCRIPTION: Converts the full name comparison to a dictionary representation that can be used as a basis for more customized comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfull_name_comparison.get_comparison(\"duckdb\").as_dict()\n```\n\n----------------------------------------\n\nTITLE: Initializing Splink for Linking Only\nDESCRIPTION: Code to set up Splink for linking only (link_only) across multiple input tables. This configuration finds links between different datasets.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/link_type.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import SettingsCreator\n\nsettings = SettingsCreator(\n    link_type= \"link_only\",\n)\n\nlinker = Linker(\n    [df_1, df_2, df_n],\n    settings,\n    db_api=dbapi,\n    input_table_aliases=[\"name1\", \"name2\", \"name3\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Running pytest for Splink tests\nDESCRIPTION: Command to run all Splink tests locally using pytest and poetry. This runs tests against the default DuckDB backend.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npoetry run pytest tests/\n```\n\n----------------------------------------\n\nTITLE: Birth Place Distribution Analysis\nDESCRIPTION: Analyzes the distribution of birth_place values to demonstrate data skew analysis for string columns using profile_columns.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/profile_columns.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprofile_columns(df, column_expressions=\"birth_place\", db_api=DuckDBAPI())\n```\n\n----------------------------------------\n\nTITLE: Importing Splink Dataset in Python\nDESCRIPTION: Imports a fake dataset from Splink and displays the first two rows. This dataset contains a fully-populated ground-truth column called 'cluster' for accuracy analysis.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/accuracy_analysis_from_labels_column.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import splink_datasets\n\ndf = splink_datasets.fake_1000\ndf.head(2)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Loading Dataset\nDESCRIPTION: Imports required libraries including Splink, sets up pandas display options, and loads a fake dataset for demonstration.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/05_Predicting_results.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import Linker, DuckDBAPI, splink_datasets\n\nimport pandas as pd\n\npd.options.display.max_columns = 1000\n\ndb_api = DuckDBAPI()\ndf = splink_datasets.fake_1000\n```\n\n----------------------------------------\n\nTITLE: Configuring Splink Logging Without Basic Logging\nDESCRIPTION: Shows how to configure Splink logging by setting up a custom logger before initializing the Linker. This approach gives more control over the logging format and level.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/debug_modes.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# This code can be anywhere since set_up_basic_logging is False\nimport logging\nlogging.basicConfig(format=\"%(message)s\")\nsplink_logger = logging.getLogger(\"splink\")\nsplink_logger.setLevel(logging.INFO)\n\nlinker = Linker(df, settings, db_api, set_up_basic_logging=False)\n```\n\n----------------------------------------\n\nTITLE: Basic Chart Display in Python\nDESCRIPTION: Simple code to display the chart object. This is typically used after generating a match_weights_chart from a Splink linker object.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/match_weights_chart.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nchart\n```\n\n----------------------------------------\n\nTITLE: Returning SplinkDataFrame from SQL Query in Python\nDESCRIPTION: This snippet demonstrates how to execute a SQL query against Splink results and return the result as a SplinkDataFrame instead of the default pandas DataFrame.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/querying_splink_results.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf_query_result = linker.misc.query_sql(sql, output_type='splink_df')\n```\n\n----------------------------------------\n\nTITLE: Displaying Threshold Selection Chart in Python\nDESCRIPTION: This snippet shows how to display the threshold selection chart generated by Splink's accuracy analysis tool. It assumes the chart has been previously created and stored in a variable named 'chart'.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/threshold_selection_tool_from_labels_table.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nchart\n```\n\n----------------------------------------\n\nTITLE: Enhancing Altair Chart with Sorting and Axis Customization\nDESCRIPTION: This code improves the previous chart by sorting the bars, customizing axes and titles, and setting chart properties for better visualization of Splink comparison levels.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nalt.Chart(df).mark_bar().encode(\n    y=alt.Y(\"cl_id\",\n        sort=\"-x\",\n        title=\"Comparison level\"\n    ),\n    x=alt.X(\"log2_bayes_factor\",\n        title=\"Comparison level match weight = log2(m/u)\",\n        scale=alt.Scale(domain=[-10,10])\n    ),\n    color=\"comparison_name\"\n).properties(\n    title=\"New Chart - WOO!\"\n).configure_view(\n    step=15\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Blocking Rules Performance\nDESCRIPTION: Generates analysis chart for blocking rules effectiveness\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.blocking_analysis import (\n    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,\n)\nfrom splink import block_on\n\ncumulative_comparisons_to_be_scored_from_blocking_rules_chart(\n    table_or_tables=df,\n    db_api=db_api,\n    blocking_rules=[block_on(\"first_name\", \"surname\"), block_on(\"surname\", \"dob\")],\n    link_type=\"dedupe_only\",\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Label Data\nDESCRIPTION: Loads and registers ground truth labels for model evaluation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/07_Evaluation.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.datasets import splink_dataset_labels\n\ndf_labels = splink_dataset_labels.fake_1000_labels\nlabels_table = linker.table_management.register_labels_table(df_labels)\ndf_labels.head(5)\n```\n\n----------------------------------------\n\nTITLE: Loading and Displaying Historical Dataset (Python)\nDESCRIPTION: This code loads a 50k row dataset of historical persons using Splink's built-in datasets and displays the first few rows using pandas.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/deterministic_dedupe.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfrom splink import splink_datasets\n\npd.options.display.max_rows = 1000\ndf = splink_datasets.historical_50k\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Examining Phonetic Transformations for a Single String\nDESCRIPTION: Uses the phonetic_transform function to show how a string is transformed by various phonetic algorithms, which helps understand how phonetic matching works.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsa.phonetic_transform(\"Richard\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing Phonetic Matching Results with phonetic_match_chart\nDESCRIPTION: Uses the phonetic_match_chart function to visualize how different phonetic algorithms perform across various string variations, showing which algorithms consider different variations as matches.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsa.phonetic_match_chart(data, \"string1\", \"string2\")\n```\n\n----------------------------------------\n\nTITLE: Basic Unlinkables Chart Display\nDESCRIPTION: Simple code to display an unlinkables chart object.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/unlinkables_chart.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nchart\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Blocking Rules in Splink\nDESCRIPTION: Demonstrates proper configuration of multiple blocking rules using SettingsCreator, showing the recommended approach for implementing OR logic through separate rules rather than within a single rule.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/blocking/performance.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nSettingsCreator(\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\")\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Backend-Agnostic Test for All Dialects in Python\nDESCRIPTION: This snippet demonstrates how to write a backend-agnostic test that runs on all available SQL dialects. It uses a decorator to mark the test function and includes helper methods for loading data and instantiating a Splink linker.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom tests.decorator import mark_with_dialects_excluding\n\n@mark_with_dialects_excluding()\ndef test_feature_that_works_for_all_backends(test_helpers, dialect, some_other_test_fixture):\n    helper = test_helpers[dialect]\n\n    df = helper.load_frame_from_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n    settings = SettingsCreator(\n        link_type=\"dedupe_only\",\n        comparisons=[\n            cl.ExactMatch(\"first_name\"),\n            cl.ExactMatch(\"surname\"),\n        ],\n        blocking_rules_to_generate_predictions=[\n            block_on(\"first_name\"),\n        ],\n    )\n    linker = helper.Linker(\n        df,\n        settings,\n        **helper.extra_linker_args(),\n    )\n\n\n    # and then some actual testing logic\n```\n\n----------------------------------------\n\nTITLE: Configuring Shared Axes and Properties in Altair Chart\nDESCRIPTION: This code configures an Altair chart with shared x and y axes, adds a title, and sets the step size for the view. This is useful for creating consistent visualization components that can be combined later.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nchart.resolve_axis(\n    y=\"shared\",\n    x=\"shared\"\n).properties(\n    title=\"New Chart - WOO!\"\n).configure_view(\n    step=15\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Level Jaro-Winkler Comparison with Different Thresholds\nDESCRIPTION: Demonstrates how to create a comparison with multiple levels using the JaroWinklerAtThresholds function from the comparison library, with different thresholds to catch various types of string variations.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\nfirst_name_comparison = cl.JaroWinklerAtThresholds(\"first_name\", [0.9, 0.8, 0.7])\n```\n\n----------------------------------------\n\nTITLE: Preparing Input Datasets with DuckDB\nDESCRIPTION: Extracts two separate datasets from a table called 'data_for_matching' using DuckDB SQL queries. One dataset contains records from 'stockport' and the other contains records from 'z_all_companies'.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf_stockport = duckdb.sql(\n    \"select * from data_for_matching where source_dataset = 'stockport'\"\n)\n\ndf_all_companies = duckdb.sql(\n    \"select * from data_for_matching where source_dataset = 'z_all_companies'\"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Chart Generation Function in Splink\nDESCRIPTION: Python function that loads a chart template and populates it with provided data records.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef my_new_chart(records, as_dict=False):\n    chart_path = \"my_new_chart.json\"\n    chart = load_chart_definition(chart_path)\n\n    chart[\"data\"][\"values\"] = records\n    return altair_or_json(chart, as_dict=as_dict)\n```\n\n----------------------------------------\n\nTITLE: Basic Inline Dashboard Display in Jupyter Notebook\nDESCRIPTION: Simple code to display an existing comparison viewer dashboard HTML file within a Jupyter notebook using IFrame.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/comparison_viewer_dashboard.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import IFrame\nIFrame(\n    src=\"./img/scv.html\", width=\"100%\", height=1000\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Vega-Lite Visualization Settings for Heatmap Display in JSON\nDESCRIPTION: This code shows changes to a Vega-Lite visualization configuration, enhancing a heatmap and text display. The modifications include removing legends, customizing axis labels, adjusting dimensions with step-based sizing, and adding dynamic text sizing based on score values.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/understanding_and_editing_charts.md#2025-04-16_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n                  },\n    -              \"type\": \"quantitative\"\n    +              \"type\": \"quantitative\",\n    +              \"legend\": null\n                },\n                \"x\": {\n                  \"field\": \"comparator\",\n    -              \"type\": \"ordinal\"\n    +              \"type\": \"ordinal\",\n    +              \"title\": null,\n    +              \"axis\": {\n    +                \"labelFontSize\": 12\n    +              }\n                },\n                \"y\": {\n                  \"field\": \"strings_to_compare\",\n    -              \"type\": \"ordinal\"\n    +              \"type\": \"ordinal\",\n    +              \"axis\": null\n                }\n              },\n    -          \"height\": 300,\n    -          \"title\": \"Heatmap of Distance Scores\",\n    -          \"width\": 200\n    +          \"title\": \"Distance\",\n    +          \"width\": {\n    +            \"step\": 40\n    +          },\n    +          \"height\": {\n    +            \"step\": 30\n    +          }\n            },\n            {\n              \"mark\": {\n```\n\nLANGUAGE: json\nCODE:\n```\n                \"baseline\": \"middle\"\n              },\n              \"encoding\": {\n    +            \"size\": {\n    +              \"field\": \"score\",\n    +              \"scale\": {\n    +                \"range\": [\n    +                  8,\n    +                  14\n    +                ],\n    +                \"reverse\": true\n    +              },\n    +              \"legend\": null\n    +            },\n                \"text\": {\n                  \"field\": \"score\",\n                  \"type\": \"quantitative\"\n```\n\nLANGUAGE: json\nCODE:\n```\n      ],\n      \"resolve\": {\n        \"scale\": {\n    -      \"color\": \"independent\"\n    +      \"color\": \"independent\",\n    +      \"y\": \"shared\",\n    +      \"size\": \"independent\"\n        }\n      },\n      \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\",\n```\n\n----------------------------------------\n\nTITLE: Defining Backend-Agnostic Test Function with Decorator in Python\nDESCRIPTION: This snippet demonstrates how to define a test function and apply the backend-agnostic decorator. The decorator marks the test to run for all dialects and parameterizes it with a 'dialect' parameter.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@mark_with_dialects_excluding()\ndef test_feature_that_works_for_all_backends(test_helpers, dialect, some_other_test_fixture):\n```\n\n----------------------------------------\n\nTITLE: F1-Score Formula in LaTeX\nDESCRIPTION: Mathematical formula for calculating the F1-score (balanced F-score)\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/edge_metrics.md#2025-04-16_snippet_6\n\nLANGUAGE: latex\nCODE:\n```\nF_{1} = 2\\left[\\frac{1}{\\textsf{Precision}}+\\frac{1}{\\textsf{Recall}}\\right]^{-1} = \\frac{2 \\cdot \\textsf{Precision} \\cdot \\textsf{Recall}}{\\textsf{Precision} + \\textsf{Recall}}\n```\n\n----------------------------------------\n\nTITLE: Importing and Using ComparisonCreator.configure in Splink\nDESCRIPTION: Describes the configure() method available on all comparison functions in Splink. This method is used to customize how comparisons between record fields are handled in the record linkage process.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/comparison_library.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsplink.internals.comparison_creator.ComparisonCreator.configure\n```\n\n----------------------------------------\n\nTITLE: Displaying Cluster Metrics Dataframe in Python\nDESCRIPTION: This simple code snippet shows how to display the cluster metrics dataframe after computing graph metrics with Splink.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/clusters/how_to_compute_metrics.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf_clusters\n```\n\n----------------------------------------\n\nTITLE: Loading Test Data Using Dialect-Specific Helper in Python\nDESCRIPTION: This snippet demonstrates how to use a method from the dialect-specific test helper to load test data from a CSV file into the database.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf = helper.load_frame_from_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n```\n\n----------------------------------------\n\nTITLE: Levenshtein Distance Blocking Analysis\nDESCRIPTION: Shows how to analyze blocking rules using Levenshtein distance for fuzzy matching on surnames.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/03_Blocking.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbr = \"l.first_name = r.first_name and levenshtein(l.surname, r.surname) < 2\"\n\ncounts = count_comparisons_from_blocking_rule(\n    table_or_tables=df,\n    blocking_rule= br,\n    link_type=\"dedupe_only\",\n    db_api=db_api,\n)\ncounts\n```\n\n----------------------------------------\n\nTITLE: Validating Email Strings in Splink Comparisons with Regular Expressions\nDESCRIPTION: Demonstrates how to use the valid_string_regex parameter to ensure strings conform to a specific pattern before comparison, ensuring only valid email addresses are compared.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2023-07-27-feature_update.md#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport splink.duckdb.duckdb_comparison_library as cl\n\nemail_comparison = cl.levenshtein_at_thresholds(\"email\", valid_string_regex=\"^[^@]+\")\n```\n\n----------------------------------------\n\nTITLE: Cards Gallery Component - Blocking Configuration\nDESCRIPTION: JSON configuration for displaying blocking analysis cards, showing cumulative comparisons chart.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/index.md#2025-04-16_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n    \"title\": \"`cumulative comparisons to be scored from blocking rules chart`\",\n    \"image\": \"./img/cumulative_num_comparisons_from_blocking_rules_chart.png\",\n    \"url\": \"./cumulative_comparisons_to_be_scored_from_blocking_rules_chart.ipynb\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Displaying Side-by-Side Charts in Altair\nDESCRIPTION: This code uses the pipe operator to display bar and text charts side-by-side instead of layered. This is shown as a workaround for sorting issues that can occur when layering charts in Altair.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nbar | text\n```\n\n----------------------------------------\n\nTITLE: Basic Waterfall Chart Display in Python\nDESCRIPTION: Simple code to display a waterfall chart from an existing chart object.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/waterfall_chart.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nchart\n```\n\n----------------------------------------\n\nTITLE: Negative Predictive Value Formula in LaTeX\nDESCRIPTION: Mathematical formula for calculating negative predictive value\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/edge_metrics.md#2025-04-16_snippet_4\n\nLANGUAGE: latex\nCODE:\n```\n\\textsf{Negative Predictive Value} = \\frac{\\textsf{True Negatives}}{\\textsf{All Predicted Negatives}} = \\frac{\\textsf{True Negatives}}{\\textsf{True Negatives} + \\textsf{False Negatives}}\n```\n\n----------------------------------------\n\nTITLE: Profiling Transaction Data Columns\nDESCRIPTION: This code profiles the 'memo', 'transaction_date', and 'amount' columns of the transaction datasets using Splink's exploratory functions. It helps in understanding the distribution and characteristics of the data.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/transactions.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.exploratory import profile_columns\n\ndb_api = DuckDBAPI()\nprofile_columns(\n    [df_origin, df_destination],\n    db_api=db_api,\n    column_expressions=[\n        \"memo\",\n        \"transaction_date\",\n        \"amount\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Synthetic Datasets in Python\nDESCRIPTION: Displays the synthetic base and comparison datasets created for bias analysis in data linking.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/bias_eval.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsynthetic_base_df\n```\n\nLANGUAGE: python\nCODE:\n```\nsynthetic_comparison_df\n```\n\n----------------------------------------\n\nTITLE: Importing Splink Libraries and Creating a Linker Object\nDESCRIPTION: This snippet demonstrates how to import necessary Splink modules, create a dataset, define comparison settings, and initialize a Linker object. It sets up the foundation for parameter estimation and visualization.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/parameter_estimate_comparisons_chart.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\nimport splink.comparison_template_library as ctl\nfrom splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets\n\ndf = splink_datasets.fake_1000\n\nsettings = SettingsCreator(\n    link_type=\"dedupe_only\",\n    comparisons=[\n        cl.JaroWinklerAtThresholds(\"first_name\", [0.9, 0.7]),\n        cl.JaroAtThresholds(\"surname\", [0.9, 0.7]),\n        ctl.DateComparison(\n            \"dob\",\n            input_is_string=True,\n            datetime_metrics=[\"year\", \"month\"],\n            datetime_thresholds=[1, 1],\n        ),\n        cl.ExactMatch(\"city\").configure(term_frequency_adjustments=True),\n        ctl.EmailComparison(\"email\"),\n    ],\n    blocking_rules_to_generate_predictions=[\n        block_on(\"first_name\"),\n        block_on(\"surname\"),\n    ],\n)\n\nlinker = Linker(df, settings, DuckDBAPI())\nlinker.training.estimate_u_using_random_sampling(max_pairs=1e6)\n```\n\n----------------------------------------\n\nTITLE: Estimating Match Probability Parameters\nDESCRIPTION: Estimates the probability of random record matches using deterministic matching rules with an assumed 70% recall rate.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndeterministic_rules = [\n    block_on(\"first_name\", \"dob\"),\n    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\",\n    block_on(\"email\")\n]\n\nlinker.training.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)\n```\n\n----------------------------------------\n\nTITLE: Installing Splink with PostgreSQL backend support\nDESCRIPTION: Command to install Splink with additional packages required for the PostgreSQL backend.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/getting_started.md#2025-04-16_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip install 'splink[postgres]'\n```\n\n----------------------------------------\n\nTITLE: Chart Template Path Reference\nDESCRIPTION: File path reference to the JSON template that defines the match weights waterfall chart structure.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/understanding_and_editing_charts.md#2025-04-16_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmatch_weights_waterfall.json\n```\n\n----------------------------------------\n\nTITLE: Cards Gallery Component - Comparison Helpers\nDESCRIPTION: JSON configuration for displaying comparison helper cards, including comparator scores and phonetic matching charts.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/index.md#2025-04-16_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n    \"title\": \"`comparator score chart`\",\n    \"image\": \"./img/comparator_score_chart.png\",\n    \"url\": \"../topic_guides/comparisons/choosing_comparators.ipynb#comparing-string-similarity-and-distance-scores\"\n    },\n    {\n    \"title\": \"`comparator score threshold chart`\",\n    \"image\": \"./img/comparator_score_threshold_chart.png\",\n    \"url\": \"../topic_guides/comparisons/choosing_comparators.ipynb#choosing-thresholds\"\n    },\n    {\n    \"title\": \"`phonetic match chart`\",\n    \"image\": \"./img/phonetic_match_chart.png\",\n    \"url\": \"../topic_guides/comparisons/choosing_comparators.ipynb#phonetic-matching\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: P4 Score Formula in LaTeX\nDESCRIPTION: Mathematical formula for calculating the P4 score as harmonic mean of four confusion matrix metrics\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/edge_metrics.md#2025-04-16_snippet_7\n\nLANGUAGE: latex\nCODE:\n```\n4\\left[\\frac{1}{\\textsf{Recall}}+\\frac{1}{\\textsf{Specificity}}+\\frac{1}{\\textsf{Precision}}+\\frac{1}{\\textsf{Negative Predictive Value}}\\right]^{-1}\n```\n\n----------------------------------------\n\nTITLE: Enabling Fast EM Training in Splink\nDESCRIPTION: Shows how to enable the faster Expectation-Maximization training option by setting the estimate_without_term_frequencies parameter to True. This can result in significant speed improvements during model training.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2023-07-27-feature_update.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nlinker.estimate_parameters_using_expectation_maximisation(..., estimate_without_term_frequencies=True)\n```\n\n----------------------------------------\n\nTITLE: Converting PostcodeComparison to Dictionary in Splink\nDESCRIPTION: Converts the postcode comparison to a dictionary representation that can be used as a basis for more customized comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npc_comparison.get_comparison(\"duckdb\").as_dict()\n```\n\n----------------------------------------\n\nTITLE: Loading and Configuring Splink Model\nDESCRIPTION: Loads model settings from a JSON file and configures blocking rules for predictions.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/07_Evaluation.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport urllib\n\nfrom splink import block_on\n\nurl = \"https://raw.githubusercontent.com/moj-analytical-services/splink/847e32508b1a9cdd7bcd2ca6c0a74e547fb69865/docs/demos/demo_settings/saved_model_from_demo.json\"\n\nwith urllib.request.urlopen(url) as u:\n    settings = json.loads(u.read().decode())\n\n# The data quality is very poor in this dataset, so we need looser blocking rules\n# to achieve decent recall\nsettings[\"blocking_rules_to_generate_predictions\"] = [\n    block_on(\"first_name\"),\n    block_on(\"city\"),\n    block_on(\"email\"),\n    block_on(\"dob\"),\n]\n\nlinker = Linker(df, settings, db_api=DuckDBAPI())\ndf_predictions = linker.inference.predict(threshold_match_probability=0.01)\n```\n\n----------------------------------------\n\nTITLE: Initializing EmailComparison in Splink\nDESCRIPTION: Creates a comparison for email address fields with appropriate string matching and domain-specific logic for email addresses.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport splink.comparison_library as cl\n\nemail_comparison = cl.EmailComparison(\"email\")\n```\n\n----------------------------------------\n\nTITLE: Setting up PostgreSQL Database for Splink\nDESCRIPTION: Commands to initialize, start a PostgreSQL database, create a database named 'splink_db', and set up a user for Splink development.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/development_quickstart.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ninitdb splink_db\npg_ctl -D splink_db start --wait -l ./splink_db_log\ncreatedb splink_db # The inner database\npsql -d splink_db <<SQL\n  CREATE USER splinkognito CREATEDB CREATEROLE password 'splink123!' ;\nSQL\n```\n\n----------------------------------------\n\nTITLE: Data Preparation Function Reference\nDESCRIPTION: Reference to the Python function that transforms raw records into the data format required for waterfall chart visualization.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/understanding_and_editing_charts.md#2025-04-16_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nrecords_to_waterfall_data\n```\n\n----------------------------------------\n\nTITLE: Implementing Backend-Agnostic Test Excluding SQLite in Python\nDESCRIPTION: This snippet demonstrates how to write a backend-agnostic test that excludes the SQLite dialect. It uses a decorator to mark the test function and specify which dialect to exclude.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom tests.decorator import mark_with_dialects_excluding\n\n@mark_with_dialects_excluding(\"sqlite\")\ndef test_feature_that_doesnt_work_with_sqlite(test_helpers, dialect, some_other_test_fixture):\n    helper = test_helpers[dialect]\n\n    df = helper.load_frame_from_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n\n    # and then some actual testing logic\n```\n\n----------------------------------------\n\nTITLE: SQL Pipeline CTE Structure Example\nDESCRIPTION: Demonstrates the basic structure of how multiple SQL select statements are combined into a single query using Common Table Expressions (CTEs). Shows how statements a, b, and c are pipelined together.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/caching.md#2025-04-16_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nwith\na as (a_sql),\nb as (b_sql),\nc_sql\n```\n\n----------------------------------------\n\nTITLE: Testing Soundex Phonetic Transformation with phonetics Package\nDESCRIPTION: This code snippet demonstrates how to use the phonetics package to apply the Soundex algorithm to compare two similar-sounding but differently spelled names. The example shows that 'Smith' and 'Smyth' produce identical Soundex codes (S5030).\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/phonetic.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport phonetics\nprint(phonetics.soundex(\"Smith\"), phonetics.soundex(\"Smyth\"))\n```\n\n----------------------------------------\n\nTITLE: Setting Up Debug Mode in Splink\nDESCRIPTION: Shows how to enable debug mode in Splink by setting the linker's _debug_mode attribute to True. This turns off pipelining and displays SQL statements and their results to help understand Splink's calculations.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/debug_modes.md#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nlinker._debug_mode = True\n```\n\n----------------------------------------\n\nTITLE: Converting DateOfBirthComparison to Dictionary in Splink\nDESCRIPTION: Converts the date of birth comparison to a dictionary representation that can be used as a basis for more customized comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndate_of_birth_comparison.get_comparison(\"duckdb\").as_dict()\n```\n\n----------------------------------------\n\nTITLE: Customizing Vega-Lite JSON Specification for Improved Chart Visualization\nDESCRIPTION: This JSON diff shows changes made to a Vega-Lite specification to improve a visualization. Changes include adding a proper title, improving color schemes (removing red-green for accessibility), adding text size encoding, adjusting chart dimensions, and improving axis labels and legends.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/understanding_and_editing_charts.md#2025-04-16_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n@@ -1,9 +1,8 @@\n{\n-  \"config\": {\n-    \"view\": {\n-      \"continuousWidth\": 400,\n-      \"continuousHeight\": 300\n-    }\n+  \"title\": {\n+    \"text\": \"Heatmaps of string comparison metrics\",\n+    \"anchor\": \"middle\",\n+    \"fontSize\": 16\n  },\n  \"hconcat\": [\n    {\n@@ -18,25 +17,32 @@\n                  0,\n                  1\n                ],\n-                \"range\": [\n-                  \"red\",\n-                  \"green\"\n-                ]\n+                \"scheme\": \"greenblue\"\n              },\n-              \"type\": \"quantitative\"\n+              \"type\": \"quantitative\",\n+              \"legend\": null\n            },\n            \"x\": {\n              \"field\": \"comparator\",\n-              \"type\": \"ordinal\"\n+              \"type\": \"ordinal\",\n+              \"title\": null\n            },\n            \"y\": {\n              \"field\": \"strings_to_compare\",\n-              \"type\": \"ordinal\"\n+              \"type\": \"ordinal\",\n+              \"title\": \"String comparison\",\n+              \"axis\": {\n+                \"titleFontSize\": 14\n+              }\n            }\n          },\n-          \"height\": 300,\n-          \"title\": \"Heatmap of Similarity Scores\",\n-          \"width\": 300\n+          \"title\": \"Similarity\",\n+          \"width\": {\n+            \"step\": 40\n+          },\n+          \"height\": {\n+            \"step\": 30\n+          }\n        },\n        {\n          \"mark\": {\n@@ -44,6 +50,16 @@\n            \"baseline\": \"middle\"\n          },\n          \"encoding\": {\n+            \"size\": {\n+              \"field\": \"score\",\n+              \"scale\": {\n+                \"range\": [\n+                  8,\n+                  14\n+                ]\n+              },\n+              \"legend\": null\n+            },\n            \"text\": {\n              \"field\": \"score\",\n              \"format\": \".2f\",\n@@ -51,7 +67,10 @@\n            },\n            \"x\": {\n              \"field\": \"comparator\",\n-              \"type\": \"ordinal\"\n+              \"type\": \"ordinal\",\n+              \"axis\": {\n+                \"labelFontSize\": 12\n+              }\n            },\n            \"y\": {\n              \"field\": \"strings_to_compare\",\n@@ -72,29 +91,33 @@\n            \"color\": {\n              \"field\": \"score\",\n              \"scale\": {\n-                \"domain\": [\n-                  0,\n-                  5\n-                ],\n-                \"range\": [\n-                  \"green\",\n-                  \"red\"\n-                ]\n+                \"scheme\": \"yelloworangered\",\n+                \"reverse\": true\n```\n\n----------------------------------------\n\nTITLE: Locking and Installing Dependencies with Poetry in Shell\nDESCRIPTION: These commands demonstrate how to lock the project dependencies and install them using Poetry. It includes options for installing optional dependencies like Spark support.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/building_env_locally.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npoetry lock\n```\n\nLANGUAGE: sh\nCODE:\n```\npoetry install\n```\n\nLANGUAGE: sh\nCODE:\n```\npoetry install -E spark\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Tearing Down PostgreSQL Docker Environment for Splink Testing\nDESCRIPTION: Bash commands to set up and tear down a Docker environment for PostgreSQL, used for Splink testing and development.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/backends/postgres.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/postgres_docker/setup.sh\n```\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/postgres_docker/teardown.sh\n```\n\n----------------------------------------\n\nTITLE: Implementing Backend-Specific Test for DuckDB in Python\nDESCRIPTION: This snippet shows how to write a backend-specific test for the DuckDB dialect using the mark_with_dialects_including decorator.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@mark_with_dialects_including(\"duckdb\")\ndef test_some_specific_duckdb_feature():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Cards Gallery Component - Exploratory Analysis\nDESCRIPTION: JSON configuration for displaying exploratory analysis cards, including profile columns and completeness charts.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/index.md#2025-04-16_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"title\": \"`profile columns`\",\n    \"image\": \"./img/profile_columns.png\",\n    \"url\": \"./profile_columns.ipynb\"\n  },\n  {\n    \"title\": \"`completeness chart`\",\n    \"image\": \"./img/completeness_chart.png\",\n    \"url\": \"./completeness_chart.ipynb\"\n  }]\n```\n\n----------------------------------------\n\nTITLE: Converting NameComparison to Dictionary in Splink\nDESCRIPTION: Converts the surname comparison to a dictionary representation that can be used as a basis for more customized comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsurname_comparison.get_comparison(\"duckdb\").as_dict()\n```\n\n----------------------------------------\n\nTITLE: Using Jaccard Similarity UDF in Spark SQL\nDESCRIPTION: Example of using a registered Jaccard similarity UDF in Spark SQL. This UDF compares two name columns and returns true if the Jaccard similarity is greater than or equal to 0.9.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/udfs.md#2025-04-16_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\njaccard(\"name_column_1\", \"name_column_2\") >= 0.9\n```\n\n----------------------------------------\n\nTITLE: Running ruff linter and formatter for Splink project\nDESCRIPTION: Commands to run the ruff formatter and linter in the Splink project using Poetry. The formatter applies consistent code style, while the linter checks for errors and code quality issues.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/lint_and_format.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npoetry run ruff format\npoetry run ruff check .\n```\n\n----------------------------------------\n\nTITLE: Converting EmailComparison to Dictionary in Splink\nDESCRIPTION: Converts the email comparison to a dictionary representation that can be used as a basis for more customized comparisons.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nemail_comparison.as_dict()\n```\n\n----------------------------------------\n\nTITLE: Installing Splink Package\nDESCRIPTION: Installs Splink version 4.0 from the conda-forge channel using conda package manager.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/demo_settings/model_create_h50k.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#!conda install -c conda-forge splink=4.0 --yes\n```\n\n----------------------------------------\n\nTITLE: Using Named On-Disk Database with DuckDB in Splink\nDESCRIPTION: Example of connecting to a named on-disk database file with DuckDB and using it with Splink. This approach helps manage memory usage by persisting data to disk.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/performance/optimising_duckdb.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncon = duckdb.connect(database='my-db.duckdb')\nlinker = Linker(\n    df, settings, DuckDBAPI(connection=con)\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Backend-Specific Test for SQLite in Python\nDESCRIPTION: This snippet shows how to write a backend-specific test for the SQLite dialect using the mark_with_dialects_including decorator.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n@mark_with_dialects_including(\"sqlite\")\ndef test_some_specific_sqlite_feature():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Starting PostgreSQL Docker Container for Splink\nDESCRIPTION: Command to start a PostgreSQL server using Docker for Splink development.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/development_quickstart.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/postgres_docker/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Adding Tooltip to Splink Altair Chart\nDESCRIPTION: This snippet enhances the Altair chart by adding a tooltip with detailed information about each comparison level, improving the interactivity of the visualization.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nalt.Chart(df).mark_bar().encode(\n    y=alt.Y(\"cl_id\",\n            sort=\"-x\",\n            title=\"Comparison level\"\n            ),\n    x=alt.X(\"log2_bayes_factor\",\n            title=\"Comparison level match weight = log2(m/u)\",\n            scale=alt.Scale(domain=[-10, 10])\n            ),\n    color=\"comparison_name\",\n    tooltip=[\n        \"comparison_name\",\n        \"label_for_charts\",\n        \"sql_condition\",\n        \"m_probability\",\n        \"u_probability\",\n        \"bayes_factor\",\n        \"log2_bayes_factor\"\n        ]\n).properties(\n    title=\"New Chart - WOO!\"\n).configure_view(\n    step=15\n)\n```\n\n----------------------------------------\n\nTITLE: Examining Phonetic Transformations for Another String\nDESCRIPTION: Demonstrates phonetic transformations for another string to compare with the previous example, showing how different names may be encoded similarly or differently.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsa.phonetic_transform(\"Steven\")\n```\n\n----------------------------------------\n\nTITLE: Adding Packages with Poetry in Shell\nDESCRIPTION: These commands demonstrate how to add new packages to the Splink project using Poetry. They include options for specifying package versions.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/building_env_locally.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npoetry add <package-name>\n```\n\nLANGUAGE: sh\nCODE:\n```\npoetry add <package-name>==<version>\n```\n\nLANGUAGE: sh\nCODE:\n```\npoetry add \"<package-name> >= <version>\"\n```\n\n----------------------------------------\n\nTITLE: Processing Benchmark Data for Splink Comparison Functions\nDESCRIPTION: This snippet loads benchmark data from a JSON file, processes it into a pandas DataFrame, and calculates performance metrics for different comparison functions in Splink. It prepares the data for visualization and analysis.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/performance/performance_of_comparison_functions.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport json\npd.options.display.max_colwidth = 10009\npath = \".benchmarks/Darwin-CPython-3.11-64bit/0006_b5b7ee569dab10ff304d1123984a2f446917fe9e_20241205_124128.json\"\n\n# Load the JSON data from the file\nwith open(path, 'r') as file:\n    data = json.load(file)\n\n# Extract the benchmark statistics\nbenchmarks = data['benchmarks']\n\n# Create a DataFrame from the benchmark statistics\ndf = pd.DataFrame([{\n    'name': benchmark['name'],\n    'rounds': benchmark['stats']['rounds'],\n    'median': benchmark['stats']['median'],\n\n    'iterations': benchmark['stats']['iterations']\n} for benchmark in benchmarks])\n\ndf['comparison_type'] = df['name'].str.extract(r'\\[(.*?)-(?:duckdb|spark)\\]')[0]\ndf['backend'] = df['name'].str.extract(r'-(duckdb|spark)\\]')[0]\n\n\ndf['name'] = df['name'].str.replace(r'test_comparison_execution_\\w+\\[.*?\\]', '', regex=True)\n\ndf = df.drop('name', axis=1)\n\n# Get exact match times for each backend\nexact_match_times = df[df['comparison_type'] == 'Exact Match'].set_index('backend')['median']\n\n# Calculate multiples using merge and division\ndf['multiple_of_exact_match'] = df.apply(\n    lambda x: x['median'] / exact_match_times[x['backend']],\n    axis=1\n)\n\ndf['comparison_type'] = df['comparison_type'].apply(\n    lambda x: f\"{x}*\" if x == 'Cosine Similarity Level' else x\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Sample Dataset\nDESCRIPTION: Imports and loads fake sample data from splink_datasets module for tutorial purposes.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Begin by reading in the tutorial data again\nfrom splink import splink_datasets\n\ndf = splink_datasets.fake_1000\n```\n\n----------------------------------------\n\nTITLE: Implementing Backend-Specific Test for Apache Spark in Python\nDESCRIPTION: This snippet demonstrates how to write a backend-specific test for the Apache Spark dialect using the mark_with_dialects_including decorator.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n@mark_with_dialects_including(\"spark\")\ndef test_some_specific_spark_feature():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Spark DataFrame Conversion Example\nDESCRIPTION: Shows conversion of small result sets to Pandas DataFrames instead of saving to disk, used for efficiency with smaller tables like __splink__m_u_counts.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/spark_pipelining_and_caching.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntoPandas()\n```\n\n----------------------------------------\n\nTITLE: Configuring In-Memory DuckDB with Disk Spilling in Splink\nDESCRIPTION: Example of setting up an in-memory DuckDB connection that can spill to disk when needed. This approach maintains performance while providing overflow capacity for large operations.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/performance/optimising_duckdb.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncon = duckdb.connect(\":memory:\")\n\ncon.execute(\"SET temp_directory='/path/to/temp';\")\nlinker = Linker(\n    df, settings, DuckDBAPI(connection=con)\n)\n```\n\n----------------------------------------\n\nTITLE: Automated Conda Environment Setup for Splink\nDESCRIPTION: Script to automatically set up a conda environment with all dependencies needed for Splink development.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/development_quickstart.md#2025-04-16_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n./scripts/conda/development_setup_with_conda.sh\n```\n\n----------------------------------------\n\nTITLE: Using comparator_score Function to Compare String Similarity\nDESCRIPTION: Demonstrates how to use the comparator_score function from Splink's similarity_analysis module to compare two strings and get scores from all available comparators.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.exploratory import similarity_analysis as sa\n\nsa.comparator_score(\"Richard\", \"iRchard\")\n```\n\n----------------------------------------\n\nTITLE: Installing Splink with pip\nDESCRIPTION: Command to install the latest version of Splink from PyPI using pip. Splink supports Python 3.8+.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/getting_started.md#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install splink\n```\n\n----------------------------------------\n\nTITLE: Basic Chart Generation with Splink's Blocking Rules\nDESCRIPTION: This snippet demonstrates the basic usage of the chart function, showing the chart output. This is a placeholder showing that the chart variable would be displayed.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/cumulative_comparisons_to_be_scored_from_blocking_rules_chart.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nchart\n```\n\n----------------------------------------\n\nTITLE: Visualizing Splink Model Match Weights in Python\nDESCRIPTION: Generates a match weights chart to visualize the relative importance of different data fields in the Splink linking model.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/bias_eval.ipynb#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.match_weights_chart()\n```\n\n----------------------------------------\n\nTITLE: Running Splink tests for specific backend with options\nDESCRIPTION: Command to run tests for training probability_two_random_records_match on DuckDB backend with specific pytest options.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npytest -W ignore -q -x -m duckdb tests/test_estimate_prob_two_rr_match.py\n```\n\n----------------------------------------\n\nTITLE: Installing Splink with Athena backend support\nDESCRIPTION: Command to install Splink with additional packages required for the AWS Athena backend.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/getting_started.md#2025-04-16_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip install 'splink[athena]'\n```\n\n----------------------------------------\n\nTITLE: Installing Splink Package\nDESCRIPTION: Optional installation of Splink package for Google Colab environment.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/06_Visualising_predictions.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment and run this cell if you're running in Google Colab.\n# !pip install splink\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for Splink\nDESCRIPTION: Command to create a conda environment for Splink development using a predefined environment file.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/development_quickstart.md#2025-04-16_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nmamba env create -n splink --file ./scripts/conda/development_environment.yaml\n```\n\n----------------------------------------\n\nTITLE: Generated SQL Query for Salted Blocking Rules in Splink\nDESCRIPTION: This SQL query is generated by Splink to implement the salted blocking rules. It demonstrates how the data is partitioned and joined based on the specified blocking rules and salting configuration.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/performance/salting.md#2025-04-16_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '0' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.dob = r.dob\nWHERE\n  l.unique_id < r.unique_id\nUNION ALL\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '1' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.first_name = r.first_name\n  AND CEIL(l.__splink_salt * 4) = 1\n  AND NOT (\n    COALESCE((\n        l.dob = r.dob\n    ), FALSE)\n  )\nWHERE\n  l.unique_id < r.unique_id\nUNION ALL\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '1' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.first_name = r.first_name\n  AND CEIL(l.__splink_salt * 4) = 2\n  AND NOT (\n    COALESCE((\n        l.dob = r.dob\n    ), FALSE)\n  )\nWHERE\n  l.unique_id < r.unique_id\nUNION ALL\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '1' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.first_name = r.first_name\n  AND CEIL(l.__splink_salt * 4) = 3\n  AND NOT (\n    COALESCE((\n        l.dob = r.dob\n    ), FALSE)\n  )\nWHERE\n  l.unique_id < r.unique_id\nUNION ALL\nSELECT\n  l.unique_id AS unique_id_l,\n  r.unique_id AS unique_id_r,\n  l.first_name AS first_name_l,\n  r.first_name AS first_name_r,\n  l.surname AS surname_l,\n  r.surname AS surname_r,\n  l.dob AS dob_l,\n  r.dob AS dob_r,\n  l.city AS city_l,\n  r.city AS city_r,\n  l.tf_city AS tf_city_l,\n  r.tf_city AS tf_city_r,\n  l.email AS email_l,\n  r.email AS email_r,\n  l.`group` AS `group_l`,\n  r.`group` AS `group_r`,\n  '1' AS match_key\nFROM __splink__df_concat_with_tf AS l\nINNER JOIN __splink__df_concat_with_tf AS r\n  ON l.first_name = r.first_name\n  AND CEIL(l.__splink_salt * 4) = 4\n  AND NOT (\n    COALESCE((\n        l.dob = r.dob\n    ), FALSE)\n  )\nWHERE\n  l.unique_id < r.unique_id\n```\n\n----------------------------------------\n\nTITLE: SQL Query Example\nDESCRIPTION: Demonstrates how to query the prediction results using SQL with a limit of 2 rows.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/05_Predicting_results.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsql = f\"\"\"\nselect *\nfrom {df_predictions.physical_name}\nlimit 2\n\"\"\"\nlinker.misc.query_sql(sql)\n```\n\n----------------------------------------\n\nTITLE: Displaying Cluster Studio Dashboard in IPython\nDESCRIPTION: A simple code snippet that displays the cluster studio dashboard HTML file in an IPython notebook using IFrame.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/cluster_studio_dashboard.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import IFrame\nIFrame(src=\"./img/cluster_studio.html\", width=\"100%\", height=1000)\n```\n\n----------------------------------------\n\nTITLE: Configuring AthenaAPI Setup\nDESCRIPTION: Sets up the AthenaAPI with bucket, database, and filepath configurations for Splink integration\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.backends.athena import AthenaAPI\n\n\nbucket = \"MYTESTBUCKET\"\ndatabase = \"MYTESTDATABASE\"\nfilepath = \"MYTESTFILEPATH\"  # file path inside of your bucket\n\naws_filepath = f\"s3://{bucket}/{filepath}\"\ndb_api = AthenaAPI(\n    boto3_session,\n    output_bucket=bucket,\n    output_database=database,\n    output_filepath=filepath,\n)\n```\n\n----------------------------------------\n\nTITLE: Example Match Weight Calculation\nDESCRIPTION: This equation demonstrates a practical example of calculating the total match weight by adding the prior weight and individual feature weights with actual values.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/theory/fellegi_sunter.md#2025-04-16_snippet_4\n\nLANGUAGE: latex\nCODE:\n```\n\\begin{equation}\n\\begin{aligned}\n    M_\\textsf{obs} &= M_\\textsf{prior} + M_\\textsf{forename} + M_\\textsf{surname} + M_\\textsf{dob} + M_\\textsf{city} + M_\\textsf{email} \\\\[10pt]\n     &= -6.67 + 4.74 + 6.49 - 1.97 - 1.12 + 8.00 \\\\[10pt]\n     &= 9.48\n\\end{aligned}\n\\end{equation}\n```\n\n----------------------------------------\n\nTITLE: Running pytest with ignore warnings and output display\nDESCRIPTION: Command to run pytest while ignoring warnings and displaying test output in the terminal.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npytest -W ignore -s tests/\n```\n\n----------------------------------------\n\nTITLE: Installing Splink with conda\nDESCRIPTION: Alternative command to install Splink using conda from the conda-forge channel.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/getting_started.md#2025-04-16_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nconda install -c conda-forge splink\n```\n\n----------------------------------------\n\nTITLE: Installing Poetry for Splink Development\nDESCRIPTION: Commands to install Poetry version 1.4.2 globally as a package manager for Splink development.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/development_quickstart.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install --upgrade pip\npip install poetry==1.4.2\n```\n\n----------------------------------------\n\nTITLE: Adding Packages with Poetry in Splink\nDESCRIPTION: Commands for adding new packages to the Splink project using Poetry. Demonstrates how to add a package with or without version constraints.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/managing_dependencies_with_poetry.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npoetry add <package-name>\n```\n\nLANGUAGE: sh\nCODE:\n```\npoetry add <package-name>==<version>\n# Add quotes if you want to use other equality calls\npoetry add \"<package-name> >= <version>\"\n```\n\n----------------------------------------\n\nTITLE: Complete Dataset Profiling\nDESCRIPTION: Shows how to profile all columns in a dataset using the profile_columns function with default parameters.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/profile_columns.ipynb#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import splink_datasets, DuckDBAPI\nfrom splink.exploratory import profile_columns\n\ndf = splink_datasets.historical_50k\nprofile_columns(df, db_api=DuckDBAPI())\n```\n\n----------------------------------------\n\nTITLE: Installing Splink with Spark backend support\nDESCRIPTION: Command to install Splink with additional packages required for the Apache Spark backend.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/getting_started.md#2025-04-16_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install 'splink[spark]'\n```\n\n----------------------------------------\n\nTITLE: Running Splink tests using Docker\nDESCRIPTION: Commands to build a Docker image and run Splink tests within a container, useful for testing against specific Python versions.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ndocker build -t run_tests:testing -f scripts/run_tests.Dockerfile . && docker run --rm --name splink-test run_tests:testing\n```\n\n----------------------------------------\n\nTITLE: Generating Data for Splink Chart\nDESCRIPTION: This code extracts relevant data from the Linker object and prepares it for chart creation by selecting specific columns and filtering records.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Take linker object and extract complete settings dict\nrecords = linker._settings_obj._parameters_as_detailed_records\n\ncols_to_keep = [\n    \"comparison_name\",\n    \"sql_condition\",\n    \"label_for_charts\",\n    \"m_probability\",\n    \"u_probability\",\n    \"bayes_factor\",\n    \"log2_bayes_factor\",\n    \"comparison_vector_value\"\n]\n\n# Keep useful information for a match weights chart\nrecords = [{k: r[k] for k in cols_to_keep}\n           for r in records\n           if r[\"comparison_vector_value\"] != -1 and r[\"comparison_sort_order\"] != -1]\n\nrecords[:3]\n```\n\n----------------------------------------\n\nTITLE: Defining SQL Dialect Options in Markdown\nDESCRIPTION: This snippet defines the 'sql_dialect' parameter, its default value, and provides examples of valid SQL dialects. It specifies that the dialect must be a valid SQLGlot dialect.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/settings_dict_guide.md#2025-04-16_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## `sql_dialect`\n\nThe SQL dialect in which `sql_conditions` are written.  Must be a valid SQLGlot dialect\n\n**Default value**: `None`\n\n**Examples**: `['spark', 'duckdb', 'presto', 'sqlite']`\n\n<hr>\n```\n\n----------------------------------------\n\nTITLE: Installing Splink in Google Colab\nDESCRIPTION: Optional setup code to install Splink package when running in Google Colab environment.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/business_rates_match.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment and run this cell if you're running in Google Colab.\n# !pip install splink\n```\n\n----------------------------------------\n\nTITLE: Modifying Packages with Poetry in Shell\nDESCRIPTION: These commands show how to remove packages, update existing packages to specific versions, and update packages to their latest versions using Poetry.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/building_env_locally.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npoetry remove <package-name>\n```\n\nLANGUAGE: sh\nCODE:\n```\npoetry add <package-name>==<version>\n```\n\nLANGUAGE: sh\nCODE:\n```\npoetry add \"<package-name> >= <version>\"\n```\n\nLANGUAGE: sh\nCODE:\n```\npoetry add <package-name>==<version>\n```\n\nLANGUAGE: sh\nCODE:\n```\npoetry update <package-name>\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Poetry in Splink\nDESCRIPTION: Commands for installing project dependencies, including options for installing with extras and optional dependencies.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/managing_dependencies_with_poetry.md#2025-04-16_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npoetry install\n```\n\nLANGUAGE: sh\nCODE:\n```\npoetry install -E spark\n```\n\nLANGUAGE: sh\nCODE:\n```\npoetry install --with dev --with linting --with testing --with benchmarking --with typechecking --with demos --all-extras\n```\n\n----------------------------------------\n\nTITLE: Viewing PostcodeComparison Structure in Splink\nDESCRIPTION: Displays the human-readable description of the postcode comparison structure for the DuckDB backend, showing how postcodes are compared.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/out_of_the_box_comparisons.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(pc_comparison.get_comparison(\"duckdb\").human_readable_description)\n```\n\n----------------------------------------\n\nTITLE: Displaying Term Frequency Adjustment Chart in Python\nDESCRIPTION: This code snippet shows how to display the term frequency adjustment chart generated by the tf_adjustment_chart function. It assumes the chart object has already been created.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/tf_adjustment_chart.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nchart\n```\n\n----------------------------------------\n\nTITLE: Running pytest with verbose output and fail-fast\nDESCRIPTION: Command to run pytest with verbose output, ignoring warnings, and stopping on the first error or failure.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npytest -W ignore -v -x tests/\n```\n\n----------------------------------------\n\nTITLE: Exporting Altair Chart to Vega-Lite JSON\nDESCRIPTION: This code exports an Altair chart to its underlying Vega-Lite JSON representation. This is useful when reaching Altair's limitations, allowing manual editing of the Vega-Lite specification directly.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nchart.to_json()\n```\n\n----------------------------------------\n\nTITLE: Legacy Blocking Rule Implementation - Python\nDESCRIPTION: Shows the previous verbose method of implementing blocking rules using exact_match_rule and logical operators.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2023-12-06-feature_update.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport splink.duckdb.blocking_rule_library as brl\nbrl.and_(\n  brl.exact_match_rule(\"first_name\"),\n  brl.exact_match_rule(\"surname\")\n)\n```\n\n----------------------------------------\n\nTITLE: Python MkDocs Configuration Block\nDESCRIPTION: MkDocs configuration block for rendering the splink.comparison_level_library documentation with specific display options and filters.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/comparison_level_library.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n::: splink.comparison_level_library\n    handler: python\n    options:\n      show_root_heading: false\n      show_root_toc: false\n      show_source: false\n      members_order: source\n      inherited_members: false\n      merge_init_into_class: true\n      filters:\n        - \"!^create_sql$\"\n```\n\n----------------------------------------\n\nTITLE: Installing Splink in Google Colab (Python)\nDESCRIPTION: This snippet shows how to install Splink in a Google Colab environment. The code is commented out and needs to be uncommented to run.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/deterministic_dedupe.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment and run this cell if you're running in Google Colab.\n# !pip install splink\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environments with Shell Script\nDESCRIPTION: These commands show how to use the create_venv.sh script to automate the creation of virtual environments for Splink. It includes options for using the default environment name or specifying a custom name.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/building_env_locally.md#2025-04-16_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nsource scripts/create_venv.sh\n```\n\nLANGUAGE: sh\nCODE:\n```\nsource scripts/create_venv.sh <name_of_venv>\n```\n\n----------------------------------------\n\nTITLE: Locking Dependencies with Poetry in Splink\nDESCRIPTION: Command for updating the poetry.lock file to ensure consistent dependency installation across different environments.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/managing_dependencies_with_poetry.md#2025-04-16_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npoetry lock\n```\n\n----------------------------------------\n\nTITLE: Visualizing Parameter Estimate Comparisons in Splink using Python\nDESCRIPTION: This code shows how to generate a visualization comparing different parameter estimates in Splink. This is useful for checking consistency of estimates across different training rounds and can help identify potential convergence issues.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/training/training_rationale.md#2025-04-16_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.parameter_estimate_comparisons_chart()\n```\n\n----------------------------------------\n\nTITLE: Simplified Vega-Lite Data Structure\nDESCRIPTION: Shows the simplified data structure for charts requiring only a single dataset.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n\"data\": {\"values\": [...]}\n```\n\n----------------------------------------\n\nTITLE: Running a single pytest function in Splink\nDESCRIPTION: Command to run a specific test function within a test file in Splink using pytest and poetry.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npoetry run pytest tests/test_u_train.py::test_u_train_multilink\n```\n\n----------------------------------------\n\nTITLE: Using Jaro Similarity with DuckDB in Python\nDESCRIPTION: Demonstrates calculating the Jaro similarity score between two strings using DuckDB in Python. Jaro similarity measures the number and order of matching characters and the number of transpositions needed, returning a value between 0 and 1 where higher values indicate more similarity.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/comparators.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nduckdb.sql(\"SELECT jaro_similarity('MARTHA', 'MARHTA')\").df().iloc[0,0]\n```\n\n----------------------------------------\n\nTITLE: Joining Tables with Unique Identifiers in SQL\nDESCRIPTION: This SQL snippet demonstrates how to perform a simple inner join between two tables (A and B) using a unique identifier (UID) in an ideal scenario where such identifiers exist for all records.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/theory/record_linkage.md#2025-04-16_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *\nFROM A\nINNER JOIN B\nON A.UID = B.UID\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Colab\nDESCRIPTION: Commented-out installation commands for setting up Splink in Google Colab, including installing ipywidgets, splink, and enabling widget extensions.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb_no_test/comparison_playground.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment and run this cell if you're running in Google Colab.\n# !pip install ipywidgets\n# !pip install splink\n# !jupyter nbextension enable --py widgetsnbextension\n```\n\n----------------------------------------\n\nTITLE: Fixing SSL Certificate Verification Error in Python\nDESCRIPTION: Workaround for SSL certificate verification errors when downloading datasets. This snippet disables SSL certificate verification by setting the default HTTPS context to an unverified one.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/datasets.md#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nssl._create_default_https_context = ssl._create_unverified_context\n```\n\n----------------------------------------\n\nTITLE: Running Local Documentation Build Script in Shell\nDESCRIPTION: This snippet shows how to execute a shell script that builds the Splink documentation locally. The script should be run outside the Poetry virtual environment for faster builds and immediate visibility of changes.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/contributing_to_docs.md#2025-04-16_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nsource scripts/make_docs_locally.sh\n```\n\n----------------------------------------\n\nTITLE: Date of Birth Distribution Analysis\nDESCRIPTION: Examines the distribution of date of birth values to identify potential data quality issues and patterns.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/charts/profile_columns.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprofile_columns(df, column_expressions=\"dob\", db_api=DuckDBAPI())\n```\n\n----------------------------------------\n\nTITLE: Example Notebooks Documentation Structure\nDESCRIPTION: Markdown structure defining example notebooks organization by database type (DuckDB, PySpark, Athena, SQLite) with links to Colab notebooks demonstrating different record linkage scenarios.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/examples_index.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nhide:\n  - toc\ntags:\n  - Examples\n  - DuckDB\n  - Spark\n  - Athena\n---\n\n# Example Notebooks\n\nThis section provides a series of examples to help you get started with Splink. You can find the underlying notebooks in the [demos folder](https://github.com/moj-analytical-services/splink/tree/master/docs/demos/examples) of the Splink repository.\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-release Splink 4\nDESCRIPTION: Command to install the pre-release version of Splink 4 using pip package manager.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2024-03-19-splink4.md#2025-04-16_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --pre splink\n```\n\n----------------------------------------\n\nTITLE: Using InvalidCols for Column Validation Logging\nDESCRIPTION: Shows how to use the InvalidCols named tuple to generate log strings for invalid column errors in Splink's validation system.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/settings_validation/extending_settings_validator.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Store our invalid columns\nmy_invalid_cols = MissingColumnsLogGenerator([\"first_col\", \"second_col\"])\n# Construct the corresponding log string\nmy_invalid_cols.construct_log_string()\n```\n\n----------------------------------------\n\nTITLE: Example Probability Calculation from Match Weight\nDESCRIPTION: This formula demonstrates calculating the match probability from a given match weight using the example where the total match weight is 9.48.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/theory/fellegi_sunter.md#2025-04-16_snippet_6\n\nLANGUAGE: latex\nCODE:\n```\nPr(\\textsf{Match | Observation}) = \\frac{2^{9.48}}{1+2^{9.48}} \\approx 0.999\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages in Google Colab for Splink\nDESCRIPTION: Installation commands for the required packages when running in Google Colab environment. This installs Splink, ipywidgets, and enables the widget extension.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/real_time_record_linkage.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment and run this cell if you're running in Google Colab.\n# !pip install ipywidgets\n# !pip install splink\n# !jupyter nbextension enable --py widgetsnbextension\n```\n\n----------------------------------------\n\nTITLE: Running Spellchecker Script in Shell\nDESCRIPTION: This shell command runs a custom spellchecker script on either a single markdown file or a folder of markdown files. It uses PySpelling and Aspell to check for spelling errors in the Splink documentation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/contributing_to_docs.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./scripts/pyspelling/spellchecker.sh <path_to_file_or_folder>\n```\n\n----------------------------------------\n\nTITLE: Running a single pytest file in Splink\nDESCRIPTION: Command to run a specific test file in Splink using pytest and poetry.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npoetry run pytest tests/test_u_train.py\n```\n\n----------------------------------------\n\nTITLE: Breaking Lineage with Parquet in SparkAPI for Python\nDESCRIPTION: This code snippet shows how to configure the SparkAPI to break lineage using the parquet method. It sets the break_lineage_method parameter to 'parquet' and specifies the number of partitions for repartitioning.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/performance/optimising_spark.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink import SparkAPI\n\ndb_api = SparkAPI(\n    spark_session=spark,\n    break_lineage_method=\"parquet\",\n    num_partitions_on_repartition=80,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Splink Package in Google Colab\nDESCRIPTION: Optional installation command for Splink package when running the notebook in Google Colab environment.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/quick_and_dirty_persons.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment and run this cell if you're running in Google Colab.\n# !pip install splink\n```\n\n----------------------------------------\n\nTITLE: Splink Spark Backend Installation - Bash\nDESCRIPTION: Command to install Splink with only Spark-specific dependencies using pip.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2023-12-06-feature_update.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install 'splink[spark]'\n```\n\n----------------------------------------\n\nTITLE: Importing Comparison Libraries in Splink with Simplified Syntax\nDESCRIPTION: Shows the new, more concise way to import comparison libraries in Splink without the previously required backend name duplication.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2023-07-27-feature_update.md#2025-04-16_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport splink.duckdb.comparison_level_library as cll\n```\n\n----------------------------------------\n\nTITLE: Matthews Correlation Coefficient Formula in LaTeX\nDESCRIPTION: Mathematical formula for calculating the Matthews Correlation Coefficient\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/edge_metrics.md#2025-04-16_snippet_8\n\nLANGUAGE: latex\nCODE:\n```\n\\phi = \\sqrt{\\textsf{Recall} \\cdot \\textsf{Specificity} \\cdot \\textsf{Precision} \\cdot \\textsf{Negative Predictive Value}} - \\sqrt{(1 - \\textsf{Recall})(1 - \\textsf{Specificity})(1 - \\textsf{Precision})(1 - \\textsf{Negative Predictive Value})}\n```\n\n----------------------------------------\n\nTITLE: Creating a Conda Environment using an explicit specification file\nDESCRIPTION: This is a conda environment specification file that lists all required packages with their exact versions and download URLs. It's designed to create a reproducible environment for the Splink project on Linux-64 platforms.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/scripts/conda/development_environment_lock_Linux-x86_64.txt#2025-04-16_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# This file may be used to create an environment using:\n# $ conda create --name <env> --file <this file>\n# platform: linux-64\n@EXPLICIT\n```\n\n----------------------------------------\n\nTITLE: Excluding Content in MkDocs YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates how to exclude certain directories from the MkDocs build process. This can be used to speed up builds when working on specific sections of the documentation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/contributing_to_docs.md#2025-04-16_snippet_1\n\nLANGUAGE: yml\nCODE:\n```\nexclude_docs: |\n  dev_guides/**\n  charts/**\n  topic_guides/**\n  demos/**\n  blog/**\n```\n\n----------------------------------------\n\nTITLE: Running Splink tests for DuckDB backend\nDESCRIPTION: Command to run all DuckDB tests and core tests in Splink.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npoetry run pytest tests/ -m duckdb\n```\n\n----------------------------------------\n\nTITLE: Installing Splink Package in Google Colab\nDESCRIPTION: Installation command for the Splink package in Google Colab environment, commented out for optional use.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/pairwise_labels.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment and run this cell if you're running in Google Colab.\n# !pip install splink\n```\n\n----------------------------------------\n\nTITLE: Installing Miniforge for Splink Development\nDESCRIPTION: Commands to install Miniforge, a conda distribution that includes the mamba package manager for faster dependency installation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/development_quickstart.md#2025-04-16_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh -b\n```\n\n----------------------------------------\n\nTITLE: Configuring MkDocs for Python Module Documentation in Splink\nDESCRIPTION: This code snippet configures MkDocs to render the documentation for the blocking_rule_library module in Splink. It specifies options for displaying the module's contents, including hiding the root heading and table of contents, not showing source code, ordering members by source, and merging __init__ into the class documentation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/blocking.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n::: splink.blocking_rule_library\n    handler: python\n    options:\n      show_root_heading: false\n      show_root_toc: false\n      show_source: false\n      members_order: source\n      inherited_members: false\n      merge_init_into_class: true\n```\n\n----------------------------------------\n\nTITLE: Basic Vega-Lite Data Structure\nDESCRIPTION: Shows the typical data structure pattern used in Vega-Lite chart specifications with named datasets.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/charts/building_charts.ipynb#2025-04-16_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n\"data\": {\"name\": \"data-a6c84a9cf1a0c7a2cd30cc1a0e2c1185\"},\n\"datasets\": {\n  \"data-a6c84a9cf1a0c7a2cd30cc1a0e2c1185\": [\n    ...\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Recall Formula in LaTeX\nDESCRIPTION: Mathematical formula for calculating recall (true positive rate)\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/edge_metrics.md#2025-04-16_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n\\textsf{Recall} = \\frac{\\textsf{True Positives}}{\\textsf{All Positives}} = \\frac{\\textsf{True Positives}}{\\textsf{True Positives} + \\textsf{False Negatives}}\n```\n\n----------------------------------------\n\nTITLE: F-Score Formula in LaTeX\nDESCRIPTION: Mathematical formula for calculating the F-score with weight parameter beta\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/edge_metrics.md#2025-04-16_snippet_5\n\nLANGUAGE: latex\nCODE:\n```\nF_{\\beta} = \\frac{(1 + \\beta^2) \\cdot \\textsf{Precision} \\cdot \\textsf{Recall}}{\\beta^2 \\cdot \\textsf{Precision} + \\textsf{Recall}}\n```\n\n----------------------------------------\n\nTITLE: Configuring API Documentation YAML\nDESCRIPTION: YAML configuration block for documenting Splink Linker visualization methods, specifying filters and display options for the API documentation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/visualisations.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntags:\n  - API\n  - Visualisations\n\n::: splink.internals.linker_components.visualisations.LinkerVisualisations\n    handler: python\n    filters:\n      - \"!^__init__$\"\n    options:\n      show_root_heading: false\n      show_source: false\n      members_order: source\n```\n\n----------------------------------------\n\nTITLE: Importing Backend-Agnostic Test Decorator in Python\nDESCRIPTION: This snippet shows how to import the decorator factory used for marking backend-agnostic tests.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom tests.decorator import mark_with_dialects_excluding\n```\n\n----------------------------------------\n\nTITLE: Installing Java on Ubuntu for Splink\nDESCRIPTION: Command to install Java 11 on Ubuntu which is required for Splink's Spark backend.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/development_quickstart.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt install openjdk-11-jre-headless\n```\n\n----------------------------------------\n\nTITLE: Visualizing Match Weights Distribution\nDESCRIPTION: Generates a chart showing the distribution of match weights, which helps in understanding the model's discrimination power.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/pairwise_labels.ipynb#2025-04-16_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlinker.visualisations.match_weights_chart()\n```\n\n----------------------------------------\n\nTITLE: Running PostgreSQL-specific Splink Tests in Bash\nDESCRIPTION: Command to run only the Splink tests that are specific to PostgreSQL backend.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/backends/postgres.md#2025-04-16_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest -m postgres_only tests/\n```\n\n----------------------------------------\n\nTITLE: Instantiating Splink Linker with Dialect-Specific Arguments in Python\nDESCRIPTION: This snippet shows how to instantiate a Splink linker using the dialect-specific helper, including any extra arguments required by specific dialects.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlinker = helper.Linker(df, settings_dict, **helper.extra_linker_args())\n```\n\n----------------------------------------\n\nTITLE: Installing Splink in Google Colab\nDESCRIPTION: Code to install the Splink package when running in Google Colab environment. This is commented out by default.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/02_Exploratory_analysis.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment and run this cell if you're running in Google Colab.\n# !pip install splink\n```\n\n----------------------------------------\n\nTITLE: Updating Packages with Poetry in Splink\nDESCRIPTION: Commands for updating packages to specific versions or to the latest version in the Splink project using Poetry.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/managing_dependencies_with_poetry.md#2025-04-16_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npoetry add <package-name>==<version>\npoetry add \"<package-name> >= <version>\"\n```\n\nLANGUAGE: sh\nCODE:\n```\npoetry add <package-name>==<version>\npoetry update <package-name>\n```\n\n----------------------------------------\n\nTITLE: Deriving Match Probability from m and u Parameters in LaTeX\nDESCRIPTION: This LaTeX equation demonstrates the mathematical derivation of match probability using m and u parameters. It shows three equivalent forms of the probability calculation, including the final simplified version.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/theory/fellegi_sunter.md#2025-04-16_snippet_7\n\nLANGUAGE: LaTeX\nCODE:\n```\n$$\n\\begin{equation}\n\\begin{aligned}\nPr(\\textsf{Match | Observation}) &= \\frac{2^{\\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right) + \\log_2\\left(\\prod_{i}^\\textsf{features}\\frac{m_{i}}{u_{i}}\\right)}}{1+2^{\\log_2\\left(\\frac{\\lambda}{1-\\lambda}\\right) + \\log_2\\left(\\prod_{i}^\\textsf{features}\\frac{m_{i}}{u_{i}}\\right)}} \\\\[20pt]\n &= \\frac{\\left(\\frac{\\lambda}{1-\\lambda}\\right)\\prod_{i}^\\textsf{features}\\frac{m_{i}}{u_{i}}}{1+\\left(\\frac{\\lambda}{1-\\lambda}\\right)\\prod_{i}^\\textsf{features}\\frac{m_{i}}{u_{i}}} \\\\[20pt]\n &= 1 - \\left[1+\\left(\\frac{\\lambda}{1-\\lambda}\\right)\\prod_{i}^\\textsf{features}\\frac{m_{i}}{u_{i}}\\right]^{-1}\n\\end{aligned}\n\\end{equation}\n$$\n```\n\n----------------------------------------\n\nTITLE: Running Splink Tests Without PostgreSQL\nDESCRIPTION: Command to run Splink tests while ignoring the PostgreSQL-specific tests if PostgreSQL is not installed.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/development_quickstart.md#2025-04-16_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\npytest tests/ --ignore tests/test_full_example_postgres.py\n```\n\n----------------------------------------\n\nTITLE: Python MkDocs Configuration for ComparisonLevelCreator\nDESCRIPTION: MkDocs configuration block for displaying the configuration method documentation from the ComparisonLevelCreator class.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/comparison_level_library.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n::: splink.internals.comparison_creator.ComparisonLevelCreator.configure\n    handler: python\n    options:\n      show_root_heading: false\n      show_root_toc: true\n      show_source: false\n```\n\n----------------------------------------\n\nTITLE: Additivity of Match Weights\nDESCRIPTION: This formula shows how match weights are additive across features, with the total weight being the sum of the prior weight and all feature weights.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/theory/fellegi_sunter.md#2025-04-16_snippet_2\n\nLANGUAGE: latex\nCODE:\n```\nM_\\textsf{obs} = M_\\textsf{prior} + M_\\textsf{features}\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Version Specifications\nDESCRIPTION: A requirements list specifying exact versions of Python packages needed for the Splink project. The dependencies include libraries for Jupyter widgets, testing frameworks, notebook testing, parallel test execution, JupyterLab, and fuzzy string matching.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/data/demos_requirements.txt#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nipywidgets==8.1.2\npytest==8.0.0\nnbmake==1.5.0\npytest-xdist==3.5.0\njupyterlab==4.1.1\nrapidfuzz==3.6.1\n```\n\n----------------------------------------\n\nTITLE: Removing Packages with Poetry in Splink\nDESCRIPTION: Command for removing packages from the Splink project using Poetry.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/managing_dependencies_with_poetry.md#2025-04-16_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npoetry remove <package-name>\n```\n\n----------------------------------------\n\nTITLE: Adding Note Block in Markdown\nDESCRIPTION: A markdown note block using MkDocs material syntax to encourage breaking up text content with visuals and sections.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/blog_posts.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n!!! note\n\n    In this blog we want to make content as easily digestible as possible. We encourage breaking up and big blocks of text into sections and using visuals/emojis/gifs to bring your post to life!\n```\n\n----------------------------------------\n\nTITLE: Specificity Formula in LaTeX\nDESCRIPTION: Mathematical formula for calculating specificity (true negative rate)\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/edge_metrics.md#2025-04-16_snippet_2\n\nLANGUAGE: latex\nCODE:\n```\n\\textsf{Specificity} = \\frac{\\textsf{True Negatives}}{\\textsf{All Negatives}} = \\frac{\\textsf{True Negatives}}{\\textsf{True Negatives} + \\textsf{False Positives}}\n```\n\n----------------------------------------\n\nTITLE: Dataset Catalog Table in Markdown\nDESCRIPTION: A markdown table listing available datasets with their descriptions, sizes, number of unique entities, and source links. Includes synthetic, historical, medical, and transaction datasets.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/includes/generated_files/datasets_table.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|dataset name|description|rows|unique entities|link to source|\n|-|-|-|-|-|\n|`fake_1000`|Fake 1000 from splink demos.  Records are 250 simulated people, with different numbers of duplicates, labelled.|1,000|250|[source](https://raw.githubusercontent.com/moj-analytical-services/splink_datasets/master/data/fake_1000.csv)|\n|`historical_50k`|The data is based on historical persons scraped from wikidata. Duplicate records are introduced with a variety of errors.|50,000|5,156|[source](https://raw.githubusercontent.com/moj-analytical-services/splink_datasets/master/data/historical_figures_with_errors_50k.parquet)|\n|`febrl3`|The Freely Extensible Biomedical Record Linkage (FEBRL) datasets consist of comparison patterns from an epidemiological cancer study in Germany.FEBRL3 data set contains 5000 records (2000 originals and 3000 duplicates), with a maximum of 5 duplicates based on one original record.|5,000|2,000|[source](https://raw.githubusercontent.com/moj-analytical-services/splink_datasets/master/data/febrl/dataset3.csv)|\n|`febrl4a`|The Freely Extensible Biomedical Record Linkage (FEBRL) datasets consist of comparison patterns from an epidemiological cancer study in Germany.FEBRL4a contains 5000 original records.|5,000|5,000|[source](https://raw.githubusercontent.com/moj-analytical-services/splink_datasets/master/data/febrl/dataset4a.csv)|\n|`febrl4b`|The Freely Extensible Biomedical Record Linkage (FEBRL) datasets consist of comparison patterns from an epidemiological cancer study in Germany.FEBRL4b contains 5000 duplicate records, one for each record in FEBRL4a.|5,000|5,000|[source](https://raw.githubusercontent.com/moj-analytical-services/splink_datasets/master/data/febrl/dataset4b.csv)|\n|`transactions_origin`|This data has been generated to resemble bank transactions leaving an account. There are no duplicates within the dataset and each transaction is designed to have a counterpart arriving in 'transactions_destination'. Memo is sometimes truncated or missing.|45,326|45,326|[source](https://raw.githubusercontent.com/moj-analytical-services/splink_datasets/master/data/transactions_origin.parquet)|\n|`transactions_destination`|This data has been generated to resemble bank transactions arriving in an account. There are no duplicates within the dataset and each transaction is designed to have a counterpart sent from 'transactions_origin'. There may be a delay between the source and destination account, and the amount may vary due to hidden fees and foreign exchange rates. Memo is sometimes truncated or missing.|45,326|45,326|[source](https://raw.githubusercontent.com/moj-analytical-services/splink_datasets/master/data/transactions_destination.parquet)|\n```\n\n----------------------------------------\n\nTITLE: Stopping PostgreSQL Docker Container for Splink\nDESCRIPTION: Command to stop the PostgreSQL Docker container used for Splink development.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/development_quickstart.md#2025-04-16_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/postgres_docker/teardown.sh\n```\n\n----------------------------------------\n\nTITLE: Path Configuration Example\nDESCRIPTION: Project path configuration showing the location of the Splink project in moj-analytical-services\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2024-07-10-splink4_release.md#2025-04-16_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nProject: /moj-analytical-services/splink\n```\n\n----------------------------------------\n\nTITLE: Creating Test Data for String Comparison Analysis\nDESCRIPTION: Creates a pandas DataFrame containing various string variations to test different comparison methods, including deletions, transpositions, shortenings, nicknames, and unrelated names.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndata = [\n    {\"string1\": \"Richard\", \"string2\": \"Richard\", \"error_type\": \"None\"},\n    {\"string1\": \"Richard\", \"string2\": \"ichard\", \"error_type\": \"Deletion\"},\n    {\"string1\": \"Richard\", \"string2\": \"Richar\", \"error_type\": \"Deletion\"},\n    {\"string1\": \"Richard\", \"string2\": \"iRchard\", \"error_type\": \"Transposition\"},\n    {\"string1\": \"Richard\", \"string2\": \"Richadr\", \"error_type\": \"Transposition\"},\n    {\"string1\": \"Richard\", \"string2\": \"Rich\", \"error_type\": \"Shortening\"},\n    {\"string1\": \"Richard\", \"string2\": \"Rick\", \"error_type\": \"Nickname/Alias\"},\n    {\"string1\": \"Richard\", \"string2\": \"Ricky\", \"error_type\": \"Nickname/Alias\"},\n    {\"string1\": \"Richard\", \"string2\": \"Dick\", \"error_type\": \"Nickname/Alias\"},\n    {\"string1\": \"Richard\", \"string2\": \"Rico\", \"error_type\": \"Nickname/Alias\"},\n    {\"string1\": \"Richard\", \"string2\": \"Rachael\", \"error_type\": \"Different Name\"},\n    {\"string1\": \"Richard\", \"string2\": \"Stephen\", \"error_type\": \"Different Name\"},\n]\n\ndf = pd.DataFrame(data)\ndf\n```\n\n----------------------------------------\n\nTITLE: Precision Formula in LaTeX\nDESCRIPTION: Mathematical formula for calculating precision (positive predictive value)\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/edge_metrics.md#2025-04-16_snippet_3\n\nLANGUAGE: latex\nCODE:\n```\n\\textsf{Precision} = \\frac{\\textsf{True Positives}}{\\textsf{All Predicted Positives}} = \\frac{\\textsf{True Positives}}{\\textsf{True Positives} + \\textsf{False Positives}}\n```\n\n----------------------------------------\n\nTITLE: Resulting JSON Configuration for Levenshtein Comparison in Splink\nDESCRIPTION: Shows the final JSON configuration structure that all three implementation approaches produce in Splink. This represents how the comparison will be stored in Splink's settings dictionary regardless of which implementation approach is used.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/customising_comparisons.ipynb#2025-04-16_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n    'output_column_name': 'email',\n    'comparison_levels': [\n        {\n            'sql_condition': '\"email_l\" IS NULL OR \"email_r\" IS NULL',\n            'label_for_charts': 'Null',\n            'is_null_level': True},\n        {\n            'sql_condition': '\"email_l\" = \"email_r\"',\n            'label_for_charts': 'Exact match'\n        },\n        {\n            'sql_condition': 'levenshtein(\"email_l\", \"email_r\") <= 2',\n            'label_for_charts': 'Levenshtein <= 2'\n        },\n        {\n            'sql_condition': 'levenshtein(\"email_l\", \"email_r\") <= 4',\n            'label_for_charts': 'Levenshtein <= 4'\n        },\n        {\n            'sql_condition': 'ELSE', \n            'label_for_charts': 'All other comparisons'\n        }],\n    'comparison_description': 'Exact match vs. Email within levenshtein thresholds 2, 4 vs. anything else'\n}\n```\n\n----------------------------------------\n\nTITLE: Dataset Schema Definition Table - Markdown\nDESCRIPTION: Markdown table defining the schema for the fake_1000_labels dataset, including column names, row count, and source link information.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/includes/generated_files/dataset_labels_table.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|dataset name|description|rows|unique entities|link to source|\n|-|-|-|-|-|\n|`fake_1000_labels`|Clerical labels for fake_1000 |3,176|NA|[source](https://raw.githubusercontent.com/moj-analytical-services/splink_datasets/master/data/fake_1000_labels.csv)|\n```\n\n----------------------------------------\n\nTITLE: Initializing AWS Boto3 Session\nDESCRIPTION: Creates a boto3 session for AWS services with eu-west-1 region configuration\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/athena/deduplicate_50k_synthetic.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport boto3\n\nboto3_session = boto3.Session(region_name=\"eu-west-1\")\n```\n\n----------------------------------------\n\nTITLE: MkDocs Dependencies with Version Requirements\nDESCRIPTION: A comprehensive list of MkDocs packages and their required versions for setting up documentation. Includes core MkDocs packages, various plugins for extended functionality, and specific version constraints for compatibility.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/scripts/docs-requirements.txt#2025-04-16_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\nmkdocs==1.6.0\nmknotebooks==0.8.0\nmkdocs-schema-reader==0.11.1\nmkdocs-material==9.5.2\nmkdocs-gen-files==0.5.0\nmkdocs-autorefs==1.0.1\nmkdocs-material-extensions==1.3.1\nmkdocs-mermaid2-plugin==1.1.1\nmkdocs-monorepo-plugin==1.1.0\nmkdocstrings==0.25.1\nmkdocstrings-python==1.10.5\nmkdocstrings-python-legacy==0.2.3\nmkdocs-click==0.8.1\njinja2==3.0.3\nmkdocs-charts-plugin==0.0.10\nneoteroi-mkdocs==1.0.5\nmkdocs-video==1.5.0\nmkdocs-rss-plugin==1.15.0\ngriffe==0.49.0\n```\n\n----------------------------------------\n\nTITLE: Disabling PySpark Warnings\nDESCRIPTION: Suppresses warning messages from PySpark by setting the log level to ERROR and filtering out UserWarnings.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/spark/deduplicate_1k_synthetic.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Disable warnings for pyspark - you don't need to include this\nimport warnings\n\nspark.sparkContext.setLogLevel(\"ERROR\")\nwarnings.simplefilter(\"ignore\", UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Selecting Dialect-Specific Test Helper in Python\nDESCRIPTION: This snippet shows how to select the appropriate dialect-specific test helper from the test_helpers dictionary provided as a fixture.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nhelper = test_helpers[dialect]\n```\n\n----------------------------------------\n\nTITLE: Installing Splink Package\nDESCRIPTION: Optional installation of Splink package for Google Colab environment.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/05_Predicting_results.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment and run this cell if you're running in Google Colab.\n# !pip install splink\n```\n\n----------------------------------------\n\nTITLE: MkDocs Configuration Block\nDESCRIPTION: Configuration block for MkDocs documentation generator, specifying Python handler and display options for the splink.clustering module documentation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/clustering.md#2025-04-16_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n::: splink.clustering\n    handler: python\n    options:\n      show_root_heading: false\n      show_root_toc: false\n      show_source: false\n```\n\n----------------------------------------\n\nTITLE: Installing Splink Package\nDESCRIPTION: Optional installation command for Google Colab environment.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/07_Evaluation.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment and run this cell if you're running in Google Colab.\n# !pip install splink\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure for splink.blocking_analysis\nDESCRIPTION: A markdown structure defining API documentation for the splink.blocking_analysis module. It uses the MkDocs documentation system with specific directives to render Python module documentation.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/blocking_analysis.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n::: splink.blocking_analysis\n    handler: python\n    options:\n      show_root_heading: false\n      show_root_toc: false\n      show_source: false\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Model Settings\nDESCRIPTION: Fetches and loads previously saved model settings from a JSON file, then initializes a new Linker instance with these settings.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/05_Predicting_results.ipynb#2025-04-16_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport urllib\n\nurl = \"https://raw.githubusercontent.com/moj-analytical-services/splink/847e32508b1a9cdd7bcd2ca6c0a74e547fb69865/docs/demos/demo_settings/saved_model_from_demo.json\"\n\nwith urllib.request.urlopen(url) as u:\n    settings = json.loads(u.read().decode())\n\n\nlinker = Linker(df, settings, db_api=DuckDBAPI())\n```\n\n----------------------------------------\n\nTITLE: Python Module Path Declaration\nDESCRIPTION: Module path declarations for Splink exploratory functionality and similarity analysis components.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/exploratory.md#2025-04-16_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n::: splink.exploratory\n    handler: python\n    options:\n      show_root_heading: false\n      show_root_toc: false\n      show_source: false\n```\n\nLANGUAGE: markdown\nCODE:\n```\n::: splink.exploratory.similarity_analysis\n    handler: python\n    options:\n      show_root_heading: false\n      show_root_toc: false\n      show_source: false\n```\n\n----------------------------------------\n\nTITLE: YAML Frontmatter Configuration\nDESCRIPTION: YAML configuration block defining documentation tags and metadata for the clustering API documentation page.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/api_docs/clustering.md#2025-04-16_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntags:\n  - API\n  - clustering\n---\n```\n\n----------------------------------------\n\nTITLE: Running core Splink tests\nDESCRIPTION: Command to run only the core tests in Splink, which are backend-independent.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npoetry run pytest tests/ -m core\n```\n\n----------------------------------------\n\nTITLE: Generating Record Linkage Predictions\nDESCRIPTION: Performs the record linkage inference to predict matching records with a threshold match probability of 0.9, which determines the pairs considered as matches.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/quick_and_dirty_persons.ipynb#2025-04-16_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresults = linker.inference.predict(threshold_match_probability=0.9)\n```\n\n----------------------------------------\n\nTITLE: Loading Historical Person Dataset\nDESCRIPTION: Loads a sample dataset of 50,000 historical person records and displays the first 5 rows to inspect the data structure.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/quick_and_dirty_persons.ipynb#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom splink.datasets import splink_datasets\n\ndf = splink_datasets.historical_50k\ndf.head(5)\n```\n\n----------------------------------------\n\nTITLE: Running Splink tests with custom pytest options in Docker\nDESCRIPTION: Command to run Splink tests in a Docker container with custom pytest options, specifically for Spark backend tests.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/testing.md#2025-04-16_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ndocker run --rm --name splink-test run_tests:testing pytest -W ignore -m spark tests/test_u_train.py\n```\n\n----------------------------------------\n\nTITLE: Accuracy Formula in LaTeX\nDESCRIPTION: Mathematical formula for calculating accuracy as the proportion of correct classifications\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/evaluation/edge_metrics.md#2025-04-16_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n\\textsf{Accuracy} = \\frac{\\textsf{True Positives}+\\textsf{True Negatives}}{\\textsf{All Predictions}}\n```\n\n----------------------------------------\n\nTITLE: Customizing PostgreSQL Schema for Splink in Python\nDESCRIPTION: Shows how to specify a custom schema name and additional schemas to search when initializing the PostgresAPI for Splink.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/splink_fundamentals/backends/postgres.md#2025-04-16_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndbapi = PostgresAPI(engine=engine, schema=\"another_splink_schema\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndbapi = PostgresAPI(engine=engine, other_schemas_to_search=[\"my_data_schema_1\", \"my_data_schema_2\"])\n```\n\n----------------------------------------\n\nTITLE: Citation Format for Splink in BibTeX\nDESCRIPTION: BibTeX citation format for referencing the Splink software in academic research papers. The citation includes author information, publication details, DOI, and URL for the paper published in the International Journal of Population Data Science.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/index.md#2025-04-16_snippet_0\n\nLANGUAGE: BibTeX\nCODE:\n```\n@article{Linacre_Lindsay_Manassis_Slade_Hepworth_2022,\n\ttitle        = {Splink: Free software for probabilistic record linkage at scale.},\n\tauthor       = {Linacre, Robin and Lindsay, Sam and Manassis, Theodore and Slade, Zoe and Hepworth, Tom and Kennedy, Ross and Bond, Andrew},\n\tyear         = 2022,\n\tmonth        = {Aug.},\n\tjournal      = {International Journal of Population Data Science},\n\tvolume       = 7,\n\tnumber       = 3,\n\tdoi          = {10.23889/ijpds.v7i3.1794},\n\turl          = {https://ijpds.org/article/view/1794},\n}\n```\n\n----------------------------------------\n\nTITLE: Viewing the Generated SQL for a Comparison\nDESCRIPTION: Shows how to view the underlying SQL generated by a Splink comparison, which helps understand how the comparison will be implemented in the database.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/topic_guides/comparisons/choosing_comparators.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfirst_name_comparison.get_comparison(\"duckdb\").as_dict()\n```\n\n----------------------------------------\n\nTITLE: Displaying Cluster Results (Python)\nDESCRIPTION: This snippet displays the first 5 rows of the generated clusters as a pandas DataFrame.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/examples/duckdb/deterministic_dedupe.ipynb#2025-04-16_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclusters.as_pandas_dataframe(limit=5)\n```\n\n----------------------------------------\n\nTITLE: Legacy Version Installation - Bash\nDESCRIPTION: Command to install a specific version of Splink (3.9.6) for Python 3.7 compatibility.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/blog/posts/2023-12-06-feature_update.md#2025-04-16_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install splink==3.9.6\n```\n\n----------------------------------------\n\nTITLE: Installing Splink Package\nDESCRIPTION: Installation command for the Splink package using pip (commented out for Google Colab usage).\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/demos/tutorials/04_Estimating_model_parameters.ipynb#2025-04-16_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Uncomment and run this cell if you're running in Google Colab.\n# !pip install splink\n```\n\n----------------------------------------\n\nTITLE: Running Splink Tests\nDESCRIPTION: Command to run the full Splink test suite using pytest to verify the development environment is set up correctly.\nSOURCE: https://github.com/moj-analytical-services/splink/blob/master/docs/dev_guides/changing_splink/development_quickstart.md#2025-04-16_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\npytest tests/\n```"
  }
]
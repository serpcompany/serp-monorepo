[
  {
    "owner": "quixio",
    "repo": "quix-docs",
    "content": "TITLE: Saving the Trained Model using Pickle\nDESCRIPTION: Saves the trained Decision Tree model object (`decision_tree`) to a file named 'decision_tree_5_depth.sav' in the current working directory using the Python `pickle` library. The 'wb' mode indicates writing in binary format. This allows the model to be loaded later for deployment or further analysis without retraining.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/train-ml.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npickle.dump(decision_tree, open('./decision_tree_5_depth.sav', 'wb'))\n```\n\n----------------------------------------\n\nTITLE: Publishing Video Frame as Time Series Binary Data with Quix Streams in Python\nDESCRIPTION: This Python snippet publishes a grabbed video frame, along with geolocation and timestamp, to a Quix Streams timeseries buffer. It uses Quix Streams's stream_producer, adds a nanosecond timestamp and values for image (as binary), longitude, and latitude, then publishes the message. Requires Quix Streams SDK in Python, access to the stream_producer object, and valid frame_bytes, lon, and lat variables. Input: frame_bytes (image as bytes), lon, lat. Output: Published message with time series format. Limitation: Proper Quix Streams setup is required for execution.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/tfl-frame-grabber.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nself.stream_producer.timeseries.buffer.add_timestamp_nanoseconds(time.time_ns()) \\\n    .add_value(\"image\", bytearray(frame_bytes)) \\\n    .add_value(\"lon\", lon) \\\n    .add_value(\"lat\", lat) \\\n    .publish()\n```\n\n----------------------------------------\n\nTITLE: Installing Quix Streams using pip in Bash\nDESCRIPTION: This command uses pip, the Python package installer (specifically targeting python3), to install the `quixstreams` library. It's the standard method for adding the Quix Streams dependency to a Python environment.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/quix-streams.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install quixstreams\n```\n\n----------------------------------------\n\nTITLE: Calculating Average in Time Window with Quix Streams (Python)\nDESCRIPTION: This Python code processes event-delimited time windows on streaming data, resetting and accumulating averages when pressure crosses thresholds. It initializes producers and consumers for different topics, maintains a running count and total, and outputs the average at the end of each 'ON' period. Required dependencies: quix-streams ('client', 'qx'), pandas, datetime, and proper environment variable setup for topic names. The handler expects a DataFrame with 'Pressure' input and publishes the average when the window closes.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/process/timeseries-events.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n...\ntopic_consumer = client.get_topic_consumer(os.environ[\"pressure_values\"], consumer_group = \"empty-transformation\")\ntopic_producer = client.get_topic_producer(os.environ[\"pressure_events\"])\ntopic_averages = client.get_topic_producer(os.environ[\"pressure_averages\"])\n\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\n    global triggered, average, count, total\n    stream_producer = topic_producer.get_or_create_stream(stream_id = stream_consumer.stream_id)\n    stream_averages = topic_averages.get_or_create_stream(\"pressure_averages\")\n    pressure = df['Pressure'][0]\n    if not triggered:\n        if  pressure > 0:\n            print('State ON')\n            triggered = True\n            count = 0\n            average = 0\n            total = 0\n    else:\n        count = count + 1\n        total = (total + pressure)\n        average = total / count\n        if pressure <= 0 :\n            print('State OFF')\n            triggered = False\n            print('average : --> {:.2f}'.format(average))\n            stream_averages.timeseries.buffer \\\n                .add_timestamp(datetime.datetime.utcnow()) \\\n                .add_value(\"PressureAverage\", float(average)) \\\n                .publish()\n```\n\n----------------------------------------\n\nTITLE: Connecting to Quix and Reading Stream Data with SignalR in Node.js\nDESCRIPTION: This code demonstrates how to establish a connection to Quix, subscribe to a parameter data stream, receive data, and unsubscribe from the event using the SignalR client library in Node.js. It requires the @microsoft/signalr package and a valid Quix access token.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/reading-data.md#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Use with Demo Data (for example)\nvar signalR = require(\"@microsoft/signalr\");\n\nconst options = {\n    accessTokenFactory: () => 'YOUR_ACCESS_TOKEN'\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://reader-YOUR_ENVIRONMENT_ID.platform.quix.io/hub\", options)\n    .build();\n\n// Establish connection (1)\nconnection.start().then(() => {\n    console.log(\"Connected to Quix.\");\n\n    // Subscribe to parameter data stream (2)\n    connection.invoke(\"SubscribeToParameter\", \"your-topic-name\", \"your-stream-id\", \"your-parameter-id\");\n\n    // Read data from the stream (3)\n    connection.on(\"ParameterDataReceived\", data => {\n        console.log(\"topicId: \" + data.topicId);\n        console.log(\"streamId: \" + data.streamId);\n        console.log(\"EngineRPM values: \" + data.numericValues.EngineRPM);\n\n        // Unsubscribe from stream (4)\n        connection.invoke(\"UnsubscribeFromParameter\", \"your-topic-name\", \"your-stream-id\", \"your-parameter-id\");\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Transformation with Quix Streams in Python\nDESCRIPTION: This Python script uses the Quix Streams library to connect to a Kafka broker, consume messages from the 'cpu-load' topic, calculate the average 'cpu_load' value over a 20-second tumbling window, and publish the resulting average and timestamp to the 'average-cpu-load' topic. It defines a streaming data frame (SDF) pipeline for processing, prints processed rows, and runs the application. Requires the `quixstreams` library.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/get-started/process.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n``` python\nfrom quixstreams import Application\nfrom datetime import timedelta\n\n# connect to your kafka broker\napp = Application(\n    broker_address=\"localhost:9092\",\n    consumer_group=\"transform-v1\",\n    auto_offset_reset=\"earliest\",\n)\n\n# consume data from the input topic\ninput_topic = app.topic(\"cpu-load\")\n# publish data to the output topic\noutput_topic = app.topic(\"average-cpu-load\")\n\n# sdf reads messages from the input topic\nsdf = app.dataframe(topic=input_topic)\n\n# calculate average cpu load over 20 seconds tumbling window\nsdf = sdf.apply(lambda row: row[\"cpu_load\"]) \\\n    .tumbling_window(timedelta(seconds=20)).mean().final() \\\n        .apply(lambda value: {\n            'average-cpu-load': value['value'],\n            'time': value['end']\n            })\n\n# print every row\nsdf = sdf.update(lambda row: print(row))\n\n# publish to output topic\nsdf = sdf.to_topic(output_topic)\n\nif __name__ == \"__main__\":\n    # run the application and process all inbound messages using the sdf pipeline\n    app.run(sdf)\n```\n```\n\n----------------------------------------\n\nTITLE: Decompressing Kafka Messages with zlib in Python using Quix Streams\nDESCRIPTION: This Python script showcases how to decompress messages consumed from a Kafka topic using the `zlib` library within a Quix Streams application. It defines a `decompress_data` function that uses `zlib.decompress` and decodes the result from UTF-8 bytes. The script sets up a Quix application with an input topic configured for `bytes` deserialization and an output topic for `json` serialization. It processes the streaming data by applying the decompression function and then publishes the decompressed data to the output topic. Dependencies include `os`, `zlib`, `quixstreams`, and `dotenv`.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/compressed-data.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os, zlib # make sure zlib is imported (you don't need to install it though)\nfrom quixstreams import Application\n\n# for local dev, load env vars from a .env file\nfrom dotenv import load_dotenv\nload_dotenv()\n\ndef decompress_data(compressed_data):\n    data = zlib.decompress(compressed_data)\n    data = data.decode(\"utf-8\") # decode UTF-8 packed bytes\n    return data\n\napp = Application.Quix(\"decompress-service\", auto_offset_reset=\"earliest\")\n\n# inbound compressed data is a series of bytes so the bytes deserializer is used for this topic\ninput_topic = app.topic(os.environ[\"input\"], value_deserializer=\"bytes\")\n# data is to be published as JSON for onward processing in the pipeline.\n# JSON is the default serializer / message format, but is explicitly specified here for clarity\noutput_topic = app.topic(os.environ[\"output\"], value_serializer=\"json\")\n\nsdf = app.dataframe(input_topic)\nsdf = sdf.apply(decompress_data) # call the `decompress_data` function for each message\nsdf = sdf.update(lambda msg: print(msg)) # print out each message for debugging\nsdf = sdf.to_topic(output_topic) # publish messages to output topic\n\nif __name__ == \"__main__\":\n    app.run(sdf)\n```\n\n----------------------------------------\n\nTITLE: Installing Quix Streams with pip in Bash\nDESCRIPTION: This snippet demonstrates how to install the Quix Streams client library for Python using the pip package manager. It is a prerequisite step for running any Python code that relies on the Quix Streams API. This command must be run in a command-line interface where Python3 and pip are available. Ensure you run this in the correct environment (e.g., virtualenv) if required.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/read-csv.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install quixstreams\n```\n\n----------------------------------------\n\nTITLE: Writing Mouse Positions to Quix Topic via SignalR in JavaScript/HTML\nDESCRIPTION: This code snippet demonstrates how to send mouse cursor positions from an HTML canvas to a Quix stream in real time using the Quix Streaming Writer API and the SignalR JavaScript client. Requires including the Microsoft SignalR library and a valid Quix Personal Access Token (PAT), environment id, and pre-existing topic. Upon mouse movement on the canvas, the script captures the coordinates and current timestamp, displays them, paints locally, and sends them as a structured event packet to Quix via the 'SendEventData' method over a persistent SignalR WebSockets connection. Inputs are mouse events, outputs are both UI updates and data packets sent to Quix. The snippet expects the server to be reachable and the credentials to be valid.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/streaming-apis.md#2025-04-23_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<!DOCTYPE html>\\n<html lang=\\\"en\\\">\\n  <head>\\n    <title>Hello WebSockets</title>\\n    <link rel=\\\"stylesheet\\\" href=\\\"/style.css\\\" />\\n    <script src=\\\"https://cdnjs.cloudflare.com/ajax/libs/microsoft-signalr/6.0.1/signalr.js\\\"></script>\\n  </head>\\n  <body>\\n    <hr />\\n    <canvas\\n      id=\\\"myCanvas\\\"\\n      width=\\\"500\\\"\\n      height=\\\"300\\\"\\n      style=\\\"border: 5px solid #00ff00\\\"\\n      onmousemove=\\\"getCursorPosition(event)\\\"\\n    >\\n    </canvas>\\n\\n    <hr />\\n    <div>Timestamp: <span id=\\\"timestamp\\\"></span>.</div>\\n    <div>X Mouse position: <span id=\\\"c_p_x\\\"></span>.</div>\\n    <div>Y Mouse position: <span id=\\\"c_p_y\\\"></span>.</div>\\n    <button onclick=\\\"clearCanvas()\\\">Clear canvas</button>\\n    <hr />\\n\\n    <script>\\n      const token = \\\"<your_pat_token>\\\"; // Obtain your PAT token from Quix\\n      const environmentId = \\\"<your_environment>\\\";\\n      const topic = \\\"websocket-topic\\\";\\n      const streamId = \\\"mouse-pos\\\";\\n\\n      var mouseX;\\n      var mouseY;\\n      var timestamp;\\n\\n      const canvas = document.getElementById(\\\"myCanvas\\\");\\n      const ctx = canvas.getContext(\\\"2d\\\");\\n      ctx.fillStyle = \\\"#00FF00\\\";\\n\\n      const options = {\\n        accessTokenFactory: () => token,\\n      };\\n\\n      const connection = new signalR.HubConnectionBuilder()\\n        .withUrl(\\n          \\\"https://writer-\\\" + environmentId + \\\".platform.quix.io/hub\\\",\\n          options\\n        )\\n        .build();\\n\\n      connection.start().then(async () => {\\n        console.log(\\\"Connected to Quix.\\\");\\n      });\\n\\n      async function getCursorPosition(event) {\\n        timestamp = new Date().getTime();\\n        mouseX = event.clientX;\\n        mouseY = event.clientY;\\n\\n        document.getElementById(\\\"timestamp\\\").textContent = timestamp;\\n        document.getElementById(\\\"c_p_x\\\").textContent = mouseX;\\n        document.getElementById(\\\"c_p_y\\\").textContent = mouseY;\\n        ctx.fillRect(mouseX, mouseY, 10, 10);\\n\\n        let mousePos = JSON.stringify({ x: mouseX, y: mouseY });\\n\\n        let mousePacket = [\\n          {\\n            timestamp: new Date().getTime(),\\n            tags: {\\n              mousestatus: \\\"tracking\\\",\\n            },\\n            id: \\\"mouse position\\\",\\n            value: mousePos,\\n          },\\n        ];\\n\\n        console.log(\\\"Sending mouse data\\\");\\n        await connection.invoke(\\\"SendEventData\\\", topic, streamId, mousePacket);\\n        console.log(\\\"Sent mouse data\\\");\\n      }\\n\\n      function clearCanvas() {\\n        ctx.clearRect(0, 0, 500, 300);\\n      }\\n    </script>\\n  </body>\\n</html>\n```\n\n----------------------------------------\n\nTITLE: Filtering and Republishing Car Count Data with Quix Streams (Python)\nDESCRIPTION: This Python code defines a Quix Streams transformation that listens to a specified input topic, extracts only the number of detected cars from timeseries data, and publishes this data to a separate output topic. It relies on the quixstreams library, pandas, and uses environment variables to identify input/output topics. The main handler processes pandas dataframes, checks for \\'car\\' entries, and republishes minimal cleaned data (timestamp, car count) for real-time downstream consumption. Ensure quixstreams and pandas are installed, and set environment variables \\\"input\\\" and \\\"output\\\" appropriately. The service subscribes to streams, handles graceful termination, and is suitable for pipeline deployments in streaming analytics scenarios.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/add-service.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport quixstreams as qx\nimport os\nimport pandas as pd\nimport datetime\n\nclient = qx.QuixStreamingClient()\n\ntopic_consumer = client.get_topic_consumer(os.environ[\"input\"], consumer_group = \"empty-transformation\")\ntopic_producer = client.get_topic_producer(os.environ[\"output\"])\n\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\n    d = df.to_dict()\n    if 'car' in d:\n        # Create a clean data frame\n        data = qx.TimeseriesData()\n        data.add_timestamp(datetime.datetime.utcnow()) \\\n            .add_value(\"Cars\", d['car'][0])\n    \n        stream_producer = topic_producer.get_or_create_stream(stream_id = stream_consumer.stream_id)\n        stream_producer.timeseries.buffer.publish(data)\n\ndef on_stream_received_handler(stream_consumer: qx.StreamConsumer):\n    stream_consumer.timeseries.on_dataframe_received = on_dataframe_received_handler\n\n# subscribe to new streams being received\ntopic_consumer.on_stream_received = on_stream_received_handler\n\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n\n# Handle termination signals and provide a graceful exit\nqx.App.run()\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Hopping Windows with Quix Streams in Python\nDESCRIPTION: This code demonstrates how to define a hopping window over a StreamingDataFrame using Quix Streams. It shows two configurations: one that emits results only when the window is closed, and another that emits results for each incoming message. The example uses a 1-hour window with a 10-minute step interval.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/windowing-stream-processing-guide.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom datetime import timedelta\n\napp = Application(broker_address='localhost:9092')\ntopic = app.topic('input-topic')\nsdf = app.dataframe(topic)\n\n# Create a hopping window of 1 hour with a 10-minute step \n# and emit results only when the window is closed\nsdf = (\n    sdf.hopping_window(duration_ms=timedelta(hours=1), step_ms=timedelta(minutes=10))\n    .sum()\n    .final()\n)\n\n# Same hopping window, but emitting results for each incoming message\nsdf = (\n    sdf.hopping_window(duration_ms=timedelta(hours=1), step_ms=timedelta(minutes=10))\n    .sum()\n    .current()\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Model Loading and Preprocessing in Python\nDESCRIPTION: This snippet imports the necessary Python libraries: `pickle` for deserializing the machine learning model from a file, and `math` for performing mathematical calculations required during the data preprocessing step. These imports are added to the `quix_function.py` file.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/deploy-ml.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\nimport math\n```\n\n----------------------------------------\n\nTITLE: Initializing Data Enrichment Service with Quix and Redis in Python\nDESCRIPTION: This code sets up a Quix application for data enrichment. It initializes connections to Quix topics and Redis, and applies an enrichment function to incoming data streams.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/data-enrichment-with-quix-and-redis.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom quixstreams import Application\nimport redis\n\n# Initialize Quix application\napp = Application(consumer_group=\"transformation-v1\", auto_offset_reset=\"earliest\")\n\n# Get input and output topics from environment variables\ninput_topic = app.topic(os.environ[\"input\"])\noutput_topic = app.topic(os.environ[\"output\"])\n\n# Create a Redis connection\nr = redis.Redis(\n\thost=os.environ['redis_host'],\n\tport=int(os.environ['redis_port']),\n\tpassword=os.environ['redis_password'],\n\tusername=os.environ['redis_username'] if 'redis_username' in os.environ else None,\n\tdecode_responses=True)\n\ndef enrich_data(row):\n   # Function truncated for brevity (covered separately in the next section)\n   ####\n   \nsdf = app.dataframe(input_topic)\n\n# The enrich_data function will be implemented below\nsdf = sdf.apply(enrich_data)\n```\n\n----------------------------------------\n\nTITLE: Setting a 1-Week Tumbling Window in Quix Streams (Python)\nDESCRIPTION: This snippet shows how to configure a tumbling window of one week using `timedelta(weeks=1)` within the `tumbling_window` function in Quix Streams for aggregation purposes like calculating the mean.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/integrations/databases/influxdb/replacing-flux.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n``` python\nsdf = sdf.tumbling_window(timedelta(weeks=1)).mean().final()\n```\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output from Transformation Script\nDESCRIPTION: This JSON object shows an example message produced by the 'transform.py' script and published to the 'average-cpu-load' Kafka topic. It contains the calculated 'average-cpu-load' for a 20-second time window and the 'time' field representing the end timestamp of that window (likely in milliseconds since epoch).\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/get-started/process.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n``` json\n{\n    \"average-cpu-load\": 14.9,\n    \"time\": 1712149460000\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Handling TopicMetricsUpdated Event in JavaScript with SignalR\nDESCRIPTION: This code snippet demonstrates how to establish a SignalR connection, subscribe to topic metrics, and handle the TopicMetricsUpdated event. It includes connecting to the Quix hub, invoking the SubscribeToTopicMetrics method, and logging the received metrics data.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/subscriptions.md#2025-04-23_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\n\nconst options = {\n    accessTokenFactory: () => '<your-PAT>'\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n      .withUrl(\"https://reader-joeengland-apitests-testing.platform.quix.io/hub\", options)\n      .build();\n\n// Establish connection \nconnection.start().then(() => {\n    console.log(\"Connected to Quix.\");\n\n    connection.invoke(\"SubscribeToTopicMetrics\", \"f1-data\");\n\n    connection.on(\"TopicMetricsUpdated\", (metrics) => {\n        console.log('metrics -----> ', metrics);\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Illustrating 'over-forecast' JSON Alert Message Format\nDESCRIPTION: Presents the JSON structure for an 'over-forecast' alert. This alert is triggered when a forecasted parameter ('forecast_fluctuated_ambient_temperature') is predicted to exceed its maximum threshold (55ºC). The message includes the 'over-forecast' status, parameter name, timestamp, the forecasted temperature value, and a message indicating the predicted time until the threshold is breached.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/predictive-maintenance/alert-service.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"over-forecast\",\n  \"parameter_name\": \"forecast_fluctuated_ambient_temperature\",\n  \"alert_temperature\": 55.014602460947586,\n  \"timestamp\": 1701278280000000000,\n  \"message\": \"'Ambient temperature' is forecasted to go over 55ºC in 1:36:29.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Joining Sensor Data Streams with Equipment Status in Python\nDESCRIPTION: This code joins event streams that correlate sensor readings with equipment status updates. It enriches sensor data with the latest equipment status whenever a status update occurs, demonstrating a stateful join of two streams.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/navigating-stateful-stream-processing.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Simulated stream of sensor readings\nsensor_readings = [\n    {'timestamp': 1, 'machine_id': 'A', 'sensor_value': 0.5},\n    {'timestamp': 2, 'machine_id': 'B', 'sensor_value': 0.7},\n    # more sensor readings...\n]\n\n# Simulated stream of equipment status updates\nstatus_updates = [\n    {'timestamp': 1, 'machine_id': 'A', 'status': 'normal'},\n    {'timestamp': 2, 'machine_id': 'B', 'status': 'warning'},\n    # more status updates...\n]\n\n# Function to join sensor readings with the latest status updates\ndef join_streams(sensor_readings, status_updates):\n    # Keeping the latest status for each machine\n    latest_status = {}\n    joined_data = []\n\n    # Update the latest status based on status_updates stream\n    for update in status_updates:\n        latest_status[update['machine_id']] = update['status']\n\n    # Join sensor readings with the latest known status\n    for reading in sensor_readings:\n        machine_id = reading['machine_id']\n        if machine_id in latest_status:\n            # Enrich sensor reading with the latest status\n            enriched_reading = dict(reading)\n            enriched_reading['status'] = latest_status[machine_id]\n            joined_data.append(enriched_reading)\n\n    return joined_data\n\n# Example of processing the streams\njoined_data = join_streams(sensor_readings, status_updates)\nprint(joined_data)\n```\n\n----------------------------------------\n\nTITLE: Loading and Publishing CSV Data to Quix Topic with Quix Streams in Python\nDESCRIPTION: This Python script loads data from a CSV file and publishes each row to a Quix topic using the Quix Streams Python client. It loads environment variables using python-dotenv, reads data via the csv library, and sends data serialized as JSON. Dependencies include quixstreams and python-dotenv, both installable via pip. Key parameters are the CSV file name (USERS_FILE) and environment variables \\\"output\\\" (topic) and \\\"Quix__Sdk__Token\\\" (SDK token). The script authenticates, connects to the Quix broker, and produces messages at a fixed interval. Input: CSV file path and environment configuration; Output: CSV rows streamed to the designated Quix topic. Limitations: The script expects the specified environment variables to be set, the CSV file to be present, and the Quix Streams libraries to be installed.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/read-csv.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# pip install quixstreams python-dotenv\nimport csv\nimport os\nimport time\nimport json\n\nfrom dotenv import load_dotenv\nfrom quixstreams import Application\n\n# Load environment variables from the \".env\" file\nload_dotenv()\n\n# Create an Application to connect to the Quix broker with SDK token\napp = Application()\n\n# Define an output topic\noutput_topic = app.topic(os.environ[\"output\"])\n\n# Path to a CSV file with data\nUSERS_FILE = \"user-data.csv\"\n\ndef load_csv(path: str):\n    rows = []\n    with open(path, \"r\") as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            rows.append(row)\n    return rows\n\n\ndef main():\n    # Load data from the CSV file\n    users = load_csv(USERS_FILE)\n\n    print(f'Writing CSV data to the \"{output_topic.name}\" topic ...')\n    with app.get_producer() as producer:\n        for user in users:\n            # Send data to the output topic\n            producer.produce(\n                topic=output_topic.name,\n                key=\"users-csv\",\n                value=json.dumps(user),\n            )\n            time.sleep(1)\n\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(\"Quitting\")\n```\n\n----------------------------------------\n\nTITLE: Defining Pipeline Deployments and Topics in Quix with YAML\nDESCRIPTION: This YAML snippet represents the main descriptor file (quix.yaml) for a Quix project. It defines metadata such as the pipeline version, deployment configurations for applications (like resource allocation, deployment type, commit version, and variables for runtime configuration), and the setup for topics (including persistence and partitioning). The file orchestrates how all components of the project interoperate, and is required at the root of the project. Inputs include application and topic definitions; outputs are consumed by the Quix platform to instantiate and manage the pipeline accordingly. Prerequisite: Quix environment configured to use this descriptor.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/create/project-structure.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Quix Project Descriptor\\n# This file describes the data pipeline and configuration of resources of a Quix Project.\\n\\nmetadata:\\n  version: 1.0\\n\\n# This section describes the Deployments of the data pipeline\\ndeployments:\\n  - name: Demo Data\\n    application: Demo Data\\n    deploymentType: Job\\n    version: ada522b5199fc9667505b4dd19980995804ca764\\n    resources:\\n      cpu: 200\\n      memory: 200\\n      replicas: 1\\n    libraryItemId: 7abe02e1-1e75-4783-864c-46b930b42afe\\n    variables:\\n      - name: Topic\\n        inputType: OutputTopic\\n        description: Name of the output topic to write into\\n        required: true\\n        value: f1-data\\n\\n# This section describes the Topics of the data pipeline\\ntopics:\\n  - name: f1-data\\n    persisted: false\\n    configuration:\\n      partitions: 2\\n      replicationFactor: 2\\n      retentionInMinutes: -1\\n      retentionInBytes: 262144000\n```\n\n----------------------------------------\n\nTITLE: Polling REST API and Producing to Quix Topic in Python\nDESCRIPTION: This Python script initializes a Quix application, defines an output topic based on the 'output' environment variable, and enters an infinite loop. In each iteration, it sends a GET request to 'https://random-data-api.com/api/v2/users', parses the JSON response, serializes it using `JSONSerializer`, and produces the serialized data to the configured Quix topic using the Quix producer. A 4-second pause is introduced between polls using `time.sleep(4)`. Dependencies include `quixstreams`, `time`, `os`, and `requests`.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/polling.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n``` python\nfrom quixstreams import Application\nfrom quixstreams.models.serializers.quix import JSONSerializer, SerializationContext\nimport time, os, requests\n\napp = Application()\n\nserializer = JSONSerializer()\noutput_topic = app.topic(os.environ[\"output\"])\nproducer = app.get_producer()\n\nwhile True:\n    response = requests.get(\"https://random-data-api.com/api/v2/users\")\n    json_response = response.json()\n\n    with producer:\n        serialized_value = serializer(\n            value=json_response, ctx=SerializationContext(topic=output_topic.name)\n        )\n        producer.produce(\n            topic=output_topic.name,\n            key=\"polling-sample\",\n            value=serialized_value\n        )\n    time.sleep(4)\n```\n```\n\n----------------------------------------\n\nTITLE: Handling Received Parameter Data with SignalR in JavaScript\nDESCRIPTION: This snippet shows how the UI listens for the \\\"ParameterDataReceived\\\" event from the SignalR connection, filtering by stream ID to process relevant data. When the event contains data matching the expected stream suffix, application-specific logic can be executed in response. Dependencies include the SignalR JavaScript client, a valid \\\"connection\\\" object, and the naming convention for stream IDs. The input is a SignalR event providing a data payload (should contain streamId); the output is the effect of processing the relevant data. Custom logic for handling the data should be placed within the if-block.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/data-stream-processing/data-stream-processing.md#2025-04-23_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nconnection.on(\"ParameterDataReceived\", data => {\\n    if (data.streamId.endsWith(\"-car-game-input-control\")) { ... }\\n}\n```\n\n----------------------------------------\n\nTITLE: Asynchronously Generating Data for Multiple Printers in Python\nDESCRIPTION: This Python snippet demonstrates how the Data Generator service initiates data generation for multiple printers concurrently using asyncio. It iterates through the specified number of printers, calculates a staggered start delay for each, and creates an asynchronous task (`generate_data_async`) to handle data generation and publishing for that specific printer, identified by a unique name and stream ID. The `asyncio.gather` function runs all created tasks.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/predictive-maintenance/data-generator.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntasks = []\nprinter_data = generate_data()\n\n# Distribute all printers over the data length (defaults to 60 seconds)\ndelay_seconds = get_data_length() / replay_speed / number_of_printers\n\nfor i in range(number_of_printers):\n    # Set MessageKey/StreamID or leave parameters empty to get a generated message key.\n    name = f\"Printer {i + 1}\"  # We don't want a Printer 0, so start at 1\n\n    # Start sending data, each printer will start with some delay after the previous one\n    tasks.append(asyncio.create_task(generate_data_async(topic, producer, name, printer_data.copy(), int(delay_seconds * i))))\n\nawait asyncio.gather(*tasks)\n```\n\n----------------------------------------\n\nTITLE: Streaming CSV Data to Quix Topic using Python\nDESCRIPTION: This Python script uses the `quixstreams` library to stream data from a CSV file (`data.csv`) into a Quix topic defined by the 'output' environment variable. It first downloads the CSV file if it doesn't exist locally. It utilizes `pandas` to read the CSV and renames specific columns to be treated as Quix tags. The script then iterates through the DataFrame rows, publishing each row to the designated topic producer's stream with a small delay between sends. Dependencies include `quixstreams`, `pandas`, and `urllib3` (which needs to be added to `requirements.txt`).\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/event-detection/data-acquisition.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport quixstreams as qx\nimport time\nimport os\nimport pandas as pd\nfrom urllib import request\n\n# Quix injects credentials automatically to the client. \n# Alternatively, you can always pass an SDK token manually as an argument.\nclient = qx.QuixStreamingClient()\n\n# Open the output topic where to write data out\ntopic_producer = client.get_topic_producer(topic_id_or_name = os.environ[\"output\"])\n\n# Set stream ID or leave parameters empty to get stream ID generated.\nstream = topic_producer.create_stream()\n\n# download data.csv\n# if you want to use your own data file just comment these lines\n# and place your data.csv file in the same folder as main.py\nf = request.urlopen(\"https://quixtutorials.blob.core.windows.net/tutorials/event-detection/data.csv\")\nwith open(\"data.csv\", \"wb\") as data_file:\n    data_file.write(f.read())\n\ndf = pd.read_csv(\"data.csv\")\n\nfor col_i in ['device_id','rider','streamId','team','version']:\n    df = df.rename(columns={col_i: \"TAG__\" + col_i})\n    \nprint(\"Writing data\")\nseconds_to_wait = 0.5\n\nwhile True:\n    for i in range(len(df)):\n        start_loop = time.time()\n        df_i = df.iloc[[i]]\n        stream.timeseries.publish(df_i)\n        print(\"Sending \" + str(i) + \"/\" + str(len(df)))\n        end_loop = time.time()\n        time.sleep(max(0.0, seconds_to_wait - (end_loop - start_loop)))\n\nqx.App.run()\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Batched Write Operation to DuckDB with Transaction Management in Python\nDESCRIPTION: This snippet implements the write method for the DuckDBSink, enabling efficient batched inserts within a transactional context. The function connects to the database, processes streaming data batches, formats records (including timestamp normalization), and performs inserst using executemany for performance. On errors, it raises SinkBackpressureError for retry logic and handles transactions with commit/rollback as needed. Dependencies: duckdb, logging, datetime, time, SinkBatch, SinkBackpressureError. Inputs are batches of streaming data with a schema; outputs are DB insertions or error handling with backpressure signals. Constraints include transaction size affecting memory footprint.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    def write(self, batch: SinkBatch):\n        conn = duckdb.connect(self.database_path)\n        try:\n            conn.execute(\"BEGIN TRANSACTION\")\n            \n            for write_batch in batch.iter_chunks(n=self.batch_size):\n                records = []\n                min_timestamp = None\n                max_timestamp = None\n\n                for item in write_batch:\n                    value = item.value\n                    if 'timestamp' not in value:\n                        value['timestamp'] = datetime.utcnow().isoformat()\n                    else:\n                        value['timestamp'] = datetime.fromisoformat(value['timestamp']).isoformat()\n                    records.append(value)\n                    if min_timestamp is None or value['timestamp'] < min_timestamp:\n                        min_timestamp = value['timestamp']\n                    if max_timestamp is None or value['timestamp'] > max_timestamp:\n                        max_timestamp = value['timestamp']\n\n                columns = \", \".join(records[0].keys())\n                placeholders = \", \".join(['?' for _ in records[0].keys()])\n                query = f\"INSERT INTO {self.table_name} ({columns}) VALUES ({placeholders})\"\n                values = [list(record.values()) for record in records]\n\n                try:\n                    _start = time.monotonic()\n                    conn.executemany(query, values)\n\t\t\tconn.execute(\"COMMIT\")\n                    elapsed = round(time.monotonic() - _start, 2)\n                    logger.info(\n                        f\"Sent data to DuckDB; \"\n                        f\"total_records={len(records)} \"\n                        f\"min_timestamp={min_timestamp} \"\n                        f\"max_timestamp={max_timestamp} \"\n                        f\"time_elapsed={elapsed}s\"\n                    )\n                except duckdb.Error as exc:\n                    logger.error(\"Error writing to DuckDB: %s\", exc)\n                     raise SinkBackpressureError(\n                         retry_after=60,# hardcoded at a minute\n                         topic=batch.topic,\n                         partition=batch.partition,\n                     ) \n                    \n            \n        except duckdb.Error as exc:\n            logger.error(\"Transaction failed, rolling back. Error: %s\", exc)\n            conn.execute(\"ROLLBACK\")\n            raise\n        finally:\n            conn.close()\n\n```\n\n----------------------------------------\n\nTITLE: Viewing Chat Message Structure in Quix Data Explorer (JSON)\nDESCRIPTION: This JSON snippet shows the structure of a message retrieved from the 'chat-messages' topic within the Quix Data Explorer. It includes timestamps, string values (like the chat message itself), and tag values (metadata such as room, user name, and role). This structure is typical for data flowing through Quix topics.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/sentiment-analysis/sentiment-analysis-service.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"Epoch\": 0,\n\"Timestamps\": [\n    1695303751958000000\n],\n\"NumericValues\": {},\n\"StringValues\": {\n    \"chat-message\": [\n    \"Can you check on my order please?\"\n    ]\n},\n\"TagValues\": {\n    \"room\": [\n    \"channel\"\n    ],\n    \"name\": [\n    \"gabbybe\"\n    ],\n    \"role\": [\n    \"Customer\"\n    ]\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Product Data and Writing to Redis Cloud in Python\nDESCRIPTION: This Python function reads the 'products.json' file using Pandas, iterates over each product item, and batches insertion of the category ('cat') and title into Redis under keys formatted as 'product:{id}'. It uses a Redis pipeline for efficient bulk writes, printing a summary upon completion. Dependencies include the pandas library for JSON parsing and a configured Redis client instance ('r'); input is a JSON file, and output is data written to Redis with minimal error handling.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/data-ingestion.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Read products from products.tsv and store the category in Redis\\ndef load_products():\\n    products = pd.read_json('products.json')\\n    pipe = r.pipeline()\\n\\n    for index, row in products.iterrows():\\n        key = f'product:{row[\"id\"]}'\\n        pipe.hset(key, 'cat', row['category'])\\n        pipe.hset(key, 'title', row['title'])\\n\\n    pipe.execute()\\n    print(f\"Imported {len(products)} products\")\n```\n\n----------------------------------------\n\nTITLE: Ingesting System Metrics with Quix Streams in Python\nDESCRIPTION: This Python script uses the `quixstreams` library to create a data producer. It periodically fetches CPU load and swap memory usage using `psutil` and publishes this information as JSON messages to a Quix topic defined by the 'output' environment variable. The script runs continuously until manually stopped.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-cloud/quixtour/ingest.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport psutil, time, os, json\nfrom quixstreams import Application\n\napp = Application()\n\noutput_topic = app.topic(os.environ[\"output\"])\n    \ndef get_cpu_load():\n    cpu_load = psutil.cpu_percent(interval=1)\n    memory = psutil.swap_memory()\n    return {\n        \"cpu_load\": cpu_load,\n        \"memory\": memory._asdict(),\n        \"timestamp\": int(time.time_ns()),\n    }\n\ndef main():\n    # Create a Producer to send data to the topic\n    with app.get_producer() as producer:\n        while True:                \n            # Get the current CPU and memory usage\n            message = get_cpu_load()\n            print(\"CPU load: \", message[\"cpu_load\"])\n\n            # Produce message to the topic\n            producer.produce(\n            topic=output_topic.name,\n                key=\"server-1-cpu\",\n                value=json.dumps(message)\n            )\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Listening for Parameter Data Using SignalR in Typescript\nDESCRIPTION: This snippet shows how an Angular (Typescript) client subscribes to the 'ParameterDataReceived' event via SignalR to stream parameter data from a Quix topic. It uses a HubConnection object to listen for incoming parameter data and emits the received payload to a local Subject or Observable for further processing in the application. 'readerHubConnection' is expected to be a configured SignalR connection, and 'this.paramDataReceived' is typically a Subject or BehaviorSubject used for reactive data flow. Parameters include the event name and the callback handling the received data payload.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/sentiment-analysis/ui-service.md#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Listen for parameter data and emit\\nreaderHubConnection.on(\\\"ParameterDataReceived\\\", (payload: ParameterData) => {\\n    this.paramDataReceived.next(payload);\\n});\n```\n\n----------------------------------------\n\nTITLE: Enriching DataFrame with Metadata and Publishing Results - Python\nDESCRIPTION: This snippet defines the dataframe handler function invoked for each received timeseries batch. It enriches the pandas DataFrame with category, title, birthdate, country, device type, age, and gender, leveraging custom lookup functions, then adapts to pre-existing fields (choosing appropriate transformations if age/gender already exist). The function creates or reuses a producer stream keyed by visitor ID and publishes the enriched dataframe. Dependencies include pandas, a streaming/producer API (likely from Quix Streams), and custom data access functions for Redis.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/data-enrichment.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Callback triggered for each new timeseries data. This method will enrich the data\\ndef on_dataframe_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\\n    # Enrich data\\n    df['category'] = df['productId'].apply(get_product_category)\\n    df['title'] = df['productId'].apply(get_product_title)\\n    df['birthdate'] = df['userId'].apply(get_visitor_birthdate)\\n    df['country'] = df['ip'].apply(get_country_from_ip)\\n    df['deviceType'] = df['userAgent'].apply(get_device_type)\\n\\n    # For synthetic data (from csv) we don't have age. For data generated from our live web, we have age and gender\\n    if 'age' not in df.columns:\\n        df['age'] = df['birthdate'].apply(calculate_age)\\n    else:\\n        df['age'] = df['age'].apply(convert_age_to_int)\\n\\n    if 'gender' not in df.columns:\\n        df['gender'] = df['userId'].apply(get_visitor_gender)\\n    else:\\n        df['gender'] = df['gender'].apply(get_first_letter_of_gender)\\n\\n    # Create a new stream (or reuse it if it was already created).\\n    # We will be using one stream per visitor id, so we can parallelise the processing\\n    # because the partitioning key will be the stream id\\n    producer_stream = producer_topic.get_or_create_stream(stream_consumer.stream_id)\\n    producer_stream.properties.parents.append(stream_consumer.stream_id)\\n    producer_stream.timeseries.buffer.publish(df)\n```\n\n----------------------------------------\n\nTITLE: Producing and Publishing CPU Metrics to Kafka - Quix Streams - Python\nDESCRIPTION: This Python code connects to a local Kafka broker using Quix Streams, collects real-time CPU and memory usage data with psutil, serializes the metrics as JSON, and publishes them to a specified Kafka topic ('cpu-load'). The main dependencies are psutil, json, time, and quixstreams, and the script defines a reusable main() function, handling graceful shutdown with a keyboard interrupt. Expected input is none (it is self-running), and output is continuous metric messages sent to Kafka; ensure Kafka is running locally and the required Python packages are installed.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/get-started/produce.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport psutil, time, json\nfrom quixstreams import Application\n\n# connect to your local Kafka broker\napp = Application(broker_address=\"localhost:9092\", consumer_group=\"producer-v1\")\n\n# configure the output topic to publish data to\noutput_topic = app.topic(\"cpu-load\")\n\n# get the cpu load (and memory usage)\ndef get_cpu_load():\n    cpu_load = psutil.cpu_percent(interval=1)\n    memory = psutil.swap_memory()\n    return {\n        \"cpu_load\": cpu_load,\n        \"memory\": memory._asdict(),\n        \"timestamp\": int(time.time_ns()),\n    }\n\ndef main():\n    # create a Producer to send data to the topic\n    with app.get_producer() as producer:\n        while True:                \n            # Get the current CPU and memory usage\n            message = get_cpu_load()\n            print(\"CPU load: \", message[\"cpu_load\"])\n\n            # publish a message to the output topic\n            producer.produce(\n            topic=output_topic.name,\n                key=\"server-1-cpu\",\n                value=json.dumps(message)\n            )\n\nif __name__ == '__main__':\n    try:\n        main()\n    except KeyboardInterrupt:\n        print('Exiting due to keyboard interrupt')\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Telemetry Parameters with SignalR in JavaScript\nDESCRIPTION: This snippet demonstrates how the UI uses the SignalR client to subscribe to real-time telemetry parameters (\\\"x\\\", \\\"y\\\", \\\"angle\\\", \\\"speed\\\") from the \\\"car-game-control\\\" topic by invoking the \\\"SubscribeToParameter\\\" method. Each subscription uses a unique stream ID that helps distinguish the data source. Dependencies include the SignalR JavaScript client, an established WebSocket connection variable named \\\"connection\\\", and context-specific variables such as \\\"stream_id\\\". The expected input is parameter names as strings; the output is real-time parameter data delivered via SignalR events. The code must be placed after connection initialization.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/data-stream-processing/data-stream-processing.md#2025-04-23_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nconnection.invoke(\"SubscribeToParameter\", \"car-game-control\", stream_id + \"-car-game-input-control\", \"x\");\\nconnection.invoke(\"SubscribeToParameter\", \"car-game-control\", stream_id + \"-car-game-input-control\", \"y\");\\nconnection.invoke(\"SubscribeToParameter\", \"car-game-control\", stream_id + \"-car-game-input-control\", \"angle\");\\nconnection.invoke(\"SubscribeToParameter\", \"car-game-control\", stream_id + \"-car-game-input-control\", \"speed\");\n```\n\n----------------------------------------\n\nTITLE: Stateful Sustained CPU Load Alerting with Tumbling Window using Quix Streams in Python\nDESCRIPTION: This advanced Python script uses Quix Streams and state management to detect sustained high CPU load and send only a single alert per event. It calculates the average 'cpu_load' over 10-second tumbling windows. A stateful filter function (`is_alert`) checks if the average load exceeds 20 and if an alert has already been sent for the current high-load period using `State`. An alert is generated and sent to the output topic only when the threshold is first crossed. The state resets when the load drops below the threshold. Requires `quixstreams`, `os`, `datetime.timedelta`, `quixstreams.State`, and topic environment variables.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/influxdb-alerting/threshold-detection.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom quixstreams import Application, State\nfrom datetime import timedelta\n\napp = Application()\n\ninput_topic = app.topic(os.environ[\"input\"])\noutput_topic = app.topic(os.environ[\"output\"])\n\nsdf = app.dataframe(input_topic)\n\nsdf = sdf.apply(lambda row: row[\"cpu_load\"]) \\\n    .tumbling_window(timedelta(seconds=10)).mean().final()\n\nsdf[\"window_duration_s\"] = (sdf[\"end\"] - sdf[\"start\"]) / 1000\n\ndef is_alert(row: dict, state: State):\n    \n    is_alert_sent_state = state.get(\"is_alert_sent\", False)\n    \n    if row[\"cpu_load\"] > 20:\n        if not is_alert_sent_state:\n            state.set(\"is_alert_sent\", True)\n            return True        \n        else:\n            return False\n    else:\n        state.set(\"is_alert_sent\", False)\n        return False\n        \n\nsdf = sdf.filter(is_alert, stateful=True)\n\n# Produce message payload with alert.\nsdf = sdf.apply(lambda row: {\n    \"summary\": \"CPU overload\",\n    \"source\": \"custom_event\",\n    \"severity\": \"critical\",\n    \"custom_details\": {\n        \"timestamp\": row[\"timestamp\"],\n        \"message\": \"CPU value is \" + str(row[\"cpu_load\"])\n    }\n})\n\nsdf = sdf.to_topic(output_topic)\n\nif __name__ == \"__main__\":\n    app.run(sdf)\n\n```\n\n----------------------------------------\n\nTITLE: Embedding Function for Document Rows – Python\nDESCRIPTION: Defines a function to create dense vector embeddings for a document's description field, using the loaded encoder, and converting results to a compatible list for Qdrant. Dependencies: encoder (SentenceTransformer), time. Input: a row with 'doc_descr'. Output: list of floats (embedding), and logs embedding vector. Waits briefly to throttle processing.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef create_embeddings(row):\\n    text = row['doc_descr']\\n    embeddings = encoder.encode(text)\\n    embedding_list = embeddings.tolist() # Conversion step because SentenceTransformer outputs a numpy array but Qdrant expects a plain list\\n    print(f'Created vector: \\\"{embedding_list}\\\"')\\n    time.sleep(0.2) # Adding small pause since Colab sometimes chokes\\n\\n    return embedding_list\n```\n\n----------------------------------------\n\nTITLE: Handling PackageReceived Event in JavaScript with SignalR\nDESCRIPTION: This code snippet demonstrates how to establish a SignalR connection, subscribe to packages, and handle the PackageReceived event. It includes connecting to the Quix hub, invoking the SubscribeToPackages method, and logging the received package data.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/subscriptions.md#2025-04-23_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\n\nconst options = {\n    accessTokenFactory: () => '<your-PAT>'\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n      .withUrl(\"https://reader-joeengland-apitests-testing.platform.quix.io/hub\", options)\n      .build();\n\n// Establish connection \nconnection.start().then(() => {\n    console.log(\"Connected to Quix.\");\n\n    connection.invoke(\"SubscribeToPackages\", \"f1-data\");\n\n    connection.on(\"PackageReceived\", (package) => {\n        console.log('package -----> ', package);\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Serving SMS Alerts on CPU Spikes using Quix Streams and Vonage API in Python\nDESCRIPTION: This Python code implements a streaming application using the Quix Streams SDK to monitor incoming CPU spike events and optionally send SMS notifications to administrators by integrating with the Vonage API. Key dependencies are the 'quixstreams' (Quix SDK) and, optionally, 'vonage', which must be added to requirements.txt and environment variables (VONAGE_API_KEY, VONAGE_API_SECRET, TO_NUMBER) set if SMS functionality is enabled by setting 'SEND_SMS = True'. The application defines input topics, processes incoming DataFrames, triggers alert logic, and runs as a service without requiring manual polling. Input is a CPU spike data row; output is an SMS notification or console log. If 'SEND_SMS' is False, SMS logic is bypassed but messages are still printed for monitoring. The code is self-contained for deployment but depends on external environment configuration.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-cloud/quixtour/serve-sms.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nimport os\n\n# Set this to True if you want to actually send an SMS (you'll need a free Vonage account)\nSEND_SMS = False \n\nif SEND_SMS:\n    # Configure Vonage API\n    # add vonage module to requirements.txt to pip install it\n    import vonage \n    vonage_key = os.environ[\"VONAGE_API_KEY\"]\n    vonage_secret = os.environ[\"VONAGE_API_SECRET\"]\n    to_number = os.environ[\"TO_NUMBER\"]\n\n    client = vonage.Client(key=vonage_key, secret=vonage_secret)\n    sms = vonage.Sms(client)\n\ndef send_sms(message):\n    \"\"\"\n    Send an SMS using Vonage API\n    \"\"\"\n    print(\"Sending SMS message to admin...\")\n    response_data = sms.send_message(\n        {\n            \"from\": \"Vonage APIs\",\n            \"to\": to_number,\n            \"text\": message,\n        }\n    )\n\n    if response_data[\"messages\"][0][\"status\"] == \"0\":\n        print(\"Message sent successfully.\")\n    else:\n        print(f\"Message failed with error: {response_data['messages'][0]['error-text']}\")\n    return\n\ndef send_alert(row):\n    \"\"\"\n    Trigger a CPU spike alert and send an SMS notification\n    \"\"\"\n    if SEND_SMS:\n        send_sms(row)\n\n# Create an Application\n# It will get the SDK token from environment variables to connect to Quix Kafka\napp = Application()\n\n# Define an input topic\ninput_topic = app.topic(os.environ[\"input\"])\n\n# Create a StreamingDataFrame to process data\nsdf = app.dataframe(input_topic)\n\n# Trigger the \"send_alert\" function for each incoming message\nsdf = sdf.update(send_alert)\n\n# Print messages to the console\nsdf = sdf.update(lambda row: print(row))\n\nif __name__ == \"__main__\":\n    # Run the Application\n    app.run(sdf)    \n```\n\n----------------------------------------\n\nTITLE: Creating Tumbling Windows with Different Output Behaviors in Quix Streams\nDESCRIPTION: This snippet demonstrates two ways of creating tumbling windows in Quix Streams. The first example creates a 1-hour tumbling window that emits results only when the window closes. The second example creates a similar window but emits results for each incoming message.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/windowing-stream-processing-guide.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create a tumbling window of 1 hour and emit results only when the window is closed\nsdf = (\n    sdf.tumbling_window(duration_ms=timedelta(hours=1))\n    .sum()\n    .final()\n)\n\n# Same tumbling window, but emitting results for each incoming message\nsdf = (\n    sdf.tumbling_window(duration_ms=timedelta(hours=1))\n    .sum()\n    .current()\n)\n```\n\n----------------------------------------\n\nTITLE: Reading Mouse Positions from Quix Topic via SignalR in JavaScript/HTML\nDESCRIPTION: This code snippet illustrates how to consume real-time event data from a Quix stream using the Quix Streaming Reader API, JavaScript, and SignalR. After including the SignalR client library, a WebSocket connection is established to a Quix reader hub using a PAT token and environment id. Upon connection, the script subscribes to a topic and listens for incoming packages, parses the mouse position payload, logs metadata, and draws it onto an HTML canvas. Inputs are real-time event packets from Quix, outputs are visual canvas updates. Prerequisites include correct PAT, environment id, and matching topic/stream. The snippet assumes appropriate permissions and that the writer is sending matching data.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/streaming-apis.md#2025-04-23_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<!DOCTYPE html>\\n<script\\n  src=\\\"https://cdnjs.cloudflare.com/ajax/libs/microsoft-signalr/6.0.1/signalr.js\\\"\\n  crossorigin=\\\"anonymous\\\"\\n  referrerpolicy=\\\"no-referrer\\\"\\n></script>\\n<html>\\n  <body>\\n    <h2>Quix JavaScript Hello WebSockets</h2>\\n\\n    <canvas\\n      id=\\\"myCanvas\\\"\\n      width=\\\"500\\\"\\n      height=\\\"300\\\"\\n      style=\\\"border: 5px solid #000000\\\"\\n    >\\n    </canvas>\\n    <hr />\\n    <button onclick=\\\"clearCanvas()\\\">Clear canvas</button>\\n\\n    <script>\\n      const token = \\\"<your_pat_token>\\\"; // Obtain your PAT token from Quix\\n\\n      // Set the environment and Topic\\n      const environmentId = \\\"<your_environment>\\\";\\n      const topicName = \\\"transform\\\";\\n      const streamId = \\\"mouse-pos\\\";\\n      const canvas = document.getElementById(\\\"myCanvas\\\");\\n      const ctx = canvas.getContext(\\\"2d\\\");\\n      ctx.fillStyle = \\\"#FF0000\\\";\\n\\n      const options = {\\n        accessTokenFactory: () => token,\\n      };\\n\\n      const connection = new signalR.HubConnectionBuilder()\\n        .withUrl(`https://reader-${environmentId}.platform.quix.io/hub`, options)\\n        .build();\\n\\n      connection.start().then(() => {\\n        console.log(\\\"Connected to Quix\\\");\\n\\n        connection.invoke(\\\"SubscribeToPackages\\\", topicName);\\n\\n        connection.on(\\\"PackageReceived\\\", (data) => {\\n          let payload = JSON.parse(data.value);\\n          console.log(\\\"DATA (payload): ---->>>\", payload);\\n          console.log(\\n            \\\"DATA (payload - timestamp): ---->>>\",\\n            payload[0].Timestamp\\n          );\\n          console.log(\\\"DATA (payload - value): ---->>>\", payload[0].Value);\\n          let mousePos = JSON.parse(payload[0].Value);\\n          console.log(\\\"DATA (payload - value.x): ---->>>\", mousePos.x);\\n          console.log(\\\"DATA (payload - value.y): ---->>>\", mousePos.y);\\n          ctx.fillRect(mousePos.x, mousePos.y, 10, 10);\\n        });\\n      });\\n\\n      function clearCanvas() {\\n        ctx.clearRect(0, 0, 500, 300);\\n      }\\n    </script>\\n  </body>\\n</html>\n```\n\n----------------------------------------\n\nTITLE: Reading and Publishing CPU Load to Quix Streams with Python\nDESCRIPTION: This Python code initializes a Quix Streams application that reads the local CPU load using psutil, packages it with a Unix timestamp (in nanoseconds), and publishes the readings continuously to a Quix topic. It utilizes environment variables (loaded via python-dotenv) for configuration, including output topic and SDK token. The main dependencies are psutil, quixstreams, dotenv, json, os, and time. The output is a JSON object per message containing 'timestamp' and 'cpu_load', identified by a static stream key ('server-1-cpu'). Execution is interrupted with Ctrl+C, which triggers a graceful shutdown.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/influxdb-alerting/python-client.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport psutil\nimport os\nimport time\nimport json\nfrom quixstreams import Application\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\ndef get_cpu_load():\n    cpu_load = psutil.cpu_percent(interval=1)\n    return cpu_load\n\napp = Application()\noutput_topic = app.topic(os.environ[\"output\"])\n\ndef main():\n    with app.get_producer() as producer:\n        while True:        \n            cpu_load = get_cpu_load()\n            print(\"CPU load: \", cpu_load)\n            timestamp = int(time.time_ns()) # Quix timestamp is nano seconds\n            message = {\"timestamp\": timestamp, \"cpu_load\": cpu_load}\n                \n            producer.produce(\n                topic=output_topic.name,\n                key=\"server-1-cpu\",\n                value=json.dumps(message)\n            )\n\nif __name__ == '__main__':\n    try:\n        main()\n    except KeyboardInterrupt:\n        print('Exiting due to keyboard interrupt')\n```\n\n----------------------------------------\n\nTITLE: SignalR Writer API Connection\nDESCRIPTION: JavaScript code demonstrating how to establish a SignalR connection to the Writer API using PAT authentication\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/setup.md#2025-04-23_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_PAT\"\nconst environmentId = \"YOUR_ENVIRONMENT_ID\"\n\nconst options = {\n    accessTokenFactory: () => token\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://writer-\" + environmentId + \".platform.quix.io/hub\", options)\n    .build();\n\nconnection.start().then(() => console.log(\"SignalR connected.\"));\n```\n\n----------------------------------------\n\nTITLE: Defining a Quix Streams CPU Threshold Transformation in Python\nDESCRIPTION: This Python script uses the Quix Streams library (`quixstreams`) to define a data processing pipeline. It initializes an application, connects to input and output Kafka topics specified by environment variables ('input', 'output'), reads dataframes from the input topic, filters rows where the 'cpu_load' field is greater than 25, transforms the filtered rows into alert strings, prints the alerts to the console for monitoring, and sends them to the output topic. The application runs when the script is executed directly.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-cloud/quixtour/process-threshold.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n``` python\nfrom quixstreams import Application\nimport os\n\n# Create an Application\n# It will get the SDK token from environment variables to connect to Quix Kafka\napp = Application()\n\n# Define input and output topics\ninput_topic = app.topic(os.environ[\"input\"])\noutput_topic = app.topic(os.environ[\"output\"])\n\n# Create a StreamingDataFrame to process data\nsdf = app.dataframe(input_topic)\n\n# Filter in all rows where CPU load is over 25.\nsdf = sdf.filter(lambda row: row[\"cpu_load\"] > 25)\n\n# Produce message payload with alert.\nsdf = sdf.apply(lambda row: \"CPU value is \" + str(row[\"cpu_load\"]))\n\n# Print messages to the console\nsdf = sdf.update(lambda row: print(row))\n\n# Send messages to the output topic\nsdf = sdf.to_topic(output_topic)\n\nif __name__ == \"__main__\":\n    # Run the Application\n    app.run(sdf)\n```\n```\n\n----------------------------------------\n\nTITLE: Authenticating and Forwarding Webhooks with Flask, Quixstreams, and Waitress in Python\nDESCRIPTION: This snippet sets up a Flask web application that listens for POST requests on a '/webhook' endpoint, authenticates requests using an HMAC SHA-1 signature matched against a shared secret (from environment variables), and upon successful validation, serializes the received JSON and produces it to a Quix topic using Quixstreams. Dependencies include quixstreams, Flask, Waitress, hmac, hashlib, and correct environment variables (for 'shared_secret' and 'output'). The endpoint expects a signed JSON body with an 'x-signature' header, and only produces to the output topic if the signature is valid. In production, Waitress is used as the WSGI server for robustness and security. Limitations: the webhook endpoint as shown is only secure for services that properly sign requests; for others, authentication logic may need simplification.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/inbound-webhooks.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.models.serializers.quix import JSONSerializer, SerializationContext\nfrom flask import Flask, request\nfrom datetime import datetime\nfrom waitress import serve\nimport os\nimport json\nimport hmac\nimport hashlib\n\napp = Application()\n\nserializer = JSONSerializer()\noutput_topic = app.topic(os.environ[\"output\"])\nproducer = app.get_producer()\n\nflask_app = Flask(\"Sample Webhook\")\n\n# this is unauthenticated, anyone could post anything to you!\n@flask_app.route(\"/webhook\", methods=['POST'])\ndef webhook():\n    \n    # get the shared secret from environment variables\n    secret = os.environ[\"shared_secret\"]\n    # convert to a byte array\n    secret_bytes = bytearray(secret, \"utf-8\")\n\n    # get the signature from the headers\n    header_sig = request.headers['x-signature']\n\n    # compute a hash-based message authentication code (HMAC)\n    hex_digest = hmac.new(secret_bytes, request.get_data(), hashlib.sha1).hexdigest()\n\n    # compare the HMAC to the header signature provided by Segment\n    if(header_sig != hex_digest):\n        # if they don't match its no bueno\n        return \"ERROR\", 401\n    \n    with producer:\n        serialized_value = serializer(\n            value=request.json, ctx=SerializationContext(topic=output_topic.name)                                              \n        )\n        producer.produce(\n            topic=output_topic.name,\n            key=\"sample-webhook\",\n            value=serialized_value\n        )\n\n    return \"OK\", 200\n\nprint(\"CONNECTED!\")\n\n# you can use app.run for dev, but its not secure, stable or particularly efficient\n# qx.App.run(debug=True, host=\"0.0.0.0\", port=80)\n\n# use waitress instead for production\nserve(flask_app, host='0.0.0.0', port = 80)\n```\n\n----------------------------------------\n\nTITLE: Sending Parameter Data using cURL\nDESCRIPTION: Example of sending parameter data using cURL with timestamps and numeric values\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/send-data.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"https://${domain}.platform.quix.io/topics/${topicName}/streams/${streamId}/parameters/data\" \\\n    -X POST \\\n    -H \"Authorization: Bearer ${token}\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n             \"timestamps\": [1591733989000000000, 1591733990000000000, 1591733991000000000],\n             \"numericValues\": {\n                 \"SomeParameter1\": [10.01, 202.02, 303.03],\n                 \"SomeParameter2\": [400.04, 50.05, 60.06]\n             }\n        }'\n```\n\n----------------------------------------\n\nTITLE: Pushing System Metrics to Quix using Quix Streams in Python\nDESCRIPTION: This Python script utilizes the `quixstreams` library to send system metrics (CPU load and memory usage obtained via `psutil`) to a Quix topic named 'cpu-load'. It initializes a Quix Application, defines a JSON serializer, gets a producer, and continuously produces messages containing the metrics, timestamp, and a static key ('server-1-cpu') in an infinite loop until interrupted. Environment variables for Quix connection details are expected and loaded using `python-dotenv`.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/quix-streams.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport psutil, time, os\nfrom quixstreams import Application\nfrom quixstreams.models.serializers.quix import JSONSerializer, SerializationContext\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\ndef get_cpu_load():\n    cpu_load = psutil.cpu_percent(interval=1)\n    memory = psutil.swap_memory()\n    return {\n        \"cpu_load\": cpu_load,\n        \"memory\": memory._asdict(),\n        \"timestamp\": int(time.time_ns()),\n    }\n\napp = Application()\n\nserializer = JSONSerializer()\noutput_topic = app.topic(\"cpu-load\")\nproducer = app.get_producer()\n\ndef main():\n    while True:\n        cpu_load = get_cpu_load()\n        print(\"CPU load: \", cpu_load)\n        with producer:\n            serialized_value = serializer(\n                value=cpu_load, ctx=SerializationContext(topic=output_topic.name)\n            )\n            producer.produce(\n                topic=output_topic.name,\n                key=\"server-1-cpu\",\n                value=serialized_value\n            )\n\nif __name__ == '__main__':\n    try:\n        main()\n    except KeyboardInterrupt:\n        print('Exiting due to keyboard interrupt')\n```\n\n----------------------------------------\n\nTITLE: Sending Aggregated Data to Redis Cloud Periodically in Python\nDESCRIPTION: This function (`send_data_to_redis`) runs in an infinite loop, designed to be executed in a background thread. It periodically retrieves data from RocksDB (`db`), performs time-window filtering (last hour), calls various aggregation calculation functions (e.g., `aggregate_eight_hours`, `calculate_visits_last_15min`), sends the latest 100 raw data points as JSON to Redis Cloud (`r`), and then pauses for 1 second using `time.sleep(1)`. It includes basic error handling.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/data-aggregation.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef send_data_to_redis():\n    while True:\n        try:\n            # Append data and discard data older than 1 hour\n            last_hour_data = db[\"last_hour_data\"]\n            one_hour = pd.to_datetime(pd.Timestamp.now()) - pd.Timedelta(hours=1)\n            updated_last_hour_data = last_hour_data[last_hour_data[\"datetime\"] > one_hour]\n\n            if not updated_last_hour_data.equals(last_hour_data):\n                last_hour_data = updated_last_hour_data\n                db[\"last_hour_data\"] = last_hour_data\n\n            # This method uses its own rolling window, so we only have to pass the buffer\n            aggregate_eight_hours(db[\"eight_hours_aggregation\"].copy())\n\n            # Get average visits in the last 15 minutes\n            calculate_visits_last_15min(last_hour_data.copy())\n\n            # Get top viewed productId in the last hour, keep only productId, category and count\n            calculate_products_last_hour(last_hour_data.copy())\n\n            # Get latest 10 visitors details (date and time, ip and country)\n            calculate_10_last_visitors(last_hour_data.copy())\n\n            # Get category popularity in the last hour\n            calculate_category_popularity(last_hour_data.copy())\n\n            # Get device type popularity in the last 10 minutes\n            calculate_device_popularity(last_hour_data.copy())\n\n            # Send all data\n            sorted_data = last_hour_data.sort_values(by='datetime', ascending=False)\n            r.set(\"raw_data\", sorted_data.head(100).to_json())\n\n            # Sleep for 1 second\n            time.sleep(1)\n        except Exception as e:\n            print(\"Error in sender thread\", e)\n```\n\n----------------------------------------\n\nTITLE: Event Data Stream Subscription Handler\nDESCRIPTION: Example demonstrating how to subscribe to event data streams, handle received events, and perform unsubscription using SignalR connection.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/subscriptions.md#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\n\nconst options = {\n    accessTokenFactory: () => '<your-PAT>'\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n      .withUrl(\"https://reader-joeengland-apitests-testing.platform.quix.io/hub\", options)\n      .build();\n\n// Establish connection \nconnection.start().then(() => {\n    console.log(\"Connected to Quix.\");\n\n    // Subscribe to event data stream \n    connection.invoke(\"SubscribeToEvent\", \"topic-1\", \"*\", \"EventA\");\n\n    // Read event data from the stream \n    connection.on(\"EventDataReceived\", data => {\n        console.log('data --> ', data);\n\n        // Unsubscribe from stream \n        connection.invoke(\"UnsubscribeFromEvent\", \"topic-1\", \"*\", \"EventA\");\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Publishing Twitch Chat Messages to Quix Topic using Python\nDESCRIPTION: This Python function, `publish_chat_message`, demonstrates how to format and publish a single Twitch chat message to a Quix topic. It utilizes the Quix SDK (`qx`) to create a `TimeseriesData` object, adding the timestamp, message content ('chat-message'), and relevant tags (room, name, role). It then obtains or creates a specific stream on the `topic_producer` based on the channel name and publishes the structured time series data.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/sentiment-analysis/twitch-service.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n``` python\ndef publish_chat_message(user: str, message: str, channel: str, timestamp: datetime, role: str = \"Customer\"):\n    timeseries_data = qx.TimeseriesData()\n    timeseries_data \\\n        .add_timestamp(timestamp) \\\n        .add_value(\"chat-message\", message) \\\n        .add_tags({\"room\": \"channel\", \"name\": user, \"role\": role})\n\n    stream_producer = topic_producer.get_or_create_stream(channel)\n    stream_producer.timeseries.publish(timeseries_data)\n```\n```\n\n----------------------------------------\n\nTITLE: Detailed Format of Camera Data Value - JSON\nDESCRIPTION: This JSON snippet illustrates the detailed format of the 'Value' field within a published event, containing an enriched camera object with attributes such as id, geolocation, common name, and an array of additional properties (like stream URLs or status). No special dependencies are needed to parse this other than a standard JSON parser. Key parameters include metadata such as imageUrl and videoUrl, physical location, and descriptive camera data. This structure is essential for downstream consumers to extract links to video files or interpret camera metadata.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/tfl-camera-feed.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"$type\": \"Tfl.Api.Presentation.Entities.Place, Tfl.Api.Presentation.Entities\",\n  \"id\": \"JamCams_00001.03766\",\n  \"url\": \"/Place/JamCams_00001.03766\",\n  \"commonName\": \"A20 Sidcup Bypass/Perry St\",\n  \"placeType\": \"JamCam\",\n  \"additionalProperties\": [\n    {\n      \"$type\": \"Tfl.Api.Presentation.Entities.AdditionalProperties, Tfl.Api.Presentation.Entities\",\n      \"category\": \"payload\",\n      \"key\": \"available\",\n      \"sourceSystemKey\": \"JamCams\",\n      \"value\": \"false\",\n      \"modified\": \"2023-08-31T15:46:06.093Z\"\n    },\n    {\n      \"$type\": \"Tfl.Api.Presentation.Entities.AdditionalProperties, Tfl.Api.Presentation.Entities\",\n      \"category\": \"payload\",\n      \"key\": \"imageUrl\",\n      \"sourceSystemKey\": \"JamCams\",\n      \"value\": \"https://s3-eu-west-1.amazonaws.com/jamcams.tfl.gov.uk/00001.03766.jpg\",\n      \"modified\": \"2023-08-31T15:46:06.093Z\"\n    },\n    {\n      \"$type\": \"Tfl.Api.Presentation.Entities.AdditionalProperties, Tfl.Api.Presentation.Entities\",\n      \"category\": \"payload\",\n      \"key\": \"videoUrl\",\n      \"sourceSystemKey\": \"JamCams\",\n      \"value\": \"https://s3-eu-west-1.amazonaws.com/jamcams.tfl.gov.uk/00001.03766.mp4\",\n      \"modified\": \"2023-08-31T15:46:06.093Z\"\n    },\n    {\n      \"$type\": \"Tfl.Api.Presentation.Entities.AdditionalProperties, Tfl.Api.Presentation.Entities\",\n      \"category\": \"cameraView\",\n      \"key\": \"view\",\n      \"sourceSystemKey\": \"JamCams\",\n      \"value\": \"West - A222 Perry St Twds Chislehurst\",\n      \"modified\": \"2023-08-31T15:46:06.093Z\"\n    },\n    {\n      \"$type\": \"Tfl.Api.Presentation.Entities.AdditionalProperties, Tfl.Api.Presentation.Entities\",\n      \"category\": \"Description\",\n      \"key\": \"LastUpdated\",\n      \"sourceSystemKey\": \"JamCams\",\n      \"value\": \"Aug 31 2023  3:46PM\",\n      \"modified\": \"2023-08-31T15:46:06.093Z\"\n    }\n  ],\n  \"children\": [],\n  \"childrenUrls\": [],\n  \"lat\": 51.4183,\n  \"lon\": 0.09822\n}\n\n```\n\n----------------------------------------\n\nTITLE: Handling Events from Quix Streams (Python)\nDESCRIPTION: This event handler receives event data from a Quix stream and executes logic depending on the event's value ('ON' or 'OFF'). The snippet assumes integration with the Quix Streams event model and expects a 'data' parameter representing event payloads. It requires the 'qx' library and an event-consumer context.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/process/timeseries-events.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef on_event_data_received_handler(stream_consumer: qx.StreamConsumer, data: qx.EventData):\n    if data.value == \"ON\":\n        print (\"Process ON event\")\n        ...\n    if data.value == \"OFF\":\n        print (\"Process OFF event\")\n        ...\n    ...\n```\n\n----------------------------------------\n\nTITLE: Compressing Kafka Messages with zlib in Python using Quix Streams\nDESCRIPTION: This Python script demonstrates how to compress data using the `zlib` library before publishing it to a Kafka topic with Quix Streams. It defines a function `compress_message` that converts a dictionary to a string, encodes it as UTF-8 bytes, and then compresses it. The main part initializes a Quix application, retrieves system metrics (CPU load, memory), compresses the data, and produces it to the configured output topic using a `bytes` serializer. Dependencies include `psutil`, `time`, `os`, `zlib`, `quixstreams`, and `dotenv` for environment variable management.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/compressed-data.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport psutil, time, os, zlib\nfrom quixstreams import Application\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# compress message with zlib\ndef compress_message(data):\n    data = str(data) # convert dict to string before encoding as bytes\n    data = zlib.compress(data.encode('utf-8')) # encode as UTF-8 bytes and compress\n    return data\n\napp = Application()\n\noutput_topic = app.topic(os.environ[\"output_topic\"], value_serializer=\"bytes\")\n\ndef get_cpu_load():\n    cpu_load = psutil.cpu_percent(interval=1)\n    memory = psutil.swap_memory()\n    data = {\n        \"cpu_load\": cpu_load,\n        \"memory\": memory._asdict(),\n        \"timestamp\": int(time.time_ns()),\n    }\n    print(data[\"cpu_load\"])\n    return data\n\ndef main():\n    with app.get_producer() as producer:\n        while True:                \n            message = get_cpu_load()\n            compressed_message = compress_message(message)\n\n            producer.produce(\n            topic=output_topic.name,\n                key=\"server-1-cpu\",\n                value=compressed_message\n            )\n\nif __name__ == '__main__':\n    try:\n        main()\n    except KeyboardInterrupt:\n        print('Exiting due to keyboard interrupt')\n```\n\n----------------------------------------\n\nTITLE: Defining Pandas DataFrame Schema for Last Hour Data in Python\nDESCRIPTION: Initializes an empty Pandas DataFrame (`initial_df`) with a predefined schema. The columns represent various attributes of clickstream data, such as timestamps, user identifiers, product details, and user demographics, specifying their respective data types using `pd.Series`. This structure is intended to hold data for the most recent hour.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/data-aggregation.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncolumns = {\n    \"timestamp\": pd.Series(dtype='datetime64[ns]'),\n    \"original_timestamp\": pd.Series(dtype='int'),\n    \"userId\": pd.Series(dtype='str'),\n    \"ip\": pd.Series(dtype='str'),\n    \"userAgent\": pd.Series(dtype='str'),\n    \"productId\": pd.Series(dtype='str'),\n    \"category\": pd.Series(dtype='str'),\n    \"title\": pd.Series(dtype='str'),\n    \"gender\": pd.Series(dtype='str'),\n    \"country\": pd.Series(dtype='str'),\n    \"deviceType\": pd.Series(dtype='str'),\n    \"age\": pd.Series(dtype='int'),\n    \"birthdate\": pd.Series(dtype='datetime64[ns]'),\n    \"datetime\": pd.Series(dtype='datetime64[ns]')\n}\n\ninitial_df = pd.DataFrame(columns)\n```\n\n----------------------------------------\n\nTITLE: Converting MessagePack to JSON with Quix Streams in Python\nDESCRIPTION: This Python script uses Quix Streams to read binary MessagePack data from an input topic (specified by the 'input' environment variable), unpacks it using 'msgpack.unpackb' via the 'apply' function, prints the unpacked row, and writes the result as JSON to an output topic (specified by 'output' environment variable). It requires 'quixstreams', 'os', 'msgpack', and 'dotenv'.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/integrations/databases/influxdb/replacing-flux.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n``` python\nimport os\nfrom quixstreams import Application\nimport msgpack\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\ndef unpack(row):\n    return msgpack.unpackb(row)\n\napp = Application()\n\ninput_topic = app.topic(os.environ[\"input\"], value_deserializer=\"bytes\")\noutput_topic = app.topic(os.environ[\"output\"], value_serializer=\"json\")\n\nsdf = app.dataframe(input_topic)\nsdf = sdf.apply(unpack)\nsdf = sdf.update(lambda row: print(row))\nsdf = sdf.to_topic(output_topic)\n\nif __name__ == \"__main__\":\n    app.run(sdf)\n```\n```\n\n----------------------------------------\n\nTITLE: Appending Incoming DataFrames to RocksDB for Last Hour Aggregation in Python\nDESCRIPTION: This code snippet, part of an `on_dataframe_handler` function, demonstrates appending new data to an existing DataFrame stored in RocksDB. It retrieves the 'last_hour_data' DataFrame from the RocksDB instance (`db`), uses `pd.concat` to append the incoming DataFrame (`df`), and then persists the updated combined DataFrame back into RocksDB under the same key.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/data-aggregation.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef on_dataframe_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\n    ...\n    # Append data to last_hour_data and save to database\n    last_hour_data = db[\"last_hour_data\"]\n    last_hour_data = pd.concat([last_hour_data, df], ignore_index=True)\n    db[\"last_hour_data\"] = last_hour_data\n```\n\n----------------------------------------\n\nTITLE: Implementing Balance Tracking with Python State Management\nDESCRIPTION: Example showing how to maintain running balance state while processing a stream of financial transactions. The code demonstrates basic stateful processing by tracking an account balance that updates with each transaction.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/navigating-stateful-stream-processing.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntransactions = [100, -50, 200, -100]  # Positive values are deposits, negative values are withdrawals\ncurrent_balance = 150 # Initial balance\n\n# Iterate over each transaction\nfor transaction in transactions:\n    current_balance += transaction  # Update the balance with each transaction\n    print(f\"Current balance: {current_balance}\")  # Print the balance after each transaction\n```\n\n----------------------------------------\n\nTITLE: Complete Quix Python Script for Crash Event Detection\nDESCRIPTION: This is the complete Python script (`main.py`) for the event detection service. It imports necessary libraries (`quixstreams`, `os`, `pandas`, `pickle`, `urllib.request`), downloads and loads the XGBoost model, sets up Quix stream consumers and producers connected to input/output topics defined by environment variables, defines handlers for incoming dataframes (`on_dataframe_received_handler` containing the detection logic) and events (`on_event_data_received_handler`), connects these handlers to the stream consumer, and starts the Quix application loop (`qx.App.run()`) to listen for streams.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/event-detection/crash-detection.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport quixstreams as qx\nimport os\nimport pandas as pd\nimport pickle\nfrom urllib import request\n\n# download the model with urllib\nf = request.urlopen(\"https://quixtutorials.blob.core.windows.net/tutorials/event-detection/XGB_model.pkl\")\nwith open(\"XGB_model.pkl\", \"wb\") as model_file:\n\tmodel_file.write(f.read())\n\n# load it with pickle\nloaded_model = pickle.load(open(\"XGB_model.pkl\", 'rb'))\n\nclient = qx.QuixStreamingClient()\n\ntopic_consumer = client.get_topic_consumer(os.environ[\"input\"], consumer_group = \"empty-transformation\")\ntopic_producer = client.get_topic_producer(os.environ[\"output\"])\n\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\n\n\t# Transform data frame here in this method. You can filter data or add new features.\n\t# Pass modified data frame to output stream using stream producer.\n\t# Set the output stream id to the same as the input stream or change it,\n\t# if you grouped or merged data with different key.\n\tstream_producer = topic_producer.get_or_create_stream(stream_id = stream_consumer.stream_id)\n\n\tif \"gForceX\" in df:\n\t\tdf[\"gForceTotal\"] = df[\"gForceX\"].abs() +  df[\"gForceY\"].abs() + df[\"gForceZ\"].abs()\n\t\tdf[\"shaking\"] = loaded_model.predict(df[[\"gForceZ\", \"gForceY\", \"gForceX\", \"gForceTotal\"]])\n\n\t\tif df[\"shaking\"].max() == 1: \n\t\t\tprint(\"Crash detected.\")\n\n\t\t\tstream_producer.events.add_timestamp_nanoseconds(df.iloc[0][\"timestamp\"]) \\\n\t\t\t\t.add_value(\"crash\", \"Crash detected.\") \\\n\t\t\t\t.publish()\n\n# Handle event data from samples that emit event data\ndef on_event_data_received_handler(stream_consumer: qx.StreamConsumer, data: qx.EventData):\n\tprint(data)\n\t# handle your event data here\n\ndef on_stream_received_handler(stream_consumer: qx.StreamConsumer):\n\t# subscribe to new DataFrames being received\n\t# if you aren't familiar with DataFrames there are other callbacks available\n\tstream_consumer.events.on_data_received = on_event_data_received_handler # register the event data callback\n\tstream_consumer.timeseries.on_dataframe_received = on_dataframe_received_handler\n\n\n# subscribe to new streams being received\ntopic_consumer.on_stream_received = on_stream_received_handler\n\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n\n# Handle termination signals and provide a graceful exit\nqx.App.run()\n```\n\n----------------------------------------\n\nTITLE: Configuring a Tumbling Window Aggregation Pipeline in Python\nDESCRIPTION: This snippet configures a Python streaming data pipeline to process input data with a 10-second tumbling window. The pipeline applies custom reducer and initializer functions to aggregate values as each window closes, outputting final aggregated metrics. Key dependencies include a data stream 'sdf', the use of 'timedelta' for window sizing, and pre-defined reducer/initializer callbacks. The output is a windowed, aggregated dataset ready for further publication. Parameters include the tumbling window size, reducer, and initializer, and correct configuration is required for accurate aggregation.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/predictive-maintenance/downsampling.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# create a tumbling window of 10 seconds\\n# use the reducer and initializer configured above\\n# get the 'final' values for the window once the window is closed.\\nsdf = (\\n    sdf.tumbling_window(timedelta(seconds=10))\\n    .reduce(reducer=reducer, initializer=initializer)\\n    .final()\\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Event Detection Logic in Quix Dataframe Handler (Python)\nDESCRIPTION: This Python function `on_dataframe_received_handler` is triggered when a new data frame (`df`) arrives on the input topic (`stream_consumer`). It calculates the total g-force (`gForceTotal`) from X, Y, and Z components, uses the pre-loaded XGBoost model (`loaded_model`) to predict if 'shaking' occurred, and if the prediction indicates shaking (value 1), it prints a detection message and publishes a 'crash' event to the output topic stream (`topic_producer`) using the Quix producer. Requires `pandas` (pd) and `quixstreams` (qx) libraries, and the previously loaded `loaded_model`.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/event-detection/crash-detection.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\n\n\t# Transform data frame here in this method. You can filter data or add new features.\n\t# Pass modified data frame to output stream using stream producer.\n\t# Set the output stream id to the same as the input stream or change it,\n\t# if you grouped or merged data with different key.\n\tstream_producer = topic_producer.get_or_create_stream(stream_id = stream_consumer.stream_id)\n\n\tif \"gForceX\" in df:\n\t\tdf[\"gForceTotal\"] = df[\"gForceX\"].abs() +  df[\"gForceY\"].abs() + df[\"gForceZ\"].abs()\n\t\tdf[\"shaking\"] = loaded_model.predict(df[[\"gForceZ\", \"gForceY\", \"gForceX\", \"gForceTotal\"]])\n\n\t\tif df[\"shaking\"].max() == 1: \n\t\t\tprint(\"Crash detected.\")\n\n\t\t\tstream_producer.events.add_timestamp_nanoseconds(df.iloc[0][\"timestamp\"]) \\\n\t\t\t\t.add_value(\"crash\", \"Crash detected.\") \\\n\t\t\t\t.publish()\n```\n\n----------------------------------------\n\nTITLE: Filtering Streams by Parameter ID using cURL\nDESCRIPTION: Shows how to filter streams based on the presence of specific parameters. This example finds streams containing the 'Gear' parameter.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-filtered.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"https://${domain}.platform.quix.io/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"parameterIds\": [ \"Gear\"] }'\n```\n\n----------------------------------------\n\nTITLE: Producing MessagePack Data with Quix Streams in Python\nDESCRIPTION: This Python script demonstrates how to collect system metrics (CPU load, swap memory) using 'psutil', pack them into MessagePack format using 'msgpack', and produce them to a Quix topic named 'cpu-load' using the Quix Streams producer. It specifies 'bytes' as the value serializer for the output topic and uses 'dotenv' to load environment variables.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/integrations/databases/influxdb/replacing-flux.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n``` python\nimport psutil, time, os, msgpack\nfrom quixstreams import Application\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\napp = Application()\n\noutput_topic = app.topic(\"cpu-load\", value_serializer=\"bytes\")\n    \ndef get_cpu_load():\n    cpu_load = psutil.cpu_percent(interval=1)\n    memory = psutil.swap_memory()\n    return {\n        \"cpu_load\": cpu_load,\n        \"memory\": memory._asdict(),\n        \"timestamp\": int(time.time_ns()),\n    }\n\ndef main():\n    with app.get_producer() as producer:\n        while True:                \n            message = get_cpu_load()\n            packed_message = msgpack.packb(message) # pack data in MessagePack format\n            \n            producer.produce(\n            topic=output_topic.name,\n                key=\"server-1-cpu\",\n                value=packed_message\n            )\n\nif __name__ == '__main__':\n    try:\n        main()\n    except KeyboardInterrupt:\n        print('Exiting due to keyboard interrupt')    \n```\n```\n\n----------------------------------------\n\nTITLE: Filtering TfL Video URL Metadata with Python\nDESCRIPTION: This Python code filters a list of additional properties from a TfL camera object to retrieve the property whose key is 'videoUrl'. It assumes the 'camera' dictionary has an 'additionalProperties' field containing key-value pairs. The snippet results in the extraction of the video URL needed for frame grabbing. Dependency: standard Python functions (filter, lambda, list). Input: a camera dictionary with additionalProperties. Output: a single dictionary entry with the video URL. Limitation: Assumes the 'videoUrl' exists; an IndexError occurs otherwise.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/tfl-frame-grabber.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncamera_video_feed = list(filter(lambda x: x[\"key\"] == \"videoUrl\", camera[\"additionalProperties\"]))[0]\n```\n\n----------------------------------------\n\nTITLE: Integrating PagerDuty Alerts in a Quix Pipeline using Python\nDESCRIPTION: This Python script uses the Quix Streams library to create a data processing application. It reads messages from an input topic, representing events like high CPU load. For each message, it formats an alert payload and sends it to PagerDuty's Events v2 API using an HTTPS POST request. The script requires the `PAGERDUTY_ROUTING_KEY` environment variable for authentication and includes a `USE_PAGER_DUTY` flag to toggle between sending real alerts and logging messages to the console. Dependencies include `quixstreams`, `os`, `json`, `datetime`, `http.client`, and `typing`.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/influxdb-alerting/add-alerting.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nfrom datetime import datetime\nfrom http.client import HTTPSConnection\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Optional\nfrom quixstreams import Application\n\nUSE_PAGER_DUTY = True # Set to False if you just want to log messages to the console\n\ndef build_alert(row: str, dedup: str) -> Dict[str, Any]:\n    routing_key = os.environ[\"PAGERDUTY_ROUTING_KEY\"]\n    return {\n        \"routing_key\": routing_key,\n        \"event_action\": \"trigger\",\n        \"dedup_key\": dedup,\n        \"payload\": row\n    }\n\ndef send_alert(row: str, dedup: Optional[str] = None) -> None:\n    # If no dedup is given, use epoch timestamp\n    if dedup is None:\n        dedup = str(datetime.utcnow().timestamp())\n    url = \"events.pagerduty.com\"\n    route = \"/v2/enqueue\"\n\n    conn = HTTPSConnection(host=url, port=443)\n    msg = json.dumps(build_alert(row, dedup))\n    conn.request(\"POST\", route, msg)\n    result = conn.getresponse()\n\n    print(\"Alert status: {}\".format(result.status))\n    print(\"Alert response: {}\".format(result.read()))\n\ndef pg_message(row):\n    if USE_PAGER_DUTY:\n        print(\"Sending PagerDuty alert\")\n        send_alert(row)  \n    else:\n        print(row)  \n    return\n\napp = Application()\ninput_topic = app.topic(os.environ[\"input\"])\n\nsdf = app.dataframe(input_topic)\nsdf = sdf.update(pg_message)\n\nif __name__ == \"__main__\":\n    app.run(sdf)\n```\n\n----------------------------------------\n\nTITLE: Stream Creation JSON Payload Example\nDESCRIPTION: Example JSON payload for creating a stream with name, location, and metadata configuration.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/create-stream.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"cardata\",\n    \"location\": \"simulations/trials\",\n    \"metadata\": {\n        \"rain\": \"light\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Training Decision Tree and Logging with MLflow\nDESCRIPTION: Trains a Decision Tree classifier model using KFold cross-validation. It defines features, sets hyperparameters (class_weight, max_depth), initializes MLflow logging for the run, logs parameters, iterates through KFold splits, trains the model, calculates training/testing accuracy, computes baseline accuracy, stores results in a DataFrame, and logs average metrics (train_accuracy, test_accuracy, fit_quality) to MLflow.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/train-ml.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel_accuracy = pd.DataFrame(columns=[\n    'Baseline Training Accuracy',\n    'Model Training Accuracy',\n    'Baseline Testing Accuracy',\n    'Model Testing Accuracy',\n])\n\nkfold = KFold(5, shuffle=True, random_state=1)\n\nwith mlflow.start_run():\n    class_weight = None\n    max_depth = 5\n    features = [\"Motion_WorldPositionX_cos\", \"Motion_WorldPositionX_sin\", \"Steer\", \"Speed\", \"Gear\"]\n\n    mlflow.log_param(\"class_weight\", class_weight)\n    mlflow.log_param(\"max_depth\", max_depth)\n    mlflow.log_param(\"features\", features)\n    mlflow.log_param(\"model_type\", \"DecisionTreeClassifier\")\n\n    X = df[features]\n    decision_tree = DecisionTreeClassifier(class_weight=class_weight, max_depth=max_depth)\n\n    for train, test in kfold.split(X):\n        X_train = X.iloc[train]\n        Y_train = Y.iloc[train]\n        X_test = X.iloc[test]\n        Y_test = Y.iloc[test]\n\n        # Train model\n        decision_tree.fit(X_train, Y_train)\n        Y_pred = decision_tree.predict(X_test)\n\n        # Assess accuracy\n        train_accuracy = round(decision_tree.score(X_train, Y_train) * 100, 2)\n        test_accuracy = round(decision_tree.score(X_test, Y_test) * 100, 2)\n\n        Y_baseline_zeros = np.zeros(Y_train.shape)\n        baseline_train_accuracy = round(accuracy_score(Y_train, Y_baseline_zeros) * 100, 2)\n        Y_baseline_zeros = np.zeros(Y_test.shape)\n        baseline_test_accuracy = round(accuracy_score(Y_test, Y_baseline_zeros) * 100, 2)\n\n        model_accuracy_i = pd.DataFrame({\n            \"Baseline Training Accuracy\": [baseline_train_accuracy],\n            \"Model Training Accuracy\": [train_accuracy],\n            \"Baseline Testing Accuracy\": [baseline_test_accuracy],\n            \"Model Testing Accuracy\": [test_accuracy]})\n        model_accuracy = pd.concat([model_accuracy, model_accuracy_i]).reset_index(drop=True)\n\n    mlflow.log_metric(\"train_accuracy\", model_accuracy[\"Model Training Accuracy\"].mean())\n    mlflow.log_metric(\"test_accuracy\", model_accuracy[\"Model Testing Accuracy\"].mean())\n    mlflow.log_metric(\"fit_quality\", 1/abs(model_accuracy[\"Model Training Accuracy\"].mean() - model_accuracy[\"Model Testing Accuracy\"].mean()))\n```\n\n----------------------------------------\n\nTITLE: Handling ActiveStreamsChanged Event in JavaScript with SignalR\nDESCRIPTION: This code snippet demonstrates how to establish a SignalR connection, subscribe to active streams, and handle the ActiveStreamsChanged event. It includes connecting to the Quix hub, invoking the SubscribeToActiveStreams method, and logging the received stream and action data.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/subscriptions.md#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\n\nconst options = {\n    accessTokenFactory: () => '<your-PAT>'\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n      .withUrl(\"https://reader-joeengland-apitests-testing.platform.quix.io/hub\", options)\n      .build();\n\n// Establish connection \nconnection.start().then(() => {\n    console.log(\"Connected to Quix.\");\n\n    connection.invoke(\"SubscribeToActiveStreams\", \"f1-data\");\n\n    connection.on(\"ActiveStreamsChanged\", (stream, action) => {\n        console.log('stream -----> ', stream);\n        console.log('action -----> ', action);\n\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Producing Documents into Kafka using Quix Producer – Python\nDESCRIPTION: Reads previously saved CSV data, generates unique document IDs, and produces each document as a Kafka message with a custom key and headers using the Quix Streams Producer. Dependencies: pandas, quixstreams, uuid, json, time. Key parameters: outputtopicname, offsetlimit. Input: DataFrame rows from CSV. Output: messages to specified Kafka topic. Stops after last row.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv('/content/documents.csv')\\noutputtopicname = docs_topic_name\\noffsetlimit = len(df)-2\\nprint(f\\\"Producing to output topic: {outputtopicname}...\\\\n\\\\n\\\")\\n\\nwith Producer(\\n    broker_address=\\\"127.0.0.1:9092\\\",\\n    extra_config={\\\"allow.auto.create.topics\\\": \\\"true\\\"},\\n) as producer:\\n    for index, row in df.iterrows():\\n        doc_id = index\\n        doc_key = f\\\"A{'0'*(10-len(str(doc_id)))}{doc_id}\\\"\\n        doc_uuid = str(uuid.uuid4())\\n        value = {\\n            \\\"Timestamp\\\": time.time_ns(),\\n            \\\"doc_id\\\": doc_id,\\n            \\\"doc_uuid\\\": doc_uuid,\\n            \\\"doc_name\\\": row['name'],\\n            \\\"doc_descr\\\": row['description'],\\n            \\\"doc_year\\\": row['year'],\\n        }\\n        print(f\\\"Producing value: {value}\\\")\\n        producer.produce(\\n            topic=outputtopicname,\\n            headers=[(\\\"uuid\\\", doc_uuid)],  # a dict is also allowed here\\n            key=doc_key,\\n            value=json.dumps(value),  # needs to be a string\\n        )\n```\n\n----------------------------------------\n\nTITLE: Illustrating 'under-forecast' JSON Alert Message Format\nDESCRIPTION: Shows the JSON structure for an 'under-forecast' alert, generated when a forecasted parameter ('forecast_fluctuated_ambient_temperature') is predicted to fall below its minimum threshold (45ºC). The message contains the 'under-forecast' status, parameter name, timestamp, the forecasted temperature, and a message stating the estimated time until the temperature drops below the threshold.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/predictive-maintenance/alert-service.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"under-forecast\",\n  \"parameter_name\": \"forecast_fluctuated_ambient_temperature\",\n  \"alert_temperature\": 44.98135836928914,\n  \"timestamp\": 1701277320000000000,\n  \"message\": \"'Ambient temperature' is forecasted to fall below 45ºC in 1:20:28.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Publishing Camera Event Data to Quix Stream Topic - Python\nDESCRIPTION: This Python snippet publishes camera event data to a Quix Streams topic, creating a separate stream for each camera based on its unique ID. It attaches a timestamp (in nanoseconds) and the serialized camera data (as a JSON string), then publishes the event. Dependencies include a Quix Streams producer topic object, a `camera_id`, a `camera` data structure, and the `json` and `time` modules. Inputs are individual camera details, and output is an event published to the corresponding stream partition. Stream naming is critical for horizontal scalability and message ordering.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/tfl-camera-feed.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nproducer_topic.get_or_create_stream(camera_id).events.add_timestamp_nanoseconds(time.time_ns()) \\\n    .add_value(\"camera\", json.dumps(camera)) \\\n    .publish()    \n\n```\n\n----------------------------------------\n\nTITLE: Generating Test Data for Simulink Model with Quix Streams in Python\nDESCRIPTION: This Python script acts as a data source, generating test data for the Simulink engine model. It uses the Quix Streams library to connect to an output Kafka topic specified by the `output` environment variable. In an infinite loop, it generates a random `throttle_angle` value every second, packages it into a `TimeseriesData` object with the current timestamp, and publishes it to the specified output topic. It depends on `quixstreams`, `time`, `datetime`, `os`, and `random` libraries.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport quixstreams as qx\nimport time\nimport datetime\nimport os\nimport random\t\nclient = qx.QuixStreamingClient()\ntopic_producer = client.get_topic_producer(topic_id_or_name = os.environ[\"output\"])\nstream = topic_producer.create_stream()\t\nwhile True:\n    throttle_angle = random.uniform(8, 12)\n    data = qx.TimeseriesData()\n    data.add_timestamp(datetime.datetime.utcnow()) \\\n        .add_value(\"throttle_angle\", throttle_angle)\n    stream.timeseries.publish(data)\n    time.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Tumbling Windows with Quix Streams in Python\nDESCRIPTION: This code snippet shows the beginning of an implementation for tumbling windows over a StreamingDataFrame using Quix Streams. It initializes the application, connects to a Kafka topic, and prepares the data frame, but the actual tumbling window implementation is incomplete in the provided code.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/windowing-stream-processing-guide.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom datetime import timedelta\n\napp = Application(broker_address='localhost:9092')\ntopic = app.topic('input-topic')\nsdf = app.dataframe(topic)\n```\n\n----------------------------------------\n\nTITLE: Generating Random 2D Unit Vectors in Python for Quix Streaming\nDESCRIPTION: This Python script serves as a Quix Streams data source, producing randomly generated 2D unit vectors at regular intervals. It connects to a Quix output topic using 'quixstreams', publishes vector values with normalized coordinates ('x', 'y'), and supports live pipeline integration for testing deployed MATLAB functions. Dependencies: 'quixstreams', standard Python math/time/os/random libraries, and appropriate Quix environment variables.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport quixstreams as qx\nimport time\nimport datetime\nimport os\nimport random, math\n\nclient = qx.QuixStreamingClient()\ntopic_producer = client.get_topic_producer(topic_id_or_name = os.environ[\"output\"])\n\nstream = topic_producer.get_or_create_stream(\"vectors\")\nindex = 0\nwhile True:\n    xa = random.uniform(-1, 1)\n    ya = random.uniform(-1, 1)\n    vlen = math.sqrt(xa ** 2 + ya ** 2)\n    stream.timeseries \\\n        .buffer \\\n        .add_timestamp(datetime.datetime.utcnow()) \\\n        .add_value(\"x\", xa / vlen) \\\n        .add_value(\"y\", ya / vlen) \\\n        .publish()\n    time.sleep(0.5)\n```\n\n----------------------------------------\n\nTITLE: Initializing CSV Data Source with Quix Streams in Python\nDESCRIPTION: This code initializes a Quix Streams application, reads data from a CSV file, and publishes it to a Kafka topic. It uses pandas for CSV handling and implements a continuous streaming loop.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/data-enrichment-with-quix-and-redis.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Import the Quix Streams modules for interacting with Kafka:\nfrom quixstreams import Application\nfrom quixstreams.models.serializers.quix import JSONSerializer, SerializationContext\n\nimport pandas as pd\nimport random\nimport time\nimport os\n\n# for local dev, load env vars from a .env file\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Create an Application\napp = Application(consumer_group=\"csv_sample\", auto_create_topics=True)\n\n# Define the topic using the \"output\" environment variable\ntopic = app.topic(name=os.environ[\"output\"], value_serializer=\"json\")\n\n# Get the directory of the current script and construct the path to the CSV file\nscript_dir = os.path.dirname(os.path.realpath(__file__))\ncsv_file_path = os.path.join(script_dir, \"demo-data.csv\")\n\n# this function loads the file and sends each row to the publisher\ndef read_csv_file(file_path: str):\n    \"\"\"\n    Code truncated for brevity. The \"read_csv_file\" function is covered in more detail in the next section.\n    ....\n    \"\"\"\n\n\ndef main():\n    \"\"\"\n    Create a pre-configured Producer object.\n    Producer is already setup to use Quix brokers.\n    It will also ensure that the topics exist before producing to them if\n    Application() is initialized with \"auto_create_topics=True\".\n    \"\"\"\n    \n    with app.get_producer() as producer:\n        # Iterate over the data from CSV file\n        # read_csv_file will be implemented further down\n        for message_key, row_data in read_csv_file(file_path=csv_file_path):\n            # Serialize row value to bytes\n            serialized_value = serializer(\n                value=row_data, ctx=SerializationContext(topic=topic.name)\n            )\n\n            # publish the data to the topic\n            producer.produce(\n                topic=topic.name,\n                key=message_key,\n                value=serialized_value,\n            )\n\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(\"Exiting.\")\n```\n\n----------------------------------------\n\nTITLE: Connecting to Quix SignalR Hub and Handling Parameter Data (JavaScript)\nDESCRIPTION: This JavaScript snippet, likely from an Angular component's `ngAfterViewInit` method, demonstrates connecting to the Quix platform using a `quixService`. It establishes a SignalR connection, registers a callback function to process incoming 'ParameterDataReceived' events, subscribes to the relevant data stream, and includes logic to re-subscribe upon reconnection. It depends on a `quixService` for initialization and connection management and assumes a `ParameterData` type is defined.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/web-ui.md#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nngAfterViewInit(): void {\nthis.getInitialData();\n\nthis.quixService.initCompleted$.subscribe((topicName) => {\n    this._topicName = topicName;\n\n    this.quixService.ConnectToQuix().then(connection => {\n    this.connection = connection;\n    this.connection.on('ParameterDataReceived', (data: ParameterData) => {\n        this._parameterDataReceived$.next(data);\n    });\n    this.subscribeToData();\n\n    this.connection.onreconnected((connectionId?: string) => {\n        if (connectionId) this.subscribeToData();\n    });\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Example Output Message Format for Object Detection Service in JSON\nDESCRIPTION: This JSON object illustrates the structure of messages published by the object detection service to its output topic. It includes timestamps, numeric values (such as counts of detected objects like 'car', 'truck', 'person', geographical coordinates 'lat', 'lon', and a 'delta' value), string values, and tag values. The primary data points are the object counts within the 'NumericValues' field.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/object-detection.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Epoch\": 0,\n  \"Timestamps\": [\n    1694003142728625200\n  ],\n  \"NumericValues\": {\n    \"car\": [\n      5\n    ],\n    \"truck\": [\n      2\n    ],\n    \"person\": [\n      2\n    ],\n    \"traffic light\": [\n      1\n    ],\n    \"lat\": [\n      51.4739\n    ],\n    \"lon\": [\n      -0.09045\n    ],\n    \"delta\": [\n      -2.597770929336548\n    ]\n  },\n  \"StringValues\": {},\n  \"TagValues\": {}\n}\n```\n\n----------------------------------------\n\nTITLE: Detecting Sustained CPU Load with Tumbling Window using Quix Streams in Python\nDESCRIPTION: This Python script utilizes Quix Streams to detect sustained high CPU load. It reads data from the input topic, applies a 10-second tumbling window to the 'cpu_load' field, calculates the mean for each window, and filters windows where the average exceeds 20. It then calculates the window duration, creates a JSON alert payload indicating the average load and duration, and sends it to the output topic. Dependencies include `quixstreams`, `os`, `datetime.timedelta`, and environment variables for topics.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/influxdb-alerting/threshold-detection.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom quixstreams import Application\nfrom datetime import timedelta\n\napp = Application()\n\ninput_topic = app.topic(os.environ[\"input\"])\noutput_topic = app.topic(os.environ[\"output\"])\n\nsdf = app.dataframe(input_topic)\n\nsdf = sdf.apply(lambda row: row[\"cpu_load\"]) \\\n    .tumbling_window(timedelta(seconds=10)).mean().final()\n\n# Filter all rows where CPU load is over 20.\nsdf = sdf.filter(lambda row: row[\"cpu_load\"] > 20)\n\nsdf[\"window_duration_s\"] = (sdf[\"end\"] - sdf[\"start\"]) / 1000\n\n# Produce message payload with alert.\nsdf = sdf.apply(lambda row: {\n    \"summary\": \"Windowed CPU overload\",\n    \"source\": \"custom_event\",\n    \"severity\": \"critical\",\n    \"custom_details\": {\n        \"timestamp\": row[\"end\"],\n        \"message\": f\"CPU {row[\\\"cpu_load\\\"]} for duration of {row[\\\"window_duration_s\\\"]} seconds.\"\n    }\n})\n\nsdf = sdf.to_topic(output_topic)\n\nif __name__ == \"__main__\":\n    app.run(sdf)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Consumer for Vector Ingestion in Python\nDESCRIPTION: Sets up a Kafka consumer application to process vector embeddings. The consumer reads from a specified topic, deserializes JSON messages, and calls the ingestion function for each message, storing the vectors in Qdrant.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ninputtopicname = vectors_topic_name\n\n# Create a special stop condition just for this Notebook (otherwise the cell will run indefinitely)\nprint(f\"Using offset limit {offsetlimit}\")\ndef on_message_processed(topic, partition, offset):\n    if offset > offsetlimit:\n        app.stop()\n\n# Define the consumer application and settings\napp = Application(\n    broker_address=\"127.0.0.1:9092\",\n    consumer_group=\"vectorizer\",\n    auto_offset_reset=\"earliest\",\n    on_message_processed=on_message_processed,\n    consumer_extra_config={\"allow.auto.create.topics\": \"true\"},\n)\n\n# Define an input topic with JSON deserializer\ninput_topic = app.topic(inputtopicname, value_deserializer=\"json\")\nprint(f\"Consuming from input topic: {inputtopicname}\")\n\n# Initialize a streaming dataframe based on the stream of messages from the input topic:\nsdf = app.dataframe(topic=input_topic)\n\n# INGESTION HAPPENS HERE\n### Trigger the embedding function for any new messages(rows) detected in the filtered SDF\nsdf = sdf.update(lambda row: ingest_vectors(row))\napp.run(sdf)\n```\n\n----------------------------------------\n\nTITLE: Dynamically Installing TA-Lib Dependency in Python\nDESCRIPTION: This Python script checks if the TA-Lib library and its underlying system dependencies are installed. If not present (common in the online IDE environment but not in deployment), it uses OS commands to download the TA-Lib source, verify its integrity via MD5 checksum, build it using `make`, install it system-wide, and finally install the corresponding Python `TA-Lib` pip package. This script is intended to be saved as `preinstall.py` and imported at the beginning of the main application script (`main.py`) to ensure TA-Lib is available when using the 'Run' feature in the IDE. It relies on `apt-get`, `curl`, `tar`, `make`, and `pip` being available in the environment.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/manage/troubleshooting.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport sys\n\nta_lib_pip_details = os.system(\"python3 -m pip show TA-Lib\")\nif ta_lib_pip_details == 0:\n    print(\"TA-Lib already installed\")\nelse:\n    if os.system(\"apt-get update\") != 0:\n        print(\"Failed apt-get update\")\n        sys.exit(1)\n    if os.popen(\"if [ -e ta-lib-0.4.0-src.tar.gz ]; then echo \\\"ok\\\"; else echo \\\"nok\\\"; fi\").read().strip() == \"ok\":\n        print(\"TA-Lib already downloaded\")\n    else:\n        print(\"Downloading ta-lib\")\n        if os.system(\"apt-get install curl -y\") != 0:\n            print(\"Failed apt-get install curl -y\")\n            sys.exit(1)\n        if os.system(\"curl https://jztkft.dl.sourceforge.net/project/ta-lib/ta-lib/0.4.0/ta-lib-0.4.0-src.tar.gz -O\") != 0:\n            print(\"Failed to download ta-lib\")\n            sys.exit(1)\n\n    zipmdsum = os.popen(\"md5sum ta-lib-0.4.0-src.tar.gz | cut -d ' ' -f 1\").read().strip()\n    if zipmdsum == \"308e53b9644213fc29262f36b9d3d9b9\":\n        print(\"TA-Lib validated\")\n    else:\n        print(\"TA-Lib has incorrect hash value, can't trust it. Found hash: '\" + str(zipmdsum) + \"'\")\n        sys.exit(1)\n\n    if os.system(\"tar -xzf ta-lib-0.4.0-src.tar.gz\") != 0:\n        print(\"Failed to extract TA-Lib zip\")\n        sys.exit(1)\n\n    if os.system(\"apt-get install build-essential -y\") != 0:\n        print(\"Failed apt-get install build-essential -y\")\n        sys.exit(1)\n\n    os.chdir(os.path.abspath(\".\") + \"/ta-lib\")\n\n    if os.system(\"./configure --prefix=/usr\") != 0:\n        print(\"Failed to configure TA-Lib for build\")\n        sys.exit(1)\n\n    if os.system(\"make\") != 0:\n        print(\"Failed to make TA-Lib\")\n        sys.exit(1)\n\n    if os.system(\"make install\") != 0:\n        print(\"Failed to make install TA-Lib\")\n        sys.exit(1)\n\n    print(\"Installed dependencies for TA-Lib pip package\")\n\n    if os.system(\"python3 -m pip install TA-Lib\") != 0:\n        print(\"Failed to pip install TA-Lib\")\n        sys.exit(1)\n\n    print(\"Installed TA-Lib pip package\")\n```\n\n----------------------------------------\n\nTITLE: Sample Quix Streams Time Series Message Format in JSON\nDESCRIPTION: This JSON snippet demonstrates the payload format of a published frame message sent to the next pipeline stage via Quix Streams. It includes epoch, timestamps, numeric geolocation values (longitude and latitude), a binary image field, and placeholder for other field types. Used for understanding output structure when debugging or integrating with downstream services. No dependencies. Input: None (example only). Output: Dictionary structure for time series data.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/tfl-frame-grabber.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Epoch\": 0,\n  \"Timestamps\": [\n    1693998068342837200\n  ],\n  \"NumericValues\": {\n    \"lon\": [\n      0.22112\n    ],\n    \"lat\": [\n      51.50047\n    ]\n  },\n  \"StringValues\": {},\n  \"BinaryValues\": {\n    \"image\": [\n      \"(Binary of 31.67 KB)\"\n    ]\n  },\n  \"TagValues\": {}\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Quix Streams Consumer Application in Python\nDESCRIPTION: This Python script demonstrates how to set up a simple Kafka consumer using the Quix Streams library. It initializes an `Application` to connect to a local Kafka broker (`localhost:9092`), defines a consumer group (`consume-v1`), and sets the offset reset policy. It then subscribes to the `cpu-load` topic, creates a streaming DataFrame (`sdf`), and defines a processing step to print each consumed message row as a JSON string. The `app.run(sdf)` command starts the application's processing loop.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/get-started/consume.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom datetime import timedelta\nimport json\n\n# connect to your local Kafka broker\napp = Application(\n    broker_address=\"localhost:9092\",\n    consumer_group=\"consume-v1\",\n    auto_offset_reset=\"earliest\",\n)\n\n# configure the input topic to subscribe to (you'll read data from this topic)\ninput_topic = app.topic(\"cpu-load\")\n\n# consume (read) messages from the input topic\nsdf = app.dataframe(topic=input_topic)\n\n# print every row\nsdf = sdf.update(lambda row: print(json.dumps(row)))\n\nif __name__ == \"__main__\":\n    # run the application and process all inbound messages using the sdf pipeline\n    app.run(sdf)\n```\n\n----------------------------------------\n\nTITLE: Example Message Format on Quix 'messages' Topic\nDESCRIPTION: This JSON object illustrates the structure of a message as published to the 'messages' Quix topic by the Twitch service. It follows the Quix time series data format, including an Epoch value, an array of nanosecond Timestamps, empty NumericValues, StringValues containing the 'chat-message', and TagValues holding metadata like 'room', 'name', and 'role'.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/sentiment-analysis/twitch-service.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n``` python\n{\n  \"Epoch\": 0,\n  \"Timestamps\": [\n    1695378597074000000\n  ],\n  \"NumericValues\": {},\n  \"StringValues\": {\n    \"chat-message\": [\n      \"@CaalvaVoladora Boomerdemons is also up\"\n    ]\n  },\n  \"TagValues\": {\n    \"room\": [\n      \"channel\"\n    ],\n    \"name\": [\n      \"benkebultsax\"\n    ],\n    \"role\": [\n      \"Customer\"\n    ]\n  }\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Running the Python Transformation Script via Shell\nDESCRIPTION: This shell command executes the Python script named 'transform.py' using the python3 interpreter. Running this command starts the Quix Streams application, which begins consuming messages from the input topic, processing them according to the defined pipeline, and publishing results to the output topic.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/get-started/process.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n```\npython3 transform.py\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Processing with Quix Streams Application - Python\nDESCRIPTION: Uses Quix Streams' Application, State, and Dataframe constructs for stateful message processing and persistent state management in the 'state' folder. Dependencies include the 'quixstreams' and 'dotenv' Python packages. The 'count_messages' function maintains a running count of messages processed, saving the total with each state update. Inputs are messages consumed from the specified topic, and outputs are augmented message payloads with the current total. Designed to work either in cloud services or local environments where Quix Streams creates the 'state' directory automatically.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/state-management.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application, State\nfrom dotenv import load_dotenv\n\ndef count_messages(value: dict, state: State):\n    total = state.get('total', default=0)\n    print('total: --> ', total)\n    total += 1\n    state.set('total', total)\n    return {**value, 'total': total}\n\nload_dotenv()\n\napp = Application()\n\ntopic = app.topic('cpu-load')\nsdf = app.dataframe(topic)\nsdf = sdf.apply(count_messages, stateful=True)\napp.run(sdf)\n```\n\n----------------------------------------\n\nTITLE: Aggregating DataFrames for Eight Hour Session Counts in Python\nDESCRIPTION: Within an `on_dataframe_handler`, this snippet performs aggregation for an eight-hour window. It retrieves the current 'eight_hours_aggregation' DataFrame from RocksDB (`db`), aggregates the incoming DataFrame (`df`) by 30-minute intervals ('datetime') and 'userId' to count occurrences, and then concatenates this result with the existing aggregation. `groupby().sum()` is used to combine counts for matching 'datetime' and 'userId' pairs before saving the updated aggregation back to RocksDB.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/data-aggregation.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef on_dataframe_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\n    ...\n    eight_hours_aggregation = db[\"eight_hours_aggregation\"]\n    aggregated = df.groupby([pd.Grouper(key='datetime', freq='30min'), 'userId']).size().reset_index(name='count')\n\n    # Add df_copy to eight_hours_aggregation. If the datetime is the same, add both counts\n    eight_hours_aggregation = (pd.concat([eight_hours_aggregation, aggregated])\n                               .groupby(['datetime', 'userId']).sum().reset_index())\n    db[\"eight_hours_aggregation\"] = eight_hours_aggregation\n```\n\n----------------------------------------\n\nTITLE: Handling and Publishing DataFrame Events - Quix - Python\nDESCRIPTION: This Python snippet demonstrates a parameter callback used in a Quix Streams context to process incoming dataframe events. It augments the dataframe with stream IDs and encodes images in Base64 before publishing to the output topic using a buffered publish command. Dependencies include pandas, base64, and Quix Streams' Python client. Inputs are parameter data as pandas DataFrames, and the output is a processed dataframe sent to the output stream; ensure your environment provides the qx library and appropriately configured producer/consumer streams.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/other-services.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n  # Callback triggered for each new parameter data.\\n  def on_dataframe_handler(self, stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\\n\\n      df[\\\"TAG__parent_streamId\\\"] = self.consumer_stream.stream_id\\n      df['image'] = df[\\\"image\\\"].apply(lambda x: str(base64.b64encode(x).decode('utf-8')))\\n\\n      self.producer_topic.get_or_create_stream(\\\"image-feed\\\") \\\\\\n          .timeseries.buffer.publish(df)\n```\n\n----------------------------------------\n\nTITLE: Filtering CPU Load Above Threshold using Quix Streams in Python\nDESCRIPTION: This Python script uses the Quix Streams library to process data from an input topic (specified by the 'input' environment variable). It filters incoming data frames, keeping only rows where the 'cpu_load' field exceeds 20. For these filtered rows, it constructs a JSON alert payload containing a summary, source, severity, timestamp, and a message indicating the CPU load, then sends this payload to an output topic (specified by the 'output' environment variable). It requires the `quixstreams` library and environment variables for input/output topics.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/influxdb-alerting/threshold-detection.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom quixstreams import Application\n\napp = Application()\n\ninput_topic = app.topic(os.environ[\"input\"])\noutput_topic = app.topic(os.environ[\"output\"])\n\nsdf = app.dataframe(input_topic)\n\n# Filter in all rows where CPU load is over 20.\nsdf = sdf.filter(lambda row: row[\"cpu_load\"] > 20)\n\n# Build an alert payload\nsdf = sdf.apply(lambda row: {\n    \"summary\": \"CPU overload\",\n    \"source\": \"custom_event\",\n    \"severity\": \"critical\",\n    \"custom_details\": {\n        \"timestamp\": row[\"timestamp\"],\n        \"message\": \"CPU value is \" + str(row[\"cpu_load\"])\n    }\n})\n\nsdf = sdf.to_topic(output_topic)\n\nif __name__ == \"__main__\":\n    app.run(sdf)\n\n```\n\n----------------------------------------\n\nTITLE: Producing Book Data to Kafka Topic in Python\nDESCRIPTION: Reads book data from a CSV file and produces it to a Kafka topic. Each record includes a timestamp, document ID, UUID, name, description, and publication year, formatted as JSON for the Kafka stream.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv('/content/documents.csv')\noutputtopicname = docs_topic_name\noffsetlimit2 = len(df)\noffsetlimit = offsetlimit + offsetlimit2\n\nprint(f\"Producing to output topic: {outputtopicname}\")\nwith Producer(\n    broker_address=\"127.0.0.1:9092\",\n    extra_config={\"allow.auto.create.topics\": \"true\"},\n) as producer:\n    for index, row in df.iterrows():\n        doc_id = index\n        doc_key = f\"A{'0'*(10-len(str(doc_id)))}{doc_id}\"\n        doc_uuid = str(uuid.uuid4())\n        value = {\n            \"Timestamp\": time.time_ns(),\n            \"doc_id\": doc_id,\n            \"doc_uuid\": doc_uuid,\n            \"doc_name\": row['name'],\n            \"doc_descr\": row['description'],\n            \"doc_year\": row['year'],\n        }\n        print(f\"Producing value: {value}\")\n        producer.produce(\n            topic=outputtopicname,\n            headers=[(\"uuid\", doc_uuid)],  # a dict is also allowed here\n            key=doc_key,\n            value=json.dumps(value),  # needs to be a string\n        )\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Streaming Token in Bash\nDESCRIPTION: This snippet shows the format for setting the Quix SDK streaming token as an environment variable, often placed in a .env file. This environment variable is required for authenticating with Quix when running command-line programs that publish or subscribe to topics. Replace \\\"<your_streaming_token>\\\" with your actual SDK token. Additional variables such as \\\"output\\\" may also be set to specify the target topic name.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/read-csv.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nQuix__Sdk__Token=\"<your_streaming_token>\"\n```\n\n----------------------------------------\n\nTITLE: Handling InfluxDB Sink Backpressure with Exception in Python\nDESCRIPTION: This code block shows how to handle InfluxDB write backpressure by catching a specific InfluxDBError with status code 429. If the error indicates that the write limit has been reached and includes a retry-after value, it raises a SinkBackpressureError to temporarily pause processing for the affected partition. This pattern is essential for robust streaming pipelines targeting rate-limited sinks.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nexcept influxdb_client_3.InfluxDBError as exc:\n    if exc.response and exc.response.status == 429 and exc.retry_after:\n        # The write limit is exceeded, raise a SinkBackpressureError\n        # to pause the partition for a certain period of time.\n        raise SinkBackpressureError(\n            retry_after=int(exc.retry_after),\n            topic=batch.topic,\n            partition=batch.partition,\n        ) from exc\n    raise\n\n```\n\n----------------------------------------\n\nTITLE: Updating State Transition Logic for Offer Detection in Python\nDESCRIPTION: This Python code defines a state machine using a dictionary named 'transitions'. It modifies the conditions for triggering an offer based on user behavior. Specifically, it changes the target demographic to ages 60-65 for both genders and alters the required navigation sequence to Shoes -> Clothing -> Shoes. The lambda functions within define the conditions for moving between states ('init', 'shoes_visited', 'clothes_visited', 'offer') based on incoming data rows containing user and product information.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/change-offer.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n``` python\ntransitions = {\n    \"init\": [\n        {\n            \"condition\": lambda row, current_state: row[\"category\"] == \"shoes\"\n                                                    and ((row[\"gender\"] == \"M\" and 60 <= row[\"age\"] <= 65)\n                                                            or (row[\"gender\"] == \"F\" and 60 <= row[\"age\"] <= 65)),\n            \"next_state\": \"shoes_visited\",\n        }\n    ],\n    \"shoes_visited\": [\n        {\n            \"condition\": lambda row, current_state: row[\"category\"] == \"clothing\",\n            \"next_state\": \"clothes_visited\"\n        },\n        {\n            \"condition\": lambda row, current_state: row[\"category\"] == \"shoes\",\n            \"next_state\": \"shoes_visited\"\n        }\n    ],\n    \"clothes_visited\": [\n        {\n            \"condition\": lambda row, current_state: row[\"category\"] == \"shoes\"\n                                                    and row[\"productId\"] != current_state[\"rows\"][0][\"productId\"],\n            \"next_state\": \"offer\"\n        },\n        {\n            \"condition\": lambda row, current_state: row[\"category\"] == \"shoes\"\n                                                    and row[\"productId\"] == current_state[\"rows\"][0][\"productId\"],\n            \"next_state\": \"shoes_visited\"\n        }\n    ]\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Writing Test Data to InfluxDB v2 using Python\nDESCRIPTION: This Python script demonstrates how to connect to a local InfluxDB v2 instance (http://localhost:8086) using the `influxdb_client` library. It retrieves the InfluxDB token from the `INFLUXDB_TOKEN` environment variable, specifies the organization ('Quix Docs'), and targets the 'telemetry' bucket. The script writes 5 data points synchronously to the 'measurement1' measurement, each containing 'tagname1' and an incrementing 'field1', pausing for one second between writes. Requires `influxdb_client`, `os`, and `time` libraries.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/influxdb-migration/influxdb-v2-install.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport influxdb_client, os, time\nfrom influxdb_client import InfluxDBClient, Point, WritePrecision\nfrom influxdb_client.client.write_api import SYNCHRONOUS\n\ntoken = os.environ.get(\"INFLUXDB_TOKEN\")\norg = \"Quix Docs\"\nurl = \"http://localhost:8086\"\nbucket=\"telemetry\"\n\nwrite_client = influxdb_client.InfluxDBClient(url=url, token=token, org=org)\nwrite_api = write_client.write_api(write_options=SYNCHRONOUS)\n   \nfor value in range(5):\n  point = (\n    Point(\"measurement1\")\n    .tag(\"tagname1\", \"tagvalue1\")\n    .field(\"field1\", value)\n  )\n  print(\"Writing value to InfluxDB v2: --> \", value)\n  write_api.write(bucket=bucket, org=\"Quix Docs\", record=point)\n  time.sleep(1) # separate points by 1 second\n```\n\n----------------------------------------\n\nTITLE: Calculating Average Speed with Quix Streams in Python\nDESCRIPTION: This Python script utilizes the Quix Streams library to perform a transformation on streaming data. It reads messages from an input topic (defined by the 'input' environment variable), extracts the 'Speed' field, calculates the average speed over a 15-second tumbling window, and structures the output message with 'average-speed' and 'time'. The transformed data is then printed to the console and published to an output topic (defined by the 'output' environment variable). The script requires the `quixstreams` library and depends on 'input' and 'output' environment variables for topic names.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/integrations/databases/influxdb/quickstart.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom quixstreams import Application\nfrom datetime import timedelta\n\n# for local dev, load env vars from a .env file\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# create a Quix Streams application\napp = Application()\n\n# JSON deserializers/serializers used by default\ninput_topic = app.topic(os.environ[\"input\"])\noutput_topic = app.topic(os.environ[\"output\"])\n\n# consume from input topic\nsdf = app.dataframe(input_topic)\n\n# calculate average speed using 15 second tumbling window\nsdf = sdf.apply(lambda row: row[\"Speed\"]) \\\n    .tumbling_window(timedelta(seconds=15)).mean().final() \\\n        .apply(lambda value: {\n            'average-speed': value['value'],\n            'time': value['end']\n            })\n\n# print every row\nsdf = sdf.update(lambda row: print(row))\n\n# publish to output topic\nsdf = sdf.to_topic(output_topic)\n\nif __name__ == \"__main__\":\n    app.run(sdf)\n```\n\n----------------------------------------\n\nTITLE: Generating Threshold Events with Quix Streams (Python)\nDESCRIPTION: This Python snippet detects when a pressure value crosses above and then below zero, generating 'ON' and 'OFF' events using Quix Streams APIs. It relies on a global 'triggered' state and uses stream producers to publish timestamped, value-tagged events. Required dependencies: quix-streams library ('qx'), pandas, and datetime. The handler expects a DataFrame with a 'Pressure' column from the incoming stream.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/process/timeseries-events.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntriggered = False\n\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\n    global triggered\n    stream_producer = topic_producer.get_or_create_stream(stream_id = stream_consumer.stream_id)\n    pressure = df['Pressure'][0]\n    if not triggered:\n        if  pressure > 0:\n            print('State ON')\n            triggered = True\n            stream_producer.events \\\n                .add_timestamp(datetime.datetime.utcnow()) \\\n                .add_value(\"PressureState\", \"ON\") \\\n                .publish()\n    else:\n        if pressure <= 0 :\n            print('State OFF')\n            triggered = False\n            stream_producer.events \\\n                .add_timestamp(datetime.datetime.utcnow()) \\\n                .add_value(\"PressureState\", \"OFF\") \\\n                .publish()\n```\n\n----------------------------------------\n\nTITLE: Generating and Saving Sample CSV Data for Kafka – Python\nDESCRIPTION: Creates a dataset of sci-fi book summaries, constructs a DataFrame and writes it to a CSV file. Dependencies: pandas. Input: hardcoded list of dictionaries. Output: 'documents.csv' file in the working directory. No parameters; intended for testing subsequent Kafka ingestion.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Let's create a dataset based on sci-fi books.\\ndocuments = [\\n  { \\\"name\\\": \\\"The Time Machine\\\", \\\"description\\\": \\\"A man travels through time and witnesses the evolution of humanity.\\\", \\\"author\\\": \\\"H.G. Wells\\\", \\\"year\\\": 1895 },\\n  { \\\"name\\\": \\\"Ender's Game\\\", \\\"description\\\": \\\"A young boy is trained to become a military leader in a war against an alien race.\\\", \\\"author\\\": \\\"Orson Scott Card\\\", \\\"year\\\": 1985 },\\n  { \\\"name\\\": \\\"Brave New World\\\", \\\"description\\\": \\\"A dystopian society where people are genetically engineered and conditioned to conform to a strict social hierarchy.\\\", \\\"author\\\": \\\"Aldous Huxley\\\", \\\"year\\\": 1932 },\\n  {\\\"name\\\": \\\"An Absolutely Remarkable Thing\\\", \\\"description\\\": \\\"A young woman becomes famous after discovering a mysterious alien artifact in New York City.\\\", \\\"author\\\": \\\"Hank Green\\\", \\\"year\\\": 2018},\\n  { \\\"name\\\": \\\"Dune\\\", \\\"description\\\": \\\"A desert planet is the site of political intrigue and power struggles.\\\", \\\"author\\\": \\\"Frank Herbert\\\", \\\"year\\\": 1965 },\\n  { \\\"name\\\": \\\"Foundation\\\", \\\"description\\\": \\\"A mathematician develops a science to predict the future of humanity and works to save civilization from collapse.\\\", \\\"author\\\": \\\"Isaac Asimov\\\", \\\"year\\\": 1951 },\\n  { \\\"name\\\": \\\"Snow Crash\\\", \\\"description\\\": \\\"A futuristic world where the internet has evolved into a virtual reality metaverse.\\\", \\\"author\\\": \\\"Neal Stephenson\\\", \\\"year\\\": 1992 },\\n  { \\\"name\\\": \\\"Neuromancer\\\", \\\"description\\\": \\\"A hacker is hired to pull off a near-impossible hack and gets pulled into a web of intrigue.\\\", \\\"author\\\": \\\"William Gibson\\\", \\\"year\\\": 1984 },\\n  { \\\"name\\\": \\\"The War of the Worlds\\\", \\\"description\\\": \\\"A Martian invasion of Earth throws humanity into chaos.\\\", \\\"author\\\": \\\"H.G. Wells\\\", \\\"year\\\": 1898 },\\n  { \\\"name\\\": \\\"The Hunger Games\\\", \\\"description\\\": \\\"A dystopian society where teenagers are forced to fight to the death in a televised spectacle.\\\", \\\"author\\\": \\\"Suzanne Collins\\\", \\\"year\\\": 2008 },\\n  { \\\"name\\\": \\\"The Andromeda Strain\\\", \\\"description\\\": \\\"A deadly virus from outer space threatens to wipe out humanity.\\\", \\\"author\\\": \\\"Michael Crichton\\\", \\\"year\\\": 1969 },\\n  { \\\"name\\\": \\\"The Left Hand of Darkness\\\", \\\"description\\\": \\\"A human ambassador is sent to a planet where the inhabitants are genderless and can change gender at will.\\\", \\\"author\\\": \\\"Ursula K. Le Guin\\\", \\\"year\\\": 1969 },\\n  { \\\"name\\\": \\\"The Time Traveler's Wife\\\", \\\"description\\\": \\\"A love story between a man who involuntarily time travels and the woman he loves.\\\", \\\"author\\\": \\\"Audrey Niffenegger\\\", \\\"year\\\": 2003 }\\n]\\n\\n# Convert the list of dictionaries to a DataFrame\\ndf = pd.DataFrame(documents)\\n# Save the DataFrame to a CSV file - producing from a CSV is a common test case when setting up a producer application.\\ndf.to_csv('documents.csv')\n```\n\n----------------------------------------\n\nTITLE: Filtering and Aggregating Stream Data with Quix Streams in Python\nDESCRIPTION: This snippet demonstrates reading data from an input topic using Quix Streams, filtering for the 'Speed' parameter, calculating the mean average over a 10-second tumbling window (downsampling), printing the result, and publishing it to an output topic. It requires the 'quixstreams' library and environment variables 'input' and 'output' for topic names.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/integrations/databases/influxdb/replacing-flux.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n``` python\nimport os\nfrom quixstreams import Application, State\nfrom datetime import timedelta\n\napp = Application()\n\ninput_topic = app.topic(os.environ[\"input\"])\noutput_topic = app.topic(os.environ[\"output\"])\n\n# Read from input topic\nsdf = app.dataframe(input_topic)\n\n# Filter in Speed\nsdf = sdf[[\"Speed\"]]\n\n# Calculate mean of speed over 10 second tumbling window\nsdf = sdf.tumbling_window(timedelta(seconds=10)).mean().final()\n\n# Print every row\nsdf = sdf.update(lambda row: print(row))\n\n# Publish to output topic\nsdf = sdf.to_topic(output_topic)\n\nif __name__ == \"__main__\":\n    app.run(sdf)\n```\n```\n\n----------------------------------------\n\nTITLE: Example Quix Parameter Data Payload (JSON)\nDESCRIPTION: This JSON object exemplifies the structure of parameter data received from a Quix topic via the Streaming Reader API. It includes timestamps, numeric values (e.g., 'car' count, 'lat', 'lon'), string values (e.g., a Base64 encoded 'image'), and tag values used for metadata ('parent_streamId'). This structure is processed by the web UI client.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/web-ui.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Epoch\": 0,\n  \"Timestamps\": [\n    1693573934793346000\n  ],\n  \"NumericValues\": {\n    \"car\": [\n      7\n    ],\n    \"traffic light\": [\n      1\n    ],\n    \"person\": [\n      3\n    ],\n    \"lat\": [\n      51.5107\n    ],\n    \"lon\": [\n      -0.11512\n    ],\n    \"delta\": [\n      -4.353343725204468\n    ]\n  },\n  \"StringValues\": {\n    \"image\": [\n        \"iVBO…to/v37HG18UyZ1Qz/fby/<snipped>+yXUGc5UVWZfIHnX0iqM6aEAAAAASUVORK5CYII=\"\n    ]\n  },\n  \"TagValues\": {\n    \"parent_streamId\": [\n      \"JamCams_00001.02500\"\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Application Metadata with app.yaml in Quix using YAML\nDESCRIPTION: This YAML configuration (app.yaml) defines metadata and operational parameters for a single application in a Quix pipeline. It specifies the application name, implementation language (Python), user-definable variables (such as output topic), paths to the Dockerfile, and the Python script to execute. Required for Quix deployment pipelines, its inputs are application properties and paths, and outputs are used by build and runtime orchestrators. Constraints include required variables and adherence to Quix application conventions.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/create/project-structure.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: Demo Data\\nlanguage: python\\nvariables:\\n  - name: Topic\\n    inputType: OutputTopic\\n    description: Name of the output topic to write into\\n    defaultValue: f1-data\\n    required: true\\ndockerfile: build/dockerfile\\nrunEntryPoint: main.py\\ndefaultFile: main.py\n```\n\n----------------------------------------\n\nTITLE: Iterating DataFrame Rows for TFL BikePoint Alerts - Python\nDESCRIPTION: This Python snippet demonstrates how to process a pandas DataFrame containing TFL BikePoint data to extract and print the number of available bikes at each location. It iterates through DataFrame rows using iterrows(), grabs 'NbBikes' and 'Name' for each row, and prints a formatted message to stdout. Required dependencies are pandas for DataFrame manipulation. Inputs: a DataFrame (df) with at least 'NbBikes' and 'Name' columns. Output: Console logs indicating the bike count at each location. Suitable for event handler functions within Quix integrations.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/slack-alerting/slack-alerting.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# iterate the data frame\\nfor i, row in df.iterrows():\\n    # get the number of bikes\\n    num_bikes = row[\\\"NbBikes\\\"]\\n    # get the location\\n    bike_loc = row[\\\"Name\\\"]\\n    # print a message\\n    print(\\\"{} has {} bikes available\\\".format(bike_loc, num_bikes))\n```\n\n----------------------------------------\n\nTITLE: Accessing Environment Variables in Python using os.environ\nDESCRIPTION: This Python snippet demonstrates how to access an environment variable named `API_SECRET` within the Quix platform. It utilizes the `os.environ` dictionary, which requires the `os` module (a standard Python library). The retrieved value is assigned to the `api_secret` variable and then printed to the console.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/environment-variables.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\napi_secret = os.environ[\"API_SECRET\"]\nprint(api_secret)\n```\n\n----------------------------------------\n\nTITLE: Processing Simulink Input Data with Quix Streams in C#\nDESCRIPTION: This C# application utilizes the Quix Streams library and the MATLAB Runtime to interact with a Simulink model deployed via a CTF archive (`quixmatlab.ctf`). It connects to input and output Kafka topics defined by environment variables (`input`, `output`). Upon receiving data on the input stream, it extracts `throttle_angle` and timestamp, calls the compiled MATLAB `engine` function, adds the returned `engine_speed` to the timeseries data, prints the values, and forwards the updated data to the output topic. It requires the `QuixStreams.Streaming`, `MathWorks.MATLAB.Runtime`, `MathWorks.MATLAB.Types`, and `MathWorks.MATLAB.Exceptions` namespaces.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_9\n\nLANGUAGE: cs\nCODE:\n```\nusing System;\nusing MathWorks.MATLAB.Runtime;\nusing MathWorks.MATLAB.Types;\nusing MathWorks.MATLAB.Exceptions;\nusing QuixStreams.Streaming;\nusing QuixStreams.Streaming.Models;\n\nnamespace Service\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            dynamic matlab = MATLABRuntime.StartMATLAB(\"quixmatlab.ctf\");\n\n            var quix = new QuixStreamingClient();\n            using var producer = quix.GetTopicProducer(Environment.GetEnvironmentVariable(\"output\"));\n            using var consumer = quix.GetTopicConsumer(Environment.GetEnvironmentVariable(\"input\"));\n            consumer.OnStreamReceived += (_, stream) =>\n            {\n                Console.WriteLine($\"New stream received: {stream.StreamId}\");\n                stream.Timeseries.OnDataReceived += (_, args) =>\n                {\n                    foreach (var ts in args.Data.Timestamps)\n                    {\n                        var tin = new double[] { ts.TimestampMilliseconds / 1000.0 };\n                        var din = new double[] { ts.Parameters[\"throttle_angle\"].NumericValue.Value };\n                        double rv = matlab.engine(din, tin);\n                        ts.AddValue(\"engine_speed\", rv);\n                        Console.WriteLine($\"throttle angle:{din[0]}, engine speed:{rv}\");\n                    }\n                    producer.GetOrCreateStream(stream.StreamId).Timeseries.Publish(args.Data);\n                };\n            };\n            Console.WriteLine(\"Listening for streams\");\n            App.Run();\n            Console.WriteLine(\"Exiting\");\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Flask Webhook to Publish Data to Quix Streams in Python\nDESCRIPTION: This Python code sets up a Flask web server using `waitress`. It defines an endpoint `/data` that accepts HTTP POST requests with JSON payloads. Received data is serialized using `quixstreams.models.serializers.quix.JSONSerializer` and published to a Quix topic specified by the 'output' environment variable using a `quixstreams.Producer`. Note that this endpoint is unauthenticated. Dependencies include `quixstreams`, `flask`, and `waitress`.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/web-app.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom quixstreams.models.serializers.quix import JSONSerializer, SerializationContext\nfrom flask import Flask, request\nfrom datetime import datetime\nfrom waitress import serve\nimport os, json\n\napp = Application()\n\nserializer = JSONSerializer()\noutput_topic = app.topic(os.environ[\"output\"])\nproducer = app.get_producer()\n\nflask_app = Flask(\"Post Data\")\n\n# this is unauthenticated, anyone could post anything to you!\n@flask_app.route(\"/data\", methods=['POST'])\ndef webhook():\n\n    # publish data to output topic\n    with producer:\n        serialized_value = serializer(\n            value=request.json, ctx=SerializationContext(topic=output_topic.name)\n        )\n        producer.produce(\n            topic=output_topic.name,\n            key=\"webapp-sample\",\n            value=serialized_value\n        )\n\n    return \"OK\", 200\n\n\nprint(\"CONNECTED!\")\n\n# use waitress for production\nserve(flask_app, host='0.0.0.0', port = 80)\n```\n\n----------------------------------------\n\nTITLE: Defining a Data Preprocessing Function in Python\nDESCRIPTION: This snippet defines a `preprocess` method within the Quix transform class in `quix_function.py`. It takes a pandas DataFrame as input, clamps specified signal values ('Speed', 'Steer', etc.) to predefined ranges using a nested `clamp` function, and calculates sine and cosine components for the 'Motion_WorldPositionX' feature, preparing the data for the ML model. Requires the `math` library and expects specific columns in the input DataFrame.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/deploy-ml.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n## To get the correct output, we preprocess data before we feed them to the trained model\n    def preprocess(self, df):\n\n        signal_limits = {\n            \"Speed\": (0, 400),\n            \"Steer\": (-1, 1),\n            \"Gear\": (0, 8),\n            \"Motion_WorldPositionX\": (-math.pi, math.pi),\n            \"Brake\": (0, 1),\n        }\n\n        def clamp(n, minn, maxn):\n            return max(min(maxn, n), minn)\n\n        for signal, limits in signal_limits.items():\n            df[signal] = df[signal].map(lambda x: clamp(x, limits[0], limits[1]))\n\n        df[\"Motion_WorldPositionX_sin\"] = df[\"Motion_WorldPositionX\"].map(lambda x: math.sin(x))\n        df[\"Motion_WorldPositionX_cos\"] = df[\"Motion_WorldPositionX\"].map(lambda x: math.cos(x))\n\n        return df\n```\n\n----------------------------------------\n\nTITLE: Reporting Sliding-Window Max Vehicle Counts - Event Payload - JSON\nDESCRIPTION: This sample JSON shows the payload sent from the max vehicle window service to the Data API. It includes maximum observed vehicles in the specified time window, window start/end times, and camera ID. Consumers extract max vehicle counts and window metadata from NumericValues and TagValues, respectively. Used for providing rolling-capacity data and capacity visualizations.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/other-services.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"Epoch\\\": 0,\\n  \\\"Timestamps\\\": [\\n    1694088514402644000\\n  ],\\n  \\\"NumericValues\\\": {\\n    \\\"max_vehicles\\\": [\\n      8\\n    ]\\n  },\\n  \\\"StringValues\\\": {},\\n  \\\"TagValues\\\": {\\n    \\\"window_start\\\": [\\n      \\\"2023-09-06 12:08:12.394372\\\"\\n    ],\\n    \\\"window_end\\\": [\\n      \\\"2023-09-07 12:08:12.394372\\\"\\n    ],\\n    \\\"window\\\": [\\n      \\\"1d 0h 0m\\\"\\n    ],\\n    \\\"cam\\\": [\\n      \\\"JamCams_00001.08959\\\"\\n    ]\\n  }\\n}\n```\n\n----------------------------------------\n\nTITLE: Testing the Quix Web App Endpoint using Curl in Shell\nDESCRIPTION: This Shell command uses Curl to send an HTTP POST request to a specified URL (the web app's endpoint, e.g., `https://app-workspace-project-branch.deployments.quix.io/data`). It sets the `Content-Type` header to `application/json` and sends the content of the local file `data.json` as the request body. This is used to test the data ingestion functionality of the Flask web app. The URL needs to be updated to the actual deployment URL.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/web-app.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST -H \"Content-Type: application/json\"  https://app-workspace-project-branch.deployments.quix.io/data -d @data.json\n```\n\n----------------------------------------\n\nTITLE: Unpacking MessagePack Data using Quix Streams in Python\nDESCRIPTION: This Python snippet establishes a Quix Streams data pipeline that reads binary MessagePack data from an input topic, deserializes it using the msgpack library, and outputs the resulting JSON objects to another topic. It uses the dotenv library to manage environment variables and expects 'msgpack' to be listed in requirements.txt. Key parameters are the input and output topic names (via environment variables), and the unpack function that decodes the raw MessagePack bytes. The code assumes a Quix Streams environment with the appropriate configuration, and requires both the 'quixstreams' and 'msgpack' packages.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/iot-data.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\\nfrom quixstreams import Application\\nimport msgpack\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\ndef unpack(row):\\n    return msgpack.unpackb(row)\\n\\napp = Application()\\n\\ninput_topic = app.topic(os.environ[\\\"input\\\"], value_deserializer=\\\"bytes\\\")\\noutput_topic = app.topic(os.environ[\\\"output\\\"], value_serializer=\\\"json\\\")\\n\\nsdf = app.dataframe(input_topic)\\nsdf = sdf.apply(unpack)\\nsdf = sdf.update(lambda row: print(row))\\nsdf = sdf.to_topic(output_topic)\\n\\nif __name__ == \\\"__main__\\\":\\n    app.run(sdf)\\n\n```\n\n----------------------------------------\n\nTITLE: Initializing QdrantClient for Vector Database – Python\nDESCRIPTION: Initializes a Qdrant vector database stored on the local filesystem using the QdrantClient. Dependency: qdrant-client. Parameter: collectionname (defines the DB directory). Effect: creates DB directory and prepares Qdrant for vector ingestions; prints the collection name for context. No direct output unless the DB already exists.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(f\\\"Using collection name {collectionname}\\\")\\n# Initialize the vector db\\nqdrant = QdrantClient(path=f\\\"./{collectionname}\\\") # persist a Qdrant DB on the filesystem\n```\n\n----------------------------------------\n\nTITLE: Enriching Row Data with Redis Lookup in Python\nDESCRIPTION: This snippet defines the 'enrich_data' function, which enriches individual data rows by retrieving corresponding latitude and longitude values for a named device from Redis. It handles Redis exceptions, attempts to fetch 'lat' and 'long' fields from a Redis hash, and augments the row dictionary with this information if available. Dependencies include a connected Redis client 'r' and the 'redis' Python package. Expected input is a dictionary with the key 'Device Name', and the output is an updated row dictionary enriched with geolocation data if found.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/data-enrichment-with-quix-and-redis.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef enrich_data(row):\\n\\tdevice_name = row[\\\"Device Name\\\"]\\n\\ttry:\\n    \\t# Attempt to retrieve enriched data from Redis\\n    \\tdevice_info = r.hgetall(f'sensor:{device_name}')\\n    \\tif device_info:\\n        \\t# Extract desired information from Redis hash (handle potential errors)\\n        \\tlatitude = device_info.get(\\\"lat\\\")\\n        \\tlongitude = device_info.get(\\\"long\\\")\\n\\n        \\t# Add enriched data to the row if retrieved successfully\\n        \\tif latitude:\\n            \\t\\trow[\\\"lat\\\"] = latitude\\n        \\tif longitude:\\n            \\t\\trow[\\\"long\\\"] = longitude\\n\\texcept redis.exceptions.RedisError as e:\\n    \\t\\tprint(f\\\"Error retrieving data from Redis for {device_name}: {e}\\\")\\n\\n\\treturn row\n```\n\n----------------------------------------\n\nTITLE: Handling DataFrames for Real-time ML Prediction in Quix Python\nDESCRIPTION: This Python function replaces the default `on_dataframe_handler` in `quix_function.py`. It preprocesses incoming data using `self.preprocess`, selects features, performs predictions using the loaded `self.model`, shifts timestamps forward by 5 seconds for the prediction time, merges predictions with original brake data, and publishes the resulting DataFrame to the output stream via `self.producer_stream`. Requires `pandas`, Quix (`qx`), the loaded model (`self.model`), the `preprocess` method, and an initialized `producer_stream`.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/deploy-ml.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Callback triggered for each new parameter data.\n    def on_dataframe_handler(self, stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\n\n        # if no speed column, skip this record        \n        if not \"Speed\" in df.columns:\n            return df\n            \n        output_df = pd.DataFrame()\n\n        # Preprocessing\n        df = self.preprocess(df)\n\n        features = [\"Motion_WorldPositionX_cos\", \"Motion_WorldPositionX_sin\", \"Steer\", \"Speed\", \"Gear\"]\n        X = df[features]\n\n        # Shift data into the future by 5 seconds. (Note that time column is in nanoseconds).\n        output_df[\"timestamp\"] = df[\"timestamp\"].apply(lambda x: int(x) + int((5 * 1000 * 1000 * 1000)))\n        output_df[\"brake-prediction\"] = self.model.predict(X)\n\t\t\n        print(\"Prediction\")\n        print(output_df[\"brake-prediction\"])\t\t\n\n\t\t# Merge the original brake value into the output data frame\n        output_df = pd.concat([df[[\"timestamp\", \"Brake\"]], output_df]).sort_values(\"timestamp\", ascending=True)\n\n        self.producer_stream.timeseries.buffer.publish(output_df)  # Send filtered data to output topic\n```\n\n----------------------------------------\n\nTITLE: Sending Enriched Data to Output Topic with Quix in Python\nDESCRIPTION: This snippet demonstrates how to forward an enriched streaming data frame (sdf) to a specified output topic within a Quix-based data pipeline. It further shows how to launch the application by running the defined Stream DataFrame (sdf) when the script is executed directly. The key parameter is 'output_topic', which defines the target topic, and 'app.run(sdf)', which starts processing the stream. Prerequisites include a configured Quix environment and the corresponding imports and pipeline setup.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/data-enrichment-with-quix-and-redis.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Send enriched data to the output topic\\nsdf = sdf.to_topic(output_topic)\\n\\nif __name__ == \"__main__\":\\n\\tapp.run(sdf)\n```\n\n----------------------------------------\n\nTITLE: Example Structure of TfL Video Feed Metadata in JSON\nDESCRIPTION: This JSON snippet represents the structure of metadata for a TfL camera video feed property, as extracted by the previous Python code. It documents the expected fields: type, category, key, source system, value (video URL), and modification timestamp. Used for reference and validation of extracted metadata. No dependencies. Input: None (example only). Output: Dictionary structure for metadata.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/tfl-frame-grabber.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"$type\": \"Tfl.Api.Presentation.Entities.AdditionalProperties, Tfl.Api.Presentation.Entities\",\n    \"category\": \"payload\",\n    \"key\": \"videoUrl\",\n    \"sourceSystemKey\": \"JamCams\",\n    \"value\": \"https://s3-eu-west-1.amazonaws.com/jamcams.tfl.gov.uk/00001.03766.mp4\",\n    \"modified\": \"2023-08-31T15:46:06.093Z\"\n},\n```\n\n----------------------------------------\n\nTITLE: Creating Stream using Node.js HTTPS\nDESCRIPTION: Creates a new stream using Node.js native HTTPS module to make a POST request to the Quix Platform API.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/create-stream.md#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst https = require('https');\n\nconst data = \"{}\";\n\nconst options = {\n    hostname: domain + '.platform.quix.io',\n    path: '/topics/' + topicName + '/streams',\n    method: 'POST',\n    headers: {\n        'Authorization': 'Bearer ' + token,\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options, res => {\n    res.on('data', d => {\n        let streamId = JSON.parse(d).streamId;\n        console.log(streamId);\n    });\n});\n\nreq.write(data);\nreq.end();\n```\n\n----------------------------------------\n\nTITLE: Defining External Image Deployment in Quix Pipeline - YAML\nDESCRIPTION: This snippet shows a YAML configuration block to deploy an external image as part of a Quix pipeline via the command line. It must be placed within the quix.yaml pipeline descriptor and contains parameters such as the deployment name, container image reference, deployment type, resource allocations, replica count, and desired status. Required dependencies include an accessible container registry and the Quix CLI tool. Key parameters: 'name' identifies the deployment, 'image' specifies the container image URI, 'deploymentType' sets mode (Service or Job), 'resources' defines CPU/memory, 'replicas' controls scaling, and 'desiredStatus' manages lifecycle. Inputs are YAML-parsed configuration values; upon sync, Quix provisions the specified deployment. Ensure correct indenting and that all referenced images and registries are accessible.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/deploy-external-image.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- name: custom-service\n    image: my-registry.com/my-service:1.2.3\n    deploymentType: Service\n    resources:\n      cpu: 300\n      memory: 600\n      replicas: 2\n    desiredStatus: Running\n```\n\n----------------------------------------\n\nTITLE: Defining Quix Stream Dataframe Handler in Python\nDESCRIPTION: This Python function signature defines the callback handler `on_dataframe_received_handler` used in a Quix service. This handler is automatically invoked whenever time series data, structured as a pandas DataFrame, arrives on the input stream it's subscribed to. It takes the stream consumer and the received DataFrame as arguments.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/object-detection.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\n\n```\n\n----------------------------------------\n\nTITLE: Device Lookup Data in CSV Format\nDESCRIPTION: Sample CSV data containing IoT device information that will be loaded into Redis. Each entry includes the device name, GPS coordinates (latitude and longitude), and local time information.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/data-enrichment-with-quix-and-redis.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nname,latitude,longitude,local_time\nGreenhouse Temperature Sensor, 52.3749, -0.1278, 2024-03-27 15:54:10\nCoop Temperature Sensor, 48.8566, 2.3522, 2024-03-27 15:54:10\nBarn Temperature Sensor, 40.7128, -74.0059, 2024-03-27 15:54:10\nVineyard Irrigation, 34.0522, -118.2437, 2024-03-27 15:54:10\nWindmill Controller, 51.1841, -1.8606, 2024-03-27 15:54:10\nRice Paddy Water Management, 31.1466, 121.4737, 2024-03-27 15:54:10\nCattle Shed Temperature Sensor, -34.9288, 138.6000, 2024-03-27 15:54:10\nChicken Coop Temperature Sensor, 50.8500, 4.3500, 2024-03-27 15:54:10\nOlivine Grove Irrigation, 38.0902, 27.1475, 2024-03-27 15:54:10\nSheep Farm Shelter, 45.5188, -111.0407, 2024-03-27 15:54:10\n```\n\n----------------------------------------\n\nTITLE: Configuring DuckDB Sink Batch Size in Python\nDESCRIPTION: This snippet shows how to initialize a DuckDBSink for use with Quix Streams, specifying the 'batch_size' parameter to control how many records are processed in each batch. Adjust 'batch_size' to optimize write throughput versus memory consumption for your workload. The sink is initialized with required parameters, and 'batch_size' is critical for balancing database performance.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nduckdb_sink = DuckDBSink(\n    # ...other parameters...\n    batch_size=50\n)\n\n```\n\n----------------------------------------\n\nTITLE: Ingesting Lookup Data into Redis\nDESCRIPTION: Python script that reads device data from a CSV file into a pandas DataFrame and loads it into Redis for later enrichment. Each device entry includes name, latitude, longitude, and local time information.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/data-enrichment-with-quix-and-redis.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport os\nimport time\nimport redis\n\n# Define the Redis connection\nr = redis.Redis(\n    host=os.environ['redis_host'],\n    port=int(os.environ['redis_port']),\n    password=os.environ['redis_password'],\n    username=os.environ['redis_username'] if 'redis_username' in os.environ else None,\n    decode_responses=True)\n\n# Function to ingest data from a CSV file\ndef ingest_csv_data():\n    csv_file_path = './sensor-data.csv'\n\n    # Read the CSV data into a DataFrame\n    df = pd.read_csv(csv_file_path)\n    pipe = r.pipeline()\n\n  # Itrate through the file\n    for index, row in df.iterrows():\n        # Convert row to a dictionary\n        entry = row.to_dict()\n        entry[\"timestamp\"] = int(time.time_ns())  # Add a timestamp\n        # Write the entry to Redis\n        key = f'sensor:{row[\"name\"]}'\n        pipe.delete(key)\n        pipe.hset(key, 'lat', row['latitude'])\n        pipe.hset(key, 'long', row['longitude'])\n        pipe.hset(key, 'local_time', row['local_time'])\n        print(pipe.execute(), row, entry)\n        print(row[\"name\"]);\n\n    print(\"CSV data ingested successfully.\")\n# Run the main function\nif __name__ == '__main__':\n    ingest_csv_data()\n```\n\n----------------------------------------\n\nTITLE: Complete Quix Streams Consumer Application with DuckDB Sink in Python\nDESCRIPTION: This full script sets up a Quix Streams consumer application that reads CS:GO telemetry data, tracks processed message count, and writes batches to DuckDB using a custom sink. Dependencies include 'quixstreams', 'os', and 'dotenv'. It takes environmental parameters for Kafka topic selection, configures consumer settings, and uses schema definitions for the DuckDB table. The application reads from the input topic, logs message details, sinks data to DuckDB, and starts the main event loop. 'TOTAL_PROCESSED' is used to optionally pause or stop after a set number of messages.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom quixstreams import Application\nfrom sinks.duckdbsink import DuckDBSink\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Define a processing function to stop the Application after processing a certain amount of messages.\nTOTAL_PROCESSED = 0\ndef on_message_processed(topic:str, partition: int, offset: int):\n    \"\"\"\n    Stop application and/or print status after processing {X} number of messages.\n    \"\"\"\n    global TOTAL_PROCESSED\n    TOTAL_PROCESSED += 1\n    ### Check if processing limit is exceeded.\n    # Note: Jsonl file only contains 1000 records anyway\n    # Keeping this check regardless, in case you want to work with a larger dataset.\n    if TOTAL_PROCESSED == 1000:\n        print(\"1000 messages processed\")\n        # app.stop()\n\n\napp = Application(\n    consumer_group=\"sink_consumer_v1\",\n    auto_offset_reset=\"earliest\",\n    consumer_extra_config={'max.poll.interval.ms': 300000},\n    commit_every=250,\n    on_message_processed=on_message_processed,\n\n)\ntopic = app.topic(os.getenv(\"input\", \"raw_data\"))\n\n# Initialize DuckDBSink\nduckdb_sink = DuckDBSink(\n    database_path=\"csgo_data_v1.db\",\n    table_name=\"tick_metadata\",\n    batch_size=50,\n\n    schema={\n        \"timestamp\": \"TIMESTAMP\",\n        \"tick\": \"INTEGER\",\n        \"inventory\": \"TEXT\",\n        # Other fields...\n    }  \n)\n\n# Create a StreamingDataFrame from the topic\nsdf = app.dataframe(topic)\nsdf = sdf.update(lambda message: print(f\"Received message for tick: {message['tick']}, message count {TOTAL_PROCESSED}\"))\n\n# Sink data to InfluxDB\nsdf.sink(duckdb_sink)\n\napp.run(sdf)\n\n```\n\n----------------------------------------\n\nTITLE: Forecasting Temperature Using scikit-learn Polynomial Regression - Python\nDESCRIPTION: This snippet implements temperature forecasting using a second-order polynomial regression model from scikit-learn within Python. The function extracts the mean fluctuated ambient temperature from input rows, fits a polynomial regression, and predicts future values for a specified forecast length. Required dependencies include numpy and scikit-learn, specifically PolynomialFeatures and LinearRegression. Inputs are a list of readings (with timestamp and mean_fluctuated_ambient_temperature), and output is a list of dictionaries each containing a predicted timestamp and temperature; timestamps are incremented in one-minute steps. The code assumes proper input structure and that all required libraries are imported elsewhere.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/predictive-maintenance/forecast-service.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nforecast_input = list(map(lambda row: row[\\\"mean_fluctuated_ambient_temperature\\\"], rows))\\n\\n# Define the degree of the polynomial regression model\\ndegree = 2\\n# Create a polynomial regression model\\nmodel = make_pipeline(PolynomialFeatures(degree), LinearRegression())\\n# Fit the model to the data\\nmodel.fit(np.array(range(len(forecast_input))).reshape(-1, 1), forecast_input)\\n# Forecast the future values\\nforecast_array = np.array(range(len(forecast_input), len(forecast_input) + forecast_length)).reshape(-1, 1)\\nforecast_values = model.predict(forecast_array)\\n\\nresult = []\\ntimestamp = rows[-1][\\\"timestamp\\\"]\\n\\nfor value in forecast_values:\\n    timestamp += 60 * 1000\\n    result.append({\\n        \\\"timestamp\\\": timestamp,\\n        \\\"forecast\\\": float(value)\\n    })\\nreturn result\n```\n\n----------------------------------------\n\nTITLE: Implementing Reducer and Initializer Functions for Streaming Aggregation in Python\nDESCRIPTION: This snippet provides the Python functions needed for stateful aggregation within the tumbling window pipeline. The 'reducer' accumulates sums of multiple temperature readings and a count, designed to be called for every message except the first in the window. The 'initializer' sets up the aggregation state when the first message in a window is received, capturing the initial values and metadata. Dependencies for this code include the input data structure, which is expected to contain all referenced temperature and metadata fields. The output is a dictionary representing the aggregate state, which must be correctly initialized and updated for accurate computation.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/predictive-maintenance/downsampling.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef reducer(state: dict, value: dict) -> dict:\\n    \"\"\"\\n    'reducer' will be called for every message except the first.\\n    We add the values to sum them so we can later divide by the \\n    count to get an average.\\n    \"\"\"\\n\\n    state['sum_hotend_temperature'] += value['hotend_temperature']\\n    state['sum_bed_temperature'] += value['bed_temperature']\\n    state['sum_ambient_temperature'] += value['ambient_temperature']\\n    state['sum_fluctuated_ambient_temperature'] += value['fluctuated_ambient_temperature']\\n    state['sum_count'] += 1\\n    return state\\n\\ndef initializer(value: dict) -> dict:\\n    \"\"\"\\n    'initializer' will be called only for the first message.\\n    This is the time to create and initialize the state for \\n    use in the reducer funciton.\\n    \"\"\"\\n\\n    return {\\n        'sum_hotend_temperature': value['hotend_temperature'],\\n        'sum_bed_temperature': value['bed_temperature'],\\n        'sum_ambient_temperature': value['ambient_temperature'],\\n        'sum_fluctuated_ambient_temperature': value['fluctuated_ambient_temperature'],\\n        'sum_timestamp': value['timestamp'],\\n        'sum_original_timestamp': value['original_timestamp'],\\n        'sum_printer': value['printer'],\\n        'sum_count': 1\\n    }\n```\n\n----------------------------------------\n\nTITLE: Example Message Format for Quix Timeseries Data (JSON)\nDESCRIPTION: This JSON snippet represents the expected format of incoming timeseries messages from the Quix stream merge service output. It includes numeric values (such as numbers of trucks and cars, location coordinates, and delta), string fields (like encoded image data), and tags (for linkage to parent streams). This structure provides the context needed to correctly extract the number of cars in the transformation code, and users can inspect similar structures in the Quix Data Explorer.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/add-service.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Epoch\": 0,\n    \"Timestamps\": [\n        1694788651367069200\n    ],\n    \"NumericValues\": {\n        \"truck\": [\n        1\n        ],\n        \"car\": [\n        3\n        ],\n        \"lat\": [\n        51.55164\n        ],\n        \"lon\": [\n        -0.01853\n        ],\n        \"delta\": [\n        -0.43226194381713867\n        ]\n    },\n    \"StringValues\": {\n        \"image\": [\n        \"iVBOR/snip/QmCC\"\n        ]\n    },\n    \"TagValues\": {\n        \"parent_streamId\": [\n        \"JamCams_00002.00820\"\n        ]\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Defining Product Data Structure in JSON\nDESCRIPTION: This JSON snippet specifies the format for representing a product item, including fields for ID, category, title, description, image, and price. It is intended to guide developers and data engineers in structuring product catalog records for ingestion. Each key represents a required property; all properties must be provided for proper processing by the ingestion pipeline.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/data-ingestion.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\\n    \"id\": \"VD55181667\",\\n    \"category\": \"books\",\\n    \"title\": \"Chronicles of the Ancient Forest by John Smith\",\\n    \"description\": \"Immerse yourself in the captivating world created by John Smith. Chronicles of the Ancient Forest takes you on a journey through an ancient forest filled with mystery and wonder. A must-read for fans of fantasy and adventure.\",\\n    \"image\": \"VD55181667.png\",\\n    \"price\": 92.04\\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Quix Application Polling and Commit Settings in Python\nDESCRIPTION: This code snippet demonstrates how to initialize a Quix Streams Application with custom polling and commit settings. It configures 'max.poll.interval.ms' for the Kafka consumer and 'commit_every' to control the frequency of offset commits. Modify these parameters to suit your application's throughput and reliability needs. The parameters take effect when constructing the Application instance and influence the consumer's rebalancing behavior and memory usage.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\napp = Application(\n     # ...other parameters...\n    consumer_extra_config={\n        'max.poll.interval.ms': 300000, # Adjust this value\n    },\n    commit_every=250  # Adjust this value\n)\n\n```\n\n----------------------------------------\n\nTITLE: Preprocessing World Position X Coordinates using Sine/Cosine\nDESCRIPTION: Transforms the 'Motion_WorldPositionX' column in the Pandas DataFrame 'df' into continuous cyclical features using sine and cosine transformations. This preprocessing step helps the model understand the cyclical nature of positional data.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/train-ml.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n## Convert motion to continuous values\ndf[\"Motion_WorldPositionX_sin\"] = df[\"Motion_WorldPositionX\"].map(lambda x: math.sin(x))\ndf[\"Motion_WorldPositionX_cos\"] = df[\"Motion_WorldPositionX\"].map(lambda x: math.cos(x))\n```\n\n----------------------------------------\n\nTITLE: Defining and Initializing a DuckDB Sink Class with Quix Streams in Python\nDESCRIPTION: This snippet defines a custom sink class for DuckDB by inheriting from Quix Streams' BatchingSink. The __init__ method sets up the sink's configuration, including database path, table name, and schema, as well as batch size for writes. The _ensure_table_exists helper uses the DuckDB API to create the target table lazily if it does not already exist. Dependencies: duckdb, logging, datetime, time, and quixstreams. The sink expects a schema mapping (dict) and will create missing tables automatically. Inputs include DB connection info and schema; output is a ready-to-use sink class.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nimport logging\nfrom datetime import datetime\nimport time\nfrom quixstreams.sinks.base import BatchingSink, SinkBatch\nfrom quixstreams.sinks.exceptions import SinkBackpressureError\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\nclass DuckDBSink(BatchingSink):\n    def __init__(self, database_path, table_name, schema, batch_size=100):\n        super().__init__()\n        self.database_path = database_path\n        self.table_name = table_name\n        self.schema = schema\n        self.batch_size = batch_size\n        self._ensure_table_exists()\n\n    def _ensure_table_exists(self):\n        conn = duckdb.connect(self.database_path)\n        columns = \", \".join([f\"{key} {dtype}\" for key, dtype in self.schema.items()])\n        create_table_query = f\"CREATE TABLE IF NOT EXISTS {self.table_name} ({columns})\"\n        conn.execute(create_table_query)\n        conn.close()\n\n```\n\n----------------------------------------\n\nTITLE: Fetching TfL Camera Feed Data via REST API - Python\nDESCRIPTION: This Python snippet performs an HTTP GET request to the TfL camera API endpoint to retrieve a list of camera feeds, using the `requests` library and an API key. The required dependency is the `requests` Python package, and input parameters include a valid `api_key`. The expected output is a response object containing the JSON camera data, which will be processed in subsequent pipeline steps.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/tfl-camera-feed.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncameras = requests.get(\n    \"https://api.tfl.gov.uk/Place/Type/JamCam/?app_id=QuixFeed&app_key={}\".format(api_key))\n\n```\n\n----------------------------------------\n\nTITLE: Full Example quix.yaml using Variables\nDESCRIPTION: Provides a complete example of a `quix.yaml` file utilizing YAML variables for various configuration settings including deployment resources (`{{CPU}}`, `{{MEMORY}}`, `{{REPLICAS}}`), deployment status (`{{DISABLED}}`), and public access URL prefix (`{{URL_PREFIX}}-{{ENV_NAME}}`). It also shows standard deployment and variable definitions.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/yaml-variables.md#2025-04-23_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n# Quix Project Descriptor\n# This file describes the data pipeline and configuration of resources of a Quix Project.\n\nmetadata:\n  version: 1.0\n\n# This section describes the Deployments of the data pipeline\ndeployments:\n  - name: CPU Threshold\n    application: Starter transformation\n    deploymentType: Service\n    version: transform-v2\n    resources:\n      cpu: {{CPU}}\n      memory: {{MEMORY}}\n      replicas: {{REPLICAS}}\n    desiredStatus: Stopped\n    disabled: {{DISABLED}}\n    publicAccess:\n      enabled: true\n      urlPrefix: {{URL_PREFIX}}-{{ENV_NAME}}\n    variables:\n      - name: input\n        inputType: InputTopic\n        description: Name of the input topic to listen to.\n        required: false\n        value: cpu-load\n      - name: output\n        inputType: OutputTopic\n        description: Name of the output topic to write to.\n        required: false\n        value: transform\n```\n\n----------------------------------------\n\nTITLE: Offering Special Male Demographic Promo via JSON Data Output\nDESCRIPTION: This JSON snippet demonstrates the format for communicating an offer targeting the male demographic. Each entry includes a high-precision Timestamp, empty Tags dictionary, an identifier (\"offer\"), and a value (\"offer1\") indicating the specific promotion. The output is designed to be consumed by downstream services (such as the webshop frontend) for rendering the appropriate user interface elements. No external processing dependencies are needed, but accurate population of the fields is required. The main expected input is user-behavior-triggered event data, and the output is a JSON-encoded offer object.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/event-detection.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"Timestamp\": 1700221907989992000,\n    \"Tags\": {},\n    \"Id\": \"offer\",\n    \"Value\": \"offer1\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Using Variables for Environment-Specific Data Tiers in quix.yaml (YAML)\nDESCRIPTION: This YAML configuration demonstrates using a variable (`{{dataTier}}`) to define the data tier for a topic named 'topic-per-environment' within `quix.yaml`. This technique allows assigning different data tiers (e.g., Bronze in development, Silver in staging) to the same logical topic across various deployment environments by setting the variable's value accordingly for each environment. Requires the `dataTier` variable to be defined elsewhere (e.g., environment variables or configuration files).\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/create/data-tiers.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntopics:\n  - name: topic-per-environment\n    dataTier: {{dataTier}}\n```\n\n----------------------------------------\n\nTITLE: Processing Simulink Input Data with Quix Streams in Python\nDESCRIPTION: This Python script uses the Quix Streams library to process data for a Simulink engine model. It initializes a connection to the MATLAB engine via `quixmatlab`, consumes data from an input Kafka topic specified by the `input` environment variable, extracts `throttle_angle` and timestamp from incoming `TimeseriesData`, calls the `engine` function in the MATLAB model, adds the resulting `engine_speed` back to the data, and publishes the augmented data to an output topic specified by the `output` environment variable. It depends on `quixstreams`, `os`, `quixmatlab`, and `matlab` libraries.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport quixstreams as qx\nimport os\nimport quixmatlab\nimport matlab\n\nqxmlm = quixmatlab.initialize()\n\nclient = qx.QuixStreamingClient()\ninput_topic = client.get_topic_consumer(os.environ[\"input\"])\noutput_topic = client.get_topic_producer(os.environ[\"output\"])\n\ndef on_data_received_handler(input_stream: qx.StreamConsumer, data: qx.TimeseriesData):\n    with data:\n        for ts in data.timestamps:    \n\t\t\t\t\tthrottle_angles = matlab.double([ts.parameters[\"throttle_angle\"].numeric_value])\n            \t\ttimestamps = matlab.double([ts.timestamp_milliseconds / 1000])\n            \t\trv = qxmlm.engine(throttle_angles, timestamps)\n            \t\tts.add_value(\"engine_speed\", rv)\n            \t\tprint(\"throttle angle:{}, engine speed:{}\".format(throttle_angles[0][0], rv))\n        output_stream = output_topic.get_or_create_stream(input_stream.stream_id)\n        output_stream.timeseries.publish(data)\n\ndef on_stream_received_handler(stream: qx.StreamConsumer):\n    print(\"New stream: {}\".format(stream.stream_id))\n    stream.timeseries.on_data_received = on_data_received_handler\n\ninput_topic.on_stream_received = on_stream_received_handler\n\nprint(\"Listening to streams. Press CTRL-C to exit.\")\nqx.App.run()\n```\n\n----------------------------------------\n\nTITLE: Basic Tag Filtering with Equal Operator in JSON\nDESCRIPTION: Example of filtering parameter data for a specific lap number using the Equal operator. The filter will only return data tagged with LapNumber equal to 2.0.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/filter-tags.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"tagFilters\": [{\n        \"tag\": \"LapNumber\",\n        \"operator\": \"Equal\",\n        \"value\": \"2.0\"\n    }]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting a Specific Data Tier for a Topic in quix.yaml (YAML)\nDESCRIPTION: This YAML snippet shows how to configure a topic named 'bronze-topic' within the `topics` list in a `quix.yaml` file. It specifically assigns the 'Bronze' data tier to this topic using the `dataTier` property. This approach is used for Infrastructure as Code configurations.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/create/data-tiers.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntopics:\n  - name: bronze-topic\n    dataTier: Bronze\n```\n\n----------------------------------------\n\nTITLE: Setting a 10-Minute Tumbling Window in Quix Streams (Python)\nDESCRIPTION: This snippet shows how to configure a tumbling window of ten minutes using `timedelta(minutes=10)` within the `tumbling_window` function in Quix Streams for aggregation purposes like calculating the mean.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/integrations/databases/influxdb/replacing-flux.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n``` python\nsdf = sdf.tumbling_window(timedelta(minutes=10)).mean().final()\n```\n```\n\n----------------------------------------\n\nTITLE: Converting Brake Values to Boolean\nDESCRIPTION: Creates a new column 'Brake_bool' in the DataFrame 'df' by converting the continuous 'Brake' column values to boolean (0 or 1). It uses the `round()` function within a lambda expression applied to each element, effectively thresholding the brake values.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/train-ml.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n## Conversion of label\ndf[\"Brake_bool\"] = df[\"Brake\"].map(lambda x: round(x))\n```\n\n----------------------------------------\n\nTITLE: Using Transactions for Consistent Batched Database Writes in DuckDB with Python\nDESCRIPTION: This example emphasizes the use of transactions in the DuckDBSink write method, showing how to wrap batched streaming inserts in BEGIN TRANSACTION and COMMIT, with ROLLBACK on errors. This ensures atomicity and consistency, and offers significant performance gains for DuckDB bulk inserts. Dependencies: duckdb, logging, SinkBatch. Inputs are streaming data batches; outputs are atomic DB insertions. Care should be taken to size transactions appropriately to control memory usage.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef write(self, batch: SinkBatch):\n    conn = duckdb.connect(self.database_path)\n    try:\n        conn.execute(\"BEGIN TRANSACTION\")\n        \n        # ... (batch processing and writing logic) ...\n\n        conn.execute(\"COMMIT\")\n    except duckdb.Error as exc:\n        logger.error(\"Transaction failed, rolling back. Error: %s\", exc)\n        conn.execute(\"ROLLBACK\")\n        raise\n    finally:\n        conn.close()\n\n```\n\n----------------------------------------\n\nTITLE: Searching Qdrant Vector Database with Semantic Query\nDESCRIPTION: Performs a semantic search against the Qdrant vector database using a pre-defined query. The query string is encoded into a vector using the encoder and then searched against the collection, returning the top 10 matches with their scores.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Searching with query '{query}'...\\n\\n\")\n\nhits = qdrant.search(\n    collection_name=collectionname,\n    query_vector=encoder.encode(query).tolist(),\n    limit=10\n)\n\nprint(\"Entries matching your query:\")\nfor hit in hits:\n  print(hit.payload['doc_name'], \" | \", hit.payload['doc_descr'], \"score:\", hit.score)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Persisting a Model File to State Directory - Python\nDESCRIPTION: Downloads a model file using the requests library and saves it to the './state' directory for persistence across service restarts. Requires the 'requests' Python package to be available. The key parameter is the model file URL, and the code demonstrates writing binary data to a persistent file, then reopening it for subsequent reading. Input is the model URL and output is the written/loaded file in the 'state' folder. Limitations include relying on valid URLs and write permission to the state directory.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/state-management.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nmodel = requests.get('https://acme.com/models/model1.dat')\nf = open('./state/model1.dat', 'wb')\nf.write(model)\nf.close()\n...\nf = open('./state/model1.dat', 'rb')\n...\n```\n\n----------------------------------------\n\nTITLE: Complete Curl Request Example\nDESCRIPTION: Comprehensive example showing a complete curl command with authentication, content type, and payload.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/portal-api/http-requests.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d \"@data.json\" \\\n     https://${api-name}-${environment-id}.platform.quix.io/<action>\n```\n\n----------------------------------------\n\nTITLE: Console Output of CSV Data Streaming Script\nDESCRIPTION: This snippet shows the typical console log output generated when running the Python CSV streaming script. It confirms that the script has started, is attempting to connect to the output topic (`phone-data`), and is actively sending data rows from the CSV, indicated by the 'Sending x/y' messages.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/event-detection/data-acquisition.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n[xx-xx-xx xx:xx:xx.xxx (8) INF] Topic phone-data is still creating\nOpening output topic\nWriting data\nSending 0/18188\nSending 1/18188\nSending 2/18188\n```\n\n----------------------------------------\n\nTITLE: Downloading and Loading Pre-trained XGBoost Model in Python\nDESCRIPTION: This Python code snippet downloads a pre-trained machine learning model file (`XGB_model.pkl`) from a specified URL using `urllib.request`. It saves the downloaded content to a local file named \"XGB_model.pkl\" and then uses the `pickle` library to load the model from the file into the `loaded_model` variable for later use in predictions.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/event-detection/crash-detection.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# download the model with urllib\nf = request.urlopen(\"https://quixtutorials.blob.core.windows.net/tutorials/event-detection/XGB_model.pkl\")\nwith open(\"XGB_model.pkl\", \"wb\") as model_file:\n\tmodel_file.write(f.read())\n\n# load it with pickle\nloaded_model = pickle.load(open(\"XGB_model.pkl\", 'rb'))\n```\n\n----------------------------------------\n\nTITLE: Connecting to Streaming Reader API using SignalR in JavaScript\nDESCRIPTION: This code snippet demonstrates how to connect to the Streaming Reader API using the Microsoft SignalR JavaScript client library. It's provided as a reference for implementation.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/overview.md#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// No actual code provided in the content\n```\n\n----------------------------------------\n\nTITLE: Kafka Streaming DataFrame: Consuming, Enriching, and Producing Document Embeddings – Python\nDESCRIPTION: Sets up and runs a Quix Streams Application to consume messages from a Kafka topic, apply embedding enrichment, update timestamps, and produce to another topic. Includes offset-based stop condition for notebook execution. Dependencies: quixstreams, time, encoder, create_embeddings, Application. Input: messages from Kafka input topic. Output: Kafka messages on output topic with added 'embeddings'.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Create a special stop condition just for this Notebook (otherwise the cell will run indefinitely)\\nprint(f\\\"Using offset limit {offsetlimit}\\\")\\ndef on_message_processed(topic, partition, offset):\\n    if offset > offsetlimit:\\n        app.stop()\\n\\n# Define the consumer application and settings\\napp = Application(\\n    broker_address=\\\"127.0.0.1:9092\\\",\\n    consumer_group=consumergroup_name,\\n    auto_offset_reset=\\\"earliest\\\",\\n    on_message_processed=on_message_processed,\\n    consumer_extra_config={\\\"allow.auto.create.topics\\\": \\\"true\\\"},\\n)\\n\\n# Define an input topic with JSON deserializer\\ninput_topic = app.topic(inputtopicname, value_deserializer=\\\"json\\\")\\nprint(f\\\"Consuming from input topic: {inputtopicname}\\\")\\n\\n# Define an output topic with JSON serializer\\noutput_topic = app.topic(outputtopicname, value_serializer=\\\"json\\\")\\nprint(f\\\"Producing to output topic: {outputtopicname}\\\")\\n\\n# Initialize a streaming dataframe based on the stream of messages from the input topic:\\nsdf = app.dataframe(topic=input_topic)\\n\\nsdf = sdf.update(lambda val: print(f\\\"Received update: {val}\\\"))\\n\\n# EMBEDDING HAPPENS HERE\\n### Trigger the embedding function for any new messages(rows) detected in the filtered SDF\\nsdf[\\\"embeddings\\\"] = sdf.apply(create_embeddings, stateful=False)\\n\\n# Update the timestamp column to the current time in nanoseconds\\nsdf[\\\"Timestamp\\\"] = sdf[\\\"Timestamp\\\"].apply(lambda row: time.time_ns())\\n\\n# Publish the processed SDF to a Kafka topic specified by the output_topic object.\\nsdf = sdf.to_topic(output_topic)\\n\\napp.run(sdf)\n```\n\n----------------------------------------\n\nTITLE: Testing Semantic Search with Qdrant Vector Database in Python\nDESCRIPTION: Performs a similarity search test on the Qdrant vector database. It encodes a test query, searches for the most similar vectors in the collection, and returns the top 10 matching book entries with their similarity scores.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nquery = \"books like star wars\" # Leave the test query as-is for the first attempt\n\nhits = qdrant.search(\n    collection_name=collectionname,\n    query_vector=encoder.encode(query).tolist(),\n    limit=10\n)\n\nprint(\"Entries matching your query:\")\nfor hit in hits:\n  print(hit.payload['doc_name'], \" | \", hit.payload['doc_descr'], \"score:\", hit.score)\n```\n\n----------------------------------------\n\nTITLE: Calculating Moving Average of Stock Prices Using Tumbling Window in Python\nDESCRIPTION: This snippet demonstrates how to calculate a moving average of stock prices using a tumbling window. It processes a list of stock prices in fixed-size, non-overlapping windows, computing the average price for each window.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/navigating-stateful-stream-processing.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstock_prices = [10, 12, 11, 13, 14, 15, 16, 17, 19]\nwindow_size = 3\n\nfor i in range(0, len(stock_prices) - window_size + 1, window_size):\n    window = stock_prices[i:i + window_size]\n    window_average = sum(window) / window_size\n    print(f\"Average price in window: {window_average}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Stream using SignalR\nDESCRIPTION: Creates a new stream using SignalR connection with Microsoft's SignalR client library, including stream details and metadata.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/create-stream.md#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst environmentId = \"YOUR_ENVIRONMENT_ID\"\nconst topic = \"YOUR_TOPIC_NAME\"\n\nconst options = {\n    accessTokenFactory: () => token\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://writer-\" + environmentId + \".platform.quix.io/hub\", options)\n    .build();\n\n// Establish connection\nconnection.start().then(async () => {\n    console.log(\"Connected to Quix.\");\n\n    // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model.\n    let streamDetails = {\n        \"name\": \"cardata\",\n        \"location\": \"simulations/trials\",\n        \"metadata\": {\n            \"rain\": \"light\"\n        }\n    }\n\n    // Send create details\n    console.log(\"Creating stream\");\n    let createdDetails = await connection.invoke(\"CreateStream\", topic, streamDetails);\n    let streamId = createdDetails.streamId\n    console.log(\"Created stream \" + streamId);\n});\n```\n\n----------------------------------------\n\nTITLE: Sending Customized Slack Alerts for Low Bike Availability - Python\nDESCRIPTION: This Python snippet extends the DataFrame loop to send customized alert messages to Slack when bike availability is low at a location. It defines a message based on a threshold (less than 3 bikes), constructs a payload as a dictionary, and sends the message to a Slack channel via an HTTP POST using requests.post and a provided webhook URL. Dependencies: pandas for DataFrame (df), requests library for HTTP, and self.webhook_url configured with the Slack webhook endpoint. Inputs: DataFrame with 'NbBikes' and 'Name' columns, webhook URL. Output: Messages delivered to the specified Slack channel with context-sensitive text. Limitations: messages may be sent for each DataFrame row processed, ensure rate control as needed.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/slack-alerting/slack-alerting.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmessage = \\\"\\\"\\n# check the number of remaining bikes\\nif num_bikes < 3: \\n    message = \\\"Hurry! {} only has {} bike left\\\".format(bike_loc, num_bikes)\\nelse:\\n    message = \\\"{} has {} bikes available\\\".format(bike_loc, num_bikes)\\n\\n# compose and send your slack message\\nslack_message = {\\\"text\\\": message}\\nrequests.post(self.webhook_url, json=slack_message)\n```\n\n----------------------------------------\n\nTITLE: Parameter Data Received Payload Example\nDESCRIPTION: Example payload structure received when handling parameter data through SignalR connection, showing the format of numeric values, string values, tag values, and binary values.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/subscriptions.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n{\n  topicId: 'joeengland-apitests-testing-f1-data',\n  topicName: 'f1-data',\n  streamId: '020aee7e-edba-4913-aee3-b1e493c78132',\n  epoch: 0,\n  timestamps: [ 1697025689418406000, 1697025689487857000 ],\n  numericValues: { EngineRPM: [ 11444, 11463 ] },\n  stringValues: {},\n  tagValues: {\n    DriverStatus: [ 'Flying_lap', 'Flying_lap' ],\n    LapNumber: [ '2', '2' ],\n    LapValidity: [ 'Valid', 'Valid' ],\n    PitStatus: [ 'None', 'None' ],\n    Sector: [ '0', '0' ],\n    streamId: [\n      '5a517ca4-efc3-4166-aedb-a5c57e2b9c59',\n      '5a517ca4-efc3-4166-aedb-a5c57e2b9c59'\n    ]\n  },\n  binaryValues: {}\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Stream Metadata using cURL in Shell\nDESCRIPTION: This snippet demonstrates how to send a PUT request to add metadata to a stream using cURL. It includes the necessary headers and JSON payload.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/stream-metadata.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl \"https://${domain}.platform.quix.io/topics/${topicName}/streams/${streamId}\" \\\n     -X PUT \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"metadata\":{\"fruit\":\"apple\"}}'\n```\n\n----------------------------------------\n\nTITLE: Parameterized Deployment Disabling using YAML Variable\nDESCRIPTION: This snippet illustrates how to use a YAML variable (`{{DISABLED}}`) to control the `disabled` status of a deployment within `quix.yaml`. This allows the deployment status to be configured per environment.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/yaml-variables.md#2025-04-23_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndeployments:\n  - name: CPU Threshold\n    disabled: {{DISABLED}}\n```\n\n----------------------------------------\n\nTITLE: Initializing InfluxDB3Sink with Authentication in Python\nDESCRIPTION: This snippet shows the initialization of an InfluxDB3Sink with required authentication and connection parameters. Used as a template for custom sink implementations, the class requires 'token', 'host', 'organization_id', 'database', and 'measurement' fields, which control access and sink behavior for InfluxDB. Adjust credentials and measurement parameters for your environment.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Initialize InfluxDB3Sink\ninflux_sink = InfluxDB3Sink(\n    token=\"<influxdb-access-token>\",\n    host=\"<influxdb-host>\",\n    organization_id=\"<influxdb-org>\",\n    database=\"<influxdb-database>\",\n    measurement=\"numbers\",\n    # ... other parameters\n)\n\n```\n\n----------------------------------------\n\nTITLE: Loading a Pickled Machine Learning Model in Python\nDESCRIPTION: This code, placed within the `__init__` method of the transformation class in `quix_function.py`, loads a serialized machine learning model stored in 'decision_tree_5_depth.sav'. It uses `pickle.load` with read-binary ('rb') mode to deserialize the model and assigns it to the `self.model` attribute for later use in predictions. Requires the `pickle` library and the model file to be present.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/deploy-ml.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n## Import ML model from file\n    self.model = pickle.load(open('decision_tree_5_depth.sav', 'rb'))\n```\n\n----------------------------------------\n\nTITLE: Defining Vector Ingestion Function for Qdrant in Python\nDESCRIPTION: Defines a function to ingest vector embeddings into Qdrant. The function creates a point structure with the document UUID as ID, the embedding vector, and the original row data as payload.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef ingest_vectors(row):\n\n  single_record = models.PointStruct(\n    id=row['doc_uuid'],\n    vector=row['embeddings'],\n    payload=row\n    )\n\n  qdrant.upload_points(\n      collection_name=collectionname,\n      points=[single_record]\n    )\n\n  print(f'Ingested vector entry id: \"{row[\"doc_uuid\"]}\"...')\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries in Python\nDESCRIPTION: Imports necessary Python libraries into the Jupyter Notebook environment. This includes math, matplotlib.pyplot, mlflow, numpy, pandas, pickle, seaborn, and specific modules from sklearn (tree, model_selection, metrics). These imports make the functions and classes from these libraries available for use in data manipulation, model training, evaluation, and visualization.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/train-ml.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport matplotlib.pyplot as plt\nimport mlflow\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport seaborn as sns\n\nfrom sklearn import tree\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n```\n\n----------------------------------------\n\nTITLE: Listing Kafka Topics using Command Line Tool\nDESCRIPTION: This command uses the Kafka command line tool to list all created topics in a Kafka deployment. It helps verify the existence of topics when troubleshooting the 'unknown topic or partition' error.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/how-to-fix-unknown-partition-error-kafka.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-topics.sh --bootstrap-server=localhost:9092 --list\n```\n\n----------------------------------------\n\nTITLE: Sample Output for ActiveStreamsChanged Event\nDESCRIPTION: This snippet shows an example of the output produced by the ActiveStreamsChanged event handler. It includes a stream object with various properties such as streamId, topicId, metadata, parameters, and status.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/subscriptions.md#2025-04-23_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n[2023-10-09T15:23:27.993Z] Information: WebSocket connected to wss://reader-joeengland-apitests-testing.platform.quix.io/hub?id=o9Ctg5zdQ7aAzdQ2Cz4eMw.\nConnected to Quix.\nstream ----->  {\n  streamId: '0a23798f-7d75-413d-9031-8d8386c2f8c7',\n  topicId: 'joeengland-apitests-testing-f1-data',\n  topicName: 'f1-data',\n  metadata: {},\n  parents: [],\n  parameters: {\n    EngineTemp: { dataType: 'Numeric' },\n    Motion_WorldPositionX: { dataType: 'Numeric' },\n    Steer: { dataType: 'Numeric' },\n    Brake: { dataType: 'Numeric' },\n    EngineRPM: { dataType: 'Numeric' },\n    Motion_WorldPositionZ: { dataType: 'Numeric' },\n    TotalLapDistance: { dataType: 'Numeric' },\n    LapDistance: { dataType: 'Numeric' },\n    Gear: { dataType: 'Numeric' },\n    original_timestamp: { dataType: 'Numeric' },\n    Motion_WorldPositionY: { dataType: 'Numeric' },\n    Speed: { dataType: 'Numeric' }\n  },\n  events: {},\n  firstSeen: '2023-10-09T15:23:29.7197985Z',\n  lastSeen: '2023-10-09T15:23:29.7198602Z',\n  status: 'Receiving',\n  lastData: '2023-10-09T15:23:29.7198603Z'\n}\naction ----->  AddUpdate\n```\n\n----------------------------------------\n\nTITLE: Specifying Source and Local Topic Names for Linked Topics in Quix (YAML)\nDESCRIPTION: This YAML snippet illustrates how to link a topic while specifying different names for the source topic and the local topic in the Quix environment. Aside from 'workspaceId', it introduces 'topicName' under 'linkedTopic' to explicitly define the name of the topic in the source workspace. This prevents confusion or conflicts where naming conventions differ between projects or teams. Both 'workspaceId' and 'topicName' must be accurate, and the user requires permission to access the source topic. The output links a local topic to a remotely named one, retaining read-only access.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/create/create-linked-project.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n\"# This section describes the Topics of the data pipeline\\ntopics:\\n  - name: f1-data  # Local name in your environment\\n    linkedTopic:\\n      workspaceId: org-source-main\\n      topicName: source-topic-name  # Actual name of the topic in the source workspace\"\n```\n\n----------------------------------------\n\nTITLE: Sending Event Data using SignalR WebSockets in Node.js\nDESCRIPTION: Node.js implementation using the SignalR library to establish a WebSocket connection and send event data to a stream. This approach provides a real-time alternative to REST API calls and follows the same data structure for events.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/send-event.md#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst environmentId = \"YOUR_ENVIRONMENT_ID\"\nconst topic = \"YOUR_TOPIC_NAME\"\nconst streamId = \"ID_OF_STREAM_TO_WRITE_TO\"\n\nconst options = {\n    accessTokenFactory: () => token\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://writer-\" + environmentId + \".platform.quix.io/hub\", options)\n    .build();\n\n// Establish connection\nconnection.start().then(async () => {\n    console.log(\"Connected to Quix.\");\n\n    // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model.\n    let eventData = [\n        {\n            \"timestamp\": Date.now() * 1000000, // set now in nanoseconds,\n            \"tags\": {\n                \"capacity\": \"over\"\n            },\n            \"id\": \"Alert\",\n            \"value\": \"Successful sample run\"\n        }\n    ]\n\n    // Send stream update details\n    console.log(\"Sending event data\");\n    await connection.invoke(\"SendEventData\", topic, streamId, eventData);\n    console.log(\"Sent event data\");\n});\n```\n\n----------------------------------------\n\nTITLE: Processing Telegraf CPU Data with Quix Streams in Python\nDESCRIPTION: This Python script uses the `quixstreams` library to process Telegraf data from a specified input Kafka topic. It filters for CPU metrics, groups them by host tag, calculates the sum of system and user CPU usage, aggregates this sum over a 1-minute tumbling window, reformats the result, prints it to the console, and publishes it to a specified output topic. It requires `quixstreams`, `python-dotenv`, and environment variables `input` and `output` for topic names.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/integrations/databases/influxdb/telegraf.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```py\nimport os\nfrom quixstreams import Application\nfrom datetime import timedelta\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\napp = Application(consumer_group=\"transformation-v1\", auto_offset_reset=\"earliest\")\n\ninput_topic = app.topic(os.environ[\"input\"])\noutput_topic = app.topic(os.environ[\"output\"])\n\nsdf = app.dataframe(input_topic)\n\nsdf = sdf[sdf[\"name\"] == \"cpu\"]\nsdf = sdf.group_by(lambda row: row[\"tags\"][\"host\"], \"group_by_host\")\nsdf = sdf.apply(lambda row: row[\"fields\"][\"usage_system\"] + row[\"fields\"][\"usage_user\"])\nsdf = sdf.tumbling_window(timedelta(minutes=1), timedelta(seconds=5)).sum().final()\nsdf = sdf.apply(lambda row, key, *_: {\n    \"time\": row[\"start\"],\n    \"cpu_total\": row[\"value\"],\n    \"tags\": {\n        \"host\": key\n    }\n}, metadata=True)\n\nsdf.print()\n\nsdf.to_topic(output_topic)\n\nif __name__ == \"__main__\":\n    app.run()\n```\n```\n\n----------------------------------------\n\nTITLE: Creating Qdrant Collection for Vector Storage in Python\nDESCRIPTION: Creates a Qdrant collection to store book vector embeddings. The collection uses the COSINE distance metric and sets the vector size based on the encoder model's embedding dimension.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nqdrant.recreate_collection(\n    collection_name=collectionname,\n    vectors_config=models.VectorParams(\n        size=encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n        distance=models.Distance.COSINE\n    )\n)\nprint(\"(re)created collection\")\n```\n\n----------------------------------------\n\nTITLE: Generating Time-Shifted Brake Signal for Prediction\nDESCRIPTION: Creates a new column 'Brake_shifted_5s' by shifting the 'Brake_bool' column values backward in time by a calculated number of periods, corresponding to approximately 5 seconds. This prepares the target variable for training a model to predict braking 5 seconds in advance. Rows with NaN values resulting from the shift are dropped.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/train-ml.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n## Offset dataset and trim it\nNUM_PERIODS = -round(5e9/53852065.77281786)\n\ndf[\"Brake_shifted_5s\"] = df[\"Brake_bool\"].shift(periods=NUM_PERIODS)\ndf = df.dropna(axis='rows') # clean out null values\n```\n\n----------------------------------------\n\nTITLE: Calculating Class Weights for Imbalanced Data\nDESCRIPTION: Calculates class weights for the target variable 'Brake_shifted_5s' (assigned to Y). This is done to address potential class imbalance by assigning higher weights to the minority class during model training. The resulting weights are stored in the 'cw' dictionary and printed.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/train-ml.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nY = df[\"Brake_shifted_5s\"]\n\ncw = {}\nfor val in set(Y):\n    cw[val] = np.sum(Y != val)\n\nprint(cw)\n```\n\n----------------------------------------\n\nTITLE: Defining Application Build and Runtime Environment with Dockerfile in YAML for Quix\nDESCRIPTION: This Dockerfile (presented in YAML code block style) configures the build environment for a Python-based Quix application. It sets up a slim Python base image, configures critical environment variables for non-interactive and Unicode-aware operation, installs dependencies from any requirements.txt found in the project, and establishes the entrypoint for running the application. Dependencies include Python 3.11.1, Docker, and a requirements.txt file; outputs are a Docker container image ready to be deployed as part of the Quix pipeline. It assumes the build context includes a valid Python main module and requirements file.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/create/project-structure.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nFROM python:3.11.1-slim-buster\\n\\nENV DEBIAN_FRONTEND=\\\"noninteractive\\\"\\nENV PYTHONUNBUFFERED=1\\nENV PYTHONIOENCODING=UTF-8\\n\\nWORKDIR /app\\nCOPY --from=git /project .\\nRUN find | grep requirements.txt | xargs -I '{}' python3 -m pip install -i http://pip-cache.pip-cache.svc.cluster.local/simple --trusted-host pip-cache.pip-cache.svc.cluster.local -r '{}' --extra-index-url https://pypi.org/simple --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/\\nENTRYPOINT [\\\"python3\\\", \\\"main.py\\\"]\n```\n\n----------------------------------------\n\nTITLE: Specifying External Image Reference - Plaintext\nDESCRIPTION: This snippet illustrates how to construct and format an external container image reference for deployment, including the registry, image name, and version tag. There are no required dependencies or prerequisites to use this reference other than having a valid image pushed to the specified registry. The syntax follows standard container reference conventions, and it may be used in deployment forms or pipeline configurations. The expected input is a string composed of registry, image path, and tag; output is the resolved image for deployment. Ensure the registry is accessible from the deployment environment.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/deploy-external-image.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndockerhub.io/myorg/my-service:1.2.3\n```\n\n----------------------------------------\n\nTITLE: Configuring Shared Folders in YAML\nDESCRIPTION: Demonstrates how to configure shared folders in app.yaml to enable code reuse across multiple applications within the same project. The configuration allows shared code to be available during the build process.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/release-2-january-2025-shared-folders.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napp.yaml\n\nincludedFolders:\n  - shared_module\n  - shared_lib\n```\n\n----------------------------------------\n\nTITLE: Setting Quix SDK Token in .env File\nDESCRIPTION: This configuration snippet shows how to define the `Quix__Sdk__Token` environment variable in a `.env` file for authenticating Quix Streams during local development. The value `\"sdk-12345\"` is a placeholder and should be replaced with a valid SDK token obtained from Quix.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/authentication/quix-streams.md#2025-04-23_snippet_0\n\nLANGUAGE: dotenv\nCODE:\n```\nQuix__Sdk__Token=\"sdk-12345\"\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU and Memory Allocation with YAML Variables in Quix (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to reference environment-scoped variables for CPU and memory allocation within the quix.yaml file. The double-brace syntax (e.g., {{ CPU_COUNT }}) allows the configuration to dynamically resolve values based on the current environment (development or production), enabling scalable and cost-efficient deployments. Prerequisites include defining the variables in the platform UI or variables file; inputs are variable names, and outputs are concrete configuration values during build/deployment. Ensure all referenced variables are present for the targeted environments to avoid runtime interpolation errors.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/yaml-variables.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nresources:\n  cpu: {{ CPU_COUNT }}\n  memory: {{ MEMORY }}\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Declaring Constants – Python\nDESCRIPTION: Imports all required Python libraries for the pipeline, including Quix Streams, Qdrant Client, and SentenceTransformer. Also sets global constants for topic, group, and collection names. Prerequisites: all prior dependencies installed, configuration values set as literals. Output: Namespaces and variables loaded for further processing.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport json\\nimport time\\nimport uuid\\nimport os\\nimport pandas as pd\\nfrom qdrant_client import models, QdrantClient\\n\\n# Quix stuff\\nfrom quixstreams.kafka import Producer\\nfrom quixstreams import Application, State, message_key\\nfrom sentence_transformers import SentenceTransformer\\n\\ndocs_topic_name = 'docs_books'\\nvectors_topic_name = 'vectors_all-minilm-l6-v2'\\nconsumergroup_name = 'qdrant-demo'\\ncollectionname = \\\"book-catalog\\\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Accuracy DataFrame\nDESCRIPTION: Displays the Pandas DataFrame 'model_accuracy' which contains the baseline and model accuracy scores calculated during the KFold cross-validation process in the previous step. This allows for a quick review of the model's performance across different folds.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/train-ml.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel_accuracy\n```\n\n----------------------------------------\n\nTITLE: Parameter Data Stream Subscription Handler\nDESCRIPTION: Example showing how to establish a SignalR connection, subscribe to parameter data streams, handle received data, and unsubscribe. Uses the @microsoft/signalr package to connect to Quix platform.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/subscriptions.md#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\n\nconst options = {\n    accessTokenFactory: () => '<your-PAT>'\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n      .withUrl(\"https://reader-joeengland-apitests-testing.platform.quix.io/hub\", options)\n      .build();\n\n// Establish connection \nconnection.start().then(() => {\n    console.log(\"Connected to Quix.\");\n\n    // Subscribe to parameter data stream \n    connection.invoke(\"SubscribeToParameter\", \"f1-data\", \"*\", \"EngineRPM\");\n\n    // Read parameter data from the stream \n    connection.on(\"ParameterDataReceived\", data => {\n        console.log('topicId ', data.topicId);\n        console.log('streamId ', data.streamId);\n        console.log('streamId ', data.numericValues.EngineRPM);\n\n        // Unsubscribe from stream \n        connection.invoke(\"UnsubscribeFromParameter\", \"f1-data\", \"*\", \"EngineRPM\");\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: REST API Endpoint Structure\nDESCRIPTION: Base URL structure for sending parameter data via REST API\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/send-data.md#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n/topics/${topicName}/streams/${streamId}/parameters/data\n```\n\n----------------------------------------\n\nTITLE: Initializing Quix Streams Consumer Application for Vector Processing\nDESCRIPTION: Sets up a Quix Streams application that consumes messages from Kafka, transforms them into vectors, and updates a Qdrant vector database. This consumer application processes incoming JSON messages and applies the ingest_vectors function to each row.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Define the consumer application and settings\napp = Application(\n    broker_address=\"127.0.0.1:9092\",\n    consumer_group=\"vectorizer\",\n    auto_offset_reset=\"earliest\",\n    on_message_processed=on_message_processed,\n    consumer_extra_config={\"allow.auto.create.topics\": \"true\"},\n)\n\n# Define an input topic with JSON deserializer\ninput_topic = app.topic(inputtopicname, value_deserializer=\"json\")\nprint(f\"Consuming from input topic: {inputtopicname}\")\n\n# Initialize a streaming dataframe based on the stream of messages from the input topic:\nsdf = app.dataframe(topic=input_topic)\n\n# INGESTION HAPPENS HERE\n### Trigger the embedding function for any new messages(rows) detected in the filtered SDF\nsdf = sdf.update(lambda row: ingest_vectors(row))\napp.run(sdf)\n\n# STOP THIS CELL MANUALLY WHEN THE BOOK ENTRIES HAVE BEEN INGESTED\n```\n\n----------------------------------------\n\nTITLE: Defining the Forecast Data Output Format - JSON\nDESCRIPTION: This snippet demonstrates the JSON schema for the published forecast data. The object contains a string timestamp and a floating-point forecast value. Consumers of this forecast data (such as alerting services) are expected to parse these fields from the topic stream. The forecast value represents the predicted temperature at the given timestamp. There are no special dependencies, but applications should ensure the timestamp conforms to the documented format.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/predictive-maintenance/forecast-service.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"timestamp\\\": \\\"2024-04-16 18:03:20\\\",\\n  \\\"forecast\\\": 72.21788743081183\\n}\n```\n\n----------------------------------------\n\nTITLE: Parameterized Resource Configuration using YAML Variables\nDESCRIPTION: This snippet demonstrates how to replace hard-coded resource values in `quix.yaml` with YAML variables (`{{CPU}}`, `{{MEMORY}}`, `{{REPLICAS}}`). This allows these values to be dynamically injected based on the environment.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/yaml-variables.md#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nresources:\n  cpu: {{CPU}}\n  memory: {{MEMORY}}\n  replicas: {{REPLICAS}}\n```\n\n----------------------------------------\n\nTITLE: Generating Additional Book Data for Vector Store in Python\nDESCRIPTION: Creates a list of science fiction book records with titles, descriptions, authors, and publication years. The data is converted to a pandas DataFrame and saved to a CSV file for subsequent streaming to Kafka.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndocuments = [\n  {\"name\": \"Childhood's End\", \"description\": \"A peaceful alien invasion leads to the end of humanity's childhood.\", \"author\": \"Arthur C. Clarke\", \"year\": 1953 },\n  {\"name\": \"The Day of the Triffids\", \"description\": \"After a meteor shower blinds most of the population, aggressive plant life starts taking over.\", \"author\": \"John Wyndham\", \"year\": 1951 },\n  {\"name\": \"Contact\", \"description\": \"Scientists receive a message from extraterrestrial beings and build a machine to meet them.\", \"author\": \"Carl Sagan\", \"year\": 1985 },\n  {\"name\": \"The Three-Body Problem\", \"description\": \"Humanity faces a potential invasion from a distant alien civilization in crisis.\", \"author\": \"Liu Cixin\", \"year\": 2008 },\n  {\"name\": \"Sphere\", \"description\": \"A team investigates a spaceship found on the ocean floor, not knowing its mysterious and possibly extraterrestrial origin.\", \"author\": \"Michael Crichton\", \"year\": 1987 },\n  {\"name\": \"Footfall\", \"description\": \"Elephant-like aliens invade Earth, and humanity must find a way to fight back.\", \"author\": \"Larry Niven and Jerry Pournelle\", \"year\": 1985 },\n  {\"name\": \"The Puppet Masters\", \"description\": \"Slug-like aliens invade Earth by attaching to humans and controlling their minds.\", \"author\": \"Robert A. Heinlein\", \"year\": 1951 },\n  {\"name\": \"The Kraken Wakes\", \"description\": \"Alien beings from the depths of the ocean start attacking humanity.\", \"author\": \"John Wyndham\", \"year\": 1953 },\n  {\"name\": \"The Invasion of the Body Snatchers\", \"description\": \"A small town discovers that some of its residents are being replaced by perfect physical copies that emerge from plantlike pods.\", \"author\": \"Jack Finney\", \"year\": 1955 },\n  {\"name\": \"Calculating God\", \"description\": \"An alien arrives on Earth, seeking to understand why God has apparently been involved in Earth's evolution.\", \"author\": \"Robert J. Sawyer\", \"year\": 2000 },\n  {\"name\": \"The Forge of God\", \"description\": \"Aliens arrive under the guise of friendship, but their true mission is to destroy Earth.\", \"author\": \"Greg Bear\", \"year\": 1987 },\n  {\"name\": \"Roadside Picnic\", \"description\": \"Aliens visited Earth, leaving behind zones filled with dangerous objects and phenomena.\", \"author\": \"Arkady and Boris Strugatsky\", \"year\": 1972 },\n  {\"name\": \"Out of the Dark\", \"description\": \"An alien race invades Earth, underestimating humanity's will to survive.\", \"author\": \"David Weber\", \"year\": 2010 },\n  {\"name\": \"Arrival (Stories of Your Life and Others)\", \"description\": \"A linguist learns to communicate with aliens who have arrived on Earth, altering her perception of reality.\", \"author\": \"Ted Chiang\", \"year\": 1998 },\n  {\"name\": \"To Serve Man\", \"description\": \"Aliens come to Earth claiming to be friends, but their true intentions are revealed in a horrifying twist.\", \"author\": \"Damon Knight\", \"year\": 1950},\n  {\"name\": \"The Mote in God's Eye\", \"description\": \"Humanity encounters an alien race that poses a unique and unforeseen challenge.\", \"author\": \"Larry Niven and Jerry Pournelle\", \"year\": 1974 },\n  {\"name\": \"Old Man's War\", \"description\": \"Earth's senior citizens are recruited to fight in an interstellar war, discovering new alien cultures and threats.\", \"author\": \"John Scalzi\", \"year\": 2005 },\n]\n\n# Convert the list of dictionaries to a DataFrame\ndf = pd.DataFrame(documents)\n# Save the DataFrame to a CSV file so that we can practice producing to Kafka from a CSV file in the next step\ndf.to_csv('documents.csv')\n```\n\n----------------------------------------\n\nTITLE: Simulink Model Bootstrap Function in MATLAB\nDESCRIPTION: Defines a MATLAB function 'engine' which initializes a Simulink model using supplied throttle angle and time arrays, runs a simulation, and returns the final engine speed from model output. Relies on MATLAB's 'timeseries', 'assignin', 'Simulink.SimulationInput', and Simulink Compiler utility functions. To use, save as 'engine.m' alongside the referenced Simulink model (e.g., 'sldemo_engine').\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_4\n\nLANGUAGE: matlab\nCODE:\n```\nfunction R = engine(throttle_angle, time)\n    ta = timeseries(throttle_angle, time);\n    assignin(\"base\", \"throttle_angle\", ta);\n    si = Simulink.SimulationInput('sldemo_engine');\n    si = simulink.compiler.configureForDeployment(si);\n    sout = sim(si);\n    R = sout.engine_speed.Data(end);\nend\n```\n\n----------------------------------------\n\nTITLE: Fetching Stream Model Data with cURL\nDESCRIPTION: This curl command fetches stream model information across all streams using the `/streams/models` endpoint. The request requires authorization via a bearer token and sends an empty JSON payload.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-models.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl \"https://${domain}.platform.quix.io/streams/models\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d \"{}\"\n```\n\n----------------------------------------\n\nTITLE: Aggregating Vehicle Counts from Detected Objects - Quix - Python\nDESCRIPTION: This handler aggregates total vehicles by summing relevant object columns ('car', 'bus', 'truck', 'motorbike') in incoming pandas DataFrames. It updates the dataframe with the computed vehicles count and publishes it to a stream using Quix Streams buffer operations. Python dependencies are pandas and the Quix SDK. Inputs are DataFrames with object counts; outputs are the same DataFrames with an extra 'vehicles' column, published downstream. Assumes 'topic_producer' and stream producer instances are defined elsewhere.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/other-services.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\\n    # List of vehicle columns\\n    vehicle_columns = ['car', 'bus', 'truck', 'motorbike']\\n\\n    # Calculate the total vehicle count based on existing columns\\n    total_vehicle_count = df.apply(lambda row: sum(row.get(column, 0) for column in vehicle_columns), axis=1)\\n\\n    # Store vehicle count in the data frame\\n    df[\\\"vehicles\\\"] = total_vehicle_count\\n    stream_producer = topic_producer.get_or_create_stream(stream_id = stream_consumer.stream_id)\\n    # Publish data frame to the producer stream\\n    stream_producer.timeseries.buffer.publish(df)\n```\n\n----------------------------------------\n\nTITLE: Fetching Parameter Data Using cURL\nDESCRIPTION: Complete cURL command example demonstrating how to make an authenticated request to the Quix Platform API to fetch parameter data.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/raw-data.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"https://${domain}.platform.quix.io/parameters/data\" \\\n    -H \"accept: text/plain\" \\\n    -H \"Authorization: bearer <token>\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"numericParameters\":[{\"parameterName\":\"Speed\"}]}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Telegraf Agent and Quix Output Plugin in YAML\nDESCRIPTION: This YAML configuration file defines settings for a Telegraf agent. It includes global agent parameters like collection interval and batch size, enables standard input plugins (`cpu`, `mem`, `disk`), and configures the `outputs.quix` plugin. The Quix output plugin sends collected data to a Quix Cloud topic, requiring environment variables for Workspace ID (`Quix__Workspace__Id`), SDK Token (`Quix__Sdk__Token`), Portal API URL (`Quix__Portal__Api`), and the output topic name (`output`). Requires Telegraf v1.3.3+.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/integrations/databases/influxdb/telegraf.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\n# Global Agent Configuration\n[agent]\n  interval = \"10s\"\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = \"0s\"\n  flush_interval = \"1s\"\n  flush_jitter = \"0s\"\n  precision = \"\"\n  debug = false\n  quiet = false\n  logfile = \"\"\n\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  collect_cpu_time = false\n  report_active = false\n\n[[inputs.mem]]\n  ## Collect memory usage metrics\n\n[[inputs.disk]]\n  ## Collect disk usage metrics\n  ignore_fs = [\"tmpfs\", \"devtmpfs\"]\n\n[[outputs.quix]]\n  workspace = \"${Quix__Workspace__Id}\"\n  auth_token = \"${Quix__Sdk__Token}\"\n  api_url = \"${Quix__Portal__Api}\"\n  topic = \"${output}\"\n  data_format = \"json\" \n  timestamp_units = \"1ns\"\n```\n```\n\n----------------------------------------\n\nTITLE: Controlling Data Publishing Rate in Python\nDESCRIPTION: This Python snippet shows the logic for controlling the timing between publishing messages. If the `keep_timing` boolean variable is `False`, indicating that the original timing from the data source should not be preserved, the code introduces a fixed delay of 200 milliseconds using `time.sleep(0.2)` before processing the next message. This requires the `time` module to be imported.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/clickstream-producer.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif not keep_timing:\n    # Don't want to keep the original timing or no timestamp? That's ok, just sleep for 200ms\n    time.sleep(0.2)\n```\n\n----------------------------------------\n\nTITLE: Hard-coded Deployment Disabling in YAML\nDESCRIPTION: This snippet shows an example section within a `quix.yaml` file where a deployment is explicitly disabled using a hard-coded boolean value (`disabled: true`).\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/yaml-variables.md#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndeployments:\n  - name: CPU Threshold\n    disabled: true\n```\n\n----------------------------------------\n\nTITLE: Continuously Writing CPU Load to InfluxDB v2 using Python\nDESCRIPTION: This Python script continuously monitors the system's CPU load using the `psutil` library and writes the data to a local InfluxDB v2 instance (http://localhost:8086). It connects to InfluxDB using a token from the `INFLUXDB_TOKEN` environment variable and the 'Quix Docs' organization, targets the 'servermetrics' bucket, and writes data points to the 'server1' measurement synchronously in an infinite loop. Each data point includes 'tagname1' and the current CPU load as the 'cpu' field. The script runs until manually interrupted (e.g., Ctrl+C). Requires `influxdb_client`, `os`, `time`, and `psutil` libraries.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/influxdb-migration/influxdb-v2-install.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport influxdb_client, os, time, psutil\nfrom influxdb_client import InfluxDBClient, Point, WritePrecision\nfrom influxdb_client.client.write_api import SYNCHRONOUS\n\ndef get_cpu_load():\n    cpu_load = psutil.cpu_percent(interval=1)\n    return cpu_load\n\ndef main():\n    token = os.environ.get(\"INFLUXDB_TOKEN\")\n    org = \"Quix Docs\"\n    url = \"http://localhost:8086\"\n    bucket=\"servermetrics\"\n\n    write_client = influxdb_client.InfluxDBClient(url=url, token=token, org=org)\n    write_api = write_client.write_api(write_options=SYNCHRONOUS)\n\n    while True:\n        cpu_load = get_cpu_load()\n        print(\"CPU load: \", cpu_load)\n        point = (\n            Point(\"server1\")\n            .tag(\"tagname1\", \"tagvalue1\")\n            .field(\"cpu\", cpu_load)\n        )\n        print(\"Writing value to InfluxDB v2: --> \", cpu_load)\n        write_api.write(bucket=bucket, org=\"Quix Docs\", record=point)\n\nif __name__ == '__main__':\n    try:\n        main()\n    except KeyboardInterrupt:\n        print('Exiting due to keyboard interrupt')\n```\n\n----------------------------------------\n\nTITLE: Example Stream Response JSON from Quix Platform\nDESCRIPTION: JSON response structure returned when querying streams. Contains stream details including ID, name, topic, timestamps, status, metadata, and location.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-paged.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"streamId\":\"e6545c18-d20d-47bd-8997-f3f825c1a45c\",\n    \"name\":\"cardata\",\n    \"topic\":\"cardata\",\n    \"createdAt\":\"2021-03-31T13:04:43.368Z\",\n    \"lastUpdate\":\"2021-03-31T13:04:44.53Z\",\n    \"dataStart\":1612191099000000000,\n    \"dataEnd\":1612191371000000000,\n    \"status\":\"Closed\",\n    \"metadata\":{},\n    \"parents\":[],\n    \"location\":\"/static data/\"\n}]\n```\n\n----------------------------------------\n\nTITLE: Describing Kafka Topic Details\nDESCRIPTION: This command provides detailed information about a specific Kafka topic, including partition count, replication factor, leader, and replicas. It's useful for confirming topic and partition details when investigating errors.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/how-to-fix-unknown-partition-error-kafka.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-topics.sh --bootstrap-server=localhost:9092 --describe --topic sampletopic\n```\n\n----------------------------------------\n\nTITLE: Tag Filtering with Regular Expression in Bash/cURL\nDESCRIPTION: Complete cURL example demonstrating how to use the Like operator with a regular expression pattern to filter parameter data. This query retrieves Speed parameter values where LapNumber is either 2 or 4 using a regex pattern.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/filter-tags.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"https://telemetry-query-testing-quickstart.platform.quix.io/parameters/data\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"tagFilters\": [{\n             \"tag\": \"LapNumber\",\n             \"operator\": \"Like\",\n             \"value\": \"^[24]\\\\.\"\n         }],\n         \"numericParameters\": [{\"parameterName\": \"Speed\"}],\n         \"from\": 1612191182000000000,\n         \"to\": 1612191189000000000\n     }'\n```\n\n----------------------------------------\n\nTITLE: Filtering Streams by Status using cURL\nDESCRIPTION: Example of filtering streams based on their status, excluding streams that are either Interrupted or Closed.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-filtered.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"https://${domain}.platform.quix.io/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"excludeStatuses\": [ \"Interrupted\", \"Closed\" ]}'\n```\n\n----------------------------------------\n\nTITLE: Adding User to Kafka Topic ACL\nDESCRIPTION: This command adds a sample user as a producer to a selected Kafka topic's ACL. It's useful for granting necessary access permissions to resolve authorization-related 'unknown topic or partition' errors.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/how-to-fix-unknown-partition-error-kafka.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-acls.sh --authorizer-properties --bootstrap-server localhost:9092 --add --allow-principal User:sampleuser --producer --topic sampletopic\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Maps API Key in Angular AppModule (TypeScript)\nDESCRIPTION: This TypeScript snippet shows how to configure the AgmCoreModule (Angular Google Maps) within an Angular application's `src/app/app.module.ts` file. It involves passing the Google Maps API key to the `forRoot` static method to initialize the map component. This is an optional step to remove the 'For development purposes only' watermark displayed on the map. Replace '<your_google_maps_api_key>' with your actual Google Maps API key.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/get-project.md#2025-04-23_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n``` typescript\nAgmCoreModule.forRoot({apiKey: '<your_google_maps_api_key>'}),\n```\n```\n\n----------------------------------------\n\nTITLE: Launching the MLflow User Interface\nDESCRIPTION: Instructs the user to run the `mlflow ui` command in a terminal window. This command starts the MLflow tracking server and opens its web-based user interface, allowing users to browse, compare, and visualize the logged experiments and their parameters/metrics.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/train-ml.md#2025-04-23_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nmlflow ui\n```\n\n----------------------------------------\n\nTITLE: Querying AWS Athena with PyAthena and Pandas in Python\nDESCRIPTION: This Python script connects to AWS Athena using the PyAthena library, executes a user-provided SQL query, and loads the results into a Pandas DataFrame. Requires replacing placeholder values for the S3 staging directory, AWS region, AWS access key ID, AWS secret access key, and the actual SQL query string. The script prints the resulting DataFrame.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-connectors/templates/awss3icebergsink-destination.md#2025-04-23_snippet_1\n\nLANGUAGE: py\nCODE:\n```\nfrom pyathena import connect\nimport pandas as pd\nimport os\n\n# Connect to Athena\nconn = connect(\n    s3_staging_dir='',  # Replace with your S3 bucket for Athena query results\n    region_name=\"eu-north-1\", # Replace with your region\n    aws_access_key_id='',  # Replace with your AWS access key ID\n    aws_secret_access_key=''  # Replace with your AWS secret access key\n)\n\ndef query_athena(sql_query):\n    # Execute the query and load the results into a Pandas DataFrame\n    df = pd.read_sql(sql_query, conn)\n    return df\n\ndata = query_athena(\"[YOUR SQL QUERY]\")\nprint(data)\n```\n\n----------------------------------------\n\nTITLE: Example Response: Stream with Two Children\nDESCRIPTION: This JSON response shows a stream with two child streams. Each stream contains properties like ID, name, topic, timestamps, status, and metadata. The children array contains the complete stream objects for direct descendants.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-models.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"children\": [{\n        \"children\": [],\n        \"streamId\": \"79bbed17-5c71-4b0e-99f6-3596577b46d8\",\n        \"name\": \"new-child\",\n        \"topic\": \"cars\",\n        \"createdAt\": \"2021-04-08T15:27:09.19Z\",\n        \"lastUpdate\": \"2021-04-13T10:21:52.572Z\",\n        \"status\": \"Open\",\n        \"metadata\": {},\n        \"parents\": [\n            \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\"\n        ],\n        \"location\": \"/\"\n    },{\n        \"children\": [],\n        \"streamId\": \"f003c1dd-9abe-49dd-afd2-f194d3d96035\",\n        \"name\": \"example1\",\n        \"topic\": \"cars\",\n        \"createdAt\": \"2021-04-12T11:50:38.504Z\",\n        \"lastUpdate\": \"2021-04-12T12:00:40.482Z\",\n        \"status\": \"Interrupted\",\n        \"metadata\": {\n            \"rain\": \"light\"\n        },\n        \"parents\": [\n            \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\"\n        ],\n        \"location\": \"/examples/first/\"\n    }],\n    \"streamId\": \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\",\n    \"topic\": \"cars\",\n    \"createdAt\": \"2021-04-08T14:12:29.807Z\",\n    \"lastUpdate\": \"2021-04-12T13:45:08.377Z\",\n    \"timeOfRecording\": \"2021-04-12T00:00:00Z\",\n    \"dataStart\": 0,\n    \"dataEnd\": 1618233869000000000,\n    \"status\": \"Interrupted\",\n    \"metadata\": {},\n    \"parents\": [],\n    \"location\": \"/\"\n}]\n```\n\n----------------------------------------\n\nTITLE: Filtering Streams by Location using cURL\nDESCRIPTION: Demonstrates filtering streams based on their location in the hierarchy. The filter matches streams at the specified location and any nested locations below it.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-filtered.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"https://${domain}.platform.quix.io/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"location\": \"/one\"}'\n```\n\n----------------------------------------\n\nTITLE: Publishing Data to Quix Stream in Python\nDESCRIPTION: This Python code snippet demonstrates how the Clickstream Producer publishes data. It first retrieves or creates a Quix stream associated with a specific `userId` using `producer_topic.get_or_create_stream`. Then, it publishes a row of data (`df_row`, presumably a Pandas DataFrame row or similar structure) to the timeseries data of that stream using the `publish` method.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/clickstream-producer.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nstream_producer = producer_topic.get_or_create_stream(row['userId'])\nstream_producer.timeseries.publish(df_row)\n```\n\n----------------------------------------\n\nTITLE: Defining Linked Topics in Quix Pipelines (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to specify linked topics within a Quix data pipeline configuration. It uses the 'linkedTopic' object with a 'workspaceId' to reference a remote workspace, thus linking a topic from an external Quix environment. Prerequisites include an existing workspace ID and access rights to the source topic. The main parameter is 'workspaceId' under 'linkedTopic', associating the local topic with one from the specified remote workspace. The snippet is part of a larger YAML configuration and must be adapted according to the environment's structure.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/create/create-linked-project.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n\"...\\n# This section describes the Topics of the data pipeline\\ntopics:\\n  - name: f1-data\\n    linkedTopic:\\n      workspaceId: org-source-main\\n...\"\n```\n\n----------------------------------------\n\nTITLE: Checking Kafka Topic Replication with Describe Command\nDESCRIPTION: Command reference for checking topic replication factor and details using the Kafka describe command. This helps verify the health and configuration of topic replicas.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/how-to-fix-unknown-partition-error-kafka.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n--describe\n```\n\n----------------------------------------\n\nTITLE: Setting Authorization Header with Bearer Token for HTTP Requests\nDESCRIPTION: Shows the format for adding a Personal Access Token (PAT) as a bearer token in the Authorization header for HTTP requests to Quix APIs.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/http-requests.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nAuthorization: bearer <token>\n```\n\n----------------------------------------\n\nTITLE: Plotting Actual vs Predicted Braking with Matplotlib\nDESCRIPTION: Generates two subplots using Matplotlib. The top plot shows the actual time-shifted braking signal (Y) and normalized speed. The bottom plot shows the braking signal predicted by the trained Decision Tree model (`decision_tree.predict(X)`) and normalized speed. This visualization helps compare the model's predictions against the actual braking behavior.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/train-ml.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nf, (ax1, ax2) = plt.subplots(2, 1, sharey=True, figsize=(50,8))\nax1.plot(Y)\nax1.plot(X[\"Speed\"]/X[\"Speed\"].max())\n\nax2.plot(decision_tree.predict(X))\nax2.plot(X[\"Speed\"]/X[\"Speed\"].max())\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for Quix Ingestion Script\nDESCRIPTION: This `requirements.txt` file lists the necessary Python packages for the accompanying data ingestion script. It includes `quixstreams` for interacting with the Quix platform, `python-dotenv` potentially for environment variable management, and `psutil` for accessing system metrics like CPU load.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-cloud/quixtour/ingest.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nquixstreams==2.5.0\npython-dotenv\npsutil\n```\n\n----------------------------------------\n\nTITLE: Parameter Data JSON Structure\nDESCRIPTION: JSON payload structure for sending parameter data with timestamps and various value types\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/send-data.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"timestamps\": [...],\n    \"numericValues\": {...},\n    \"stringValues\": {...},\n    \"tagValues\": {...}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Vector Rotation Function in MATLAB\nDESCRIPTION: Defines a MATLAB function 'rot' that performs 2D vector rotation by a provided angle theta using standard rotation matrix multiplication. This function operates on a vector 'v' and an angle 'theta', returning the rotated vector. No external dependencies; to use, save as 'rot.m' in your MATLAB working directory.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_0\n\nLANGUAGE: matlab\nCODE:\n```\nfunction M = rot(v, theta)\n    R = [cos(theta) -sin(theta); sin(theta) cos(theta)];\n    M = R * v;\nend\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Ports in Quix Deployments - YAML\nDESCRIPTION: This snippet demonstrates how to define network port mappings for a deployment in a 'quix.yaml' file for Quix projects. The 'network' section allows specifying a 'serviceName' for the internal service, and the 'ports' array defines port-forwarding rules between external and container ports. Each port entry includes an externally exposed 'port' and an optional 'targetPort', redirecting traffic to a port inside the container. The configuration supports multiple port mappings and requires valid syntax within the 'deployments' array.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/ports.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndeployments:\n    - name: Demo Data\n      ...\n      network:\n        serviceName: MyServiceInternalName\n        ports:\n        - port: 80\n          targetPort: 8080\n        - port: 81\n          targetPort: 8080\n```\n\n----------------------------------------\n\nTITLE: C# SDK Dockerfile with MATLAB R2023a Runtime\nDESCRIPTION: Dockerfile configuration for .NET SDK 6.0 with MATLAB R2023a Runtime integration. Sets up the necessary MATLAB dependencies and runtime environment on Ubuntu Jammy.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_12\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM mcr.microsoft.com/dotnet/sdk:6.0-jammy\n\n# MathWorks base dependencies\nENV DEBIAN_FRONTEND=\"noninteractive\" TZ=\"Etc/UTC\"\nCOPY --from=git /project/base-dependencies.txt /tmp/base-dependencies.txt\nRUN apt-get update && apt-get install --no-install-recommends -y `cat /tmp/base-dependencies.txt` \\\n    && apt-get clean && apt-get -y autoremove && rm -rf /var/lib/apt/lists/*\nRUN [ -d /usr/share/X11/xkb ] || mkdir -p /usr/share/X11/xkb\n...\n```\n\n----------------------------------------\n\nTITLE: Creating Stream using Curl\nDESCRIPTION: Creates a new stream using a POST request to the Quix Platform REST API with minimal configuration.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/create-stream.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl \"https://${domain}.platform.quix.io/topics/${topicName}/streams\" \\\n        -X POST \\\n        -H \"Authorization: bearer ${token}\" \\\n        -H \"Content-Type: application/json\" \\\n        -d '{}'\n```\n\n----------------------------------------\n\nTITLE: Installing PyAthena and Pandas Dependencies in Shell\nDESCRIPTION: Installs the necessary Python libraries, PyAthena and Pandas, using pip. These libraries are required to connect to AWS Athena for querying data and to handle the results as Pandas DataFrames.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-connectors/templates/awss3icebergsink-destination.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install pyathena pandas\n```\n\n----------------------------------------\n\nTITLE: Fetching Stream by ID using cURL\nDESCRIPTION: Example of how to fetch a specific stream using its ID through the Quix Platform API. The request accepts an array of stream IDs to match multiple streams.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-filtered.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"https://${domain}.platform.quix.io/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"streamIds\": [\"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\"]}'\n```\n\n----------------------------------------\n\nTITLE: Sending Parameter Data using SignalR\nDESCRIPTION: Example of sending parameter data using SignalR WebSocket connection with multiple parameter types\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/send-data.md#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst environmentId = \"YOUR_ENVIRONMENT_ID\"\nconst topic = \"YOUR_TOPIC_NAME\"\nconst streamId = \"ID_OF_STREAM_TO_WRITE_TO\"\n\nconst options = {\n    accessTokenFactory: () => token\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://writer-\" + environmentId + \".platform.quix.io/hub\", options)\n    .build();\n\n// Establish connection\nconnection.start().then(async () => {\n    console.log(\"Connected to Quix.\");\n\n    // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model.\n    let parameterData = {\n      \"epoch\": Date.now() * 1000000, // set now as time starting point, in nanoseconds\n      \"timestamps\": [\n        0,\n        5000000000, // 5 seconds from now (see epoch)\n        8000000000\n      ],\n      \"numericValues\": {\n        \"NumericParameter1\": [\n          13.37,\n          42,\n          24.72\n        ]\n      },\n      \"stringValues\": {\n        \"StringParameter1\": [\n          \"Hello\",\n          \"World\",\n          \"!\"\n        ]\n      },\n      \"tagValues\": {\n        \"Tag1\": [\n          \"A\",\n          \"B\",\n          null\n        ]\n      }\n    }\n\n    // Send stream update details\n    console.log(\"Sending parameter data\");\n    await connection.invoke(\"SendParameterData\", topic, streamId, parameterData);\n    console.log(\"Sent parameter data\");\n});\n```\n\n----------------------------------------\n\nTITLE: Checking Java Daemon Processes – Python (Shell)\nDESCRIPTION: Checks for running Java-based Kafka or Zookeeper processes via the process status command and prints any matches, which helps verify that the services are active and not accidentally filtering out the grep command. No required parameters. Output: process lines printed to stdout.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!ps aux | grep -E '[j]ava'\n```\n\n----------------------------------------\n\nTITLE: Calculating Running Total for Banking Transactions in Python\nDESCRIPTION: This snippet demonstrates how to calculate a running total of a user's transactions over time to display their current balance in a banking app. It iterates through a list of transactions, updating the balance with each transaction.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/navigating-stateful-stream-processing.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntransactions = [100, -50, 200, -100]  # Positive values are deposits, negative values are withdrawals\ncurrent_balance = 150 # Initial balance\n\n# Iterate over each transaction\nfor transaction in transactions:\n    current_balance += transaction  # Update the balance with each transaction\n    print(f\"Current balance: {current_balance}\")  # Print the balance after each transaction\n```\n\n----------------------------------------\n\nTITLE: Setting Streamlit Server Port via sys.argv in Python\nDESCRIPTION: This Python code snippet demonstrates modifying the `sys.argv` list within a `main.py` script to configure the Streamlit server. It effectively passes command-line arguments to Streamlit, specifically setting the `--server.port` to `80`. This ensures the Streamlit application runs on the correct port required for public deployment within the Quix platform.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/deploy-public-page.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nsys.argv = [\"streamlit\", \"run\", \"streamlit_file.py\", \"--server.port=80\"]\n```\n\n----------------------------------------\n\nTITLE: Error Traceback for Incorrect Quix Streams Application Initialization\nDESCRIPTION: Displays the Python traceback resulting from initializing Quix Streams `Application` incorrectly, as shown in the previous snippet. The error `ValueError: Cannot provide both broker address and Quix SDK Token` occurs because the library receives both a positional argument interpreted as the broker address and an implicitly provided SDK token.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/manage/troubleshooting.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nTraceback (most recent call last):\n  File \"/app/main.py\", line 77, in <module>\n    app = Application(\n          ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/quixstreams/app.py\", line 202, in __init__\n    raise ValueError(\"Cannot provide both broker address and Quix SDK Token\")\nValueError: Cannot provide both broker address and Quix SDK Token\nsys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=3, family=2, type=1, proto=6, laddr=('10.0.2.227', 59714), raddr=('34.149.137.116', 443)>\n\n[ - Logs stream closed - ]\n```\n\n----------------------------------------\n\nTITLE: Python Dockerfile with MATLAB R2023a Runtime\nDESCRIPTION: Dockerfile that sets up Python 3.10.11 environment with MATLAB R2023a Runtime on Ubuntu 22.04. Includes Python installation from source, pip setup, and necessary MATLAB dependencies.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_11\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM ubuntu:22.04\nENV PYTHONUNBUFFERED=1\nENV PYTHONIOENCODING=UTF-8\n\n# Python 3.10\nENV PATH /usr/local/bin:$PATH\nENV LANG C.UTF-8\nRUN set -eux; \\\n    apt-get update; \\\n    apt-get install -y --no-install-recommends \\\n        ca-certificates \\\n        netbase \\\n        tzdata \\\n    ; \\\n    rm -rf /var/lib/apt/lists/*\n...\n```\n\n----------------------------------------\n\nTITLE: Curl Authentication Example\nDESCRIPTION: Demonstrates how to set the Authorization header in a curl command using the -H flag.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/portal-api/http-requests.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -H \"Authorization: bearer <token>\" ...\n```\n\n----------------------------------------\n\nTITLE: Quix API Endpoint URL Format\nDESCRIPTION: Shows the standard format for Quix API endpoint URLs, which include the API name and environment ID in the domain structure.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/http-requests.md#2025-04-23_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nhttps://<api-name>-<environment-id>.platform.quix.io/<action>\n```\n\n----------------------------------------\n\nTITLE: Sample Response for Aggregated Speed by LapNumber in JSON\nDESCRIPTION: This JSON snippet demonstrates a sample response when aggregating Speed by LapNumber using the mean aggregation type. It shows the average speed recorded for each lap.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/aggregate-tags.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamps\": [\n    1612191286000000000,\n    1612191286000000000\n  ],\n  \"numericValues\": {\n    \"mean(Speed)\": [\n      213.36765142704252,\n      173.77710595934278\n    ]\n  },\n  \"stringValues\": {},\n  \"tagValues\": {\n    \"LapNumber\": [\n      \"3.0\",\n      \"4.0\"\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Tag Filtering with Multiple Values in JSON\nDESCRIPTION: Example of filtering parameter data using an array of values. This filter will return data where LapNumber equals either 2.0 or 4.0.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/filter-tags.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"tagFilters\": [{\n        \"tag\": \"LapNumber\",\n        \"operator\": \"Equal\",\n        \"value\": [ \"2.0\", \"4.0\" ]\n    }]\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Bike Prediction Output in Quix Console (Shell)\nDESCRIPTION: This snippet shows an example line of output displayed in the Quix console when the New York Bikes prediction service is running successfully. It indicates the current number of bikes and provides forecasts for the next hour and the next day. This output confirms the model is processing data and generating predictions.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/data-science/5-run.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nCurrent n bikes: 23742 Forecast 1h: 23663 Forecast 1 day: 22831\n```\n\n----------------------------------------\n\nTITLE: Exchanging Aggregated Vehicle Count Data - Event Payload - JSON\nDESCRIPTION: This JSON sample represents a message payload exchanged between processing stages, summarizing detected vehicles and related data. Key properties include epoch timestamp, per-category object counts, coordinates (lat/lon), and binary images. The 'vehicles' key aggregates counts for car, bus, truck, and motorbike. Consumers expect numeric, string, binary, and tag values grouped by type. Used for inter-service messaging and archiving.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/other-services.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"Epoch\\\": 0,\\n  \\\"Timestamps\\\": [\\n    1694077540745375700\\n  ],\\n  \\\"NumericValues\\\": {\\n    \\\"truck\\\": [\\n      1\\n    ],\\n    \\\"car\\\": [\\n      2\\n    ],\\n    \\\"lat\\\": [\\n      51.4075\\n    ],\\n    \\\"lon\\\": [\\n      -0.19236\\n    ],\\n    \\\"delta\\\": [\\n      -2.177236557006836\\n    ],\\n    \\\"vehicles\\\": [\\n      3\\n    ]\\n  },\\n  \\\"StringValues\\\": {},\\n  \\\"BinaryValues\\\": {\\n    \\\"image\\\": [\\n      \\\"(Binary of 157.97 KB)\\\"\\n    ]\\n  },\\n  \\\"TagValues\\\": {}\\n}\n```\n\n----------------------------------------\n\nTITLE: SignalR CDN Integration\nDESCRIPTION: HTML code showing how to include SignalR via CDN in a web page\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/setup.md#2025-04-23_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <title>Hello SignalR</title>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/microsoft-signalr/6.0.1/signalr.js\"></script>\n  </head>\n  ...\n</html>\n```\n\n----------------------------------------\n\nTITLE: Curl Data Payload Examples\nDESCRIPTION: Shows two methods of sending JSON data in curl requests: inline JSON and file-based JSON.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/http-requests.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -d '{\"key\": \"value\"}' ...\ncurl -d \"@data.json\" ...\n```\n\n----------------------------------------\n\nTITLE: Pagination Parameters JSON for Stream Requests\nDESCRIPTION: JSON structure for the paging object used to paginate stream results. Includes index (page number) and length (items per page) parameters.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-paged.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"paging\": {\n    \"index\": 1,\n    \"length\": 10\n}\n```\n\n----------------------------------------\n\nTITLE: Fetching Paginated Streams Using curl in Quix Platform\nDESCRIPTION: A curl command that retrieves a specific page of streams by sending a JSON object with pagination parameters. This example fetches the second page with 10 items per page.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-paged.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl \"https://${domain}.platform.quix.io/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"paging\":{\"index\": 1,\"length\": 10}}'\n```\n\n----------------------------------------\n\nTITLE: Summarized Output Message Format Example - JSON\nDESCRIPTION: This JSON snippet is a condensed version of the published event message, showing the structure as it appears in the Quix message browser. It contains a timestamp, tags, an event ID, and a value containing camera data. This format helps developers quickly recognize the expected structural pattern of outgoing events.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/tfl-camera-feed.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"Timestamp\": 1693925495304353500,\n        \"Tags\": {},\n        \"Id\": \"camera\",\n        \"Value\": \"<data>\"\n    }\n]\n\n```\n\n----------------------------------------\n\nTITLE: C# Runtime Dockerfile with MATLAB R2023a Runtime\nDESCRIPTION: Dockerfile setup for .NET Runtime 6.0 with MATLAB R2023a Runtime integration. Configures the runtime environment with required MATLAB dependencies on Ubuntu Jammy.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_13\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM mcr.microsoft.com/dotnet/runtime:6.0-jammy\n\n# MathWorks base dependencies\nENV DEBIAN_FRONTEND=\"noninteractive\" TZ=\"Etc/UTC\"\nCOPY --from=git /project/base-dependencies.txt /tmp/base-dependencies.txt\nRUN apt-get update && apt-get install --no-install-recommends -y `cat /tmp/base-dependencies.txt` \\\n    && apt-get clean && apt-get -y autoremove && rm -rf /var/lib/apt/lists/*\nRUN [ -d /usr/share/X11/xkb ] || mkdir -p /usr/share/X11/xkb\n...\n```\n\n----------------------------------------\n\nTITLE: Implementing CSV File Reading Function in Python\nDESCRIPTION: This function reads data from a CSV file in an endless loop, yielding rows at regular intervals. It uses pandas to read the CSV and adds a timestamp to each row before yielding.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/data-enrichment-with-quix-and-redis.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef read_csv_file(file_path: str):\n\t\"\"\"\n\tA function to read data from a CSV file in an endless manner.\n\tIt returns a generator with stream_id and rows\n\t\"\"\"\n\n\t# Read the CSV file into a pandas.DataFrame\n       print(\"CSV file loading.\")\n\tdf = pd.read_csv(file_path)\n\tprint(\"File loaded.\")\n\n\t# Get the number of rows in the dataFrame for printing out later\n\trow_count = len(df)\n\n\t# Generate a unique ID for this data stream.\n\t# It will be used as a message key in Kafka\n\tstream_id = f\"CSV_DATA_{str(random.randint(1, 100)).zfill(3)}\"\n\n\t# Get the column headers as a list\n\theaders = df.columns.tolist()\n\n\t# Continuously loop over the data\n\twhile True:\n    \t\t# Print a message to the console for each iteration\n    \t\tprint(f\"Publishing {row_count} rows.\")\n\n    \t\t# Iterate over the rows and convert them to\n    \t\tfor _, row in df.iterrows():\n        \t\t# Create a dictionary that includes both column headers and row values\n        \t\trow_data = {header: row[header] for header in headers}\n\n        \t\t# add a new timestamp column with the current data and time\n        \t\trow_data[\"Timestamp\"] = time.time_ns()\n\n        \t\t# Yield the stream ID and the row data\n        \t\tyield stream_id, row_data\n       \t \n        \t\t# Wait a moment before outputting more data.\n        \t\ttime.sleep(0.5)\n\n    \t\tprint(\"All rows published\")\n```\n\n----------------------------------------\n\nTITLE: Compiling Simulink Bootstrap Function for .NET Deployment\nDESCRIPTION: Uses MATLAB Compiler to export 'engine.m' as a .NET assembly, targeting the .NET 6.0 framework with required APIs. Outputs to the 'cs' directory for .NET deployment scenarios. Prerequisites include MATLAB Compiler SDK and compatible .NET environment.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_7\n\nLANGUAGE: cs\nCODE:\n```\nmcc -W 'dotnet:quixmatlab,api=matlab-data,framework_version=6.0' engine.m -d cs\n```\n\n----------------------------------------\n\nTITLE: Running the Python Consumer Script using Shell\nDESCRIPTION: This shell command executes the Python script named `consumer.py` using the `python3` interpreter. Running this command starts the Quix Streams application defined within the script, which will then connect to Kafka and begin consuming and printing messages from the 'cpu-load' topic.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/get-started/consume.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython3 consumer.py\n```\n\n----------------------------------------\n\nTITLE: SignalR Reader API Connection\nDESCRIPTION: JavaScript code showing how to establish a SignalR connection to the Reader API using authentication token.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/setup.md#2025-04-23_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\n\nconst options = {\n    accessTokenFactory: () => 'YOUR_PAT'\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://reader-YOUR_ENVIRONMNENT_ID.platform.quix.io/hub\", options)\n    .build();\n\nconnection.start().then(() => console.log(\"SignalR connected.\"));\n```\n\n----------------------------------------\n\nTITLE: Curl Content-Type Header Example\nDESCRIPTION: Demonstrates setting the Content-Type header for JSON payloads in curl requests.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/portal-api/http-requests.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -H \"Content-Type: application/json\" ...\n```\n\n----------------------------------------\n\nTITLE: Setting Authorization Header with Bearer Token\nDESCRIPTION: Shows how to set the Authorization header using a Personal Access Token (PAT) as a bearer token for API authentication.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/http-requests.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nAuthorization: bearer <token>\n```\n\n----------------------------------------\n\nTITLE: Adding Stream Metadata using Node.js HTTPS Module\nDESCRIPTION: This code snippet shows how to send a PUT request to add metadata to a stream using the Node.js HTTPS module. It constructs the request options and payload.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/stream-metadata.md#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst https = require('https');\n\nconst data = JSON.stringify({ metadata: { fruit: \"apple\" }});\n\nconst options = {\n    hostname: domain + '.platform.quix.io',\n    path: '/topics/' + topicName + '/streams/' + streamId,\n    method: 'PUT',\n    headers: {\n        'Authorization': 'Bearer ' + token,\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options);\n\nreq.write(data);\nreq.end();\n```\n\n----------------------------------------\n\nTITLE: Example CPU Overload Alert Message in JSON\nDESCRIPTION: This JSON object represents an example message produced by the CPU threshold detection transform. It includes fields like 'summary', 'source', 'severity', and 'custom_details' which contains the timestamp and the specific CPU load value that triggered the alert. This format is sent to the output topic by the Quix Streams application.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/influxdb-alerting/threshold-detection.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"summary\": \"CPU overload\",\n  \"source\": \"custom_event\",\n  \"severity\": \"critical\",\n  \"custom_details\": {\n    \"timestamp\": 1710947291392758000,\n    \"message\": \"CPU value is 25.1\"\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple YAML Variables in a String\nDESCRIPTION: This snippet demonstrates how to concatenate multiple YAML variables (`{{URL_PREFIX}}` and `{{ENV_NAME}}`) along with static text within a single string value for the `urlPrefix` setting in `quix.yaml`. This is useful for creating environment-specific identifiers.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/yaml-variables.md#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\npublicAccess:\n  enabled: true\n  urlPrefix: {{URL_PREFIX}}-{{ENV_NAME}}\n```\n\n----------------------------------------\n\nTITLE: Plotting Shifted vs Unshifted Brake Signals using Matplotlib\nDESCRIPTION: Uses Matplotlib to generate a plot comparing the original boolean brake signal ('Brake_bool') with the time-shifted signal ('Brake_shifted_5s'). This visualization helps confirm that the 5-second shift has been applied correctly.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/train-ml.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(15, 8))\nplt.plot(df[\"Brake_shifted_5s\"])\nplt.plot(df[\"Brake_bool\"])\nplt.legend(['Shifted', 'Unshifted'])\n```\n\n----------------------------------------\n\nTITLE: Setting Authorization Header with Bearer Token using Curl\nDESCRIPTION: Demonstrates how to set the Authorization header with a Personal Access Token (PAT) when making HTTP requests using Curl command line tool.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/http-requests.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -H \"Authorization: bearer <token>\" ...\n```\n\n----------------------------------------\n\nTITLE: Curl Data Payload Examples\nDESCRIPTION: Shows two methods of sending JSON data in curl requests: inline JSON and file reference.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/portal-api/http-requests.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -d '{\"key\": \"value\"}' ...\ncurl -d \"@data.json\" ...\n```\n\n----------------------------------------\n\nTITLE: Compiling Simulink Bootstrap Function for Python Deployment\nDESCRIPTION: Runs 'mcc' to compile 'engine.m' for Python integration. Output is a Python package (in 'py' directory) suitable for use within Quix or standalone environments. Dependencies include MATLAB Compiler and correct Simulink project setup.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmcc -W python:quixmatlab engine.m -d py\n```\n\n----------------------------------------\n\nTITLE: Setting HTTP Method with Curl\nDESCRIPTION: Shows how to specify the HTTP method (PUT in this example) when making requests with Curl.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/http-requests.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X PUT ...\n```\n\n----------------------------------------\n\nTITLE: Sending Event Data with Node.js HTTP Request\nDESCRIPTION: Node.js code that uses the https module to send an event to a stream. The request includes the same event data structure as the curl example, with an ID, timestamp, tags, and value.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/send-event.md#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst https = require('https');\n\nconst data = JSON.stringify({\n    \"id\": \"Alert\",\n    \"timestamp\": 1618133869000000000,\n    \"tags\": {\n        \"capacity\": \"over\"\n    },\n    \"value\": \"Help\"\n});\n\nconst options = {\n    hostname: domain + '.platform.quix.io',\n    path: '/topics/' + topicName + '/streams/' + streamId + '/events/data',\n    method: 'POST',\n    headers: {\n        'Authorization': 'Bearer ' + token,\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options);\n\nreq.write(data);\nreq.end();\n```\n\n----------------------------------------\n\nTITLE: Example Quix Streaming Writer API Endpoint URL\nDESCRIPTION: Provides a concrete example of a Quix streaming writer API endpoint URL with a sample environment ID (acme-weather).\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/http-requests.md#2025-04-23_snippet_3\n\nLANGUAGE: http\nCODE:\n```\nhttps://writer-acme-weather.platform.quix.io/<action>\n```\n\n----------------------------------------\n\nTITLE: Enriching Clickstream Data Format - JSON\nDESCRIPTION: This snippet presents the structure of the enriched clickstream data after it has been processed by the enrichment service. It demonstrates how numerical and string fields (such as epoch, timestamps, user information, and enriched attributes like age, category, and gender) are organized in the output. This format is expected to be published to the downstream `enriched-click-data` topic. Consumers need to handle nested objects and expect some fields (like age and gender) to be arrays.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/data-enrichment.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"Epoch\\\": 0,\\n  \\\"Timestamps\\\": [\\n    1700147866722586000\\n  ],\\n  \\\"NumericValues\\\": {\\n    \\\"original_timestamp\\\": [\\n      1331800486\\n    ],\\n    \\\"age\\\": [\\n      42\\n    ]\\n  },\\n  \\\"StringValues\\\": {\\n    \\\"userId\\\": [\\n      \\\"8D0E437E-9249-4DDA-BC4F-C1E5409E3A3B\\\"\\n    ],\\n    \\\"ip\\\": [\\n      \\\"69.76.12.213\\\"\\n    ],\\n    \\\"userAgent\\\": [\\n      \\\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:10.0.2) Gecko/20100101 Firefox/10.0.2\\\"\\n    ],\\n    \\\"productId\\\": [\\n      \\\"VD55177927\\\"\\n    ],\\n    \\\"category\\\": [\\n      \\\"clothing\\\"\\n    ],\\n    \\\"title\\\": [\\n      \\\"Deluxe Cashmere Sweater\\\"\\n    ],\\n    \\\"birthdate\\\": [\\n      \\\"1981-11-14\\\"\\n    ],\\n    \\\"country\\\": [\\n      \\\"United States\\\"\\n    ],\\n    \\\"deviceType\\\": [\\n      \\\"Desktop\\\"\\n    ],\\n    \\\"gender\\\": [\\n      \\\"F\\\"\\n    ]\\n  },\\n  \\\"TagValues\\\": {}\\n}\n```\n\n----------------------------------------\n\nTITLE: Conditionally Sending Alerts Based on Status - Python\nDESCRIPTION: This Python comment illustrates the core logic to be implemented within the alert filter transformation service. The service monitors incoming alert messages and, when an alert has a 'status' field equal to 'over-forecast', it forwards (publishes) the alert to the output topic. The key dependency is the Quix SDK (or similar) for event handling. The developer must implement this logic in an event handler function that processes incoming JSON alerts.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/predictive-maintenance/phone-alerts.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# if status == over-forecast then send alert\n```\n\n----------------------------------------\n\nTITLE: Complete Curl Request Example\nDESCRIPTION: Provides a complete example of a curl request including authorization, content type, and payload.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/http-requests.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d \"@data.json\" \\\n     https://${api-name}-${environment-id}.platform.quix.io/<action>\n```\n\n----------------------------------------\n\nTITLE: Authentication Header Format for Quix API\nDESCRIPTION: Shows the required format for the Authorization header using a Personal Access Token (PAT) as a bearer token.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/portal-api/http-requests.md#2025-04-23_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nAuthorization: bearer <token>\n```\n\n----------------------------------------\n\nTITLE: Compiling MATLAB Function for Python Deployment\nDESCRIPTION: Illustrates the command to compile a MATLAB .m file ('rot.m') into a Python package using MATLAB Compiler. This produces deployable Python bindings in a specified output directory (here, 'py'). Requires MATLAB Compiler and proper licensing.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmcc -W python:quixmatlab rot.m -d py\n```\n\n----------------------------------------\n\nTITLE: Setting Content-Type Header for JSON Payload with Curl\nDESCRIPTION: Demonstrates how to set the Content-Type header to application/json when sending JSON data in HTTP requests with Curl.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/http-requests.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -H \"Content-Type: application/json\" ...\n```\n\n----------------------------------------\n\nTITLE: Example Quix API Endpoint URL\nDESCRIPTION: Provides a concrete example of a Quix API endpoint URL for the streaming writer API.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/portal-api/http-requests.md#2025-04-23_snippet_3\n\nLANGUAGE: http\nCODE:\n```\nhttps://writer-acme-weather.platform.quix.io/<action>\n```\n\n----------------------------------------\n\nTITLE: Filtering Over-forecast Alerts - JSON Example - json\nDESCRIPTION: This JSON snippet demonstrates the expected format for an over-temperature forecast alert, providing context for the filter transformation service. It is used as a schema reference while setting up the message filter to ensure only alerts with 'status' set to 'over-forecast' and 'parameter_name' as 'ambient_temperature' trigger push notifications. The fields include status, parameter_name, alert_temperature, timestamp, and a descriptive message. The service consuming these messages expects them in this exact structure.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/predictive-maintenance/phone-alerts.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\\n  \\\"status\\\": \\\"over-forecast\\\",\\n  \\\"parameter_name\\\": \\\"ambient_temperature\\\",\\n  \\\"alert_temperature\\\": 72.82717847141305,\\n  \\\"timestamp\\\": \\\"2024-04-16 10:42:40\\\",\\n  \\\"message\\\": \\\"'Ambient temperature' is forecasted to go over 73\\u00baC at 2024-04-16 10:42:40.\\\"\\n}\n```\n\n----------------------------------------\n\nTITLE: Making HTTP POST Request to Send Event Data with curl\nDESCRIPTION: A curl command that sends a POST request to add a single event to a stream. The event includes an ID, timestamp, tags, and a value. This requires replacing variables for domain, topic name, stream ID, and authentication token.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/send-event.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -i \"https://${domain}.platform.quix.io/topics/${topicName}/streams/${streamId}/events/data\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '[{\n             \"id\": \"Alert\",\n             \"timestamp\": 1618133869000000000,\n             \"tags\": {\n                 \"capacity\": \"over\"\n             },\n             \"value\": \"Help\"\n     }]'\n```\n\n----------------------------------------\n\nTITLE: SignalR CDN Integration\nDESCRIPTION: HTML code showing how to include SignalR library via CDN in a web page.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/setup.md#2025-04-23_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <title>Hello SignalR</title>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/microsoft-signalr/6.0.1/signalr.js\"></script>\n  </head>\n  ...\n</html>\n```\n\n----------------------------------------\n\nTITLE: Starting Kafka and Zookeeper Daemons – Python (Shell)\nDESCRIPTION: Runs Kafka and Zookeeper services in the background using provided configuration files, then waits for services to be available. Dependencies: working Kafka binary, configuration files. Main parameters: paths to configuration. Output: background processes running Kafka/Zookeeper, log printed for wait.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!./kafka_2.13-3.6.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.6.1/config/zookeeper.properties\\n!./kafka_2.13-3.6.1/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.6.1/config/server.properties\\n!echo \\\"Waiting for 10 secs until kafka and zookeeper services are up and running\\\"\\n!sleep 10\n```\n\n----------------------------------------\n\nTITLE: Complete Curl Example for Quix API Requests\nDESCRIPTION: A comprehensive example showing the recommended pattern for structuring HTTP requests to Quix APIs using Curl, including authorization, content type, payload, and endpoint URL.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/http-requests.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d \"@data.json\" \\\n     https://${api-name}-${environment-id}.platform.quix.io/<action>\n```\n\n----------------------------------------\n\nTITLE: Sample Response for Grouped Speed Readings by LapNumber in JSON\nDESCRIPTION: This JSON snippet shows a sample response when grouping Speed readings by LapNumber. It includes timestamps, numeric values for Speed, and tag values for LapNumber.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/aggregate-tags.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"timestamps\": [\n        1612191286000000000,\n        1612191287000000000,\n        ...\n    ],\n    \"numericValues\": {\n        \"Speed\": [\n            307.8333333333333,\n            313.8421052631579,\n            ...\n        ]\n    },\n    \"tagValues\": {\n        \"LapNumber\": [\n            \"3.0\",\n            \"4.0\",\n            ...\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Stream Metadata using SignalR in JavaScript\nDESCRIPTION: This snippet demonstrates how to use SignalR to update stream metadata in Quix. It establishes a connection, prepares the stream details, and invokes the UpdateStream method.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/stream-metadata.md#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst environmentId = \"YOUR_ENVIRONMENT_ID\"\nconst topic = \"YOUR_TOPIC_NAME\"\nconst streamId = \"ID_OF_STREAM_TO_UPDATE\"\n\nconst options = {\n    accessTokenFactory: () => token\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://writer-\" + environmentId + \".platform.quix.io/hub\", options)\n    .build();\n\n// Establish connection\nconnection.start().then(async () => {\n    console.log(\"Connected to Quix.\");\n\n    // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model.\n    let streamDetails = {\n        \"name\": \"Example stream\",\n        \"location\": \"/sub/dir\",\n        \"metadata\": {\n            \"fruit\": \"apple\"\n        }\n    }\n\n    // Send stream update details\n    console.log(\"Updating stream\");\n    await connection.invoke(\"UpdateStream\", topic, streamDetails);\n    console.log(\"Updated stream\");\n});\n```\n\n----------------------------------------\n\nTITLE: Quix API Endpoint URL Format\nDESCRIPTION: Shows the general format for Quix API endpoint URLs including environment ID and API name.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/portal-api/http-requests.md#2025-04-23_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nhttps://<api-name>-<environment-id>.platform.quix.io/<action>\n```\n\n----------------------------------------\n\nTITLE: Detailed Stream Payload Example for ActiveStreamsChanged Event\nDESCRIPTION: This JSON snippet provides a more detailed example of a stream payload returned by the ActiveStreamsChanged event. It includes additional metadata, parameter details, and event information.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/subscriptions.md#2025-04-23_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"streamId\": \"5ecfc7ce-906c-4d3a-811c-a85ea75a24b3\",\n  \"topicName\": \"f1-data\",\n  \"name\": \"F1 Game - Swal - Sakhir Short 2022-04-08-09:00:39\",\n  \"location\": \"/Game/Codemasters/F1-2019/Sakhir Short\",\n  \"metadata\": {\n    \"GameVersion\": \"1.22\",\n    \"PacketFormat\": \"2019\",\n    \"Track\": \"Sakhir Short\",\n    \"SessionId\": \"237322236454500810\",\n    \"Player0_Name\": \"Swal\",\n    \"Player0_Id\": \"100\"\n  },\n  \"parents\": [],\n  \"timeOfRecording\": \"2022-04-08T09:00:39.3971666Z\",\n  \"parameters\": {\n    \"EngineRPM\": {\n      \"dataType\": \"Numeric\",\n      \"minimumValue\": 0,\n      \"maximumValue\": 20000,\n      \"location\": \"/Player/Telemetry/Engine\"\n    },\n    \"LapDistance\": {\n      \"dataType\": \"Numeric\",\n      \"unit\": \"m\",\n      \"location\": \"/Player/Telemetry/Misc\"\n    },\n    \"Brake\": {\n      \"description\": \"Amount of brake applied\",\n      \"dataType\": \"Numeric\",\n      \"minimumValue\": 0,\n      \"maximumValue\": 1,\n      \"location\": \"/Player/Telemetry/Input\"\n    },\n    \"Throttle\": {\n      \"dataType\": \"Unknown\",\n      \"minimumValue\": 0,\n      \"maximumValue\": 1,\n      \"location\": \"/Player/Telemetry/Input\"\n    },\n    \"Gear\": {\n      \"dataType\": \"Numeric\",\n      \"minimumValue\": -1,\n      \"maximumValue\": 8,\n      \"location\": \"/Player/Telemetry/Engine\"\n    },\n    \"Speed\": {\n      \"dataType\": \"Numeric\",\n      \"minimumValue\": 0,\n      \"maximumValue\": 400,\n      \"location\": \"/Player/Telemetry/Engine\"\n    },\n    \"Steer\": {\n      \"dataType\": \"Numeric\",\n      \"minimumValue\": -1,\n      \"maximumValue\": 1,\n      \"location\": \"/Player/Telemetry/Input\"\n    }\n  },\n  \"events\": {\n    \"Player_NewLap\": {\n      \"name\": \"Player NewLap\",\n      \"level\": \"Information\",\n      \"location\": \"\"\n    },\n    \"Player_Position_Changed\": {\n      \"name\": \"Player Position Changed\",\n      \"level\": \"Critical\",\n      \"location\": \"\"\n    },\n    \"RaceWinner\": {\n      \"name\": \"Race Winner\",\n      \"level\": \"Critical\",\n      \"location\": \"\"\n    }\n  },\n  \"firstSeen\": \"2022-04-08T08:57:40.3406586Z\",\n  \"lastSeen\": \"2022-04-08T09:00:39.6308255Z\",\n  \"status\": \"Receiving\",\n  \"lastData\": \"2022-04-08T09:00:39.6237312Z\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Python Dependencies for Event Detection Service\nDESCRIPTION: Specifies the Python packages (`urllib3` for downloading the model, `xgboost` version 0.90 for the ML model, and `scikit-learn` likely as a dependency for XGBoost or related tasks) required for the event detection service. These lines should be added to the `requirements.txt` file to ensure the packages are installed during the build process.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/event-detection/crash-detection.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nurllib3\nxgboost==0.90\nscikit-learn \n```\n\n----------------------------------------\n\nTITLE: Requesting Numeric Parameters in Quix API\nDESCRIPTION: Example of the minimal JSON request body required to fetch numeric parameters from the Quix Platform. This example shows how to request a single numeric parameter named 'Speed'.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/raw-data.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"numericParameters\": [{\n        \"parameterName\": \"Speed\"\n    }]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Response Format for Parameter Data\nDESCRIPTION: Sample JSON response showing the structure of returned parameter data, including timestamps and parameter values organized by type.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/raw-data.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"timestamps\": [\n        1612191100000000000\n    ],\n    \"numericValues\": {\n        \"Speed\": [\n            104.22222222222224\n        ]\n    },\n    \"stringValues\": {},\n    \"tagValues\": {}\n}\n```\n\n----------------------------------------\n\nTITLE: Including Shared Folders via includedFolders in Quix (YAML)\nDESCRIPTION: This app.yaml snippet extends the base configuration by demonstrating how to include extra directories, such as custom-modules, using the includedFolders property. It enables bundling shared code or modules with the Quix application during build and deployment. This approach requires the specified directories to exist in the project root and ensures their availability at runtime in various environments (CLI, IDE, deployment).\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/shared-folders.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: sample\\nlanguage: python\\nvariables: []\\ndockerfile: dockerfile\\nrunEntryPoint: main.py\\ndefaultFile: main.py\\nincludedFolders:\\n  - custom-modules\n```\n\n----------------------------------------\n\nTITLE: Curl HTTP Method Example\nDESCRIPTION: Shows how to specify the HTTP method in a curl command using the -X flag.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/portal-api/http-requests.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X PUT ...\n```\n\n----------------------------------------\n\nTITLE: Curl Authorization Header Example\nDESCRIPTION: Demonstrates setting the Authorization header in a curl command using the -H flag.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/http-requests.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -H \"Authorization: bearer <token>\" ...\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Model Handling in Python\nDESCRIPTION: Imports the `pickle` library, used for serializing and de-serializing Python objects (specifically, loading the pre-trained ML model), and the `request` module from `urllib`, used to download the model file from a URL. These imports are added to the `main.py` script.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/event-detection/crash-detection.md#2025-04-23_snippet_3\n\nLANGUAGE: py\nCODE:\n```\nimport pickle\nfrom urllib import request\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output for Detected Crash Event\nDESCRIPTION: This JSON object represents an example event message published by the service when a crash is detected. It includes a nanosecond timestamp (`Timestamp`), empty tags (`Tags`), an event identifier (`Id`: \"crash\"), and the event value (`Value`: \"Crash detected.\"). This format is generated by the `stream_producer.events.add_timestamp_nanoseconds().add_value().publish()` calls in the Python script.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/event-detection/crash-detection.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[{\n\t\"Timestamp\": 1674137774598972000,\n\t\"Tags\": {},\n\t\"Id\": \"crash\",\n\t\"Value\": \"Crash detected.\"\n}]\n```\n\n----------------------------------------\n\nTITLE: Grouping Speed Readings by LapNumber in JSON\nDESCRIPTION: This JSON snippet demonstrates how to group Speed readings by LapNumber using the groupBy property. It specifies a time range and a numeric parameter to aggregate.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/aggregate-tags.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"from\": 1612191286000000000,\n    \"to\":   1612191386000000000,\n    \"numericParameters\": [{\n        \"parameterName\": \"Speed\"\n    }],\n    \"groupBy\": [ \"LapNumber\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running the Python Client from the Command Line\nDESCRIPTION: This command is used to execute the 'cpu_load.py' Python client from the terminal. It assumes that all dependencies are installed and the working directory contains the code and '.env' file for configuration. The command will start publishing CPU load data to the specified Quix topic.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/influxdb-alerting/python-client.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 cpu_load.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Dash/Plotly Host and Port in Python for Quix Deployment\nDESCRIPTION: This Python code snippet shows how to start a Dash/Plotly application server, configuring it to listen on host `0.0.0.0` (all available network interfaces) and port `80`. This setup is required for public services deployed on Quix, ensuring they are accessible externally instead of being limited to `localhost`. The `debug=False` argument is typically used for production deployments.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/deploy-public-page.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\napp.run_server(debug=False, host=\"0.0.0.0\", port=80)\n```\n\n----------------------------------------\n\nTITLE: Example Response: Stream with Nested Hierarchy\nDESCRIPTION: This JSON response demonstrates a more complex hierarchy with a stream that has a child and a grandchild. It shows how the nested children array allows representation of multi-level relationships in the stream model.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-models.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"children\": [{\n        \"children\": [{\n            \"children\": [],\n            \"streamId\": \"79bbed17-5c71-4b0e-99f6-3596577b46d8\",\n            \"name\": \"new-child\",\n            \"topic\": \"cars\",\n            \"createdAt\": \"2021-04-08T15:27:09.19Z\",\n            \"lastUpdate\": \"2021-04-13T10:30:11.495Z\",\n            \"status\": \"Open\",\n            \"metadata\": {},\n            \"parents\": [\n                \"f003c1dd-9abe-49dd-afd2-f194d3d96035\"\n            ],\n            \"location\": \"/\"\n          }\n        ],\n        \"streamId\": \"f003c1dd-9abe-49dd-afd2-f194d3d96035\",\n        \"name\": \"example1\",\n        \"topic\": \"cars\",\n        \"createdAt\": \"2021-04-12T11:50:38.504Z\",\n        \"lastUpdate\": \"2021-04-12T12:00:40.482Z\",\n        \"status\": \"Interrupted\",\n        \"metadata\": {\n            \"rain\": \"light\"\n        },\n        \"parents\": [\n            \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\"\n        ],\n        \"location\": \"/examples/first/\"\n      }\n    ],\n    \"streamId\": \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\",\n    \"topic\": \"cars\",\n    \"createdAt\": \"2021-04-08T14:12:29.807Z\",\n    \"lastUpdate\": \"2021-04-12T13:45:08.377Z\",\n    \"timeOfRecording\": \"2021-04-12T00:00:00Z\",\n    \"dataStart\": 0,\n    \"dataEnd\": 1618233869000000000,\n    \"status\": \"Interrupted\",\n    \"metadata\": {},\n    \"parents\": [],\n    \"location\": \"/\"\n}]\n```\n\n----------------------------------------\n\nTITLE: Grouping Speed Data by Time Intervals in JSON\nDESCRIPTION: This JSON payload demonstrates how to group speed data into 2-second intervals, aggregated by mean average. It uses the groupByTime property to specify the time bucket duration for aggregation.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/aggregate-time.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"groupByTime\": {\n        \"timeBucketDuration\": 2000000000,\n    },\n    \"numericParameters\": [{\n        \"parameterName\": \"Speed\",\n        \"aggregationType\": \"Mean\"\n    }]\n}\n```\n\n----------------------------------------\n\nTITLE: Extracted Environment ID in Markdown\nDESCRIPTION: This code block shows the extracted environment ID from the previous URL example. It demonstrates the format of a Quix environment ID, which combines organization, project, and environment names.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/portal-api/setup.md#2025-04-23_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\njoeengland-apitests-testing\n```\n\n----------------------------------------\n\nTITLE: Installing libgomp1 in Dockerfile for XGBoost Support\nDESCRIPTION: This shell command uses `apt-get` to install the `libgomp1` package within the Docker container image. This library is a prerequisite for the `xgboost` Python package used in the event detection service. This command should be added to the `dockerfile` after the `COPY --from=git /project .` line.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/event-detection/crash-detection.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nRUN apt-get install libgomp1\n```\n\n----------------------------------------\n\nTITLE: Event Data Received Payload Example\nDESCRIPTION: Example payload structure received when handling event data through SignalR connection, showing the format of event information including timestamp and tags.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/subscriptions.md#2025-04-23_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{\n    topicName: 'topic-1',\n    streamId: 'b45969d2-4624-4ab7-9779-c8f90ce79420'\n    id: 'EventA',\n    timestamp: 1591733990000000000,\n    value: 'val-a',\n    tags: {\n        tag1: 'val1'\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Interpolating Missing Values in Time-Series Data using JSON\nDESCRIPTION: This JSON payload shows how to specify interpolation for missing values when aggregating time-series data. It includes options for the time range, parameter aggregation, group by time settings, and stream IDs.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/aggregate-time.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"from\": 1612191286000000000,\n    \"to\":   1612191295000000000,\n    \"numericParameters\": [{\n        \"parameterName\": \"Speed\",\n        \"aggregationType\": \"First\"\n    }],\n    \"groupByTime\": {\n        \"timeBucketDuration\": 2000000000,\n        \"interpolationType\": \"None\"\n    },\n    \"streamIds\": [ \"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Quix Python Application Using dotenv\nDESCRIPTION: This code snippet demonstrates how to define the 'Quix__Sdk__Token' environment variable in a .env file using dotenv. This token is required for authenticating with the Quix Streams SDK within the Python client. The .env file should be placed in the project directory, and the token value should be replaced with your actual streaming token.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/influxdb-alerting/python-client.md#2025-04-23_snippet_1\n\nLANGUAGE: dotenv\nCODE:\n```\nQuix__Sdk__Token=\"<your SDK token>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Application Metadata in Quix (YAML)\nDESCRIPTION: This snippet provides a minimal default app.yaml configuration for a new Quix Python application. It specifies fundamental properties such as the application name, programming language, Dockerfile reference, entry points, and variables. The configuration serves as a starting point and is required for proper Quix deployment and development workflows.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/shared-folders.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: sample\\nlanguage: python\\nvariables: []\\ndockerfile: dockerfile\\nrunEntryPoint: main.py\\ndefaultFile: main.py\n```\n\n----------------------------------------\n\nTITLE: Filtering Data by Timestamp Range\nDESCRIPTION: JSON request body example demonstrating how to restrict data retrieval to a specific time range using nanosecond timestamps.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/raw-data.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"from\": 1612191286000000000,\n    \"to\":   1612191386000000000\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Quix Environment URL\nDESCRIPTION: Example URL structure for accessing a Quix environment. The URL contains the environment ID which is needed for API requests.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/setup.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://portal.platform.quix.io/pipeline?workspace=joeengland-apitests-testing\n```\n\n----------------------------------------\n\nTITLE: Sending Parameter Data using Node.js HTTPS\nDESCRIPTION: Example of sending parameter data using Node.js HTTPS module\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/send-data.md#2025-04-23_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst https = require('https');\n\nconst data = JSON.stringify({\n    \"timestamps\": [1591733989000000000, 1591733990000000000, 1591733991000000000],\n    \"numericValues\": {\n        \"SomeParameter1\": [10.01, 202.02, 303.03],\n        \"SomeParameter2\": [400.04, 50.05, 60.06]\n    }\n});\n\nconst options = {\n    hostname: domain + '.platform.quix.io',\n    path: '/topics/' + topicName + '/streams/' + streamId + '/parameters/data',\n    method: 'POST',\n    headers: {\n        'Authorization': 'Bearer ' + token,\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options);\n\nreq.write(data);\nreq.end();\n```\n\n----------------------------------------\n\nTITLE: Displaying Kafka Message Size Error\nDESCRIPTION: Shows the specific Kafka error message encountered when a produced message exceeds the configured broker size limit (e.g., 1MB). This error indicates the need for message size reduction techniques like compression.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/compressed-data.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nKafkaError{code=MSG_SIZE_TOO_LARGE,val=10,str=\"Unable to produce message: Broker: Message size too large\"}\n```\n\n----------------------------------------\n\nTITLE: GET /detected_objects Endpoint Example Response - Data API - JSON\nDESCRIPTION: This is a sample JSON array of camera detection data returned from the /detected_objects API. Each array item maps a camera to a dictionary of detected object counts and contextual data, such as location, timestamp, and other structured fields. Used in the UI for visualization and analytics. This payload omits binary images for efficiency.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/other-services.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[\\n  \\\"JamCams_00001.01419\\\": {\\n      \\\"car\\\":{\\\"0\\\":3.0},\\n      \\\"delta\\\":{\\\"0\\\":-1.0003459453582764},\\n      \\\"image\\\":{\\\"0\\\":\\\"\\\"},\\\"lat\\\":{\\\"0\\\":51.5596},\\n      \\\"lon\\\":{\\\"0\\\":-0.07424},\\\"person\\\":{\\\"0\\\":3.0},\\n      \\\"timestamp\\\":{\\\"0\\\":1692471825406959867},\\n      \\\"traffic light\\\":{\\\"0\\\":1.0},\\n      \\\"truck\\\":{\\\"0\\\":1.0}\\n  },\\n  ...\\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Input Topic for Slack Notification Connector\nDESCRIPTION: Specifies the input topic name `filtered-rss-data` for the `Slack Notification` connector. This topic contains the data processed and filtered by the `RSS Data Filtering` connector and serves as the input for the alert sending stage. This needs to be selected when configuring the Slack connector.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/rss-tutorial/rss-processing-pipeline.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nfiltered-rss-data\n```\n\n----------------------------------------\n\nTITLE: Hard-coded Resource Configuration in YAML\nDESCRIPTION: This snippet shows an example section within a `quix.yaml` file where resource limits (CPU, memory, replicas) are defined with hard-coded values. This is the configuration before introducing variables.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/yaml-variables.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nresources:\n  cpu: 200\n  memory: 500\n  replicas: 1\n```\n\n----------------------------------------\n\nTITLE: Injecting SDK Token into Quix Streams Application - Python\nDESCRIPTION: This snippet demonstrates how to instantiate a Quix Streams Application object by directly specifying the SDK token via the 'quix_sdk_token' parameter. This approach bypasses the automatic environment variable configuration and is useful for explicitly setting the authentication token in client code. Required prerequisites include having the 'Application' class available from the Quix Streams SDK, and a valid SDK token. The key parameter is 'quix_sdk_token', which expects a valid streaming token string. The input is the token string, and the output is an authenticated Application instance connected to the desired environment.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/manage/testing-environments.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nApplication(quix_sdk_token=\\\"sdk-49..a5\\\")\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for Slack Sink Destination Card\nDESCRIPTION: This HTML snippet defines a card element for the Slack Sink destination. It includes an icon, the destination name, a brief description (consuming data from a Kafka topic and sending Slack notifications based on matching criteria), and a link to the detailed documentation page.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-connectors/templates/index.md#2025-04-23_snippet_7\n\nLANGUAGE: html\nCODE:\n```\n<li>\n<div style='display:flex'>\n<img src='slacksink-destination-icon.png' style='max-width:40px;border-radius:8px;'>\n<p style='min-width: 100px;margin-top:7px'>\n<strong style='margin-left:9px;border-radius: 8px;'>Slack Sink</strong>\n</p>\n</div>\n<hr>\n<p>Consume data from a Kafka topic and send Slack notifications based on your matching criteria.</p>\n<p><a href='slacksink-destination.html'><span class='twemoji'><svg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'><path d='M13.22 19.03a.75.75 0 0 0 1.06 0l6.25-6.25a.75.75 0 0 0 0-1.06l-6.25-6.25a.75.75 0 1 0-1.06 1.06l4.97 4.97H3.75a.75.75 0 0 0 0 1.5h14.44l-4.97 4.97a.75.75 0 0 0 0 1.06z' fill-rule='evenodd'></path></svg></span> Slack Sink</a></p>\n</li>\n```\n\n----------------------------------------\n\nTITLE: Example Environment ID URL Format\nDESCRIPTION: Example URL format showing the structure of a Quix environment ID\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/setup.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://portal.platform.quix.io/pipeline?workspace=joeengland-apitests-testing\n```\n\n----------------------------------------\n\nTITLE: SignalR Reader API Connection\nDESCRIPTION: JavaScript code demonstrating how to establish a SignalR connection to the Reader API using PAT authentication\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/setup.md#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\n\nconst options = {\n    accessTokenFactory: () => 'YOUR_PAT'\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://reader-YOUR_ENVIRONMNENT_ID.platform.quix.io/hub\", options)\n    .build();\n\nconnection.start().then(() => console.log(\"SignalR connected.\"));\n```\n\n----------------------------------------\n\nTITLE: Illustrating 'no-alert' JSON Message Format\nDESCRIPTION: Shows the JSON message format generated by the Alert service when a monitored parameter ('hotend_temperature' in this example) is within its defined thresholds. The message includes a 'no-alert' status, parameter name, descriptive message, timestamp, and the current temperature reading.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/predictive-maintenance/alert-service.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"no-alert\",\n  \"parameter_name\": \"hotend_temperature\",\n  \"message\": \"'Hotend temperature' is within normal parameters\",\n  \"timestamp\": 1701280033000000000,\n  \"alert_temperature\": 246.04148121958596\n}\n```\n\n----------------------------------------\n\nTITLE: GET /max_vehicles Endpoint Example Response - Data API - JSON\nDESCRIPTION: This JSON structure shows a sample response from the Data API's /max_vehicles endpoint, mapping each camera name to its corresponding maximum vehicle count (float). Used by the UI to display the maximum traffic for each camera over a time window. No dependencies; response is consumed directly from HTTP API calls. Output maps camera keys to count values.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/other-services.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\\n    \\\"JamCams_00001.01251\\\":2.0,\\n    \\\"JamCams_00001.01252\\\":1.0\\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring RSS Feed URL for Data Source Connector\nDESCRIPTION: Specifies the URL for the StackOverflow RSS feed tagged 'python'. This URL is used as the input `rss_url` parameter when configuring the `RSS Data Source` connector in Quix to fetch data.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/rss-tutorial/rss-processing-pipeline.md#2025-04-23_snippet_0\n\nLANGUAGE: url\nCODE:\n```\nhttps://stackoverflow.com/feeds/tag/python\n```\n\n----------------------------------------\n\nTITLE: Incorrect Initialization of Quix Streams Application (Python)\nDESCRIPTION: Shows an example of initializing the Quix Streams `Application` class (version 2.x) with the consumer group as the first positional argument. This method is incorrect in newer versions when running in the Quix environment where an SDK token is automatically provided, leading to a `ValueError` because the first argument is interpreted as the broker address.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/manage/troubleshooting.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\napp = Application(\n    \"myconsumergroup\",\n    auto_offset_reset=\"earliest\",\n    auto_create_topics=True,  # Quix app has an option to auto create topics\n)\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for Redis Sink Destination Card\nDESCRIPTION: This HTML snippet defines a card element for the Redis Sink destination. It includes an icon, the destination name, a brief description (consuming data from a Kafka topic and persisting it to Redis), and a link to the detailed documentation page.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-connectors/templates/index.md#2025-04-23_snippet_6\n\nLANGUAGE: html\nCODE:\n```\n<li>\n<div style='display:flex'>\n<img src='redissink-destination-icon.png' style='max-width:40px;border-radius:8px;'>\n<p style='min-width: 100px;margin-top:7px'>\n<strong style='margin-left:9px;border-radius: 8px;'>Redis Sink</strong>\n</p>\n</div>\n<hr>\n<p>Consume data from a Kafka topic and persist it to Redis.</p>\n<p><a href='redissink-destination.html'><span class='twemoji'><svg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'><path d='M13.22 19.03a.75.75 0 0 0 1.06 0l6.25-6.25a.75.75 0 0 0 0-1.06l-6.25-6.25a.75.75 0 1 0-1.06 1.06l4.97 4.97H3.75a.75.75 0 0 0 0 1.5h14.44l-4.97 4.97a.75.75 0 0 0 0 1.06z' fill-rule='evenodd'></path></svg></span> Redis Sink</a></p>\n</li>\n```\n\n----------------------------------------\n\nTITLE: Sending JSON Payload with Curl\nDESCRIPTION: Shows two methods of sending JSON data with Curl: inline JSON string or referencing a JSON file with the @ symbol.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/http-requests.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -d '{\"key\": \"value\"}' ...\ncurl -d \"@data.json\" ...\n```\n\n----------------------------------------\n\nTITLE: Illustrating 'under-now' JSON Alert Message Format\nDESCRIPTION: Displays the JSON structure for an 'under-now' alert, triggered when a monitored parameter ('bed_temperature') drops below its minimum threshold (105ºC). The message includes the 'under-now' status, parameter name, timestamp, the temperature that triggered the alert, and a descriptive message indicating the threshold violation.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/predictive-maintenance/alert-service.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"status\": \"under-now\",\n  \"parameter_name\": \"bed_temperature\",\n  \"timestamp\": 1701273328000000000,\n  \"alert_temperature\": 104.0852349596566,\n  \"message\": \"'Bed temperature' is under the threshold (105ºC)\"\n}\n```\n\n----------------------------------------\n\nTITLE: Structure of Published Camera Data Message - JSON\nDESCRIPTION: This JSON snippet defines the array-based structure of camera data messages published to the Quix Streams output topic. Each object contains a nanosecond-level timestamp, an event ID (e.g., 'camera'), optional tags, and a 'Value' payload referencing the camera data. This structure is used for transmitting camera event data to downstream services like the frame grabber. Inputs are produced by the publisher code, and outputs include all downstream consumers of the topic.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/tfl-camera-feed.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"Timestamp\": 1693925495304353500,\n    \"Tags\": {},\n    \"Id\": \"camera\",\n    \"Value\": \"<data>\"\n  }\n]\n\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for Confluent Kafka Sink Destination Card\nDESCRIPTION: This HTML snippet defines a card element for the Confluent Kafka Sink destination. It includes an icon, the destination name, a brief description (consuming data from a Quix Kafka topic and publishing to a Confluent Cloud topic), and a link to the detailed documentation page.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-connectors/templates/index.md#2025-04-23_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<li>\n<div style='display:flex'>\n<img src='confluentkafkasink-destination-icon.png' style='max-width:40px;border-radius:8px;'>\n<p style='min-width: 100px;margin-top:7px'>\n<strong style='margin-left:9px;border-radius: 8px;'>Confluent Kafka Sink</strong>\n</p>\n</div>\n<hr>\n<p>Consume data from a Kafka topic in Quix and publish it to a topic in Confluent Cloud</p>\n<p><a href='confluentkafkasink-destination.html'><span class='twemoji'><svg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'><path d='M13.22 19.03a.75.75 0 0 0 1.06 0l6.25-6.25a.75.75 0 0 0 0-1.06l-6.25-6.25a.75.75 0 1 0-1.06 1.06l4.97 4.97H3.75a.75.75 0 0 0 0 1.5h14.44l-4.97 4.97a.75.75 0 0 0 0 1.06z' fill-rule='evenodd'></path></svg></span> Confluent Kafka Sink</a></p>\n</li>\n```\n\n----------------------------------------\n\nTITLE: Running the Kafka Producer Script - Shell\nDESCRIPTION: This shell command runs the previously defined Python producer script called 'producer.py' using Python 3. It requires the 'producer.py' file to exist in the working directory, all necessary dependencies installed, and executes the script so it can begin publishing messages to the Kafka topic.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/get-started/produce.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython3 producer.py\n```\n\n----------------------------------------\n\nTITLE: Running Consumer Script from CLI in Bash\nDESCRIPTION: This command line snippet demonstrates how to start the Quix Streams consumer application, which will consume telemetry from the broker and write to DuckDB. The 'consumer.py' script should be in your project root directory.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython consumer.py\n\n```\n\n----------------------------------------\n\nTITLE: Downloading Large Files for Tutorials in Python\nDESCRIPTION: Python code example demonstrating how to download large files at runtime instead of checking them into GitHub. This snippet shows how to use urllib to retrieve a model file from cloud storage and save it locally for use in tutorials.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/CONTRIBUTING.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# pip install or add urllib3 to requirements.txt\nfrom urllib import request\nf = request.urlopen(\"https://quixtutorials.blob.core.windows.net/tutorials/event-detection/XGB_model.pkl\")\nwith open(\"model_file.pkl\", \"wb\") as model_file:\n    model_file.write(f.read())\n```\n\n----------------------------------------\n\nTITLE: Cloning the Quix Docs Repository with Git in Bash\nDESCRIPTION: Commands to clone the Quix documentation repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/RUNNING-DOCS-LOCALLY.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/quixio/quix-docs.git\ncd quix-docs\n```\n\n----------------------------------------\n\nTITLE: Incorrect Multiple Tag Filters Example in JSON\nDESCRIPTION: Example of an incorrect filter configuration that applies multiple contradictory conditions to the same tag. This would not return any results as a LapNumber cannot simultaneously equal both 2.0 and 4.0.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/filter-tags.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"tagFilters\": [{\n        \"tag\": \"LapNumber\",\n        \"operator\": \"Equal\",\n        \"value\": \"2.0\"\n    },{\n        \"tag\": \"LapNumber\",\n        \"operator\": \"Equal\",\n        \"value\": \"4.0\"\n    }]\n}\n```\n\n----------------------------------------\n\nTITLE: Stream Creation Response Example\nDESCRIPTION: Example JSON response from the stream creation API containing the generated stream ID.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/create-stream.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"streamId\": \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining User Data Structure in JSON\nDESCRIPTION: This JSON snippet illustrates how individual user records should be represented, containing keys for a unique UUID ('userId'), gender, and birth date. These attributes enable demographic targeting for offers and form the basis of user metadata processed by the application. All fields are expected as strings, except for birthday, which follows ISO 8601 format.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/data-ingestion.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\\n    \"userId\":\"0001BDD9-EABF-4D0D-81BD-D9EABFCD0D7D\",\\n    \"gender\":\"F\",\\n    \"birthDate\":\"1984-04-08\"\\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Slack Webhook URL Parameter Name\nDESCRIPTION: Identifies the configuration parameter name `webhook_url` required for the `Slack Notification` connector. Users must provide a valid Slack Incoming Webhook URL corresponding to this parameter name to enable sending alerts to the specified Slack channel.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/rss-tutorial/rss-processing-pipeline.md#2025-04-23_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nwebhook_url\n```\n\n----------------------------------------\n\nTITLE: Displaying Bug Fixes Section in Markdown Changelog\nDESCRIPTION: Markdown formatting used to highlight the 'BUG FIXES' section in the changelog entry for December 11, 2023. This uses backticks to style the section heading.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/changelogs/2023-archive.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n`BUG FIXES`\n```\n\n----------------------------------------\n\nTITLE: Launching Jupyter Notebook via Shell Command\nDESCRIPTION: This shell command starts the Jupyter Notebook server from a command-line interface, such as an Anaconda Powershell Prompt or a standard terminal. Executing this command launches the Jupyter Notebook application, typically opening the user interface in the default web browser, allowing users to create, open, and manage their notebooks. This command requires Jupyter Notebook to be installed in the active Python environment.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/jupyter-nb.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\njupyter notebook\n```\n\n----------------------------------------\n\nTITLE: Checking Docker Service Health in Bash\nDESCRIPTION: Commands to check the status of Docker services and view service logs for diagnosing issues with the documentation server.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/RUNNING-DOCS-LOCALLY.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# Check service status\ndocker compose ps\n# View service logs\ndocker compose logs\n```\n\n----------------------------------------\n\nTITLE: Example Quix Portal URL with Environment ID in Markdown\nDESCRIPTION: This code snippet shows an example URL for accessing a specific environment in the Quix portal. It demonstrates how the environment ID is included as a query parameter in the URL.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/portal-api/setup.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nhttps://portal.platform.quix.io/home?workspace={environment-id}\n```\n\n----------------------------------------\n\nTITLE: Ordering Stream Results using cURL\nDESCRIPTION: Shows how to order stream results using the ordering property, sorting streams by topic in ascending order.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-filtered.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"https://${domain}.platform.quix.io/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"ordering\": [{ \"by\": \"topic\", \"direction\": \"asc\" }]}'\n```\n\n----------------------------------------\n\nTITLE: Viewing 3D Printer Data Message Format in JSON\nDESCRIPTION: This JSON object illustrates the structure of messages published by the Data Generator service to the 'json-3d-printer-data' topic. It includes temperature readings (hotend, bed, ambient, fluctuated ambient), timestamps, and a printer identifier. This format can be viewed using the Quix topic explorer's 'Messages' view.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/predictive-maintenance/data-generator.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"hotend_temperature\": 249.52922614294954,\n  \"bed_temperature\": 110.12854118355098,\n  \"ambient_temperature\": 38.70099292962708,\n  \"fluctuated_ambient_temperature\": 38.70099292962708,\n  \"timestamp\": \"2024-04-16T17:07:03.717628\",\n  \"original_timestamp\": \"2024-04-16T17:07:03.717628\",\n  \"printer\": \"Printer 33\"\n}\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for Kafka Connect Sink Destination Card\nDESCRIPTION: This HTML snippet defines a card element for the Kafka Connect Sink destination. It includes an icon, the destination name, a brief description (using a Kafka Connect sink template within the Quix platform), and a link to the detailed documentation page.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-connectors/templates/index.md#2025-04-23_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<li>\n<div style='display:flex'>\n<img src='kafkaconnectsink-destination-icon.png' style='max-width:40px;border-radius:8px;'>\n<p style='min-width: 100px;margin-top:7px'>\n<strong style='margin-left:9px;border-radius: 8px;'>Kafka Connect Sink</strong>\n</p>\n</div>\n<hr>\n<p>Use a Kafka Connect sink template in the Quix platform</p>\n<p><a href='kafkaconnectsink-destination.html'><span class='twemoji'><svg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'><path d='M13.22 19.03a.75.75 0 0 0 1.06 0l6.25-6.25a.75.75 0 0 0 0-1.06l-6.25-6.25a.75.75 0 1 0-1.06 1.06l4.97 4.97H3.75a.75.75 0 0 0 0 1.5h14.44l-4.97 4.97a.75.75 0 0 0 0 1.06z' fill-rule='evenodd'></path></svg></span> Kafka Connect Sink</a></p>\n</li>\n```\n\n----------------------------------------\n\nTITLE: Python ModuleNotFoundError for quixstreams\nDESCRIPTION: Illustrates a `ModuleNotFoundError` for the `quixstreams` package. This Python error can occur if the package is not installed in the environment or if an incompatible Python version (outside the required >=3.6 and <4 range) is being used.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/manage/troubleshooting.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nModuleNotFoundError: No module named 'quixstreams'\n```\n\n----------------------------------------\n\nTITLE: Initializing Pandas DataFrame in RocksDB for Eight Hour Aggregation in Python\nDESCRIPTION: This snippet initializes the storage for an eight-hour aggregation in RocksDB. It creates an empty Pandas DataFrame with columns 'datetime', 'userId', and 'count', and assigns it to the key 'eight_hours_aggregation' within the RocksDB instance (`db`). This DataFrame will be used to accumulate session counts over an eight-hour window.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/data-aggregation.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndb[\"eight_hours_aggregation\"] = pd.DataFrame(columns=[\"datetime\", \"userId\", \"count\"])\n```\n\n----------------------------------------\n\nTITLE: Sample Output for PackageReceived Event\nDESCRIPTION: This snippet shows an example of the output produced by the PackageReceived event handler. It includes package details such as topicId, streamId, type, and a value containing telemetry data.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/subscriptions.md#2025-04-23_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n{\n  topicId: 'joeengland-apitests-testing-f1-data',\n  topicName: 'f1-data',\n  streamId: '020aee7e-edba-4913-aee3-b1e493c78132',\n  type: 'QuixStreams.Telemetry.Models.TimeseriesDataRaw',\n  value: '{\"Epoch\":0,\"Timestamps\":[1697020374684026880,1697020374737754880],\"NumericValues\":{\"original_timestamp\":[1.6871805934271606E+18,1.6871805934782164E+18],\"Motion_WorldPositionZ\":[-47.103004455566406,-51.53349304199219],\"Motion_WorldPositionY\":[91.16168212890624,91.1617202758789],\"Motion_WorldPositionX\":[-386.4772338867188,-386.242431640625],\"TotalLapDistance\":[7836.7548828125,7841.19189453125],\"Steer\":[0.0,0.0],\"Speed\":[312.0,312.0],\"LapDistance\":[184.69873046875,189.1357421875],\"Gear\":[8.0,8.0],\"EngineTemp\":[90.0,90.0],\"EngineRPM\":[11143.0,11153.0],\"Brake\":[0.0,0.0]},\"StringValues\":{},\"BinaryValues\":{},\"TagValues\":{\"DriverStatus\":[\"Flying_lap\",\"Flying_lap\"],\"LapNumber\":[\"3\",\"3\"],\"LapValidity\":[\"Valid\",\"Valid\"],\"PitStatus\":[\"None\",\"None\"],\"Sector\":[\"0\",\"0\"],\"streamId\":[\"5a517ca4-efc3-4166-aedb-a5c57e2b9c59\",\"5a517ca4-efc3-4166-aedb-a5c57e2b9c59\"]}}',\n  dateTime: '2023-10-11T10:32:54'\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Streams by Metadata Example\nDESCRIPTION: JSON example showing how to structure metadata filters to match streams with specific metadata values.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-filtered.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"metadata\": [{\n    \"key\": \"circuit\",\n    \"value\": \"Sakhir Short\"\n}]\n```\n\n----------------------------------------\n\nTITLE: Example JSON Message Structure in Quix Topic\nDESCRIPTION: This JSON object represents an example message format as seen in the Quix `phone-data` topic when data is being streamed. It includes an Epoch, Timestamps array, NumericValues containing sensor readings like location and speed, StringValues for categorical data like battery status, and TagValues which might contain metadata tags (empty here). The exact fields present can vary between messages.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/event-detection/data-acquisition.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"Epoch\": 0,\n\"Timestamps\": [\n    1673966764000000000\n],\n\"NumericValues\": {\n    \"Longitude\": [\n    0.52202169\n    ],\n    \"Latitude\": [\n    51.73491702\n    ],\n    \"Speed\": [\n    98.0639991760254\n    ],\n    \"Heading\": [\n    347.5\n    ],\n    \"BatteryLevel\": [\n    0.27\n    ],\n    \"Altitude\": [\n    59.63983154296875\n    ],\n    \"Accuracy\": [\n    3.790092468261719\n    ]\n},\n\"StringValues\": {\n    \"EnergySaverStatus\": [\n    \"Off\"\n    ],\n    \"BatteryState\": [\n    \"Discharging\"\n    ],\n    \"BatteryPowerSource\": [\n    \"Battery\"\n    ]\n},\n\"TagValues\": {}\n}\n```\n\n----------------------------------------\n\nTITLE: Representing Project Directory Structure in Python Formatted Tree\nDESCRIPTION: This code block displays the folder structure for the DuckDB tutorial repository, illustrating where to place your sink implementation and support files. It includes subfolders for completed code, sink modules, and relevant scripts. This visual guide is for organizational purposes and is not executable code.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nduck-db-tutorial/\n├── completed_code/\n├── sinks/\n│   ├── __init__.py\n│   └── duckdbsink.py\n├── .env\n├── consumer.py\n├── producer.py\n├── csgo_telemetry_sample.jsonl\n\n```\n\n----------------------------------------\n\nTITLE: Presenting InfluxDB Terminology Mapping Table - Markdown\nDESCRIPTION: This snippet shows a Markdown table mapping InfluxDB concepts to general database terminology. It is intended to clarify domain-specific language for users transitioning between database platforms. No code dependencies are required; this table is informational only, with expected input/output being a rendered table within Markdown documentation.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/influxdb-migration/influxdb-source.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| InfluxDB | General database |\\n|---|---|\\n| Bucket | Database |\\n| Measurement | Table |\\n| Point | Row |\\n| Field | Column |\\n| Tag | Metadata |\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Kafka Distribution – Python (Shell)\nDESCRIPTION: Downloads and extracts the Apache Kafka binary distribution using standard shell commands. Dependencies: curl, tar utilities, internet access. No parameters. Output: Kafka archive and extracted folder available in the working directory.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!curl -sSOL https://dlcdn.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz\\n!tar -xzf kafka_2.13-3.6.1.tgz\n```\n\n----------------------------------------\n\nTITLE: API Endpoint URL Format\nDESCRIPTION: Shows the structure of Quix API endpoint URLs which include the API name and environment ID.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/http-requests.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nhttps://<api-name>-<environment-id>.platform.quix.io/<action>\n```\n\n----------------------------------------\n\nTITLE: Offering Special Female Demographic Promo via JSON Data Output\nDESCRIPTION: This JSON snippet shows the structure of an offer intended for the female demographic. The object includes a Timestamp, empty Tags object, offer identifier, and a value (\"offer2\") that signals the tailored campaign. It is formatted for direct integration with presentation layers and relies on precise event tracking upstream. Inputs are the detected user events and demographic data; output is a formatted JSON payload for the application layer. The primary dependency is on accurate event and user data.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/event-detection.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"Timestamp\": 1700222075422209000,\n    \"Tags\": {},\n    \"Id\": \"offer\",\n    \"Value\": \"offer2\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Running Producer Script from CLI in Bash\nDESCRIPTION: This snippet demonstrates how to invoke the Python producer script from the command line to start sending telemetry events to the message broker. It assumes the presence of 'producer.py' in the working directory. No parameters are passed to the script by default.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython producer.py\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Second Vector Consumer for Qdrant Update in Python\nDESCRIPTION: Sets up a Kafka consumer application to process the second batch of vector embeddings. The consumer is configured to stop after processing the designated number of messages based on the offset limit.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ninputtopicname = vectors_topic_name\n\n# Create a special stop condition just for this Notebook (otherwise the cell will run indefinitely)\nprint(f\"Using offset limit {offsetlimit}\")\ndef on_message_processed(topic, partition, offset):\n    if offset > offsetlimit:\n        app.stop()\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for MQTT Sink Destination Card\nDESCRIPTION: This HTML snippet defines a card element for the MQTT Sink destination. It includes an icon, the destination name, a brief description (consuming data from a Kafka topic and publishing it to an MQTT broker), and a link to the detailed documentation page.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-connectors/templates/index.md#2025-04-23_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<li>\n<div style='display:flex'>\n<img src='mqttsink-destination-icon.png' style='max-width:40px;border-radius:8px;'>\n<p style='min-width: 100px;margin-top:7px'>\n<strong style='margin-left:9px;border-radius: 8px;'>MQTT Sink</strong>\n</p>\n</div>\n<hr>\n<p>Consume data from a Kafka topic and publish it to an MQTT broker</p>\n<p><a href='mqttsink-destination.html'><span class='twemoji'><svg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'><path d='M13.22 19.03a.75.75 0 0 0 1.06 0l6.25-6.25a.75.75 0 0 0 0-1.06l-6.25-6.25a.75.75 0 1 0-1.06 1.06l4.97 4.97H3.75a.75.75 0 0 0 0 1.5h14.44l-4.97 4.97a.75.75 0 0 0 0 1.06z' fill-rule='evenodd'></path></svg></span> MQTT Sink</a></p>\n</li>\n```\n\n----------------------------------------\n\nTITLE: Example JSON Payload for the Quix Web App\nDESCRIPTION: This JSON object represents sample data that can be sent to the `/data` endpoint of the Python Flask web app. It includes fields like `id`, `temp` (temperature), and `press` (pressure), demonstrating a typical structure for sensor or device data. This data format is used in the accompanying Curl example.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/web-app.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"device-012-ABC\",\n  \"temp\": 123,\n  \"press\": 456\n}\n```\n\n----------------------------------------\n\nTITLE: Logging State Machine Transitions and Offers to Redis Cloud (Log Format)\nDESCRIPTION: This log excerpt illustrates the standard output from the state machine used to track user journey and offer triggers. Entries follow a time-stamped, human-readable structure, denoting user identifiers, transitions between states, triggering events, categories involved, and offer activations. These logs are designed for storage in Redis Cloud and visualization in a monitoring UI, requiring integration with Redis and a logging facility. Each line represents either a transition state or triggered offer, consuming state updates internally and producing structured text lines.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/event-detection.md#2025-04-23_snippet_2\n\nLANGUAGE: log\nCODE:\n```\n2023-11-17 11:54:35: [User sad-music-boxing triggered offer offer2]\n2023-11-17 11:54:35: [User sad-music-boxing entered state offer][Event: clicked VD55177927][Category: clothing]\n2023-11-17 11:54:32: [User sad-music-boxing entered state shoes_visited][Event: clicked VD55147564][Category: shoes]\n2023-11-17 11:54:29: [User sad-music-boxing entered state clothes_visited][Event: clicked VD55129406][Category: clothing]\n```\n\n----------------------------------------\n\nTITLE: Specifying Aggregation Type for Speed Parameter in JSON\nDESCRIPTION: This JSON snippet shows how to specify an aggregation type (mean) for the Speed parameter when requesting data. This is used to define how the parameter values should be aggregated when grouping.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/aggregate-tags.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"numericParameters\": [{\n    \"parameterName\": \"Speed\",\n    \"aggregationType\": \"mean\"\n}]\n```\n\n----------------------------------------\n\nTITLE: Sample Output for TopicMetricsUpdated Event\nDESCRIPTION: This snippet shows an example of the output produced by the TopicMetricsUpdated event handler. It includes metrics such as timestamp, topicId, topicName, bytesPerSecond, and activeStreams.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/subscriptions.md#2025-04-23_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n{\n  timestamp: '2023-10-11T10:22:14.7787333Z',\n  topicId: 'joeengland-apitests-testing-f1-data',\n  topicName: 'f1-data',\n  bytesPerSecond: 8282,\n  activeStreams: 1\n}\n```\n\n----------------------------------------\n\nTITLE: MATLAB Base Dependencies List\nDESCRIPTION: List of required system packages and dependencies for running MATLAB Runtime in Docker containers.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nca-certificates libasound2 libc6 libcairo2 libcairo-gobject2 libcap2 libcrypt1 libcrypt-dev libcups2 libdrm2 libdw1 libgbm1 libgdk-pixbuf2.0-0 libgl1 libglib2.0-0 libgomp1 libgstreamer1.0-0 libgstreamer-plugins-base1.0-0 libgtk-3-0 libice6 libnspr4 libnss3 libodbc1 libpam0g libpango-1.0-0 libpangocairo-1.0-0 libpangoft2-1.0-0 libsndfile1 libsystemd0 libuuid1 libwayland-client0 libxcomposite1 libxcursor1 libxdamage1 libxfixes3 libxft2 libxinerama1 libxrandr2 libxt6 libxtst6 libxxf86vm1 linux-libc-dev locales locales-all make net-tools odbcinst1debian2 procps sudo unzip wget zlib1g\n```\n\n----------------------------------------\n\nTITLE: Identifying Python Script for Custom Slack Notification Logic\nDESCRIPTION: Identifies the Python script `quix_function.py` within the `Slack Notification` connector, which needs modification. The tutorial instructs replacing the default content of this file with custom Python code (from a linked GitHub repository) to refine Slack alert formatting based on specific fields from the incoming data stream.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/rss-tutorial/rss-processing-pipeline.md#2025-04-23_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nquix_function.py\n```\n\n----------------------------------------\n\nTITLE: Kafka Error Message for Message Size Too Large\nDESCRIPTION: Displays the error message (`KafkaError{code=MSG_SIZE_TOO_LARGE,...}`) encountered when attempting to produce a Kafka message that exceeds the broker's maximum size limit, which is typically 1MB in Quix. Handling compressed messages might be necessary.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/manage/troubleshooting.md#2025-04-23_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nKafkaError{code=MSG_SIZE_TOO_LARGE,val=10,str=\"Unable to produce message: Broker: Message size too large\"}\n```\n\n----------------------------------------\n\nTITLE: Environment ID URL Example\nDESCRIPTION: Example URL format showing the structure of an environment ID in Quix platform.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/setup.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://portal.platform.quix.io/pipeline?workspace=joeengland-apitests-testing\n```\n\n----------------------------------------\n\nTITLE: Example API Endpoint URL\nDESCRIPTION: Provides a concrete example of a Quix API endpoint URL for the streaming writer API.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/http-requests.md#2025-04-23_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nhttps://writer-acme-weather.platform.quix.io/<action>\n```\n\n----------------------------------------\n\nTITLE: Starting the Documentation Server with Docker Compose in Bash\nDESCRIPTION: Command to build and start the documentation server using Docker Compose while running the browser script to open the documentation.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/RUNNING-DOCS-LOCALLY.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up --build & ./open-browser.sh\n```\n\n----------------------------------------\n\nTITLE: Making the Browser Script Executable in Bash\nDESCRIPTION: Command to make the open-browser.sh script executable, which is used to automatically open the documentation in a browser.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/RUNNING-DOCS-LOCALLY.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nchmod +x open-browser.sh\n```\n\n----------------------------------------\n\nTITLE: Curl Content-Type Header Example\nDESCRIPTION: Demonstrates setting the Content-Type header for JSON payloads in a curl command.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/http-requests.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -H \"Content-Type: application/json\" ...\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for Websocket Destination Card\nDESCRIPTION: This HTML snippet defines a card element for the Websocket Destination. It includes the destination name (no icon specified in this snippet), a brief description (sending data from Kafka to a client connected via websocket), and a link to the detailed documentation page.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-connectors/templates/index.md#2025-04-23_snippet_8\n\nLANGUAGE: html\nCODE:\n```\n<li>\n<div style='display:flex'>\n<p style='min-width: 100px;margin-top:7px'>\n<strong style='margin-left:9px;border-radius: 8px;'>Websocket Destination</strong>\n</p>\n</div>\n<hr>\n<p>Send data from Kafka to a client connected to this websocket server</p>\n<p><a href='websocketdestination-destination.html'><span class='twemoji'><svg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'><path d='M13.22 19.03a.75.75 0 0 0 1.06 0l6.25-6.25a.75.75 0 0 0 0-1.06l-6.25-6.25a.75.75 0 1 0-1.06 1.06l4.97 4.97H3.75a.75.75 0 0 0 0 1.5h14.44l-4.97 4.97a.75.75 0 0 0 0 1.06z' fill-rule='evenodd'></path></svg></span> Websocket Destination</a></p>\n</li>\n```\n\n----------------------------------------\n\nTITLE: Stream Response Object Example\nDESCRIPTION: Example of a stream object returned by the API, showing the structure and available properties including metadata.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-filtered.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[{\n    \"streamId\":\"e6545c18-d20d-47bd-8997-f3f825c1a45c\",\n    \"name\":\"cardata\",\n    \"topic\":\"cardata\",\n    \"createdAt\":\"2021-03-31T13:04:43.368Z\",\n    \"lastUpdate\":\"2021-03-31T13:04:44.53Z\",\n    \"dataStart\":1612191099000000000,\n    \"dataEnd\":1612191371000000000,\n    \"status\":\"Closed\",\n    \"metadata\":{\n        \"circuit\":\"Sakhir Short\",\n        \"player\":\"Swal\",\n        \"game\":\"Codemasters F1 2019\"\n    },\n    \"parents\":[],\n    \"location\":\"/static data/\"\n}]\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Docker Network Issues in Bash\nDESCRIPTION: Commands to check Docker network configurations and inspect the network specifically for the Quix documentation service.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/RUNNING-DOCS-LOCALLY.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# Check Docker network\ndocker network ls\n# Inspect network configuration\ndocker network inspect quix-docs_default\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Port Conflicts in Bash\nDESCRIPTION: Commands to check if port 8000 is already in use for both Unix/Linux and Windows systems.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/RUNNING-DOCS-LOCALLY.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Check if port 8000 is in use\nlsof -i :8000\n# Or on Windows:\nnetstat -ano | findstr :8000\n```\n\n----------------------------------------\n\nTITLE: Displaying Connector Logos using HTML\nDESCRIPTION: This HTML snippet uses `div` and `img` tags to display logos for Kafka and AWS WAF, visually connected by an arrow image. It employs CSS classes like `connect-images`, `cards`, and `blog-grid-card` likely for styling and layout purposes within a documentation or blog page.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/connect/kafka-to-aws-waf.md#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"connect-images cards blog-grid-card\" markdown>\n<div>\n<img src=\"../images/kafka_logo.png\" width=\"40px\" />\n</div>\n<div>\n<img src=\"../images/arrow.svg\" width=\"40px\" />\n</div>\n<div>\n<img src=\"./images/aws-waf_1.jpg\" />\n</div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Listing ACLs for Kafka Topic\nDESCRIPTION: This command lists the Access Control Lists (ACLs) configured for a specific Kafka topic. It helps in verifying access permissions when troubleshooting authorization-related 'unknown topic or partition' errors.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/how-to-fix-unknown-partition-error-kafka.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./bin/kafka-acls.sh --authorizer-properties --bootstrap-server localhost:9092 --list --topic sampletopic\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Docker Permission Issues in Bash\nDESCRIPTION: Commands to ensure Docker has proper user permissions by adding the current user to the docker group.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/RUNNING-DOCS-LOCALLY.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Ensure Docker has proper permissions\nsudo usermod -aG docker $USER\n# Log out and back in for changes to take effect\n```\n\n----------------------------------------\n\nTITLE: Environment ID Value Example\nDESCRIPTION: Example of an extracted environment ID from a Quix platform URL.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/setup.md#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\njoeengland-apitests-testing\n```\n\n----------------------------------------\n\nTITLE: Curl HTTP Method Example\nDESCRIPTION: Shows how to specify the HTTP method in a curl command using the -X flag.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/http-requests.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X PUT ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Port Exposure in YAML for Quix Deployments\nDESCRIPTION: This YAML snippet demonstrates how to expose ports internally in a Quix pipeline deployment. It specifies a service name and maps an internal port to an external port.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/CHANGELOG.md#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndeployments:\n- name: Demo Data\n    ...\n    network:\n    serviceName: MyServiceInternalName\n    ports:\n        - port: 8080\n        targetPort: 4000\n```\n\n----------------------------------------\n\nTITLE: Extracting Environment ID from URL\nDESCRIPTION: Example of an environment ID extracted from a Quix environment URL. The environment ID is a combination of organization name, project name, and environment name.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/setup.md#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\njoeengland-apitests-testing\n```\n\n----------------------------------------\n\nTITLE: Correct Initialization of Quix Streams Application using Keyword Argument (Python)\nDESCRIPTION: Shows the corrected method for initializing the Quix Streams `Application` class. By explicitly using the `consumer_group` keyword argument instead of relying on positional arguments, the `ValueError` related to providing both broker address and SDK token is avoided.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/manage/troubleshooting.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napp = Application(\n    consumer_group=\"myconsumergroup\",\n    auto_offset_reset=\"earliest\",\n    auto_create_topics=True,  # Quix app has an option to auto create topics\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Transaction Stream Processing\nDESCRIPTION: Final example showing the same transaction processing logic, demonstrating consistency in state management across a stream of financial transactions.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/navigating-stateful-stream-processing.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntransactions = [100, -50, 200, -100]  # Positive values are deposits, negative values are withdrawals\ncurrent_balance = 150 # Initial balance\n\n\n\n```\n\n----------------------------------------\n\nTITLE: Default Quix Kafka Broker Addresses\nDESCRIPTION: Lists the standard Kafka broker addresses (`kafka-k1.quix.io:9093`, `kafka-k2.quix.io:9093`, `kafka-k3.quix.io:9093`) provided by Quix. This list should be verified when troubleshooting 'Broker Transport Failure' errors to ensure the correct connection details are used.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/manage/troubleshooting.md#2025-04-23_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nkafka-k1.quix.io:9093,kafka-k2.quix.io:9093,kafka-k3.quix.io:9093\n```\n\n----------------------------------------\n\nTITLE: Example Quix Environment URL in Markdown\nDESCRIPTION: This snippet provides a sample URL for a Quix environment, illustrating how the environment ID is structured within the URL. It's used to explain the components of an environment ID.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/portal-api/setup.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nhttps://portal.platform.quix.io/pipeline?workspace=joeengland-apitests-testing\n```\n\n----------------------------------------\n\nTITLE: Displaying New Features Section in Markdown Changelog\nDESCRIPTION: Markdown formatting used to highlight the 'NEW FEATURES' section in the changelog entry for November 16, 2023. This uses backticks to style the section heading.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/changelogs/2023-archive.md#2025-04-23_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n`NEW FEATURES`\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for AWS S3 Iceberg Sink Destination Card\nDESCRIPTION: This HTML snippet defines a card element for the AWS S3 Iceberg Sink destination. It includes an icon, the destination name, a brief description (consuming data from Kafka to write to an Apache Iceberg table in AWS S3 using Glue Data Catalog), and a link to the detailed documentation page.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-connectors/templates/index.md#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<li>\n<div style='display:flex'>\n<img src='awss3icebergsink-destination-icon.png' style='max-width:40px;border-radius:8px;'>\n<p style='min-width: 100px;margin-top:7px'>\n<strong style='margin-left:9px;border-radius: 8px;'>AWS S3 Iceberg Sink</strong>\n</p>\n</div>\n<hr>\n<p>Consume data from a Kafka topic and write it to an Apache Iceberg table stored in AWS S3 using the AWS Glue Data Catalog.</p>\n<p><a href='awss3icebergsink-destination.html'><span class='twemoji'><svg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'><path d='M13.22 19.03a.75.75 0 0 0 1.06 0l6.25-6.25a.75.75 0 0 0 0-1.06l-6.25-6.25a.75.75 0 1 0-1.06 1.06l4.97 4.97H3.75a.75.75 0 0 0 0 1.5h14.44l-4.97 4.97a.75.75 0 0 0 0 1.06z' fill-rule='evenodd'></path></svg></span> AWS S3 Iceberg Sink</a></p>\n</li>\n```\n\n----------------------------------------\n\nTITLE: Constructing Query API Endpoint URL in Markdown\nDESCRIPTION: This snippet demonstrates the URL pattern for Query API endpoints, which includes the environment-specific ID. It shows how to structure the base URL for making API requests to the Quix Query service.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/portal-api/setup.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n    https://telemetry-query-${environment-id}.platform.quix.io/\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Service Management Commands in Bash\nDESCRIPTION: Various Docker Compose commands for managing the documentation service including stopping, viewing logs, rebuilding, and force rebuilding the service.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/RUNNING-DOCS-LOCALLY.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose down\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose logs -f\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose build\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up --build\n```\n\n----------------------------------------\n\nTITLE: Defining Input Topic for RSS Data Filtering Connector\nDESCRIPTION: Specifies the input topic name `rss-data` for the `RSS Data Filtering` connector. This topic receives the raw data from the `RSS Data Source` connector and serves as the input for the filtering stage. It's crucial when configuring the filtering deployment if the environment has multiple applications.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/rss-tutorial/rss-processing-pipeline.md#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nrss-data\n```\n\n----------------------------------------\n\nTITLE: Creating External Links in Markdown\nDESCRIPTION: Demonstrates how to create external links that open in a new browser tab using Markdown syntax with a target attribute.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/WRITING-STYLE.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[Link text](URL){target=_blank}\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for BigQuery Sink Destination Card\nDESCRIPTION: This HTML snippet defines a card element for the BigQuery Sink destination. It includes an icon, the destination name, a brief description (persisting data from Quix to BigQuery), and a link to the detailed documentation page.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-connectors/templates/index.md#2025-04-23_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<li>\n<div style='display:flex'>\n<img src='bigquerysink-destination-icon.png' style='max-width:40px;border-radius:8px;'>\n<p style='min-width: 100px;margin-top:7px'>\n<strong style='margin-left:9px;border-radius: 8px;'>BigQuery Sink</strong>\n</p>\n</div>\n<hr>\n<p>Persist data from Quix to BigQuery</p>\n<p><a href='bigquerysink-destination.html'><span class='twemoji'><svg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'><path d='M13.22 19.03a.75.75 0 0 0 1.06 0l6.25-6.25a.75.75 0 0 0 0-1.06l-6.25-6.25a.75.75 0 1 0-1.06 1.06l4.97 4.97H3.75a.75.75 0 0 0 0 1.5h14.44l-4.97 4.97a.75.75 0 0 0 0 1.06z' fill-rule='evenodd'></path></svg></span> BigQuery Sink</a></p>\n</li>\n```\n\n----------------------------------------\n\nTITLE: Running Docker Compose in Detached Mode in Bash\nDESCRIPTION: Alternative command to run the documentation server in background (detached) mode using Docker Compose.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/RUNNING-DOCS-LOCALLY.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose up -d --build && ./open-browser.sh\n```\n\n----------------------------------------\n\nTITLE: Running Jupyter Notebook from Terminal\nDESCRIPTION: This shell command starts the Jupyter Notebook server from the terminal. Once executed, the Jupyter Notebook interface becomes accessible via a web browser, typically at `http://localhost:8888`.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/import-data.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\njupyter notebook\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for InfluxDB 3.0 Sink Destination Card\nDESCRIPTION: This HTML snippet defines a card element for the InfluxDB 3.0 Sink destination. It includes an icon, the destination name, a brief description (consuming data from a Quix Kafka topic and persisting it to an InfluxDB 3.0 database), and a link to the detailed documentation page.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/quix-connectors/templates/index.md#2025-04-23_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<li>\n<div style='display:flex'>\n<img src='influxdb30sink-destination-icon.png' style='max-width:40px;border-radius:8px;'>\n<p style='min-width: 100px;margin-top:7px'>\n<strong style='margin-left:9px;border-radius: 8px;'>InfluxDB 3.0 Sink</strong>\n</p>\n</div>\n<hr>\n<p>Consume data from a Kafka topic in Quix and persist the data to an InfluxDB 3.0 database.</p>\n<p><a href='influxdb30sink-destination.html'><span class='twemoji'><svg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'><path d='M13.22 19.03a.75.75 0 0 0 1.06 0l6.25-6.25a.75.75 0 0 0 0-1.06l-6.25-6.25a.75.75 0 1 0-1.06 1.06l4.97 4.97H3.75a.75.75 0 0 0 0 1.5h14.44l-4.97 4.97a.75.75 0 0 0 0 1.06z' fill-rule='evenodd'></path></svg></span> InfluxDB 3.0 Sink</a></p>\n</li>\n```\n\n----------------------------------------\n\nTITLE: Installing Quix Streams via pip (Python, Bash)\nDESCRIPTION: Demonstrates the command to install the Quix Streams SDK using Python's package installer, pip. Requires Python 3 and internet access. The 'quixstreams' package enables Quix data streaming capabilities, and updating to the latest version can be done by adding the '-U' flag to the command. This command should be run in a shell or command-line interface, and is commonly specified in a project's requirements.txt file for Quix Cloud environments.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/integrate-data/overview.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install quixstreams\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for MkDocs Project\nDESCRIPTION: This configuration lists the necessary Python packages and their minimum versions required for the project, primarily focusing on MkDocs and its extensions for building documentation. It includes packages for the theme ('mkdocs-material' with 'imaging' extra), lightboxes ('mkdocs-glightbox'), multi-repository support ('mkdocs-multirepo-plugin'), redirects ('mkdocs-redirects'), markdown inclusion ('mkdocs-include-markdown-plugin'), search exclusion ('mkdocs-exclude-search'), and C foreign function interface support ('cffi'). These dependencies are typically installed using `pip install -r <filename>`.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmkdocs-material[imaging]>=9.5.0\nmkdocs-glightbox>=0.1.0\nmkdocs-multirepo-plugin>=0.1.0\nmkdocs-redirects>=1.2.0\nmkdocs-include-markdown-plugin>=4.0.4\nmkdocs-exclude-search>=0.6.4\ncffi>=1.15.0\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Remote Quix Environment with Repository using CLI\nDESCRIPTION: Runs the Quix CLI command `quix envs sync` to synchronize the remote Quix environment (associated with the current CLI context) with its project repository. This ensures the cloud deployment reflects the code and configuration stored in the repository.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/overview.md#2025-04-23_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nquix envs sync\n```\n\n----------------------------------------\n\nTITLE: Displaying Improvements Section in Markdown Changelog\nDESCRIPTION: Markdown formatting used to highlight the 'IMPROVEMENTS' section in the changelog entry for December 11, 2023. This uses backticks to style the section heading.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/changelogs/2023-archive.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`IMPROVEMENTS`\n```\n\n----------------------------------------\n\nTITLE: Adding client.id Property for Kafka Tracking\nDESCRIPTION: Introduces the 'client.id' property to certain Quix services as part of an enhancement. This property is intended to track Quix Kafka usage when connecting to external Kafka providers, such as Confluent, improving monitoring capabilities.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/changelogs/2023-archive.md#2025-04-23_snippet_3\n\nLANGUAGE: Configuration\nCODE:\n```\nclient.id\n```\n\n----------------------------------------\n\nTITLE: Compiling MATLAB Function for .NET Deployment\nDESCRIPTION: Shows the command for compiling 'rot.m' as a .NET assembly targeting the .NET 6.0 framework with the MATLAB Compiler. The output is a .NET package in the 'cs' directory (or equivalent). Requires MATLAB Compiler SDK, .NET environment, and the specified framework version.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_2\n\nLANGUAGE: cs\nCODE:\n```\nmcc -W 'dotnet:quixmatlab,api=matlab-data,framework_version=6.0' rot.m -d cs\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for Release Notes\nDESCRIPTION: YAML configuration block defining metadata for the release notes blog post including title, date, authors, and categories.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/release-scratchpads.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: \"Quix Release: Scratchpads\"\ndate: 2024-10-28\nauthors: [steve-rosam]\nslug: quix-release-scratchpads\ndescription: >\n  Learn about the latest Quix release. \ncategories:\n  - releases\nhide:\n  - navigation\n```\n\n----------------------------------------\n\nTITLE: Installing SignalR via NPM\nDESCRIPTION: Command to install Microsoft SignalR package using NPM package manager\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/setup.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @microsoft/signalr\n```\n\n----------------------------------------\n\nTITLE: SignalR Writer API Connection\nDESCRIPTION: JavaScript code demonstrating how to establish a SignalR connection to the Writer API using authentication token.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/setup.md#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nvar signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_PAT\"\nconst environmentId = \"YOUR_ENVIRONMENT_ID\"\n\nconst options = {\n    accessTokenFactory: () => token\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://writer-\" + environmentId + \".platform.quix.io/hub\", options)\n    .build();\n\nconnection.start().then(() => console.log(\"SignalR connected.\"));\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies using pip – Python\nDESCRIPTION: Installs all main dependencies required for the demo using pip in a Jupyter/Colab shell. Prerequisites: access to pip, a Linux shell environment. Packages installed: quixstreams (Kafka client), qdrant-client (vector DB client), sentence-transformers (embedding creation), and pandas (data manipulation). No parameters; installs fixed versions. Output: success/error messages from pip.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install quixstreams==2.1.2a qdrant-client\\u003e=1.1.1 sentence-transformers pandas\n```\n\n----------------------------------------\n\nTITLE: Creating a Call-to-Action Card with HTML/Markdown\nDESCRIPTION: This snippet utilizes a combination of Markdown syntax extensions (interpreted within the `div` tag) and standard HTML to create a grid card layout. It includes text and an HTML anchor (`<a>`) tag styled as a primary button (`md-button md-button--primary`) that links to the Quix demo booking page, opening it in a new tab.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/connect/kafka-to-aws-waf.md#2025-04-23_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"grid cards\" markdown>\n\n- __Find out how we can help you integrate!__\n\n    <a class=\"md-button md-button--primary\" href=\"https://quix.io/book-a-demo\" target=\"_blank\" style=\"margin:.5rem;\">Book a demo</a>\n\n</div>\n```\n\n----------------------------------------\n\nTITLE: Sample Stream Metadata Message Format - JSON\nDESCRIPTION: This JSON snippet represents a metadata message structure that might be seen on the output topic, but is not directly used in the main workflow. The object includes optional fields for name, location, metadata, parent references, and time of recording. No dependencies are necessary, and all fields are nullable; these messages help inform clients about the stream context but do not contain camera event data.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/computer-vision/tfl-camera-feed.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Name\": null,\n    \"Location\": null,\n    \"Metadata\": {},\n    \"Parents\": [],\n    \"TimeOfRecording\": null\n}        \n\n```\n\n----------------------------------------\n\nTITLE: Monitoring Quix Local Pipeline Logs in Bash\nDESCRIPTION: This snippet shows the command to monitor the local Quix Streams pipeline's logs from the terminal. It displays output from the message broker (Redpanda) and the application for troubleshooting or status monitoring.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nquix local pipeline logs\n\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Quix Local Pipeline in Bash\nDESCRIPTION: This command line snippet is used to safely shut down the local Quix pipeline and related Docker containers. Execute it to clean up the Redpanda broker and terminate services.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/game-telemetry-duckdb-quixstreams.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nquix local pipeline down\n\n```\n\n----------------------------------------\n\nTITLE: Filtering Data by Stream IDs\nDESCRIPTION: JSON request body example showing how to filter data by specific stream IDs, allowing for data retrieval from multiple specific streams.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/raw-data.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"streamIds\": [\n        \"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\",\n        \"9feb07ac-b0b2-4591-bc7f-8f0c1295ed7c\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing ipykernel for Jupyter Notebook Python 3\nDESCRIPTION: These commands are executed in a terminal within a Python environment. The first command uses pip to install the `ipykernel` package, and the second command installs the kernel spec for the current user, making the 'Python 3 (ipykernel)' option available in the Jupyter Notebook 'New' menu.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/import-data.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npip install ipykernel\npython -m ipykernel install --user\n```\n\n----------------------------------------\n\nTITLE: SignalR WebSocket Connection Log Example\nDESCRIPTION: This snippet shows an example of the log message produced when SignalR successfully establishes a WebSocket connection to the Quix hub.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-reader-api/reading-data.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n[2023-10-09T15:23:27.993Z] Information: WebSocket connected to wss://reader-joeengland-apitests-testing.platform.quix.io/hub?id=ABC9Ctg5zdQ7aAzXYX.\n```\n\n----------------------------------------\n\nTITLE: Example Kafka Disconnection Log Messages\nDESCRIPTION: Provides sample log entries indicating Kafka producer disconnections (`sasl_ssl://... Disconnected`). These messages might occur due to idle connection reaping or rolling node restarts in the Quix-managed Kafka service. The underlying client library usually re-establishes the connection automatically.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/manage/troubleshooting.md#2025-04-23_snippet_3\n\nLANGUAGE: log\nCODE:\n```\n[2024-04-30 14:51:46,791] [INFO] : FAIL [rdkafka#producer-4] [thrd:sasl_ssl://kafka-k5.quix.io:9093/5]: sasl_ssl://kafka-k5.quix.io:9093/5: Disconnected (after 154139ms in state UP, 1 identical error(s) suppressed)\n[2024-04-30 14:51:46,791] [INFO] : FAIL [rdkafka#producer-4] [thrd:sasl_ssl://kafka-k5.quix.io:9093/5]: sasl_ssl://kafka-k5.quix.io:9093/5: Disconnected (after 154139ms in state UP, 1 identical error(s) suppressed)\n[2024-04-30 14:51:46,791] [ERROR] : Kafka producer error: sasl_ssl://kafka-k5.quix.io:9093/5: Disconnected (after 154139ms in state UP, 1 identical error(s) suppressed) code=\"-195\"\n[2024-04-30 14:51:46,791] [ERROR] : Kafka producer error: sasl_ssl://kafka-k5.quix.io:9093/5: Disconnected (after 154139ms in state UP, 1 identical error(s) suppressed) code=\"-195\"\n```\n\n----------------------------------------\n\nTITLE: Example JSON Output from Kafka Consumer\nDESCRIPTION: This JSON object represents an example message consumed from the `cpu-load` Kafka topic by the Python script. It illustrates the data structure, containing a `cpu_load` value, nested `memory` details (total, used, free, percent, sin, sout), and a `timestamp` indicating when the data was recorded. The consumer script prints messages in this format.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/get-started/consume.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"cpu_load\": 5.7,\n    \"memory\": {\n        \"total\": 0,\n        \"used\": 0,\n        \"free\": 0,\n        \"percent\": 0,\n        \"sin\": 90937131008,\n        \"sout\": 483672064\n    },\n    \"timestamp\": 1712238254512946000\n}\n```\n\n----------------------------------------\n\nTITLE: Fetching All Streams Using curl in Quix Platform\nDESCRIPTION: A curl command that sends an empty JSON object to the /streams endpoint to retrieve all streams within an environment. Requires authentication token and proper content-type header.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/query-api/streams-paged.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl \"https://${domain}.platform.quix.io/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d \"{}\"\n```\n\n----------------------------------------\n\nTITLE: Complete Dockerfile for Quix Python Event Detection Service\nDESCRIPTION: This Dockerfile defines the environment for running the Python event detection service. It starts from a Quix base image, sets environment variables, copies project code, installs the required system dependency `libgomp1` and `ca-certificates` using `apt-get`, installs Python dependencies from `requirements.txt` using `pip` (leveraging a local cache), and specifies `main.py` as the entry point.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/event-detection/crash-detection.md#2025-04-23_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nFROM quixpythonbaseimage\n\nENV DEBIAN_FRONTEND=\"noninteractive\"\nENV PYTHONUNBUFFERED=1\nENV PYTHONIOENCODING=UTF-8\nENV SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt\n\nWORKDIR /app\nCOPY --from=git /project .\nRUN apt-get -y install libgomp1 ca-certificates\nRUN find | grep requirements.txt | xargs -I '{}' python3 -m pip install -i http://pip-cache.pip-cache.svc.cluster.local/simple --trusted-host pip-cache.pip-cache.svc.cluster.local -r '{}' --extra-index-url https://pypi.org/simple --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/\nENTRYPOINT python3 main.py\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Docker Resource Issues in Bash\nDESCRIPTION: Commands to check Docker system resources and clean up unused resources to resolve potential resource-related issues.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/RUNNING-DOCS-LOCALLY.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Check Docker system resources\ndocker system df\ndocker system prune  # Clean up unused resources\n```\n\n----------------------------------------\n\nTITLE: Deploying Local Changes with Quix CLI\nDESCRIPTION: Executes the Quix CLI command `quix local pipeline sync --update` to synchronize local application code changes with the Quix Cloud environment. This command updates the `quix.yaml` configuration file, pushes the modified code to the project repository, and applies the deployment settings defined within `quix.yaml`. It may also prompt for synchronization of other related applications if necessary.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/deploy/overview.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nquix local pipeline sync --update\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Docker Build Issues in Bash\nDESCRIPTION: Commands for cleaning Docker builds by disabling cache and removing all containers and images to resolve build-related issues.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/RUNNING-DOCS-LOCALLY.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Clean build with no cache\ndocker compose build --no-cache\n# Remove all containers and images\ndocker compose down --rmi all\n```\n\n----------------------------------------\n\nTITLE: Processing Book Data with Embeddings in Kafka Stream Pipeline\nDESCRIPTION: Sets up a Kafka stream processing pipeline that consumes book data, creates vector embeddings for the content, updates timestamps, and produces the enriched data to another topic for vector database storage.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ninputtopicname = docs_topic_name\noutputtopicname = vectors_topic_name\n\n# Create a special stop condition just for this Notebook (otherwise the cell will run indefinitely)\nprint(f\"Using offset limit {offsetlimit}\")\ndef on_message_processed(topic, partition, offset):\n    if offset > offsetlimit:\n        app.stop()\n\n# Define your application and settings\napp = Application(\n    broker_address=\"127.0.0.1:9092\",\n    consumer_group=consumergroup_name,\n    auto_offset_reset=\"earliest\",\n    on_message_processed=on_message_processed,\n    consumer_extra_config={\"allow.auto.create.topics\": \"true\"},\n)\n\n# Define an input topic with JSON deserializer\ninput_topic = app.topic(inputtopicname, value_deserializer=\"json\")\nprint(f\"Consuming from input topic: {inputtopicname}\")\n\n# Define an output topic with JSON serializer\noutput_topic = app.topic(outputtopicname, value_serializer=\"json\")\nprint(f\"Producing to output topic: {outputtopicname}\")\n\n# Initialize a streaming dataframe based on the stream of messages from the input topic:\nsdf = app.dataframe(topic=input_topic)\n\n# Filter the SDF to include only incoming rows where the roles that dont match the bot's current role\nsdf = sdf.update(lambda val: print(f\"Received update: {val}\"))\n\n# EMBEDDING HAPPENS HERE\n### Trigger the embedding function for any new messages(rows) detected in the filtered SDF\nsdf[\"embeddings\"] = sdf.apply(create_embeddings, stateful=False)\n\n# Update the timestamp column to the current time in nanoseconds\nsdf[\"Timestamp\"] = sdf[\"Timestamp\"].apply(lambda row: time.time_ns())\n\n# Publish the processed SDF to a Kafka topic specified by the output_topic object.\nsdf = sdf.to_topic(output_topic)\n\napp.run(sdf)\n\n# STOP THIS CELL MANUALLY WHEN THE BOOK ENTRIES HAVE BEEN ENRICHED WITH EMBEDDINGS\n```\n\n----------------------------------------\n\nTITLE: Setting Up Embedding Model and Topics for Quix Stream – Python\nDESCRIPTION: Initializes input/output Kafka topic names and loads the SentenceTransformer embedding model to create document vector embeddings for downstream processing. Dependencies: sentence-transformers, variables set previously. Parameters: topic names, embedding model name. Output: topic variables and encoder instance ready for use; no direct output.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/ingest-embeddings/continuously_ingest_documents_into_a_vector_store_using_apache_kafka.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninputtopicname = docs_topic_name\\noutputtopicname = vectors_topic_name\\nencoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings\n```\n\n----------------------------------------\n\nTITLE: Changelog Entry Headers\nDESCRIPTION: Version headers and categorization tags used throughout the changelog to organize updates\nSOURCE: https://github.com/quixio/quix-docs/blob/main/changelogs/2022-archive.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`NEW FEATURES`\n`IMPROVEMENTS`\n`BUG FIXES`\n```\n\n----------------------------------------\n\nTITLE: Simulate Simulink Model via Seeded MATLAB Variables\nDESCRIPTION: Contains test data and a function invocation for running the 'engine' MATLAB Simulink bootstrap function: initializes 'throttle_a' and 'ts' arrays, then executes 'engine(throttle_a, ts)'. Used for local verification before packaging. Requires the previously defined 'engine' function, and available Simulink model assets.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/matlab/matlab-and-simulink.md#2025-04-23_snippet_5\n\nLANGUAGE: matlab\nCODE:\n```\nthrottle_a = [0.2, 0.23, 1.2, 4.2, 5.3 ];\nts = [1, 2, 3, 4, 5];\nengine(throttle_a, ts);\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries using pip\nDESCRIPTION: Installs the necessary Python libraries (seaborn for visualization, scikit-learn for machine learning, mlflow for experiment tracking, and matplotlib for plotting) using the pip package manager. These libraries are required for the subsequent data processing, model training, and visualization steps in the Jupyter Notebook.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/train-and-deploy-ml/train-ml.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install seaborn\npip install scikit-learn\npip install mlflow\npip install matplotlib\n```\n\n----------------------------------------\n\nTITLE: Viewing Quix Clickstream Message Format in JSON\nDESCRIPTION: This JSON snippet illustrates the structure of a single message as viewed in the Quix Data Explorer's 'Messages' view for the 'click-data' topic. It shows how data like timestamps, numeric values (`original_timestamp`), and string values (`userId`, `ip`, `userAgent`, `productId`) are organized within the Quix message format.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/tutorials/clickstream/clickstream-producer.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Epoch\": 0,\n  \"Timestamps\": [\n    1699973201619326000\n  ],\n  \"NumericValues\": {\n    \"original_timestamp\": [\n      1331802578\n    ]\n  },\n  \"StringValues\": {\n    \"userId\": [\n      \"F6F8B460-4204-4C26-A32C-B93826EDCB99\"\n    ],\n    \"ip\": [\n      \"75.85.165.38\"\n    ],\n    \"userAgent\": [\n      \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/534.52.7 (KHTML, like Gecko) Version/5.1.2 Safari/534.52.7\"\n    ],\n    \"productId\": [\n      \"VD55179433\"\n    ]\n  },\n  \"TagValues\": {}\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Environment Variables using dotenv in Python\nDESCRIPTION: This Python snippet demonstrates how to use the `load_dotenv` function from the `dotenv` library to load environment variables defined in a `.env` file into the application's environment. This is necessary for accessing configuration like the Quix SDK token during local development. Requires the `python-dotenv` package to be installed.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/develop/authentication/quix-streams.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n----------------------------------------\n\nTITLE: Embedding a Loom Video using HTML iframe - HTML\nDESCRIPTION: This HTML code snippet embeds a Loom video in the documentation using an iframe, wrapped in a responsive container. It allows users to watch the referenced walkthrough directly within the documentation page. Dependencies include access to the referenced video URL and basic browser support for iframes. No inputs or parameters are needed, but iframe security and compatibility should be considered for future deployments.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/create/create-project.md#2025-04-23_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<div style=\\\"position: relative; padding-bottom: 51.839080459770116%; height: 0;\\\"><iframe src=\\\"https://www.loom.com/embed/b4488be244834333aec56e1a35faf4db?sid=13c128de-df05-46b5-bb1f-03f6af3e7777\\\" frameborder=\\\"0\\\" webkitallowfullscreen mozallowfullscreen allowfullscreen style=\\\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\\\"></iframe></div>\n```\n\n----------------------------------------\n\nTITLE: Installing SignalR via NPM\nDESCRIPTION: Command to install Microsoft SignalR package using NPM package manager.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/apis/streaming-writer-api/setup.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @microsoft/signalr\n```\n\n----------------------------------------\n\nTITLE: Transaction Processing with State Maintenance\nDESCRIPTION: Another instance of the transaction processing example demonstrating stateful operations in stream processing, showing how state (balance) is maintained and updated across multiple transactions.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/docs/blog/posts/navigating-stateful-stream-processing.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntransactions = [100, -50, 200, -100]  # Positive values are deposits, negative values are withdrawals\ncurrent_balance = 150 # Initial balance\n\n# Iterate over each transaction\nfor transaction in transactions:\n    current_balance += transaction  # Update the balance with each transaction\n    print(f\"Current balance: {current_balance}\")  # Print the balance after each transaction\n```\n\n----------------------------------------\n\nTITLE: Displaying CreatedBy and UpdatedBy Information in Deployments API and CLI\nDESCRIPTION: JSON structure showing the audit information added to Deployments responses in API and CLI. It includes user identification details like userId, email, name, and timestamp for both creation and update actions.\nSOURCE: https://github.com/quixio/quix-docs/blob/main/CHANGELOG.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n...\n\"createdBy\": {\n    \"userId\": \"454bdbcc-6a17-4e8c-bcc8-2c6f5bb03e81\",\n    \"email\": \"someone@quix.io\",\n    \"firstName\": \"John\",\n    \"lastName\": \"Doe\",\n    \"dateTime\": \"2024-10-23T14:16:04.314+00:00\"\n},\n\"updatedBy\": {\n    \"userId\": \"454bdbcc-6a17-4e8c-bcc8-2c6f5bb03e81\",\n    \"email\": \"someone@quix.io\",\n    \"firstName\": \"John\",\n    \"lastName\": \"Doe\",\n    \"dateTime\": \"2024-10-25T09:49:45.223+00:00\"\n}\n...\n```"
  }
]
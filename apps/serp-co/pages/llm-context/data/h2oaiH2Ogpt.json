[
  {
    "owner": "h2oai",
    "repo": "h2ogpt",
    "content": "TITLE: Running h2oGPT with LLaMa-3 GGUF Model\nDESCRIPTION: Executes the `generate.py` script to start h2oGPT with a specific LLaMa-3 GGUF model. It includes specifying the tokenizer, model path, and maximum sequence length. This command utilizes the Hugging Face tokenizer for the LLaMa-3 model and specifies a remote GGUF file for model loading.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_quickstart.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=llama --model_path_llama=https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf?download=true --tokenizer_base_model=meta-llama/Meta-Llama-3-8B-Instruct --max_seq_len=8192\n```\n\n----------------------------------------\n\nTITLE: Running LLaMa2 7B GPTQ with RoPE scaling\nDESCRIPTION: This command runs the LLaMa2 7B model with AutoGPTQ and RoPE scaling. It loads the specified base model, enables GPTQ loading, and configures RoPE scaling. It also sets maximum and minimum new tokens for generation.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install transformers==4.31.0  # breaks load_in_8bit=True in some cases (https://github.com/huggingface/transformers/issues/25026)\npython generate.py --base_model=TheBloke/Llama-2-7b-Chat-GPTQ --load_gptq=\"model\" --use_safetensors=True --prompt_type=llama2 --score_model=None --save_dir='7bgptqrope4` --rope_scaling=\"{'type':'dynamic', 'factor':4}\"\n--max_max_new_tokens=15000 --max_new_tokens=15000 --max_time=12000\n```\n\n----------------------------------------\n\nTITLE: Running vLLM with LLaMa-2 70B AWQ on 4 A10G GPUs\nDESCRIPTION: This command launches a vLLM inference server with LLaMa-2 70B AWQ model across four A10G GPUs within a Docker container. It configures shared memory, environment variables, and volume mounts for optimal performance. Key parameters include tensor parallelism, quantization, and resource limits.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p $HOME/.cache/huggingface/hub\nmkdir -p $HOME/.cache/huggingface/modules/\nmkdir -p $HOME/.triton/cache/\nmkdir -p $HOME/.config/vllmdocker run -d \\\n    --runtime=nvidia \\\n    --gpus '\"device=0,1,2,3\"' \\\n    --shm-size=10.24gb \\\n    -p 5000:5000 \\\n    -e NCCL_IGNORE_DISABLED_P2P=1 \\\n    -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\n    -e VLLM_NO_USAGE_STATS=1 \\\n    -e VLLM_NCCL_SO_PATH=/usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2 \\\n    -e DO_NOT_TRACK=1 \\\n    -e NUMBA_CACHE_DIR=/tmp/ \\\n    -v /etc/passwd:/etc/passwd:ro \\\n    -v /etc/group:/etc/group:ro \\\n    -u `id -u`:`id -g` \\\n    -v \"${HOME}\"/.cache:$HOME/.cache/ -v \"${HOME}\"/.config:$HOME/.config/   -v \"${HOME}\"/.triton:$HOME/.triton/  \\\n    --network host \\\n    vllm/vllm-openai:latest \\\n        --port=5000 \\\n        --host=0.0.0.0 \\\n        --model=h2oai/h2ogpt-4096-llama2-70b-chat-4bit \\\n        --tensor-parallel-size=4 \\\n        --seed 1234 \\\n        --trust-remote-code \\\n\t    --max-num-batched-tokens 8192 \\\n\t    --max-num-seqs 256 \\\n\t    --quantization awq \\\n\t    --worker-use-ray \\\n\t    --enforce-eager \\\n        --download-dir=/workspace/.cache/huggingface/hub &>> logs.vllm_server.70b_awq.txt\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT in Docker with Zephyr 7B Beta Model\nDESCRIPTION: This snippet demonstrates running h2oGPT via Docker using the Zephyr 7B Beta model. It includes setting up necessary directories, exporting environment variables, and running the docker command with various volume mounts and arguments.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p ~/.cache/huggingface/hub/\nmkdir -p ~/.triton/cache/\nmkdir -p ~/.config/vllm/\nmkdir -p ~/.cache\nmkdir -p ~/save\nmkdir -p ~/user_path\nmkdir -p ~/db_dir_UserData\nmkdir -p ~/users\nmkdir -p ~/db_nonusers\nmkdir -p ~/llamacpp_path\nmkdir -p ~/h2ogpt_auth\necho '[\"key1\",\"key2\"]' > ~/h2ogpt_auth/h2ogpt_api_keys.json\nexport GRADIO_SERVER_PORT=7860\nexport OPENAI_SERVER_PORT=5000\ndocker run \\\n       --gpus all \\\n       --runtime=nvidia \\\n       --shm-size=2g \\\n       -p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT \\\n       -p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT \\\n       --rm --init \\\n       --network host \\\n       -v /etc/passwd:/etc/passwd:ro \\\n       -v /etc/group:/etc/group:ro \\\n       -u `id -u`:`id -g` \\\n       -v \"${HOME}\"/.cache/huggingface/hub/:/workspace/.cache/huggingface/hub \\\n       -v \"${HOME}\"/.config:/workspace/.config/ \\\n       -v \"${HOME}\"/.triton:/workspace/.triton/  \\\n       -v \"${HOME}\"/save:/workspace/save \\\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\n       -v \"${HOME}\"/users:/workspace/users \\\n       -v \"${HOME}\"/db_nonusers:/workspace/db_nonusers \\\n       -v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path \\\n       -v \"${HOME}\"/h2ogpt_auth:/workspace/h2ogpt_auth \\\n       -e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT \\\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 /workspace/generate.py \\\n          --base_model=HuggingFaceH4/zephyr-7b-beta \\\n          --use_safetensors=True \\\n          --prompt_type=zephyr \\\n          --save_dir='/workspace/save/' \\\n          --auth_filename='/workspace/h2ogpt_auth/auth.db' \\\n          --h2ogpt_api_keys='/workspace/h2ogpt_auth/h2ogpt_api_keys.json' \\\n          --auth='/workspace/h2ogpt_auth/h2ogpt_api_keys.json' \\\n          --use_gpu_id=False \\\n          --user_path=/workspace/user_path \\\n          --langchain_mode=\"LLM\" \\\n          --langchain_modes=\"['UserData', 'LLM']\" \\\n          --score_model=None \\\n          --max_max_new_tokens=2048 \\\n          --max_new_tokens=1024 \\\n          --use_auth_token=\"${HUGGING_FACE_HUB_TOKEN}\" \\\n          --openai_port=$OPENAI_SERVER_PORT\n```\n\n----------------------------------------\n\nTITLE: Run h2oGPT Full Installation Script\nDESCRIPTION: Runs the full h2oGPT installation script.  Prefixing the command with GPLOK=1 allows for installation of all (including GPL) packages.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nGPLOK=1 bash docs/linux_install.sh\n```\n\n----------------------------------------\n\nTITLE: CLI Chat with H2OGPT Model and LangChain\nDESCRIPTION: This command initiates a CLI chat session with a specific H2OGPT model and integrates LangChain using a specified user path.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLI.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b --cli=True --langchain_mode=UserData --user_path=user_path --answer_with_sources=False\n```\n\n----------------------------------------\n\nTITLE: Securing Gradio Server with Authentication\nDESCRIPTION: This snippet illustrates how to add authentication to the Gradio server using the `--auth` flag. It requires specifying a username and password pair to restrict access to the server.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n--auth=[('jon','password')]\n```\n\n----------------------------------------\n\nTITLE: Installing h2ogpt from wheel with CUDA support and online index\nDESCRIPTION: This snippet shows how to install h2ogpt with CUDA support using a wheel hosted online. It includes setting environment variables and using pip with specified index URLs to retrieve the package and dependencies.  flash-attn is also installed.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WHEEL.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport CMAKE_ARGS=\"-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=all\"\nexport CUDA_HOME=/usr/local/cuda-12.1\nexport PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu121 https://huggingface.github.io/autogptq-index/whl/cu121\"\npip install h2ogpt==0.2.0[cuda] --index-url https://downloads.h2ogpt.h2o.ai --extra-index-url https://pypi.org/simple --no-cache\npip install flash-attn==2.4.2\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning h2oGPT on Instruct Data\nDESCRIPTION: This torchrun command performs fine-tuning of an h2oGPT model on a single node with NVIDIA GPUs. It specifies the base model, data path, and output directory, utilizing multiple GPUs for faster training.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/FINETUNE.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport NGPUS=`nvidia-smi -L | wc -l`\ntorchrun --nproc_per_node=$NGPUS finetune.py --base_model=h2oai/h2ogpt-oasst1-512-20b --data_path=h2oai/h2ogpt-oig-oasst1-instruct-cleaned-v2 --output_dir=h2ogpt_lora_weights\n```\n\n----------------------------------------\n\nTITLE: Instantiate Embedded Weaviate instance using Python\nDESCRIPTION: This Python code instantiates an embedded Weaviate instance using the client library and uploads a sample document to it. It requires the `weaviate` package to be installed.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport weaviate\nfrom weaviate.embedded import EmbeddedOptions\n\nclient = weaviate.Client(\n  embedded_options=EmbeddedOptions()\n)\n\ndata_obj = {\n  \"name\": \"Chardonnay\",\n  \"description\": \"Goes with fish\"\n}\n\nclient.data_object.create(data_obj, \"Wine\")\n```\n\n----------------------------------------\n\nTITLE: Running a larger model with Bitsandbytes (8-bit)\nDESCRIPTION: This command runs a larger language model in 8-bit mode for production use. It specifies a Hugging Face model and enables 8-bit loading using bitsandbytes. The `--h2ocolors=False` argument disables the H2O.ai color scheme.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=HuggingFaceH4/zephyr-7b-beta --load_8bit=True\n```\n\n----------------------------------------\n\nTITLE: Downloading Model and Tokenizer (Transformers)\nDESCRIPTION: This code downloads and saves a model and its tokenizer using the `transformers` library.  It uses `AutoModelForCausalLM` and `AutoTokenizer` to load the model and tokenizer from a specified `model_name` and then saves them to the same directory.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_name = 'h2oai/h2ogpt-oasst1-512-12b'\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.save_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained(model_name)\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT Offline (Manual Download)\nDESCRIPTION: These commands illustrate manually downloading the model and then running h2oGPT offline.  The `wget` command downloads the model from a Hugging Face URL.  `TRANSFORMERS_OFFLINE=1` ensures the application does not attempt to download models from the internet, and the model is specified using `--base_model` or `--model_path_llama`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# online do:\nwget https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q5_K_M.gguf?download=true -O llamacpp_path/zephyr-7b-beta.Q5_K_M.gguf\n# Then use normally for any tasks one expects to do offline.\n# Once offline do:\nTRANSFORMERS_OFFLINE=1 python generate.py --base_model=zephyr-7b-beta.Q5_K_M.gguf --prompt_type=zephyr --gradio_offline_level=2 --share=False --add_disk_models_to_ui=False\n# or:\nTRANSFORMERS_OFFLINE=1 python generate.py --base_model=llama --model_path_llama=zephyr-7b-beta.Q5_K_M.gguf --prompt_type=zephyr --gradio_offline_level=2 --share=False --add_disk_models_to_ui=False\n# or if choosing in UI do (be sure to choose correct prompt_type too):\nTRANSFORMERS_OFFLINE=1 python generate.py --gradio_offline_level=2 --share=False --add_disk_models_to_ui=False\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT in Docker Offline\nDESCRIPTION: This snippet demonstrates how to run h2oGPT in Docker in offline mode. It sets environment variables to indicate offline operation, mounts necessary volumes, and passes arguments to the generate.py script.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport TRANSFORMERS_OFFLINE=1\nexport GRADIO_SERVER_PORT=7860\nexport OPENAI_SERVER_PORT=5000\nexport HF_HUB_OFFLINE=1\ndocker run --gpus all \\\n--runtime=nvidia \\\n--shm-size=2g \\\n-e TRANSFORMERS_OFFLINE=$TRANSFORMERS_OFFLINE \\\n-e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\n-e HF_HUB_OFFLINE=$HF_HUB_OFFLINE \\\n-e HF_HOME=\"/workspace/.cache/huggingface/\" \\\n-p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT \\\n-p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT \\\n--rm --init \\\n--network host \\\n-v /etc/passwd:/etc/passwd:ro \\\n-v /etc/group:/etc/group:ro \\\n-u `id -u`:`id -g` \\\n-v \"${HOME}\"/.cache/huggingface/:/workspace/.cache/huggingface \\\n-v \"${HOME}\"/.cache/torch/:/workspace/.cache/torch \\\n-v \"${HOME}\"/.cache/transformers/:/workspace/.cache/transformers \\\n-v \"${HOME}\"/save:/workspace/save \\\n-v \"${HOME}\"/user_path:/workspace/user_path \\\n-v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\n-v \"${HOME}\"/users:/workspace/users \\\n-v \"${HOME}\"/db_nonusers:/workspace/db_nonusers \\\n-v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path \\\n-e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT \\\n gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 \\\n /workspace/generate.py \\\n --base_model=mistralai/Mistral-7B-Instruct-v0.2 \\\n --use_safetensors=False \\\n --prompt_type=mistral \\\n --save_dir='/workspace/save/' \\\n --use_gpu_id=False \\\n --user_path=/workspace/user_path \\\n --langchain_mode=\"LLM\" \\\n --langchain_modes=\"['UserData', 'MyData', 'LLM']\" \\\n --score_model=None \\\n --max_max_new_tokens=2048 \\\n --max_new_tokens=1024 \\\n --visible_visible_models=False \\\n --openai_port=$OPENAI_SERVER_PORT \\\n --gradio_offline_level=2\n```\n\n----------------------------------------\n\nTITLE: Using Gradio Native Client with h2oGPT\nDESCRIPTION: This snippet demonstrates how to use the Gradio native client to interact with the h2oGPT API. It initializes the client with the host URL, constructs a dictionary of arguments for the API call, converts the dictionary to a string, and then uses the client to make a prediction. The response is then parsed to extract the answer.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom gradio_client import Client\nimport ast\n\nHOST_URL = \"http://localhost:7860\"\nclient = Client(HOST_URL)\n\n# string of dict for input\nkwargs = dict(instruction_nochat='Who are you?')\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\n\n# string of dict for output\nresponse = ast.literal_eval(res)['response']\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT with Mistral model\nDESCRIPTION: Executes the `generate.py` script to start h2oGPT with a specific Mistral model.  It specifies the base model, prompt type, and maximum sequence length.  This command launches the h2oGPT application, making it accessible through a web browser.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_quickstart.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# choose up to 32768 if have enough GPU memory:\npython generate.py --base_model=TheBloke/Mistral-7B-Instruct-v0.2-GGUF --prompt_type=mistral --max_seq_len=4096\n```\n\n----------------------------------------\n\nTITLE: Run h2oGPT UI with GPU (Smaller Model)\nDESCRIPTION: Runs the h2oGPT UI with a smaller 7B Llama2 model without quantization, and user-provided documents.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --langchain_mode='UserData' --user_path=user_path\n```\n\n----------------------------------------\n\nTITLE: Curl API Call Example (Bash)\nDESCRIPTION: This snippet demonstrates how to interact with the h2oGPT server using a `curl` command. It sends a POST request to the `submit_nochat_plain_api` endpoint with a JSON payload containing the instruction for the model.  The data is formatted as a JSON string with the instruction 'Who are you?'.  It's useful for interacting with the API without a dedicated client library, provided the API does not require `gr.State()` objects.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:7860/api/submit_nochat_plain_api -X POST -d '{\"data\": [\"{\\\"instruction_nochat\\\": \\\"Who are you?\\\"}\"]}' -H 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT\nDESCRIPTION: This snippet shows how to run h2oGPT with specific model and prompt settings. It uses the `generate.py` script with command-line arguments to configure the model, prompt type, and maximum sequence length.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --prompt_type=zephyr --max_seq_len=4096\n```\n\n----------------------------------------\n\nTITLE: Running with AutoGPTQ\nDESCRIPTION: This command runs the model using AutoGPTQ quantization. It loads the specified base model in GPTQ format, disables the score model, and specifies the prompt type and Langchain mode. It also benefits from specifying an hf embedding model to further reduce memory usage.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=TheBloke/Nous-Hermes-13B-GPTQ --score_model=None --load_gptq=model --use_safetensors=True --prompt_type=instruct --langchain_mode=UserData\n```\n\n----------------------------------------\n\nTITLE: h2oGPT Gradio Wrapper Example (Python)\nDESCRIPTION: This snippet demonstrates using the `GradioClient` wrapper class, which adds extra exception handling and h2oGPT specific calls. It showcases common operations like LLM querying, document Q/A, summarization, and extraction. It checks for an API key from environment variables and connects to either a local or remote h2oGPT instance. The `test_readme_example` function is intended as a self-contained example used for the project's README file.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef test_readme_example(local_server):\n    # self-contained example used for readme, to be copied to README_CLIENT.md if changed, setting local_server = True at first\n    import os\n    # The grclient.py file can be copied from h2ogpt repo and used with local gradio_client for example use\n    from gradio_utils.grclient import GradioClient\n\n    if local_server:\n        client = GradioClient(\"http://0.0.0.0:7860\")\n    else:\n        h2ogpt_key = os.getenv('H2OGPT_KEY') or os.getenv('H2OGPT_H2OGPT_KEY')\n        if h2ogpt_key is None:\n            return\n        # if you have API key for public instance:\n        client = GradioClient(\"https://gpt.h2o.ai\", h2ogpt_key=h2ogpt_key)\n\n    # LLM\n    print(client.question(\"Who are you?\"))\n\n    url = \"https://cdn.openai.com/papers/whisper.pdf\"\n\n    # Q/A\n    print(client.query(\"What is whisper?\", url=url))\n    # summarization (map_reduce over all pages if top_k_docs=-1)\n    print(client.summarize(\"What is whisper?\", url=url, top_k_docs=3))\n    # extraction (map per page)\n    print(client.extract(\"Give bullet for all key points\", url=url, top_k_docs=3))\ntest_readme_example(local_server=True)\n```\n\n----------------------------------------\n\nTITLE: Setting CMAKE Arguments for Metal Support in Docker Build\nDESCRIPTION: This snippet shows how to set the CMAKE_ARGS environment variable to enable Metal support for llama.cpp when building the Docker image. This is specifically for M1/M2 Macs.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport CMAKE_ARGS=\"-DLLAMA_METAL=on\"\n```\n\n----------------------------------------\n\nTITLE: Running Llama-2-70B-chat-AWQ on multiple GPUs\nDESCRIPTION: This command runs the Llama-2-70B-chat model with AutoAWQ quantization across multiple GPUs. It configures CUDA_VISIBLE_DEVICES to use GPUs 2 and 3, loads the model in AWQ format, and specifies the prompt type.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=2,3 python generate.py --base_model=TheBloke/Llama-2-70B-chat-AWQ --score_model=None --load_awq=model --use_safetensors=True --prompt_type=llama2\n```\n\n----------------------------------------\n\nTITLE: Running vLLM with LLaMa-2 70B AWQ in Docker\nDESCRIPTION: This command launches a vLLM inference server with LLaMa-2 70B AWQ model within a Docker container, utilizing two GPUs. It configures the container with shared memory, environment variables for Hugging Face authentication and NCCL, and volume mounts for caching. The server is exposed on port 5000 and uses the h2oai/h2ogpt-4096-llama2-70b-chat-4bit model with AWQ quantization.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p $HOME/.cache/huggingface/hub\nmkdir -p $HOME/.cache/huggingface/modules/\nmkdir -p $HOME/.triton/cache/\nmkdir -p $HOME/.config/vllmdocker run -d \\\n    --runtime=nvidia \\\n    --gpus '\"device=0,1\"' \\\n    --shm-size=10.24gb \\\n    -p 5000:5000 \\\n    -e NCCL_IGNORE_DISABLED_P2P=1 \\\n    -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\n    -e VLLM_NO_USAGE_STATS=1 \\\n    -e VLLM_NCCL_SO_PATH=/usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2 \\\n    -e DO_NOT_TRACK=1 \\\n    -e NUMBA_CACHE_DIR=/tmp/ \\\n    -v /etc/passwd:/etc/passwd:ro \\\n    -v /etc/group:/etc/group:ro \\\n    -u `id -u`:`id -g` \\\n    -v \"${HOME}\"/.cache:$HOME/.cache/ -v \"${HOME}\"/.config:$HOME/.config/   -v \"${HOME}\"/.triton:$HOME/.triton/  \\\n    --network host \\\n    vllm/vllm-openai:latest \\\n        --port=5000 \\\n        --host=0.0.0.0 \\\n        --model=h2oai/h2ogpt-4096-llama2-70b-chat-4bit \\\n        --tensor-parallel-size=2 \\\n        --seed 1234 \\\n        --trust-remote-code \\\n\t    --max-num-batched-tokens 8192 \\\n\t    --quantization awq \\\n\t    --worker-use-ray \\\n\t    --enforce-eager \\\n        --download-dir=/workspace/.cache/huggingface/hub &>> logs.vllm_server.70b_awq.txt\n```\n\n----------------------------------------\n\nTITLE: Building h2oGPT wheel from source\nDESCRIPTION: This snippet clones the h2oGPT repository, navigates into it, and then builds the Python wheel using `setup.py`. It assumes that the necessary build tools are installed and configured in the environment.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WHEEL.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/h2oai/h2ogpt.git\ncd h2ogpt\npython setup.py bdist_wheel\n```\n\n----------------------------------------\n\nTITLE: Test FasterTransformer Model\nDESCRIPTION: This script tests the converted FasterTransformer model using the `gptneox_example.py` script. It creates input files and runs the example script with the appropriate parameters.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/TRITON.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\necho \"Hi, who are you?\" > gptneox_input\necho \"And you are?\" >> gptneox_input\npython3 ${WORKSPACE}/FasterTransformer/examples/pytorch/gptneox/gptneox_example.py \\\n         --ckpt_path ${WORKSPACE}/FT-${MODEL}/1-gpu \\\n         --tokenizer_path ${WORKSPACE}/${MODEL} \\\n         --sample_input_file gptneox_input\n```\n\n----------------------------------------\n\nTITLE: Querying Document with H2OGPT\nDESCRIPTION: This code snippet demonstrates how to query a document using H2OGPT to answer a specific question. It sets up the instruction, document choice, and other parameters like `langchain_action`, `top_k_docs`, and prompts, then uses the `client.predict` method to get the response from the H2OGPT API. The response is then parsed and printed.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ninstruction = \"What is the eligibility criteria for the program?\"\ndocument_choice = \"user_path/terms-and-conditions.pdf\"\n\nlangchain_action = LangChainAction.QUERY.value\nstream_output = False\ntop_k_docs = 5\n\n#pre_prompt_summary = \"\"\"In order to write a concise single-paragraph or bulleted list summary, pay attention to the following text\\n\"\"\"\n#prompt_summary = \"Using only the text above, write a condensed and concise summary of key results as 5 bullet points:\\n\"\npre_prompt_summary = None\nprompt_summary = None\n\npre_prompt_query = \"\"\"Pay attention and remember the information below, which will help to answer the question or imperative after the context ends.\\n\"\"\"\nprompt_query = \"\"\"According to only the information in the document sources provided within the context above, \\n\"\"\"\n#pre_prompt_query = None\n#prompt_query = None\n\nkwargs = dict(instruction=instruction,\n            langchain_mode=langchain_mode,\n            langchain_action=langchain_action,  # uses full document, not vectorDB chunks\n            top_k_docs=top_k_docs,\n            stream_output=stream_output,\n            document_subset='Relevant',\n            # document_choice=document_choice,\n            max_new_tokens=256,\n            max_time=360,\n            do_sample=False,\n            pre_prompt_query=pre_prompt_query,\n            prompt_query=prompt_query,\n            pre_prompt_summary=pre_prompt_summary,\n            prompt_summary=prompt_summary,\n            h2ogpt_key=H2OGPT_KEY\n            )\n\n# get result\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\nresponse = ast.literal_eval(res)['response']\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Running LLaMa2 70B in full 16-bit\nDESCRIPTION: This command runs the LLaMa2 70B model in full 16-bit precision across multiple GPUs. It uses RoPE scaling to extend the context length.  It installs a specific version of transformers.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install transformers==4.31.0  # breaks load_in_8bit=True in some cases (https://github.com/huggingface/transformers/issues/25026)\npython generate.py --base_model=meta-llama/Llama-2-70b-chat-hf --prompt_type=llama2 --rope_scaling=\"{'type': 'linear', 'factor': 4}\" --use_gpu_id=False --save_dir=savemeta70b\n```\n\n----------------------------------------\n\nTITLE: Running Q/A with a private document collection\nDESCRIPTION: This command allows users to run question-answering on a private document collection by placing documents in a folder called `user_path`. The `langchain_mode=UserData` argument activates document Q/A mode.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b  --load_8bit=True --langchain_mode=UserData --user_path=user_path\n```\n\n----------------------------------------\n\nTITLE: Starting Fine-Tuned Chatbot\nDESCRIPTION: This torchrun command starts a fine-tuned chatbot using the specified base model and Lora weights.  It loads the model in 8-bit precision and sets the prompt type for human-bot interaction.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/FINETUNE.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun generate.py --load_8bit=True --base_model=h2oai/h2ogpt-oasst1-512-20b --lora_weights=h2ogpt_lora_weights --prompt_type=human_bot\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT with Llama 3.1 GGUF Model\nDESCRIPTION: This snippet shows how to run h2oGPT with a specific Llama 3.1 GGUF model, specifying the model path, tokenizer, and maximum sequence length.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=llama --model_path_llama=https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf?download=true --tokenizer_base_model=meta-llama/Meta-Llama-3.1-8B-Instruct --max_seq_len=8192\n```\n\n----------------------------------------\n\nTITLE: Clone h2oGPT Repository\nDESCRIPTION: Clones the h2oGPT repository from GitHub and navigates into the cloned directory. This downloads the h2oGPT source code to the local machine.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/h2oai/h2ogpt.git\ncd h2ogpt\n```\n\n----------------------------------------\n\nTITLE: Setting up Docker for GPU Inference on Ubuntu\nDESCRIPTION: This snippet configures Docker for GPU inference on Ubuntu by installing the NVIDIA Container Toolkit. It includes adding the NVIDIA repository, updating the package index, and installing the toolkit.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt-get update && sudo apt-get install -y nvidia-container-toolkit-base\nsudo apt install -y nvidia-container-runtime\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT Offline with GGUF Model\nDESCRIPTION: This command demonstrates running h2oGPT in offline mode with a GGUF model, explicitly specifying the model file using `--model_path_llama`. The `HF_DATASETS_OFFLINE` and `TRANSFORMERS_OFFLINE` environment variables are set to 1 to ensure offline operation.  `--prompt_type` is also important for ensuring the model functions correctly.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nHF_DATASETS_OFFLINE=1;TRANSFORMERS_OFFLINE=1 python generate.py --gradio_offline_level=2 --gradio_offline_level=2 --base_model=llama --model_path_llama=zephyr-7b-beta.Q5_K_M.gguf --prompt_type=zephyr\n```\n\n----------------------------------------\n\nTITLE: Setting CMAKE Arguments for CUDA (Cmdline)\nDESCRIPTION: This snippet sets CMAKE arguments related to CUDA for llama_cpp_python, including enabling CUDA and specifying CUDA architectures. This is necessary to ensure that llama_cpp_python is built with CUDA support.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md#_snippet_6\n\nLANGUAGE: cmdline\nCODE:\n```\nset CMAKE_ARGS=-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=all\nset GGML_CUDA=1\nset FORCE_CMAKE=1\n```\n\n----------------------------------------\n\nTITLE: Caching Models for Offline Use\nDESCRIPTION: This command sets `prepare_offline_level=2` to get standard models for offline use and also installs nltk and playwright.  It's used to fill the `~/.cache` folder with necessary model components.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --score_model=None --gradio_size=small --model_lock=\"[{'base_model': 'h2oai/h2ogpt-4096-llama2-7b-chat'}]\" --save_dir=save_fastup_chat --prepare_offline_level=2 --add_disk_models_to_ui=False\n# below are already in docker\npython -m nltk.downloader all\nplaywright install --with-deps\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Installs the required Python packages listed in the various requirements.txt files. This ensures all necessary libraries for h2oGPT are installed, including optional dependencies for specific features like langchain and llamacpp.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_GPU.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Install dependencies\n!for fil in requirements.txt reqs_optional/requirements_optional_langchain.txt reqs_optional/requirements_optional_llamacpp_gpt4all.txt reqs_optional/requirements_optional_langchain.gpllike.txt reqs_optional/requirements_optional_langchain.urls.txt ; do pip install -r $fil ; done\n```\n\n----------------------------------------\n\nTITLE: Run h2oGPT with OpenAI and Local Image Captioning\nDESCRIPTION: Runs h2oGPT using OpenAI for the LLM, but uses a local GPU for better image caption performance.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=<key> python generate.py  --inference_server=openai_chat --base_model=gpt-3.5-turbo --score_model=None --captions_model=microsoft/Florence-2-large\n```\n\n----------------------------------------\n\nTITLE: Installing h2ogpt using pip\nDESCRIPTION: Installs the h2ogpt package using pip. This is the simplest method for installing h2ogpt and its core dependencies.  It assumes that the environment has been properly set up with the correct Python version and virtual environment.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_quickstart.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install h2ogpt\n```\n\n----------------------------------------\n\nTITLE: CLI Chat with LangChain, User Path, and GPT-J\nDESCRIPTION: This command combines database building and CLI chat using LangChain, specifying the user path directly. It builds the database if it doesn't exist.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLI.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=gptj --cli=True --langchain_mode=UserData --user_path=user_path --answer_with_sources=False\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT with vLLM Inference Server\nDESCRIPTION: This command shows how to connect h2oGPT to a running vLLM inference server by adding the `--inference_server` argument to the h2oGPT docker run command. The `--base_model` should match the model used for vLLM.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n        --inference_server=\"vllm:0.0.0.0:5000\"\n```\n\n----------------------------------------\n\nTITLE: Context-Only Call with Parameters in h2oGPT\nDESCRIPTION: This snippet demonstrates a context-only call to the h2oGPT API, specifying various model parameters such as `visible_models`, `langchain_mode`, `max_new_tokens`, `max_time`, `repetition_penalty`, `temperature`, `top_p`, and `penalty_alpha`.  It uses `H2OGPT_KEY` for authentication.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# string of dict for input\nkwargs = dict(instruction_nochat='Who are you?',\n              visible_models=['h2oai/h2ogpt-4096-llama2-13b-chat'],\n              langchain_mode='LLM',\n              max_new_tokens=512,\n              max_time=360,\n              repetition_penalty=1.07,\n              do_sample=True,\n              temperature=0.1,\n              top_p=0.75,\n              penalty_alpha=0,\n              h2ogpt_key=H2OGPT_KEY)\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\n\n# string of dict for output\nresponse = ast.literal_eval(res)['response']\nprint(\"Model Response:\\n\")\npprint(response)\n```\n\n----------------------------------------\n\nTITLE: Running TGI Inference Server with Docker\nDESCRIPTION: This command launches a Text Generation Inference (TGI) server within a Docker container, utilizing GPU 0. It configures the container with shared memory, network host, and volume mounts for caching. The server uses the h2oai/h2ogpt-4096-llama2-7b-chat model.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL=h2oai/h2ogpt-4096-llama2-7b-chat\ndocker run -d --gpus '\"device=0\"' \\\n       --shm-size 1g \\\n       --network host \\\n       -p 6112:80 \\\n       -v $HOME/.cache/huggingface/hub/:/data ghcr.io/huggingface/text-generation-inference:0.9.3 \\\n       --model-id $MODEL \\\n       --max-input-length 4096 \\\n       --max-total-tokens 8192 \\\n       --max-stop-sequences 6 &>> logs.infserver.txt\n```\n\n----------------------------------------\n\nTITLE: Chat Completion with OpenAI API in h2oGPT\nDESCRIPTION: This snippet demonstrates how to use the OpenAI API to interact with h2oGPT for chat completion. It requires the `openai` Python package. It defines base URL and API key and uses `chat.completions.create` to send a message to h2oGPT and print the response.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nbase_url = 'https://localhost:5000/v1'\napi_key = 'INSERT KEY HERE or set to EMPTY if no key set on h2oGPT server'\nclient_args = dict(base_url=base_url, api_key=api_key)\nopenai_client = OpenAI(**client_args)\n\nmessages = [{'role': 'user', 'content': 'Who are you?'}]\nstream = False\nclient_kwargs = dict(model='h2oai/h2ogpt-4096-llama2-70b-chat', max_tokens=200, stream=stream, messages=messages)\nclient = openai_client.chat.completions\n\nresponses = client.create(**client_kwargs)\ntext = responses.choices[0].message.content\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Speech-to-Text using OpenAI Client in Python\nDESCRIPTION: This snippet demonstrates speech-to-text conversion using the OpenAI client. It reads an audio file from disk and sends it to the API for transcription. It requires an OpenAI API key and specifies the 'whisper-1' model. It assumes that h2oGPT is loaded with the appropriate STT settings.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI(base_url='http://0.0.0.0:5000/v1')\n\nfile = \"speech.wav\"\nwith open(file, \"rb\") as f:\n    audio_file= f.read()\ntranscription = client.audio.transcriptions.create(\n  model=\"whisper-1\",\n  file=audio_file\n)\nprint(transcription.text)\n```\n\n----------------------------------------\n\nTITLE: Manual Installation of h2ogpt\nDESCRIPTION: Provides instructions for manually installing h2ogpt by cloning the repository and installing the required dependencies. This method allows for more control over the installation process and is useful for development or when pip installation fails. The steps include cloning the repository, navigating to the directory, and installing required packages from various requirements files.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_quickstart.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/h2oai/h2ogpt.git\ncd h2ogpt\npip install -r requirements.txt\npip install -r reqs_optional/requirements_optional_langchain.txt\n\npip uninstall llama_cpp_python llama_cpp_python_cuda -y\npip install -r reqs_optional/requirements_optional_llamacpp_gpt4all.txt --no-cache-dir\n\npip install -r reqs_optional/requirements_optional_langchain.urls.txt\n# GPL, only run next line if that is ok:\npip install -r reqs_optional/requirements_optional_langchain.gpllike.txt\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT Generation Script\nDESCRIPTION: Executes the `generate.py` script with specified command-line arguments. This starts the h2oGPT server with configurations such as the base model, prompt type, score model, langchain mode, user path, and embedding model.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_GPU.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!GRADIO_SERVER_PORT=7860 python generate.py --base_model=togethercomputer/RedPajama-INCITE-Chat-3B-v1 --prompt_type=human_bot --score_model=None --langchain_mode=LLM --langchain_modes=\"['LLM', 'UserData', 'MyData']\" --user_path=user_path --share=False --hf_embedding_model=sentence-transformers/all-MiniLM-L6-v2\n```\n\n----------------------------------------\n\nTITLE: CLI Chat with LLaMa2 and LangChain\nDESCRIPTION: This command initiates a CLI chat session using the LLaMa2 model with a specified prompt type and LangChain integration with a user path.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLI.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model='llama' --prompt_type=llama2 --cli=True --langchain_mode=UserData --user_path=user_path --answer_with_sources=False\n```\n\n----------------------------------------\n\nTITLE: Downloading and Uploading File to H2OGPT Python\nDESCRIPTION: This snippet downloads a PDF file from a given URL, uploads it to the Gradio server using the `/upload_api` endpoint, and retrieves the local and server file paths. It leverages the `download_simple` function and `client.predict` to handle the file transfer and server-side processing.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport os\nurl = \"https://cleanvehiclerebate.org/sites/default/files/docs/nav/transportation/cvrp/documents/CVRP-Implementation-Manual.pdf\"\ntest_file1 = os.path.join('/tmp/', 'CVRP-Implementation-Manual.pdf')\ndownload_simple(url, dest=test_file1)\n\n# upload file(s).  Can be list or single file\n# test_file_server - location of the uploaded file on the Gradio server\ntest_file_local, test_file_server = client.predict(test_file1, api_name='/upload_api')\n```\n\n----------------------------------------\n\nTITLE: Building h2oGPT wheel for PyPI\nDESCRIPTION: This snippet builds a Python wheel specifically for PyPI, using a limited set of packages. The `PYPI=1` environment variable likely controls the included dependencies.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WHEEL.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nPYPI=1 python setup.py bdist_wheel\n```\n\n----------------------------------------\n\nTITLE: Setting PIP_EXTRA_INDEX_URL for Linux/Windows CPU/CUDA/ROC\nDESCRIPTION: Sets the PIP_EXTRA_INDEX_URL environment variable to the CUDA-specific PyTorch and AutoGPTQ wheel repositories. This is required for installing PyTorch and AutoGPTQ on systems with CUDA. It allows pip to find the correct CUDA-accelerated packages. `cu121` and `cu118` refer to different CUDA versions.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_quickstart.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# for windows/mac use \"set\" or relevant environment setting mechanism\nexport PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu121 https://huggingface.github.io/autogptq-index/whl/cu121\"\n# for cu118 use export PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu118 https://huggingface.github.io/autogptq-index/whl/cu118\"\n```\n\n----------------------------------------\n\nTITLE: Building h2oGPT Docker Image\nDESCRIPTION: This snippet shows how to build a Docker image for h2oGPT. It first touches the build_info.txt file and then runs the docker build command with the -t tag to name the image h2ogpt.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# build image\ntouch build_info.txt\ndocker build -t h2ogpt .\n```\n\n----------------------------------------\n\nTITLE: Select and Process Specific Document Types (bash)\nDESCRIPTION: This snippet shows how to select specific document types for processing with `src/make_db.py` and then use the resulting database with `generate.py`. It demonstrates the use of `--selected_file_types` to filter the processed files. Requires `python` and the h2ogpt environment to be set up.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npython src/make_db.py --user_path=\"/home/jon/Downloads/demo_data\" --collection_name=VAData --enable_pdf_ocr='off' --selected_file_types=\"['pdf', 'html', 'htm']\"\npython generate.py  --base_model='llama' --prompt_type=llama2 --score_model=None --langchain_mode=VAData --langchain_modes=['VAData'] --model_path_llama=https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf --max_seq_len=4096\n```\n\n----------------------------------------\n\nTITLE: Download Example Databases (bash)\nDESCRIPTION: This snippet demonstrates how to download pre-generated example databases for use with H2oGPT.  It uses `src/make_db.py` to download the databases and then runs `generate.py` with specific parameters to use the downloaded data. Requires `python` and the h2ogpt environment to be set up.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython src/make_db.py --download_some=True\npython generate.py --base_model=HuggingFaceH4/zephyr-7b-beta --langchain_mode=UserData --langchain_modes=\"['UserData', 'wiki', 'MyData', 'github h2oGPT', 'DriverlessAI docs']\"\n```\n\n----------------------------------------\n\nTITLE: Activate h2oGPT Conda Environment\nDESCRIPTION: Activates the Conda environment named 'h2ogpt'. This environment contains all the necessary dependencies for running h2oGPT.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda activate h2ogpt\n```\n\n----------------------------------------\n\nTITLE: Run make_db with Weaviate and URL using Python\nDESCRIPTION: This command runs the `make_db.py` script with a specified Weaviate URL. This requires setting the `WEAVIATE_URL` environment variable. The environment variable is defined inline for the command.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nWEAVIATE_URL=http://localhost:8080 python src/make_db.py --db_type=weaviate\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT from Python with advanced options\nDESCRIPTION: This snippet shows how to run h2oGPT directly from Python code by importing the `main` function from `h2ogpt.generate` and passing in more advanced options such as `prompt_type`, `save_dir`, `score_model`, `max_max_new_tokens`, `max_new_tokens`, `num_async`, and `top_k_docs`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WHEEL.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom h2ogpt.generate import main\nmain(base_model='meta-llama/Llama-2-7b-chat-hf',\n          prompt_type='llama2',\n          save_dir='save_gpt7',\n          score_model=None,\n          max_max_new_tokens=2048,\n          max_new_tokens=1024,\n          num_async=10,\n          top_k_docs=-1)\n```\n\n----------------------------------------\n\nTITLE: h2oGPT API Call with Image Bytes (Python)\nDESCRIPTION: This snippet demonstrates how to use h2oGPT with an image as bytes. First, the image is downloaded from a URL and converted to a base64 encoded string. Then, this base64 string is passed to the API. It showcases an alternative approach to using image URLs, suitable for handling local images or when direct URLs are not available. The image file is passed as `image_bytes` in the `kwargs` dictionary to `client.predict`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport ast\n\nfrom gradio_client import Client\n\n# can copy-paste these functions for own use\nfrom src.utils import download_image\nfrom src.vision.utils_vision import img_to_base64\n\n# without auth:\n# client = Client('http://localhost:7860')\n\n# with auth:\nclient = Client('http://localhost:7860', auth=('user', 'pass'))\n\nh2ogpt_key = 'api key here, or EMPTY if no key or do not put in kwargs'\n\n\nimage_url = 'https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg'\nsave_dir = 'datatest'\nimage_file = download_image(image_url, save_dir)\nimage_bytes = img_to_base64(image_file)\n\nkwargs = dict(\n    visible_models='THUDM/cogvlm2-llama3-chat-19B',\n    instruction_nochat=\"describe the imaged\",\n    h2ogpt_key=h2ogpt_key,\n    stream_output=False,\n    image_file=image_bytes,\n    temperature=0,\n    max_tokens=4000)\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\n\nresponse = ast.literal_eval(res)['response']\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA_HOME Environment Variable (bash)\nDESCRIPTION: This snippet sets the CUDA_HOME environment variable to the specified CUDA installation path. It's essential for ensuring that the h2oGPT environment correctly links to the CUDA libraries.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\nexport CUDA_HOME=/usr/local/cuda-12.1\n```\n\n----------------------------------------\n\nTITLE: Running model with a specific prompt type\nDESCRIPTION: This command allows users to specify the `prompt_type` when running the model from a local path. This is necessary when the `--base_model` points to a locally downloaded model. The `prompt_type` must match the model.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=<user path> --load_8bit=True --prompt_type=human_bot\n```\n\n----------------------------------------\n\nTITLE: Downloading GGUF Model Manually\nDESCRIPTION: This command uses `wget` to download a GGUF model file from a Hugging Face repository.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nwget https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf\n```\n\n----------------------------------------\n\nTITLE: Adding Uploaded File to Collection with Options Python\nDESCRIPTION: This snippet adds a previously uploaded file to an H2OGPT collection with specified chunking, embedding, and other configurations. It defines parameters such as `chunk`, `chunk_size`, `embed`, and the H2OGPT key, then calls the `/add_file_api` to integrate the file into the collection.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nchunk = True\nchunk_size = 512\nembed = True\nh2ogpt_key = H2OGPT_KEY\nloaders = tuple([None, None, None, None])\ndoc_options = tuple([langchain_mode, chunk, chunk_size, embed])\n\nres = client.predict(\n                test_file_server, *doc_options, *loaders, h2ogpt_key, api_name=\"/add_file_api\"\n            )\n```\n\n----------------------------------------\n\nTITLE: Upgrading PyTorch and TorchVision\nDESCRIPTION: This snippet demonstrates how to upgrade PyTorch and TorchVision to specific versions to resolve the BFloat16 error on macOS.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npip install -U torch==2.3.1\npip install -U torchvision==0.18.1\n```\n\n----------------------------------------\n\nTITLE: Installing Docker on Ubuntu for CPU Inference\nDESCRIPTION: This snippet demonstrates the installation of Docker on Ubuntu for CPU inference. It involves updating the package index, installing necessary packages, adding the Docker repository, and installing Docker CE.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nsudo apt install -y apt-transport-https ca-certificates curl software-properties-common\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo add-apt-repository -y \"deb [arch=amd64] https://download.docker.com/linux/ubuntu jammy stable\"\napt-cache policy docker-ce\nsudo apt install -y docker-ce\nsudo systemctl status docker\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Completions API Request using Curl\nDESCRIPTION: This snippet shows how to make a streaming request to the chat completions API with curl. The main difference from the non-streaming request is the addition of `\"stream\": true` in the JSON data.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=xxxx\ncurl http://localhost:5000/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n-d '{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a beautiful dragon who likes to breath fire.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Who are you?\"\n    }\n  ],\n  \"max_tokens\": 200,\n  \"temperature\": 0,\n  \"seed\": 1234,\n  \"h2ogpt_key\": \"$OPENAI_API_KEY\",\n  \"stream\": true\n}'\n```\n\n----------------------------------------\n\nTITLE: Creating High-Quality OIG Instruct Data\nDESCRIPTION: These pytest commands download, assemble, detoxify, chop, grade, and finalize high-quality instruction data from the OIG dataset to create a cleaned JSON file for fine-tuning.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/FINETUNE.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest -s create_data.py::test_download_useful_data_as_parquet  # downloads ~ 4.2GB of open-source permissive data\npytest -s create_data.py::test_assemble_and_detox               # ~ 3 minutes, 4.1M clean conversations\npytest -s create_data.py::test_chop_by_lengths                  # ~ 2 minutes, 2.8M clean and long enough conversations\npytest -s create_data.py::test_grade                            # ~ 3 hours, keeps only high quality data\npytest -s create_data.py::test_finalize_to_json\n```\n\n----------------------------------------\n\nTITLE: Run generation with Weaviate and URL using Python\nDESCRIPTION: This command runs the `generate.py` script using Weaviate at a specified URL. The environment variable `WEAVIATE_URL` is set before executing the script.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nWEAVIATE_URL=http://localhost:8080 python generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b \\\n   --langchain_mode=UserData \\\n   --db_type=weaviate\n```\n\n----------------------------------------\n\nTITLE: Installing h2oGPT wheel with CUDA support\nDESCRIPTION: This snippet shows how to install the h2oGPT wheel with CUDA support.  It configures environment variables for CUDA and PyTorch, and then uses pip to install the wheel and flash-attn. The path to the wheel should be adjusted based on where it was built.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WHEEL.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport CUDA_HOME=/usr/local/cuda-12.1\nexport PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu121 https://huggingface.github.io/autogptq-index/whl/cu121\"\nset CMAKE_ARGS=-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=all\nset GGML_CUDA=1\nset FORCE_CMAKE=1\npip install <h2ogpt_path>/dist/h2ogpt-0.1.0-py3-none-any.whl[cuda]\npip install flash-attn==2.4.2\n```\n\n----------------------------------------\n\nTITLE: Running vLLM server with Docker\nDESCRIPTION: This command runs a vLLM inference server within a Docker container.  It mounts several directories for caching, sets environment variables for GPU configuration, and specifies the model, port, and quantization settings. Standard output is appended to logs.vllm_server.70b_awq.txt.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p $HOME/.cache/huggingface/hub\nmkdir -p $HOME/.cache/huggingface/modules/\nmkdir -p $HOME/.triton/cache/\nmkdir -p $HOME/.config/vllm\ndocker run -d \\\n    --runtime=nvidia \\\n    --gpus '\"device=0,1\"' \\\n    --shm-size=10.24gb \\\n    -p 5000:5000 \\\n    -e NCCL_IGNORE_DISABLED_P2P=1 \\\n    -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\n    -e VLLM_NO_USAGE_STATS=1 \\\n    -e VLLM_NCCL_SO_PATH=/usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2 \\\n    -e DO_NOT_TRACK=1 \\\n    -e NUMBA_CACHE_DIR=/tmp/ \\\n    -v /etc/passwd:/etc/passwd:ro \\\n    -v /etc/group:/etc/group:ro \\\n    -u `id -u`:`id -g` \\\n    -v \"${HOME}\"/.cache:$HOME/.cache/ -v \"${HOME}\"/.config:$HOME/.config/   -v \"${HOME}\"/.triton:$HOME/.triton/  \\\n    --network host \\\n    vllm/vllm-openai:latest \\\n        --port=5000 \\\n        --host=0.0.0.0 \\\n        --model=h2oai/h2ogpt-4096-llama2-70b-chat-4bit \\\n        --tensor-parallel-size=2 \\\n        --seed 1234 \\\n        --trust-remote-code \\\n\t      --max-num-batched-tokens 8192 \\\n\t      --quantization awq \\\n        --download-dir=/workspace/.cache/huggingface/hub &>> logs.vllm_server.70b_awq.txt\n```\n\n----------------------------------------\n\nTITLE: Run h2oGPT with OpenAI Inference Server\nDESCRIPTION: Runs h2oGPT using OpenAI's API for inference. This command sets the OpenAI API key and specifies the model to use. Not recommended for private documents.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=<key> python generate.py  --inference_server=openai_chat --base_model=gpt-3.5-turbo --score_model=None\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Completion with OpenAI API in h2oGPT\nDESCRIPTION: This snippet demonstrates how to use the OpenAI API to interact with h2oGPT for streaming chat completion. It requires the `openai` Python package. It defines base URL and API key, sets `stream=True`, and iterates over the response chunks to print the delta content.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nbase_url = 'http://localhost:5000/v1'\napi_key = 'INSERT KEY HERE or set to EMPTY if no key set on h2oGPT server'\nclient_args = dict(base_url=base_url, api_key=api_key)\nopenai_client = OpenAI(**client_args)\n\nmessages = [{'role': 'user', 'content': 'Who are you?'}]\nstream = True\nclient_kwargs = dict(model='h2oai/h2ogpt-4096-llama2-70b-chat', max_tokens=200, stream=stream, messages=messages)\nclient = openai_client.chat.completions\n\nresponses = client.create(**client_kwargs)\ntext = ''\nfor chunk in responses:\n    delta = chunk.choices[0].delta.content\n    if delta:\n        text += delta\n        print(delta, end='')\n```\n\n----------------------------------------\n\nTITLE: Pulling h2oGPT Docker Image\nDESCRIPTION: This command pulls the specified h2oGPT docker image from the Google Container Registry (GCR). It ensures that the local image is up-to-date.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1\n```\n\n----------------------------------------\n\nTITLE: Example Chatbot Dataset\nDESCRIPTION: This example demonstrates the format of a chatbot dataset suitable for fine-tuning an LLM. It showcases the human-bot conversation structure for training the model to respond in a conversational manner.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/FINETUNE.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n<human>: Hi, who are you?\n<bot>: I'm h2oGPT.\n<human>: Who trained you?\n<bot>: I was trained by H2O.ai, the visionary leader in democratizing AI.\n```\n\n----------------------------------------\n\nTITLE: Installing Optional LangChain Dependencies\nDESCRIPTION: This snippet installs optional dependencies required for document question answering using LangChain.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Required for Doc Q/A: LangChain:\npip install -r reqs_optional/requirements_optional_langchain.txt -c reqs_optional/reqs_constraints.txt\n```\n\n----------------------------------------\n\nTITLE: Running Llama-2-13B-chat-AWQ\nDESCRIPTION: This command runs Llama-2-13B-chat model with AutoAWQ quantization on a single GPU. It sets CUDA_VISIBLE_DEVICES, loads the model, and specifies the prompt type.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 python generate.py --base_model=TheBloke/Llama-2-13B-chat-AWQ --score_model=None --load_awq=model --use_safetensors=True --prompt_type=llama2\n```\n\n----------------------------------------\n\nTITLE: Text-to-Speech (TTS) with OpenAI API in h2oGPT (Streaming)\nDESCRIPTION: This snippet demonstrates using the OpenAI API for text-to-speech with h2oGPT, enabling streaming. It uses `client.audio.speech.with_streaming_response.create` to stream the audio, saving it to a file.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI(base_url='http://0.0.0.0:5000/v1')\n\nwith client.audio.speech.with_streaming_response.create(\n        model=\"tts-1\",\n        voice=\"\",\n        extra_body=dict(stream=True,\n                        chatbot_role=\"Female AI Assistant\",\n                        speaker=\"SLT (female)\",\n                        stream_strip=True,\n                        ),\n        response_format='wav',\n        input=\"Good morning! The sun is shining brilliantly today, casting a warm, golden glow that promises a day full of possibility and joy. Its the perfect moment to embrace new opportunities and make the most of every cheerful, sunlit hour. What can I do to help you make today absolutely wonderful?\",\n) as response:\n    response.stream_to_file(\"speech_local.wav\")\n```\n\n----------------------------------------\n\nTITLE: Installing h2ogpt from PyPI with CUDA support\nDESCRIPTION: This snippet installs h2ogpt from PyPI with CUDA support, including the required environment variables for CUDA and PyTorch. It uses the `h2ogpt[cuda]` syntax to install CUDA-related dependencies.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WHEEL.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport CMAKE_ARGS=\"-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=all\"\nexport CUDA_HOME=/usr/local/cuda-12.1\nexport PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu121 https://huggingface.github.io/autogptq-index/whl/cu121\"\n# below [cuda] assumes CUDA 12.1 for some packages like AutoAWQ etc.\npip install h2ogpt[cuda]\npip install flash-attn==2.4.2\n```\n\n----------------------------------------\n\nTITLE: Setting Temperature and Seed Parameters in h2oGPT\nDESCRIPTION: This example demonstrates how to set parameters like `seed`, `temperature`, and `do_sample` for controlling the output of the h2oGPT model. Setting `temperature` requires setting `do_sample` to `True`. It uses `H2OGPT_KEY` for authentication.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# string of dict for input\nkwargs = dict(instruction_nochat='Who are you?',\n              seed=123,\n              temperature=0.5,\n              do_sample=True,\n              h2ogpt_key=H2OGPT_KEY)\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\n\n# string of dict for output\nresponse = ast.literal_eval(res)['response']\nprint(\"Model Response:\\n\")\npprint(response)\n```\n\n----------------------------------------\n\nTITLE: Run h2oGPT UI with LLaMa.cpp Model\nDESCRIPTION: Runs the h2oGPT UI using a LLaMa.cpp LLaMa2 model. This command specifies the model path, prompt type, and other necessary parameters for running h2oGPT with a LLaMa.cpp model.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model='llama' --prompt_type=llama2 --score_model=None --langchain_mode='UserData' --user_path=user_path --model_path_llama=https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf?download=true --max_seq_len=4096\n```\n\n----------------------------------------\n\nTITLE: Metal Configuration for llama_cpp_python (M1/M2)\nDESCRIPTION: Configures Metal support for `llama_cpp_python` on M1/M2 Macs. It enables Metal compilation and forces CMake to rebuild. Metal is Apple's GPU framework, allowing `llama_cpp_python` to utilize the Mac's GPU.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_quickstart.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport CMAKE_ARGS=\"-DLLAMA_METAL=on\"\nexport FORCE_CMAKE=1\n```\n\n----------------------------------------\n\nTITLE: Run generation with personal collection using Python\nDESCRIPTION: This command runs the `generate.py` script without the user name, leveraging the already created user database. It sets various parameters like the base model, prompt type, save directory, and uses LLM and UserData langchain modes.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\npython generate.py --base_model=https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q2_K.gguf --use_safetensors=True --prompt_type=zephyr --save_dir='save2' --use_gpu_id=False --user_path=user_path_test --langchain_mode=\"LLM\" --langchain_modes=\"['UserData', 'LLM']\" --score_model=None --add_disk_models_to_ui=False\n```\n\n----------------------------------------\n\nTITLE: CLI Chat with Base Model (GPT-J)\nDESCRIPTION: This command initiates a CLI chat session with the GPT-J base model. It disables the display of sources used for answering.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLI.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=gptj --cli=True --answer_with_sources=False\n```\n\n----------------------------------------\n\nTITLE: Install Miniconda and Set Up Python 3.10 Environment\nDESCRIPTION: Downloads and installs Miniconda, creates a Python 3.10 environment, and activates it. This ensures that h2oGPT runs in an isolated environment with the correct Python version.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nwget https://repo.anaconda.com/miniconda/Miniconda3-py310_23.1.0-1-Linux-x86_64.sh\nbash ./Miniconda3-py310_23.1.0-1-Linux-x86_64.sh -b -p $HOME/miniconda3\n\n# Manually adding Conda init to .bashrc\necho '### Conda init ###' >> $HOME/.bashrc\necho 'source $HOME/miniconda3/etc/profile.d/conda.sh' >> $HOME/.bashrc\necho 'conda activate' >> $HOME/.bashrc\nsource $HOME/.bashrc\n\n# install h2ogpt env\n\n# Run below if have existing h2ogpt env\n# conda remove -n h2ogpt --all -y\n\nconda update conda -y\nconda create -n h2ogpt -y\nconda activate h2ogpt\nconda install python=3.10 -c conda-forge -y\n```\n\n----------------------------------------\n\nTITLE: Killing Processes\nDESCRIPTION: Kills any running processes matching 'generate', 'frpc_linux_amd', or 'ngrok'.  This is useful for cleaning up previous runs and ensuring a clean environment before restarting h2oGPT.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_GPU.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# can kill old ngrok + generate and try again\ndo_kill = False\nif do_kill:\n  !pkill -f generate --signal 9\n  !pkill -f frpc_linux_amd --signal 9\n  !pkill -f ngrok --signal 9\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT in Offline Mode (Environment)\nDESCRIPTION: This command demonstrates how to run h2oGPT in offline mode by setting the `HF_DATASETS_OFFLINE` and `TRANSFORMERS_OFFLINE` environment variables to 1. This prevents the application from accessing the internet for datasets and transformers models. It also specifies `--gradio_offline_level=2` for disabling google fonts and `--share=False` to avoid attempting to create a public link.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nHF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 python generate.py --base_model='h2oai/h2ogpt-oasst1-512-12b' --gradio_offline_level=2 --share=False\n```\n\n----------------------------------------\n\nTITLE: Setting ARCHFLAGS for pip install\nDESCRIPTION: This snippet sets the ARCHFLAGS environment variable to specify the architecture for compilation during pip install. This is used to resolve the `_clang: error: the clang compiler does not support '-march=native'_` error on Intel Macs.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nARCHFLAGS=\"-arch x86_64\" pip install -r requirements.txt -c reqs_optional/reqs_constraints.txt\n```\n\n----------------------------------------\n\nTITLE: Retrieving Document Paths from Collection Python\nDESCRIPTION: This snippet retrieves the full paths of documents loaded into a specified collection, such as 'MyTest'. It uses `client.predict` with the `/get_sources_api` endpoint to fetch the document sources, which are then parsed using `ast.literal_eval` and printed.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nsources = ast.literal_eval(client.predict(langchain_mode, api_name='/get_sources_api'))\npprint(sources[:10])\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Inference Server with Docker\nDESCRIPTION: This command launches a vLLM inference server within a Docker container, utilizing two GPUs. It configures the container with shared memory, environment variables for Hugging Face authentication and NCCL, and volume mounts for caching. The server is exposed on port 5000 and uses the h2oai/h2ogpt-4096-llama2-7b-chat model.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nunset CUDA_VISIBLE_DEVICES\nmkdir -p $HOME/.cache/huggingface/hub\nmkdir -p $HOME/.cache/huggingface/modules/\nmkdir -p $HOME/.triton/cache/\nmkdir -p $HOME/.config/vllm\ndocker run \\\n    --runtime=nvidia \\\n    --gpus '\"device=0,1\"' \\\n    --shm-size=10.24gb \\\n    -p 5000:5000 \\\n    --rm --init \\\n    -e NCCL_IGNORE_DISABLED_P2P=1 \\\n    -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\n    -e VLLM_NO_USAGE_STATS=1 \\\n    -e VLLM_NCCL_SO_PATH=/usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2 \\\n    -e DO_NOT_TRACK=1 \\\n    -e NUMBA_CACHE_DIR=/tmp/ \\\n    -v /etc/passwd:/etc/passwd:ro \\\n    -v /etc/group:/etc/group:ro \\\n    -u `id -u`:`id -g` \\\n    -v \"${HOME}\"/.cache:$HOME/.cache/ -v \"${HOME}\"/.config:$HOME/.config/   -v \"${HOME}\"/.triton:$HOME/.triton/  \\\n    --network host \\\n    vllm/vllm-openai:latest \\\n        --port=5000 \\\n        --host=0.0.0.0 \\\n        --model=h2oai/h2ogpt-4096-llama2-7b-chat \\\n        --tokenizer=hf-internal-testing/llama-tokenizer \\\n        --tensor-parallel-size=2 \\\n        --seed 1234 \\\n        --trust-remote-code \\\n        --download-dir=/workspace/.cache/huggingface/hub &>> logs.vllm_server.txt\n```\n\n----------------------------------------\n\nTITLE: Utility Functions for h2oGPT Interaction\nDESCRIPTION: This section includes several utility functions used for interacting with h2oGPT, such as printing the full model response, creating directories, removing files, downloading files, and moving files atomically. These functions handle file system operations and API response parsing.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport shutil\nimport uuid\nimport requests\nfrom requests.exceptions import HTTPError\nimport contextlib\n\n\ndef print_full_model_response(response):\n    '''\n    Helper function to print full response from the h2oGPT call, including all parameters.\n        Important keys/parameters:\n        - `base_model` - model that used to answer the API call\n        - `extra_dict` - model parameters that were used to answer the API call\n        - `prompt` - actual prompt sent to LLM\n        - `where_from` - how hosted model is running: vLLM , tensor, ....\n    '''\n    print(\"Model Response with Parameters:\\n\")\n    save_dict = ast.literal_eval(res)['save_dict']\n    # Remove key from extra_dict\n    save_dict.pop('h2ogpt_key', None)\n    pprint(save_dict)\n    print(\"\\n\")\n    try:\n        sources = ast.literal_eval(response)['sources']\n        print(\"Sources:\\n\")\n        pprint(sources)\n        print(\"\\n\")\n    except:\n        print(\"No sources\\n\")\n\n\ndef makedirs(path, exist_ok=True, tmp_ok=False, use_base=False):\n    \"\"\"\n    Avoid some inefficiency in os.makedirs()\n    :param path:\n    :param exist_ok:\n    :param tmp_ok:  use /tmp if can't write locally\n    :param use_base:\n    :return:\n    \"\"\"\n    if path is None:\n        return path\n    # if base path set, make relative to that, unless user_path absolute path\n    if use_base:\n        if os.path.normpath(path) == os.path.normpath(os.path.abspath(path)):\n            pass\n        else:\n            if os.getenv('H2OGPT_BASE_PATH') is not None:\n                base_dir = os.path.normpath(os.getenv('H2OGPT_BASE_PATH'))\n                path = os.path.normpath(path)\n                if not path.startswith(base_dir):\n                    path = os.path.join(os.getenv('H2OGPT_BASE_PATH', ''), path)\n                    path = os.path.normpath(path)\n\n    if os.path.isdir(path) and os.path.exists(path):\n        assert exist_ok, \"Path already exists\"\n        return path\n    try:\n        os.makedirs(path, exist_ok=exist_ok)\n        return path\n    except FileExistsError:\n        # e.g. soft link\n        return path\n    except PermissionError:\n        if tmp_ok:\n            path0 = path\n            path = os.path.join('/tmp/', path)\n            print(\"Permission denied to %s, using %s instead\" % (path0, path), flush=True)\n            os.makedirs(path, exist_ok=exist_ok)\n            return path\n        else:\n            raise\n\n        \ndef shutil_rmtree(*args, **kwargs):\n    return shutil.rmtree(*args, **kwargs)\n\n\ndef remove(path: str):\n    try:\n        if path is not None and os.path.exists(path):\n            if os.path.isdir(path):\n                shutil_rmtree(path, ignore_errors=True)\n            else:\n                with contextlib.suppress(FileNotFoundError):\n                    os.remove(path)\n    except:\n        pass\n\n\ndef atomic_move_simple(src, dst):\n    try:\n        shutil.move(src, dst)\n    except (shutil.Error, FileExistsError):\n        pass\n    remove(src)\n\n\ndef download_simple(url, dest=None, overwrite=False, verbose=False):\n    if dest is None:\n        dest = os.path.basename(url)\n    base_path = os.path.dirname(dest)\n    if base_path:  # else local path\n        base_path = makedirs(base_path, exist_ok=True, tmp_ok=True, use_base=True)\n        dest = os.path.join(base_path, os.path.basename(dest))\n\n    if os.path.isfile(dest):\n        if not overwrite:\n            print(\"Already have %s from url %s, delete file if invalid\" % (dest, str(url)), flush=True)\n            return dest\n        else:\n            remove(dest)\n\n    if verbose:\n        print(\"BEGIN get url %s\" % str(url), flush=True)\n    if url.startswith(\"file://\"):\n        from requests_file import FileAdapter\n        s = requests.Session()\n        s.mount('file://', FileAdapter())\n        url_data = s.get(url, stream=True)\n    else:\n        url_data = requests.get(url, stream=True)\n    if verbose:\n        print(\"GOT url %s\" % str(url), flush=True)\n\n    if url_data.status_code != requests.codes.ok:\n        msg = \"Cannot get url %s, code: %s, reason: %s\" % (\n            str(url),\n            str(url_data.status_code),\n            str(url_data.reason),\n        )\n        raise requests.exceptions.RequestException(msg)\n    url_data.raw.decode_content = True\n\n    uuid_tmp = str(uuid.uuid4())[:6]\n    dest_tmp = dest + \"_dl_\" + uuid_tmp + \".tmp\"\n    with open(dest_tmp, \"wb\") as f:\n        shutil.copyfileobj(url_data.raw, f)\n    atomic_move_simple(dest_tmp, dest)\n    if verbose:\n        print(\"DONE url %s\" % str(url), flush=True)\n    return dest\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT from a Script\nDESCRIPTION: This snippet shows how to run h2oGPT by creating a shell script and then executing it. It's another way to pass the same parameters as in the direct command-line execution.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\npython generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --prompt_type=zephyr --max_seq_len=4096\n```\n\n----------------------------------------\n\nTITLE: Installing h2oGPT wheel for CPU/Metal\nDESCRIPTION: This snippet demonstrates installing the h2oGPT wheel for non-CUDA environments like CPU or Metal (M1/M2).  It uses pip to install the wheel with the `[cpu]` extra.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WHEEL.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install <h2ogpt_path>/dist/h2ogpt-0.1.0-py3-none-any.whl[cpu]\n```\n\n----------------------------------------\n\nTITLE: Install GPU FAISS requirements\nDESCRIPTION: Installs the necessary Python packages for using GPU-accelerated FAISS within the h2oGPT LangChain integration. This command uses pip to install the dependencies listed in the `requirements_optional_gpu_only.txt` file.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r reqs_optional/requirements_optional_gpu_only.txt\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies for DocTR with Conda\nDESCRIPTION: This snippet describes the steps to install `weasyprint` and `pygobject` using conda, which are prerequisites for enabling DocTR support when installing h2oGPT.  This needs to be done prior to installing the wheel.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WHEEL.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda install weasyprint pygobject -c conda-forge -y\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Up Command\nDESCRIPTION: This bash command uses Docker Compose to build and start the h2ogpt application in detached mode. The `--build` flag ensures that the Docker image is built before the container is started.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d --build\n```\n\n----------------------------------------\n\nTITLE: Running GPT4ALL Model (GPT-J)\nDESCRIPTION: This command executes `generate.py` with the GPT-J model specified using the `model_path_gptj` parameter. It sets the `base_model` to `gptj`, disables the score model, enables `UserData` langchain mode, and uses documents located in the `user_path` directory.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CPU.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=gptj --model_path_gptj=ggml-gpt4all-j-v1.3-groovy.bin --score_model=None --langchain_mode='UserData' --user_path=user_path\n```\n\n----------------------------------------\n\nTITLE: Launch Triton Inference Server\nDESCRIPTION: This script launches the Triton Inference Server using `mpirun`. It specifies the model repository and allows running as root.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/TRITON.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 mpirun -n 1 \\\n        --allow-run-as-root /opt/tritonserver/bin/tritonserver  \\\n        --model-repository=${WORKSPACE}/all_models/gptneox/ &\n```\n\n----------------------------------------\n\nTITLE: Get Supported Document Types (Python)\nDESCRIPTION: This snippet retrieves the supported file types for different document categories (non-image, image, video) using the `get_supported_types` function from `src/gpt_langchain.py`.  This allows users to determine which file types can be processed by the system. Requires `python` and the h2ogpt environment to be set up.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nsys.path.append('src')\nfrom src.gpt_langchain import get_supported_types\nnon_image_types, image_types, video_types = get_supported_types()\nprint(non_image_types)\nprint(image_types)\n```\n\n----------------------------------------\n\nTITLE: Downloading Embedding Model (LangChain)\nDESCRIPTION: This code downloads an embedding model for LangChain support using the `sentence-transformers` library. It initializes `HuggingFaceEmbeddings` with the specified `model_name` and `model_kwargs` to load the model.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nhf_embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\nmodel_kwargs = dict(device='cpu')\nfrom langchain.embeddings import HuggingFaceEmbeddings\nembedding = HuggingFaceEmbeddings(model_name=hf_embedding_model, model_kwargs=model_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Example Instruction-Output Dataset\nDESCRIPTION: This example demonstrates the format of an instruction-output dataset suitable for fine-tuning an LLM. It shows how to structure the input to teach the model to perform a specific task, such as summarization.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/FINETUNE.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nInstruction: Summarize.\nInput: This is a very very very long paragraph saying nothing much.\nOutput: Nothing was said.\n```\n\n----------------------------------------\n\nTITLE: Setup ngrok Tunnel\nDESCRIPTION: This snippet sets up an ngrok tunnel to expose the h2oGPT server to the internet. It installs the pyngrok library, prompts the user for their ngrok authtoken, and then creates an HTTP tunnel to port 7860.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_CPU.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Sign-up for free ngrok account using (e.g.) your Google email/login and get token: https://dashboard.ngrok.com/get-started/setup\n\n!pip install pyngrok\nimport getpass\nfrom pyngrok import ngrok, conf\n\nprint(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\nconf.get_default().auth_token = getpass.getpass()\n\n# Open an http ngrok tunnel\nconnection_string = ngrok.connect(7860, \"http\").public_url\nprint(\"Go to this address in about 20 seconds, and click on Visit Site: %s\" % connection_string)\n```\n\n----------------------------------------\n\nTITLE: Download LLaMA Model\nDESCRIPTION: This snippet downloads a LLaMA model from Hugging Face. It first removes any existing model files and then uses wget to download the specified model.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_CPU.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# download llama model if running that:\n!rm -rf WizardLM-7B-uncensored.ggmlv3.q8_0.bin*\n!wget https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GGML/resolve/main/WizardLM-7B-uncensored.ggmlv3.q8_0.bin\n```\n\n----------------------------------------\n\nTITLE: Use Azure OpenAI (bash)\nDESCRIPTION: This snippet shows how to use Azure OpenAI for inference.  It requires setting the `OPENAI_API_KEY` environment variable and specifying the Azure deployment details in the `--inference_server` argument. Requires `python`, the h2ogpt environment, and Azure OpenAI configured.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=<key> python generate.py --inference_server=\"openai_azure_chat:<deployment_name>:<base_url>:<api_version>\" --base_model=gpt-3.5-turbo --h2ocolors=False --langchain_mode=UserData\n```\n\n----------------------------------------\n\nTITLE: Listing Models with Gradio Client (Python)\nDESCRIPTION: This snippet shows how to use the Gradio client to retrieve the available models from the h2oGPT server and their maximum sequence lengths. It connects to the server, calls the `/model_names` API endpoint, parses the JSON response, and formats it into a dictionary of `base_model: max_seq_len`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom gradio_client import Client\nclient = Client('http://localhost:7860')\nimport ast\nres = client.predict(api_name='/model_names')\n{x['base_model']: x['max_seq_len'] for x in ast.literal_eval(res)}\n```\n\n----------------------------------------\n\nTITLE: Conda Environment Creation and Activation (Bash)\nDESCRIPTION: This snippet creates a new Conda environment named 'h2ogpt', activates it, installs Python 3.10, and verifies the Python installation. It is a crucial step in setting up the environment for h2oGPT.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n h2ogpt -y\nconda activate h2ogpt\nconda install python=3.10 -c conda-forge -y\npython --version  # should say python 3.10.xx\npython -c \"import os, sys ; print('hello world')\"  # should print \"hello world\"\n```\n\n----------------------------------------\n\nTITLE: Setting PIP Extra Index URL for CPU (Cmdline)\nDESCRIPTION: This snippet sets the PIP_EXTRA_INDEX_URL environment variable to specify the location for downloading PyTorch wheels for CPU. This ensures that the correct PyTorch version is installed for CPU usage.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md#_snippet_5\n\nLANGUAGE: cmdline\nCODE:\n```\nset PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cpu\n```\n\n----------------------------------------\n\nTITLE: Add Data to Existing Database (bash)\nDESCRIPTION: This snippet demonstrates how to add data to an existing database using the `--add_if_exists=True` flag with `src/make_db.py`.  It then runs `generate.py` to use the updated database. Requires `python` and the h2ogpt environment to be set up.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython src/make_db.py --add_if_exists=True\npython generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b --langchain_mode=UserData\n```\n\n----------------------------------------\n\nTITLE: Install Project Dependencies\nDESCRIPTION: This snippet lists the dependencies needed for the project. `xformers` version 0.0.20 is specified, while `tensorboard` and `neptune` require a minimum version of 2.13.0 and 1.2.0 respectively. These libraries support functionalities like model training and experiment logging.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_training.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n#xformers==0.0.20\n# optional for finetune\ntensorboard>=2.13.0\nneptune>=1.2.0\n```\n\n----------------------------------------\n\nTITLE: Summarizing Document with H2OGPT\nDESCRIPTION: This code snippet demonstrates how to summarize a document using H2OGPT, using `SUMMARIZE_MAP` langchain action.  It specifies the document to summarize, sets up prompts for summarization (`pre_prompt_summary`, `prompt_summary`), and calls the H2OGPT API via `client.predict`. The resulting summary is extracted from the response and printed.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ninstruction = \"What is the income eligibility criteria for the program?\"\ndocument_choice = \"user_path/CVRP-Implementation-Manual.pdf\"\nlangchain_action = LangChainAction.SUMMARIZE_MAP.value\nstream_output = False\ntop_k_docs = 5\n\npre_prompt_summary = \"\"\"In order to write a concise single-paragraph or bulleted list summary, pay attention to the following text\\n\"\"\"\nprompt_summary = \"Using only the text above, write a condensed and concise summary of key results as 5 bullet points:\\n\"\n#pre_prompt_summary = None\n#prompt_summary = None\n\n#pre_prompt_query = \"\"\"Pay attention and remember the information below, which will help to answer the question or imperative after the context ends.\\n\"\"\"\n#prompt_query = \"\"\"According to only the information in the document sources provided within the context above, \\n\"\"\"\npre_prompt_query = None\nprompt_query = None\n\nkwargs = dict(instruction=instruction,\n            langchain_mode=langchain_mode,\n            langchain_action=langchain_action,  # uses full document, not vectorDB chunks\n            top_k_docs=top_k_docs,\n            stream_output=stream_output,\n            document_subset='Relevant',\n            document_choice=document_choice,\n            max_new_tokens=1026,\n            max_time=360,\n            do_sample=False,\n            pre_prompt_query=pre_prompt_query,\n            prompt_query=prompt_query,\n            pre_prompt_summary=pre_prompt_summary,\n            prompt_summary=prompt_summary,\n            h2ogpt_key=H2OGPT_KEY\n            )\n\n# get result\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\nresponse = ast.literal_eval(res)['response']\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Git Installation (Bash)\nDESCRIPTION: This snippet installs Git using Conda. Git is required to clone the h2oGPT repository from GitHub.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge git\n```\n\n----------------------------------------\n\nTITLE: Image Understanding with OpenAI API in h2oGPT\nDESCRIPTION: This snippet demonstrates how to use the OpenAI API to perform image understanding with h2oGPT. It uses `img_to_base64` to encode images as base64 strings and then sends them to the API via a message with an 'image_url' content type. It requires the `openai` package and `src.vision.utils_vision.img_to_base64` function.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom src.vision.utils_vision import img_to_base64\n\n# local files would only work if server on same system as client\n# for img_to_base64, str_bytes=True or False will work.  True is for internal use for LLaVa gradio communication only\nurls = ['https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg',\n        img_to_base64('tests/driverslicense.jpeg'),\n        img_to_base64('tests/receipt.jpg'),\n        img_to_base64('tests/dental.png'),\n        ]\nexpecteds = ['tiger', 'license', 'receipt', ['Oral', 'Clinic']]\nfor expected, url in zip(expecteds, urls):\n    # OpenAI API\n    messages = [{\n        'role':\n            'user',\n        'content': [{\n            'type': 'text',\n            'text': 'Describe the image please',\n        }, {\n            'type': 'image_url',\n            'image_url': {\n                'url':\n                    url,\n            },\n        }],\n    }]\n\n\n\n    model = 'OpenGVLab/InternVL-Chat-V1-5'\n    base_url = 'http://localhost:5000/v1'\n    h2ogpt_key = 'fill or EMPTY'\n\n    from openai import OpenAI\n    client_args = dict(base_url=base_url,\n                       api_key=h2ogpt_key)\n    client = OpenAI(**client_args)\n\n    # auth:\n    # user = '%s:%s' % ('user', 'pass')\n    # no auth:\n    user = None\n\n    client_kwargs = dict(model=model,\n                         max_tokens=200,\n                         stream=False,\n                         messages=messages,\n                         user=user,\n                         )\n    response = client.chat.completions.create(**client_kwargs)\n    print(response)\n    if isinstance(expected, list):\n        assert any(x in response.choices[0].message.content for x in expected), \"%s %s\" % (url, response)\n    else:\n        assert expected in response.choices[0].message.content, \"%s %s\" % (url, response)\n```\n\n----------------------------------------\n\nTITLE: Helper dependencies for AutoGen\nDESCRIPTION: Lists helper libraries often used with AutoGen, including common data science and scientific computing packages like SymPy, Seaborn, scikit-learn, statsmodels, plotly, numpy, lightgbm, nltk, spacy, opencv, textblob, imageio, and bokeh. These packages provide tools for data analysis, visualization, and natural language processing.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_agents.txt#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nsympy\nseaborn\nscikit-learn\nstatsmodels\nplotly\nnumpy\nlightgbm\nnltk\nspacy\nopencv-python\nopencv-python-headless\ntextblob\nimageio\nbokeh\naltair\n```\n\n----------------------------------------\n\nTITLE: Gradio Client API Usage in Python\nDESCRIPTION: This snippet demonstrates how to use the Gradio client API to interact with the h2oGPT server. It creates a client instance, defines input parameters as a string of a dictionary for the `instruction_nochat` parameter, calls the `submit_nochat_api`, and prints the response.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom gradio_client import Client\nimport ast\n\nHOST_URL = \"http://localhost:7860\"\nclient = Client(HOST_URL)\n\n# string of dict for input\nkwargs = dict(instruction_nochat='Who are you?')\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\n\n# string of dict for output\nresponse = ast.literal_eval(res)['response']\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Streaming Speech-to-Text with httpx in Python\nDESCRIPTION: This snippet demonstrates how to perform streaming speech-to-text using httpx. It reads an audio file, constructs a multipart/form-data payload, and streams the audio data to the server.  The server sends back chunks of text.  The code extracts the text from the JSON chunks and accumulates it.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport httpx\nimport asyncio\n\nasync def stream_audio_transcription(file_path, model=\"default-model\"):\n    url = \"http://0.0.0.0:5000/v1/audio/transcriptions\"\n    headers = {\"X-API-KEY\": \"your-api-key\"}\n\n    # Read the audio file\n    with open(file_path, \"rb\") as f:\n\n        # Create the multipart/form-data payload\n        files = {\n            \"file\": (\"audio.wav\", f, \"audio/wav\"),\n            \"model\": (None, model),\n            \"stream\": (None, \"true\"),  # Note the lowercase \"true\" as the server checks for this\n            \"response_format\": (None, \"text\"),\n            \"chunk\": (None, \"none\"),\n        }\n\n        text = ''\n        async with httpx.AsyncClient() as client:\n            async with client.stream(\"POST\", url, headers=headers, files=files, timeout=120) as response:\n                async for line in response.aiter_lines():\n                    # Process each chunk of data as it is received\n                    if line.startswith(\"data:\"):\n                        try:\n                            # Remove \"data: \" prefix and strip any newlines or trailing whitespace\n                            json_data = json.loads(line[5:].strip())\n                            # Process the parsed JSON data\n                            print('json_data: %s' % json_data)\n                            text += json_data[\"text\"]\n                        except json.JSONDecodeError as e:\n                            print(\"Error decoding JSON:\", e)\n        return text\n# Run the client function\nfinal_text = asyncio.run(stream_audio_transcription(\"/home/jon/h2ogpt/tests/test_speech.wav\"))\nprint(final_text)\n```\n\n----------------------------------------\n\nTITLE: Querying Collection with H2OGPT\nDESCRIPTION: This code snippet demonstrates how to query a collection of documents using H2OGPT. It sets the instruction, action, stream output and top_k_docs parameters, and uses the `client.predict` method to query the H2OGPT API, passing the parameters as a dictionary. The result is then extracted from the API response and printed.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ninstruction = \"What is the income eligibility criteria for the Clean Vehicle Rebate Project in the state of California?\"\nlangchain_action = LangChainAction.QUERY.value\nstream_output = False\ntop_k_docs = 5\n\n#pre_prompt_summary = \"\"\"In order to write a concise single-paragraph or bulleted list summary, pay attention to the following text\\n\"\"\"\n#prompt_summary = \"Using only the text above, write a condensed and concise summary of key results as 5 bullet points:\\n\"\npre_prompt_summary = None\nprompt_summary = None\n\npre_prompt_query = \"\"\"Pay attention and remember the information below, which will help to answer the question or imperative after the context ends.\\n\"\"\"\nprompt_query = \"\"\"According to only the information in the document sources provided within the context above, \\n\"\"\"\n#pre_prompt_query = None\n#prompt_query = None\n\nkwargs = dict(instruction=instruction,\n            langchain_mode=langchain_mode,\n            langchain_action=langchain_action,  # uses full document, not vectorDB chunks\n            top_k_docs=top_k_docs,\n            stream_output=stream_output,\n            document_subset='Relevant',\n            # document_choice=document_choice,\n            max_new_tokens=1026,\n            max_time=360,\n            do_sample=False,\n            pre_prompt_query=pre_prompt_query,\n            prompt_query=prompt_query,\n            pre_prompt_summary=pre_prompt_summary,\n            prompt_summary=prompt_summary,\n            h2ogpt_key=H2OGPT_KEY\n            )\n\n# get result\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\nresponse = ast.literal_eval(res)['response']\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Kill Processes\nDESCRIPTION: This snippet provides an option to kill running processes related to generate, frpc_linux_amd and ngrok, using pkill command. This can be helpful if the server needs to be restarted or if there are conflicts.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_CPU.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# can kill old ngrok + generate and try again, or just restart entire runtime + run all cells\ndo_kill = False\nif do_kill:\n  !pkill -f generate --signal 9\n  !pkill -f frpc_linux_amd --signal 9\n  !pkill -f ngrok --signal 9\n```\n\n----------------------------------------\n\nTITLE: Build Triton Docker Image with FasterTransformer\nDESCRIPTION: This script builds a Docker image for Triton Inference Server with the FasterTransformer backend. It clones the necessary repositories, sets environment variables, and uses the Docker build command to create the image.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/TRITON.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/triton-inference-server/fastertransformer_backend.git\ncd fastertransformer_backend\ngit clone https://github.com/NVIDIA/FasterTransformer.git\nexport WORKSPACE=$(pwd)\nexport CONTAINER_VERSION=22.12\nexport TRITON_DOCKER_IMAGE=triton_with_ft:${CONTAINER_VERSION}\ndocker build --rm   \\\n    --build-arg TRITON_VERSION=${CONTAINER_VERSION}   \\\n    -t ${TRITON_DOCKER_IMAGE} \\\n    -f docker/Dockerfile \\\n    .\n```\n\n----------------------------------------\n\nTITLE: Hiding Login Tab using CLI\nDESCRIPTION: This command demonstrates how to hide the login tab within the h2oGPT user interface. It is achieved by setting the `--visible_login_tab` command-line argument to `False` during the application launch.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_ui.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n--visible_login_tab=False\n```\n\n----------------------------------------\n\nTITLE: Removing h2oGPT Header using CLI\nDESCRIPTION: This code snippet demonstrates how to remove the h2oGPT header (logo, links, and QR code) using command-line arguments. This is achieved by setting the `--visible_h2ogpt_logo`, `--visible_h2ogpt_links`, and `--visible_h2ogpt_qrcode` flags to `False`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_ui.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n--visible_h2ogpt_logo=False --visible_h2ogpt_links=False --visible_h2ogpt_qrcode=False\n```\n\n----------------------------------------\n\nTITLE: Adding Remote File to H2OGPT Collection Python\nDESCRIPTION: This snippet adds a remotely hosted file (on the Gradio server) to the H2OGPT collection. It utilizes the `client.predict` function with the `/add_file_api` endpoint, enabling chunking and specifying parameters like chunk size and the H2OGPT key for authentication and authorization.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nchunk = True\nchunk_size = 512\nh2ogpt_key = H2OGPT_KEY\nres = client.predict(test_file_server,\n                        langchain_mode, chunk, chunk_size, True,\n                        None, None, None, None,\n                        h2ogpt_key,\n                        api_name='/add_file_api')\n```\n\n----------------------------------------\n\nTITLE: Specifying AutoAWQ Kernels Dependency\nDESCRIPTION: This snippet specifies the autoawq-kernels dependency, which provides optimized kernels for AutoAWQ, enhancing its performance.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_gpu_only.txt#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nautoawq-kernels\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT with LLaMa.cpp on GPU\nDESCRIPTION: This command executes the `generate.py` script with specified parameters to utilize LLaMa.cpp with a Hugging Face model on a GPU. It sets the base model, prompt type, and user path. Ensuring that GPU usage is reflected in the logs is crucial for verifying proper setup.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=HuggingFaceH4/zephyr-7b-beta --prompt_type=zephyr --score_model=None --user_path=user_path\n```\n\n----------------------------------------\n\nTITLE: Run generate.py with UserData DB\nDESCRIPTION: This bash script demonstrates how to run the generate.py script in h2ogpt with the previously created UserData database. It mounts various directories, including the database directory, user data directory, and directories for saving generated content, into the Docker container.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p ~/.cache\nmkdir -p ~/save\nmkdir -p ~/user_path\nmkdir -p ~/db_dir_UserData\nmkdir -p ~/users\nmkdir -p ~/db_nonusers\nmkdir -p ~/llamacpp_path\ndocker run \\\n       --gpus '\"device=0\"' \\\n       --runtime=nvidia \\\n       --shm-size=2g \\\n       -p 7860:7860 \\\n       --rm --init \\\n       --network host \\\n       -v /etc/passwd:/etc/passwd:ro \\\n       -v /etc/group:/etc/group:ro \\\n       -u `id -u`:`id -g` \\\n       -v \"${HOME}\"/.cache:/workspace/.cache \\\n       -v \"${HOME}\"/save:/workspace/save \\\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\n       -v \"${HOME}\"/users:/workspace/users \\\n       -v \"${HOME}\"/db_nonusers:/workspace/db_nonusers \\\n       -v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path \\\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 /workspace/generate.py \\\n          --base_model=h2oai/h2ogpt-4096-llama2-7b-chat \\\n          --use_safetensors=True \\\n          --prompt_type=llama2 \\\n          --save_dir='/workspace/save/' \\\n          --use_gpu_id=False \\\n          --score_model=None \\\n          --max_max_new_tokens=2048 \\\n          --max_new_tokens=1024 \\\n          --langchain_mode=LLM\n```\n\n----------------------------------------\n\nTITLE: Specifying mwparserfromhell Dependency\nDESCRIPTION: This snippet specifies the required version of the mwparserfromhell library. It indicates that version 0.6.4 or greater is needed for the wiki to database conversion process.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_wikiprocessing.txt#_snippet_1\n\nLANGUAGE: TEXT\nCODE:\n```\nmwparserfromhell>=0.6.4\n```\n\n----------------------------------------\n\nTITLE: h2oGPT API Call with Image URL (Python)\nDESCRIPTION: This snippet shows how to call the h2oGPT API to describe an image using a URL. It constructs a dictionary of arguments, including the API key, model, instruction, image URL, and other parameters, and then passes this dictionary to the `client.predict` method to get a response from the API.  The response is then parsed to extract the text description.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nh2ogpt_key = 'api key here, or EMPTY if no key or do not put in kwargs'\n\nkwargs = dict(\n    visible_models='THUDM/cogvlm2-llama3-chat-19B',\n    instruction_nochat=\"describe the imaged\",\n    h2ogpt_key=h2ogpt_key,\n    stream_output=False,\n    image_file='https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg',\n    temperature=0,\n    max_tokens=4000)\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\n\nresponse = ast.literal_eval(res)['response']\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Dependencies for Google Search\nDESCRIPTION: Specifies the google-search-results package version required for Google search functionality within the h2ogpt project. This package likely allows the application to query and process results from Google Search.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_agents.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ngoogle-search-results>=2.4.2\n```\n\n----------------------------------------\n\nTITLE: Summarizing Single Document via API Python\nDESCRIPTION: This snippet demonstrates how to summarize a single document using the H2OGPT API. It constructs a dictionary of parameters including the document choice, langchain mode, action, prompts, and H2OGPT key. Then, it sends the parameters to the `/submit_nochat_api` to get the summary of the provided document.  The parameters control how the document is processed and summarized, including prompts, top_k_docs, and output settings.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ninstruction = None\ndocument_choice = \"user_path/terms-and-conditions.pdf\"\n\nlangchain_action = LangChainAction.SUMMARIZE_MAP.value\nstream_output = False\ntop_k_docs = 5\n\npre_prompt_summary = \"\"\"In order to write a concise single-paragraph or bulleted list summary, pay attention to the following text\\n\"\"\"\nprompt_summary = \"Using only the text above, write a condensed and concise summary of key results as 5 bullet points:\\n\"\n\npre_prompt_query = None\nprompt_query = None\n\nkwargs = dict(instruction=instruction,\n            langchain_mode=langchain_mode,\n            langchain_action=langchain_action,  # uses full document, not vectorDB chunks\n            top_k_docs=top_k_docs,\n            stream_output=stream_output,\n            document_subset='Relevant',\n            document_choice=document_choice,\n            max_new_tokens=256,\n            max_time=360,\n            do_sample=False,\n            pre_prompt_query=pre_prompt_query,\n            prompt_query=prompt_query,\n            pre_prompt_summary=pre_prompt_summary,\n            prompt_summary=prompt_summary,\n            h2ogpt_key=H2OGPT_KEY\n            )\n\n# get result\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\nresponse = ast.literal_eval(res)['response']\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Specifying ONNX Runtime GPU Dependency\nDESCRIPTION: This snippet specifies the exact version of the onnxruntime-gpu library required, which is 1.15.0. ONNX Runtime provides GPU-accelerated execution for ONNX models, commonly used in machine learning inference.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_gpu_only.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nonnxruntime-gpu==1.15.0\n```\n\n----------------------------------------\n\nTITLE: Downloading Tokenizers (HF Inference/OpenAI)\nDESCRIPTION: This code downloads tokenizers for Hugging Face text generation inference server and OpenAI (gpt-3.5-turbo) using the `tiktoken` library. It retrieves encodings for `cl100k_base` and `gpt-3.5-turbo`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport tiktoken\nencoding = tiktoken.get_encoding(\"cl100k_base\")\nencoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n```\n\n----------------------------------------\n\nTITLE: Install Git LFS (Ubuntu)\nDESCRIPTION: This command installs Git LFS (Large File Storage) on Ubuntu systems. Git LFS is required to properly download models from Hugging Face.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/TRITON.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install git-lfs\n```\n\n----------------------------------------\n\nTITLE: Run h2oGPT with Azure OpenAI\nDESCRIPTION: Runs h2oGPT using Azure OpenAI for inference. This command configures h2oGPT to use Azure OpenAI, specifying the deployment name, base URL, and API version.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=<key> python generate.py --inference_server=\"openai_azure_chat:<deployment_name>:<base_url>:<api_version>\" --base_model=gpt-3.5-turbo --h2ocolors=False --langchain_mode=UserData\n```\n\n----------------------------------------\n\nTITLE: Disabling h2oGPT Telemetry via Command Line\nDESCRIPTION: This snippet shows how to disable h2oGPT telemetry by setting the `H2OGPT_ENABLE_HEAP_ANALYTICS` environment variable to `False` or by passing the `--enable-heap-analytics=False` flag during execution. This prevents the application from tracking UI element clicks.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --enable-heap-analytics=False ...\n```\n\n----------------------------------------\n\nTITLE: Set CUDA Environment Variables\nDESCRIPTION: Sets the CUDA environment variables in the user's `.bashrc` file. This includes CUDA_HOME, LD_LIBRARY_PATH, and PATH. These variables are necessary for the system to locate the CUDA libraries and binaries.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\necho 'export CUDA_HOME=/usr/local/cuda-12.1' >> $HOME/.bashrc\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_HOME/lib64:$CUDA_HOME/extras/CUPTI/lib64' >> $HOME/.bashrc\necho 'export PATH=$PATH:$CUDA_HOME/bin' >> $HOME/.bashrc\n```\n\n----------------------------------------\n\nTITLE: Checking package requirements with pip\nDESCRIPTION: This snippet runs `python -m pip check` to verify that all installed packages and their dependencies are compatible. It helps identify any broken requirements after installing h2oGPT.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WHEEL.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip check\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT Offline (Smart Download)\nDESCRIPTION: These commands demonstrate running h2oGPT in offline mode after downloading the model online. `TRANSFORMERS_OFFLINE=1` ensures that the application does not attempt to download models from the internet. The examples show how to specify the model either using `--base_model` with the local filename or using `--base_model=llama` and `--model_path_llama` for Llama-based models.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# online do:\npython generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --prompt_type=zephyr --max_seq_len=4096 --add_disk_models_to_ui=False\n# Then use h2oGPT as might normally for any tasks.\n# Once offline do:\nTRANSFORMERS_OFFLINE=1 python generate.py --base_model=zephyr-7b-beta.Q5_K_M.gguf --prompt_type=zephyr --gradio_offline_level=2 --share=False --add_disk_models_to_ui=False\n# or:\nTRANSFORMERS_OFFLINE=1 python generate.py --base_model=llama --model_path_llama=zephyr-7b-beta.Q5_K_M.gguf --prompt_type=zephyr --gradio_offline_level=2 --share=False --add_disk_models_to_ui=False\n# or if choosing in UI do (be sure to choose correct prompt_type too):\nTRANSFORMERS_OFFLINE=1 python generate.py --gradio_offline_level=2 --share=False --add_disk_models_to_ui=False\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies\nDESCRIPTION: This snippet defines the required Python dependencies for the project, specifying the minimum versions of playwright, selenium, html2text, and bs4. These packages are crucial for certain functionalities and address potential issues outlined in Issue #320.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_langchain.urls.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nplaywright>=1.37.0\n# requires Chrome binary to be in path\nselenium>=4.11.2\nhtml2text>=2020.1.16\nbs4>=0.0.1\n```\n\n----------------------------------------\n\nTITLE: Image Generation using OpenAI Client in Python\nDESCRIPTION: This snippet generates an image using the OpenAI client. It sends a prompt to the API and retrieves the image data in base64 format. The image is then decoded, loaded using PIL, and saved to a file.  Requires h2oGPT to be loaded with image generation models.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI(base_url='http://0.0.0.0:5000/v1')\n# client = OpenAI()\n\nresponse = client.images.generate(\n  model=\"sdxl_turbo\",  # should be empty if do not know which model, h2oGPT will choose first if exists\n  prompt=\"A cute baby sea otter\",\n  n=1,\n  size=\"1024x1024\",\n  response_format='b64_json',\n)\nimport base64\nimage_data = base64.b64decode(response.data[0].b64_json.encode('utf-8'))\n# Convert binary data to an image\nfrom PIL import Image\nimport io\nimage = Image.open(io.BytesIO(image_data))\n# Save the image to a file or display it\nimage.save('output_image.png')\nimage.show()  # This will open the default image viewer and display the image\n```\n\n----------------------------------------\n\nTITLE: Fix Triton Preprocessing Configuration\nDESCRIPTION: This script fixes a typo in the Triton preprocessing configuration file using `sed`.  It corrects the 'postprocessing' entry to 'preprocessing'.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/TRITON.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsed -i -e 's@postprocessing@preprocessing@' all_models/gptneox/preprocessing/config.pbtxt\n```\n\n----------------------------------------\n\nTITLE: Set PIP Extra Index URL for CUDA 11.8\nDESCRIPTION: Sets the PIP_EXTRA_INDEX_URL environment variable to specify additional package repositories for PyTorch and AutoGPTQ built for CUDA 11.8. This allows pip to find the correct versions of these packages.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu118 https://huggingface.github.io/autogptq-index/whl/cu118\"\n```\n\n----------------------------------------\n\nTITLE: Installing Training Dependencies\nDESCRIPTION: This pip command installs the optional training dependencies required for fine-tuning h2oGPT. It reads the requirements from the specified text file.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/FINETUNE.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -r reqs_optional/requirements_optional_training.txt\n```\n\n----------------------------------------\n\nTITLE: Push Image Caption Model with OpenAI (bash)\nDESCRIPTION: This snippet extends the OpenAI embedding example by including an image caption model (`microsoft/Florence-2-large`). It requires sufficient GPU memory or the use of OpenAI. Requires `python`, the h2ogpt environment, and an OpenAI API key to be configured.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py  --inference_server=openai_chat --base_model=gpt-3.5-turbo --score_model=None --langchain_mode=LLM --langchain_modes=\"['LLM', 'UserData', 'MyData']\" --captions_model=microsoft/Florence-2-large\n```\n\n----------------------------------------\n\nTITLE: Extend sudo Timeout\nDESCRIPTION: Extends the sudo timeout to avoid repeated password prompts during installation. It edits the sudoers file to set a longer timeout period (60 minutes in this case).\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo visudo\n```\n\nLANGUAGE: bash\nCODE:\n```\nDefaults        timestamp_timeout=60\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo bash\nexit\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT Offline with Absolute Path\nDESCRIPTION: This snippet shows how to run h2oGPT with an absolute path to the base model. It includes specifying the prompt type when an absolute path to the base model is used. It also demonstrates setting various parameters such as inference server, score model, langchain mode, user path, authentication token, sequence length, maximum new tokens, concurrency count, batch size, and UI settings.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --inference_server=\"vllm:0.0.0.0:5000\" --base_model='$HOME/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-chat-hf/snapshots/c2f3ec81aac798ae26dcc57799a994dfbf521496' --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048 --concurrency_count=64 --batch_size=16 --prompt_type=llama2 --add_disk_models_to_ui=False\n```\n\n----------------------------------------\n\nTITLE: Running GPT4ALL Model (llama)\nDESCRIPTION: This command runs `generate.py` with a GPT4ALL-compatible LLaMa model, specified by the `model_name_gpt4all_llama` parameter.  It uses the `gpt4all_llama` base model, disables the score model, enables `UserData` langchain mode, and processes documents located in the `user_path`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CPU.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=gpt4all_llama --model_name_gpt4all_llama=ggml-wizardLM-7B.q4_2.bin --score_model=None --langchain_mode='UserData' --user_path=user_path\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT Tests in Bash\nDESCRIPTION: This bash script demonstrates how to run h2oGPT tests. It installs necessary dependencies like requirements-parser, pytest-instafail, pytest-random-order and playsound, installs gst-python using conda, installs gstreamer using apt-get and pygame using pip. Finally, it executes the pytest command with specific parameters for running tests, and optionally runs the openai server test on a local server.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install requirements-parser pytest-instafail pytest-random-order playsound==1.3.0\nconda install -c conda-forge gst-python -y\nsudo apt-get install gstreamer-1.0\npip install pygame\nGPT_H2O_AI=0 CONCURRENCY_COUNT=1 pytest --instafail -s -v tests\n# for openai server test on already-running local server\npytest -s -v -n 4 openai_server/test_openai_server.py::test_openai_client\n```\n\n----------------------------------------\n\nTITLE: Running LLaMa-2-70B-chat-GPTQ with exllama and auto_map\nDESCRIPTION: This command runs the LLaMa-2-70B-chat-GPTQ model using exllama across multiple GPUs. It loads the model, disables GPU ID usage, specifies the GPTQ model file, and sets the `set_auto_map` parameter in `exllama_dict` to distribute memory across two GPUs, each using 20GB.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=TheBloke/Llama-2-70B-chat-GPTQ --load_exllama=True --use_safetensors=True --use_gpu_id=False --load_gptq=main --prompt_type=llama2 --exllama_dict=\"{'set_auto_map':'20,20'}\"\n```\n\n----------------------------------------\n\nTITLE: Changing to h2ogpt Directory\nDESCRIPTION: This snippet changes the current directory to the cloned h2ogpt repository. This is a necessary step to execute further commands within the h2ogpt environment. It uses the standard cd command.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/blog/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ncd h2ogpt\n```\n\n----------------------------------------\n\nTITLE: Configuring ngrok Tunnel\nDESCRIPTION: Installs the pyngrok library, prompts the user for their ngrok authtoken, configures ngrok with the provided token, and opens an HTTP tunnel to the specified port (7860).  It then prints the public URL provided by ngrok to access the h2oGPT server remotely.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_GPU.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Sign-up for free ngrok account using (e.g.) your Google email/login and get token: https://dashboard.ngrok.com/get-started/setup\n\n!pip install pyngrok\nimport getpass\nfrom pyngrok import ngrok, conf\n\nprint(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\nconf.get_default().auth_token = getpass.getpass()\n\n# Open an http ngrok tunnel\nconnection_string = ngrok.connect(7860, \"http\").public_url\nprint(\"Once server is up and says Running on local URL:  http://0.0.0.0:7860, click on this link, then click on Visit Site: %s\" % connection_string)\n```\n\n----------------------------------------\n\nTITLE: Dependencies for Microsoft Bing API\nDESCRIPTION: Lists the Microsoft Bing Search SDK packages, enabling interaction with various Bing search APIs, including web, visual, video, image, and news search.  It includes general Azure dependencies used by Bing SDK.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_agents.txt#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nmsrest\nazure-core\nazure-common\nmsrestazure\nmicrosoft-bing-websearch\nmicrosoft-bing-visualsearch\nmicrosoft-bing-videosearch\nmicrosoft-bing-imagesearch\nmicrosoft-bing-newssearch\nmicrosoft-bing-customimagesearch\nmicrosoft-bing-customwebsearch\n```\n\n----------------------------------------\n\nTITLE: Running vLLM server\nDESCRIPTION: This command starts a vLLM inference server using the OpenAI API endpoint. It configures the port, host, model, tensor parallel size, and quantization method. It also sets the download directory for Hugging Face models.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npython -m vllm.entrypoints.openai.api_server \\\n        --port=5000 \\\n        --host=0.0.0.0 \\\n        --model=h2oai/h2ogpt-4096-llama2-70b-chat-4bit \\\n        --tensor-parallel-size=2 \\\n        --seed 1234 \\\n        --trust-remote-code \\\n\t    --max-num-batched-tokens 8192 \\\n\t    --quantization awq \\\n        --download-dir=/$HOME/.cache/huggingface/hub\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Logs Command\nDESCRIPTION: This bash command displays the logs of the h2ogpt application managed by Docker Compose. The `-f` flag follows the logs in real-time, allowing for continuous monitoring.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose logs -f\n```\n\n----------------------------------------\n\nTITLE: Text-to-Speech Streaming and Playback with OpenAI API in h2oGPT\nDESCRIPTION: This snippet streams audio from h2oGPT via the OpenAI API and plays it in real-time using httpx and pygame.  It initializes pygame mixer, streams the audio using httpx, converts it to a format usable by pygame, and plays the audio. It ensures that resources are cleared after playback.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport openai\nimport httpx\nimport pygame\n\nimport pygame.mixer\n\npygame.mixer.init(frequency=16000, size=-16, channels=1)\n\nsound_queue = []\n\n\ndef play_audio(audio):\n    import io\n    from pydub import AudioSegment\n\n    sr = 16000\n    s = io.BytesIO(audio)\n    channels = 1\n    sample_width = 2\n\n    audio = AudioSegment.from_raw(s, sample_width=sample_width, frame_rate=sr, channels=channels)\n    sound = pygame.mixer.Sound(io.BytesIO(audio.raw_data))\n    sound_queue.append(sound)\n    sound.play()\n\n    # Wait for the audio to finish playing\n    duration_ms = sound.get_length() * 1000  # Convert seconds to milliseconds\n    pygame.time.wait(int(duration_ms))\n\n\n# Ensure to clear the queue when done to free memory and resources\ndef clear_queue(sound_queue):\n    for sound in sound_queue:\n        sound.stop()\n\n\napi_key = 'EMPTY'\n\n# Initialize OpenAI and Pygame\nclient = openai.OpenAI(api_key=api_key)\n```\n\n----------------------------------------\n\nTITLE: Streaming Gradio Client API Usage in Python\nDESCRIPTION: This code demonstrates how to stream responses from a Gradio application using the Gradio client. It submits a request to the `/submit_nochat_api` with `stream_output=True`, then continuously checks for updates. As new text fragments become available, they are printed to the console.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom gradio_client import Client\nimport ast\nimport time\n\nHOST = 'http://localhost:7860'\nclient = Client(HOST)\napi_name = '/submit_nochat_api'\nprompt = \"Who are you?\"\nkwargs = dict(instruction_nochat=prompt, stream_output=True)\n\njob = client.submit(str(dict(kwargs)), api_name=api_name)\n\ntext_old = ''\nwhile not job.done():\n    outputs_list = job.communicator.job.outputs\n    if outputs_list:\n        res = job.communicator.job.outputs[-1]\n        res_dict = ast.literal_eval(res)\n        text = res_dict['response']\n        new_text = text[len(text_old):]\n        if new_text:\n            print(new_text, end='', flush=True)\n            text_old = text\n        time.sleep(0.01)\n# handle case if never got streaming response and already done\nres_final = job.outputs()\nif len(res_final) > 0:\n    res = res_final[-1]\n    res_dict = ast.literal_eval(res)\n    text = res_dict['response']\n    new_text = text[len(text_old):]\n    print(new_text)\n```\n\n----------------------------------------\n\nTITLE: Dependencies for SVG support\nDESCRIPTION: Includes libraries for handling Scalable Vector Graphics (SVG) files.  These packages likely enable the h2ogpt project to render and manipulate SVG images.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_agents.txt#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nsvglib\ncairosvg\n```\n\n----------------------------------------\n\nTITLE: Reinstalling flash_attn, autoawq, autoawq-kernels (bash)\nDESCRIPTION: This snippet uninstalls and reinstalls the flash_attn, autoawq, and autoawq-kernels packages. This is often necessary to resolve compatibility issues or broken installations that can lead to undefined symbol errors. The `--no-cache-dir` option ensures that the latest versions are downloaded.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\npip uninstall flash_attn autoawq autoawq-kernels -y && pip install flash_attn autoawq autoawq-kernels --no-cache-dir\n```\n\n----------------------------------------\n\nTITLE: LLaVA Installation from Wheel\nDESCRIPTION: Installs the `llava` package from a specific wheel file URL.  This ensures the correct version is used. Version `1.7.0.dev0` of llava is installed.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_langchain.txt#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nllava @ https://h2o-release.s3.amazonaws.com/h2ogpt/llava-1.7.0.dev0-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Specifying Faiss-GPU Dependency\nDESCRIPTION: This code snippet specifies the minimum version of the faiss-gpu library required for the h2ogpt project. Faiss is a library for efficient similarity search and clustering of dense vectors, optimized for GPU usage in this case.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_gpu_only.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nfaiss-gpu>=1.7.2\n```\n\n----------------------------------------\n\nTITLE: Download CUDA Toolkit for CUDA 12.1 (Repeated)\nDESCRIPTION: Downloads and installs the CUDA Toolkit 12.1.  This is a repeat of the earlier step. It is important for GPU based compilation for libraries such as llama-cpp-python.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nwget https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda_12.1.1_530.30.02_linux.run\nsudo sh cuda_12.1.1_530.30.02_linux.run\n```\n\n----------------------------------------\n\nTITLE: Running LLaMa.cpp Model with Low Memory\nDESCRIPTION: This command runs the `generate.py` script for LLaMa.cpp models on systems with limited RAM or slow CPUs. It sets `use_mlock` to `False` and `n_batch` to 256 within the `llamacpp_dict`, reduces `max_seq_len` to 512, disables the score model, activates `UserData` langchain mode, and points to the `user_path` directory for document processing.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CPU.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=llama --model_path_llama=https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf --llamacpp_dict=\"{'use_mlock':False,'n_batch':256}\" --max_seq_len=512 --score_model=None --langchain_mode='UserData' --user_path=user_path\n```\n\n----------------------------------------\n\nTITLE: Running the h2ogpt Chatbot\nDESCRIPTION: This snippet runs the h2ogpt chatbot using the `generate.py` script. It specifies the base model to use for the chatbot. This command starts the server that hosts the chatbot, allowing users to interact with the LLM.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/blog/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npython generate.py --base_model=h2oai/h2ogpt-oig-oasst1-256-6_9b\n```\n\n----------------------------------------\n\nTITLE: Programming in Fortran on IBM 1401\nDESCRIPTION: The author describes their initial attempts to program in Fortran on an IBM 1401 using punch cards. The code snippets would involve simple mathematical calculations or data processing tasks, executed by loading the program into memory from the card reader and printing the results on a loud printer. The limited input options (punched cards) posed a constraint, making it difficult to create meaningful programs.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/tests/1paul_graham.txt#_snippet_0\n\nLANGUAGE: Fortran\nCODE:\n```\nPROGRAM EXAMPLE\n  INTEGER I\n  REAL PI\n  PI = 3.14159\n  I = 1\n  10 CONTINUE\n    PRINT *, 'Approximation of PI: ', PI\n    I = I + 1\n    IF (I .LT. 10) GOTO 10\n  STOP\nEND\n```\n\n----------------------------------------\n\nTITLE: Question Answering for Single Document Python\nDESCRIPTION: This snippet performs question answering on a single document using the H2OGPT API. It constructs a dictionary of parameters, including the question (`instruction`), document choice, langchain mode and action, and prompts.  It sends the parameters as a string to the `/submit_nochat_api`, retrieves, and prints the answer to the specified question based on the selected document.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ninstruction = \"What is the eligibility criteria for the program?\"\ndocument_choice = \"user_path/terms-and-conditions.pdf\"\n\nlangchain_action = LangChainAction.SUMMARIZE_MAP.value\nstream_output = False\ntop_k_docs = 5\n\npre_prompt_summary = \"\"\"In order to write a concise single-paragraph or bulleted list summary, pay attention to the following text\\n\"\"\"\nprompt_summary = \"Using only the text above, write a condensed and concise summary of key results as 5 bullet points:\\n\"\n\n# pre_prompt_query = \"\"\"Pay attention and remember the information below, which will help to answer the question or imperative after the context ends.\\n\"\"\"\n# prompt_query = \"\"\"According to only the information in the document sources provided within the context above, \\n\"\"\"\npre_prompt_query = None\nprompt_query = None\n\nkwargs = dict(instruction=instruction,\n            langchain_mode=langchain_mode,\n            langchain_action=langchain_action,  # uses full document, not vectorDB chunks\n            top_k_docs=top_k_docs,\n            stream_output=stream_output,\n            document_subset='Relevant',\n            document_choice=document_choice,\n            max_new_tokens=256,\n            max_time=360,\n            do_sample=False,\n            pre_prompt_query=pre_prompt_query,\n            prompt_query=prompt_query,\n            pre_prompt_summary=pre_prompt_summary,\n            prompt_summary=prompt_summary,\n            h2ogpt_key=H2OGPT_KEY\n            )\n\n# get result\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\nresponse = ast.literal_eval(res)['response']\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Declaring onnxruntime dependency\nDESCRIPTION: Specifies the onnxruntime dependency with a specific version of 1.15.0. Onnxruntime is used for running machine learning models in the ONNX format. This dependency ensures compatibility for unstructured data processing within the h2ogpt project.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_cpu_only.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nonnxruntime==1.15.0\n```\n\n----------------------------------------\n\nTITLE: CUDA Installation (Bash)\nDESCRIPTION: This snippet installs the CUDA toolkit version 11.8 using Conda.  The CUDA_HOME environment variable is also set for subsequent steps in the installation.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install cudatoolkit=11.8 -c conda-forge -y\nset CUDA_HOME=$CONDA_PREFIX\n```\n\n----------------------------------------\n\nTITLE: Persist Collection without Authentication (bash)\nDESCRIPTION: This snippet demonstrates how to ensure a collection is persisted even without authentication by setting its type to 'shared'.  It utilizes `generate.py` and specifies langchain modes and types. Requires `python` and the h2ogpt environment to be set up.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model='llama' --prompt_type=llama2 --score_model=None --max_max_new_tokens=2048 --max_new_tokens=1024 \\\n       --visible_tos_tab=False --visible_hosts_tab=False --visible_models_tab=False \\\n       --langchain_modes=\"['LLM','PersistData']\" --langchain_mode=PersistData \\\n       --langchain_mode_types=\"{'PersistData':'shared'}\" \\\n       --top_k_docs=-1 --max_time=360 --save_dir=save \\\n       --model_path_llama=https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf \\\n       --max_seq_len=4096\n```\n\n----------------------------------------\n\nTITLE: Text-to-Speech (TTS) with OpenAI API in h2oGPT (Non-Streaming)\nDESCRIPTION: This snippet demonstrates using the OpenAI API for text-to-speech with h2oGPT, without streaming.  It saves the audio to a file.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\n    client = OpenAI(base_url='http://0.0.0.0:5000/v1')\n\n    response = client.audio.speech.create(\n            model=\"tts-1\",\n            voice=\"\",\n            extra_body=dict(stream=False,\n                            chatbot_role=\"Female AI Assistant\",\n                            speaker=\"SLT (female)\",\n                            format='wav',\n                            ),\n            input=\"Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! Today is a wonderful day to build something people love! \",\n    )\n    response.stream_to_file(\"speech_local2.wav\")\n```\n\n----------------------------------------\n\nTITLE: Multiple Embeddings and Sources - make_db.py (bash)\nDESCRIPTION: This snippet demonstrates creating separate databases for different embeddings and source folders using `src/make_db.py`. It specifies different collection names, langchain types, and embedding models for each database. Requires `python` and the h2ogpt environment to be set up.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython src/make_db.py --user_path=user_path --collection_name=UserData --langchain_type=shared --hf_embedding_model=BAAI/bge-large-en-v1.5\npython src/make_db.py --user_path=user_path2 --collection_name=UserData2 --langchain_type=shared --hf_embedding_model=sentence-transformers/all-MiniLM-L6-v2\n```\n\n----------------------------------------\n\nTITLE: Add Module Collection Mode to Spec File\nDESCRIPTION: This code block is added to the Analysis() section of the spec file (h2ogpt-osx-m1-cpu.spec or h2ogpt-osx-m1-gpu.spec) to explicitly instruct PyInstaller to collect all .py modules from the specified dependencies (gradio, gradio_pdf).\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/dev_installers/mac/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodule_collection_mode={\n    'gradio' : 'py',\n    'gradio_pdf' : 'py',\n},\n```\n\n----------------------------------------\n\nTITLE: Specifying Auto-GPTQ Dependency\nDESCRIPTION: This snippet defines the minimum version of the auto-gptq library as 0.7.1.  Auto-GPTQ is a library for quantizing large language models, typically used to reduce their size and improve inference speed.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_gpu_only.txt#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nauto-gptq>=0.7.1\n```\n\n----------------------------------------\n\nTITLE: Download CUDA Toolkit for CUDA 12.1\nDESCRIPTION: Downloads and installs the CUDA Toolkit 12.1 on Ubuntu 22. This is required for GPU support in h2oGPT. The user only needs to install the toolkit.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda_12.1.1_530.30.02_linux.run\nsudo sh cuda_12.1.1_530.30.02_linux.run\n```\n\n----------------------------------------\n\nTITLE: Generate Spec File for MacOS Installer (Debug Mode)\nDESCRIPTION: Generates a spec file using `pyi-makespec` for building a MacOS installer in debug mode.  It specifies hidden imports, collects all modules from various dependencies, copies metadata, and adds data files. Replace `--name` based on the desired CPU or MPS support.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/dev_installers/mac/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd h2ogpt\npyi-makespec mac_run_app.py -F --name=h2ogpt-osx-m1-cpu \\\n  --hidden-import=h2ogpt \\\n  --collect-all=h2ogpt \\\n  --recursive-copy-metadata=transformers \\\n  --collect-data=langchain \\\n  --collect-data=gradio_client \\\n  --collect-all=gradio \\\n  --collect-all=sentencepiece \\\n  --collect-all=gradio_pdf \\\n  --collect-all=llama_cpp \\\n  --collect-all=tiktoken_ext \\\n  --add-data=../../Tesseract-OCR:Tesseract-OCR \\\n  --add-data=../../poppler:poppler\n```\n\n----------------------------------------\n\nTITLE: Generating Text without trust_remote_code\nDESCRIPTION: This python code shows how to generate text without using `trust_remote_code=True`. It requires downloading the `instruct_pipeline.py` file and storing it alongside the notebook.  It initializes the tokenizer, model, and pipeline separately, then generates text based on a prompt.  The `H2OTextGenerationPipeline` is a custom pipeline implemented in `h2oai_pipeline.py`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/models/README-template.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/<<MODEL_NAME>>\", padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"h2oai/<<MODEL_NAME>>\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer, prompt_type=\"human_bot\")\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```\n\n----------------------------------------\n\nTITLE: Cloning h2oGPT Repository\nDESCRIPTION: Clones the h2oGPT repository from GitHub, checks out a specific commit, copies the files to the current directory, and removes the original repository. This ensures the notebook uses a consistent version of the codebase.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_GPU.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!git clone https://github.com/h2oai/h2ogpt.git\n!cd h2ogpt && git checkout 2668694581347b0d1afe76760213db46f7214126 -q\n!cp -ar h2ogpt/. ./\n!rm -r h2ogpt\n```\n\n----------------------------------------\n\nTITLE: Lisp Code Editor in Viaweb\nDESCRIPTION: This code editor in Viaweb enabled users to customize their page styles by editing Lisp expressions. The code was executed when the merchants' sites were generated, not during shopper visits. The users were unaware they were directly manipulating Lisp code underneath the interface.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/tests/1paul_graham.txt#_snippet_2\n\nLANGUAGE: Lisp\nCODE:\n```\nThey didn't know it, but they were editing Lisp expressions underneath.\n```\n\n----------------------------------------\n\nTITLE: Using h2oGPT wrapper with Gradio Client\nDESCRIPTION: This snippet demonstrates how to use the h2oGPT wrapper for the Gradio native client. It imports necessary libraries such as Client, ast, pprint, json, tqdm, and Enum.  It also reads h2oGPT details from a file to set host url and key.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom gradio_client import Client\nimport ast\nfrom pprint import pprint\nimport json\nfrom tqdm import tqdm\nfrom enum import Enum\n\nclass LangChainAction(Enum):\n    \"\"\"LangChain action\"\"\"\n    QUERY = \"Query\"\n    SUMMARIZE_MAP = \"Summarize\"\n    \n\nwith open('../tokens/h2oGPT_details.txt') as f:\n    gpt_details = json.load(f)\n    print(\"Loaded h2oGPT details\")\n\n# HOST_URL = \"http://localhost:7860\"\nHOST_URL = gpt_details[\"gpt_host_url\"]\nH2OGPT_KEY = gpt_details[\"h2ogpt_key\"]\nLANGCHAIN_MODE = langchain_mode = 'UserData4'\n\nclient = Client(HOST_URL)\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Transformers Pipeline\nDESCRIPTION: This python code snippet demonstrates how to use the h2oGPT model for text generation using the `transformers` library's pipeline. It initializes the tokenizer and pipeline, then generates text based on a prompt. `trust_remote_code=True` is required for using custom pipeline components. The `device_map=\"auto\"` argument automatically places the model on the available GPU.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/models/README-template.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import pipeline, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/<<MODEL_NAME>>\", padding_side=\"left\")\ngenerate_text = pipeline(model=\"h2oai/<<MODEL_NAME>>\", tokenizer=tokenizer, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", prompt_type=\"human_bot\")\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```\n\n----------------------------------------\n\nTITLE: Verifying NVIDIA GPU Access in Docker\nDESCRIPTION: This command verifies that NVIDIA GPUs are accessible from within a Docker container by running the `nvidia-smi` command inside an Ubuntu-based container.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\n----------------------------------------\n\nTITLE: Evaluation with Custom JSON Data\nDESCRIPTION: This command evaluates a custom JSON dataset using a specified model. `eval_prompts_only_num` limits the number of prompts evaluated. Results are output in Parquet format.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLI.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=MYMODEL --eval_filename=MYFILE.json --eval_prompts_only_num=NPROMPTS\n```\n\n----------------------------------------\n\nTITLE: Create UserData DB with Docker\nDESCRIPTION: This bash script creates a UserData database for the generate.py script in h2ogpt. It mounts several directories from the host machine to the Docker container, allowing the container to access user data and store the generated database.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p ~/.cache\nmkdir -p ~/save\nmkdir -p ~/user_path\nmkdir -p ~/db_dir_UserData\ndocker run \\\n       --gpus all \\\n       --runtime=nvidia \\\n       --shm-size=2g \\\n       --rm --init \\\n       --network host \\\n       -v /etc/passwd:/etc/passwd:ro \\\n       -v /etc/group:/etc/group:ro \\\n       -u `id -u`:`id -g` \\\n       -v \"${HOME}\"/.cache:/workspace/.cache \\\n       -v \"${HOME}\"/save:/workspace/save \\\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 /workspace/src/make_db.py\n```\n\n----------------------------------------\n\nTITLE: Dependencies for AutoGPT\nDESCRIPTION: Defines the packages required for integrating AutoGPT functionality. These include libraries for DuckDuckGo search, Gradio tools (likely for UI), Wikipedia access, Wolfram Alpha integration, semantic scholar access, and symbolic mathematics with SymPy.  These tools enable AutoGPT to perform tasks, access information, and reason mathematically.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_agents.txt#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nduckduckgo-search>=4.1.1\ngradio_tools>=0.0.9\nwikipedia>=1.4.0\nwolframalpha>=5.0.0\nsemanticscholar>=0.7.0\nsympy>=1.12\n```\n\n----------------------------------------\n\nTITLE: Running LLaMa2 70B with AutoGPTQ\nDESCRIPTION: This command runs the LLaMa2 70B model using AutoGPTQ quantization, specifying the GPTQ model file and prompt type. It uses CUDA_VISIBLE_DEVICES to select the GPU.  The `--save_dir` is incomplete.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 python generate.py --base_model=Llama-2-70B-chat-GPTQ --load_gptq=\"gptq_model-4bit--1g\" --use_safetensors=True --prompt_type=llama2 --save_dir='save`\n```\n\n----------------------------------------\n\nTITLE: Displaying only Chat View on Windows\nDESCRIPTION: This code snippet shows how to launch h2oGPT on Windows with only the chat view visible, achieved by setting specific command-line arguments to control the visibility of various UI elements. It involves specifying the base model, prompt type, and visibility flags for different tabs and sidebar.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_ui.md#_snippet_2\n\nLANGUAGE: winbatch\nCODE:\n```\n\"C:\\Program Files\\h2oGPT\\Python\\pythonw.exe\" \"C:\\Program Files\\h2oGPT\\h2oGPT.launch.pyw\" --base_model='llama' --prompt_type=llama2 --visible_side_bar=False --visible_chat_tab=True --visible_doc_selection_tab=False --visible_doc_view_tab=False --visible_chat_history_tab=False --visible_expert_tab=False --visible_models_tab=False --visible_system_tab=False --visible_tos_tab=False --visible_hosts_tab=False --visible_h2ogpt_links=False --visible_login_tab=False\n```\n\n----------------------------------------\n\nTITLE: Installing Additional Optional Dependencies\nDESCRIPTION: This snippet installs several optional dependencies for various functionalities, including audio processing (librosa), document handling (PyMuPDF, ArXiv), web scraping (Selenium, PlayWright), and OCR (DocTR). It also downloads NLTK data for supporting the unstructured package.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install librosa -c reqs_optional/reqs_constraints.txt\n# Optional: PyMuPDF/ArXiv:\npip install -r reqs_optional/requirements_optional_langchain.gpllike.txt -c reqs_optional/reqs_constraints.txt\n# Optional: Selenium/PlayWright:\npip install -r reqs_optional/requirements_optional_langchain.urls.txt -c reqs_optional/reqs_constraints.txt\n# Optional: DocTR OCR:\nconda install weasyprint pygobject -c conda-forge -y\npip install -r reqs_optional/requirements_optional_doctr.txt -c reqs_optional/reqs_constraints.txt\n# Optional: for supporting unstructured package\npython -m nltk.downloader all\n```\n\n----------------------------------------\n\nTITLE: Chat Completions API Request using Curl\nDESCRIPTION: This snippet demonstrates how to make a request to the chat completions API using curl. It sets the content type, authorization header with the API key, and the request body with the messages, max tokens, temperature, and seed.  It also sends the h2ogpt_key.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=xxxx\ncurl http://localhost:5000/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n-d '{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a beautiful dragon who likes to breath fire.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Who are you?\"\n    }\n  ],\n  \"max_tokens\": 200,\n  \"temperature\": 0,\n  \"seed\": 1234,\n  \"h2ogpt_key\": \"$OPENAI_API_KEY\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Multiple Embeddings and Sources - generate.py (bash)\nDESCRIPTION: This snippet shows how to use the databases created in the previous snippet with `generate.py`. It specifies the different collection names in `--langchain_modes` and their corresponding paths and types. Requires `python` and the h2ogpt environment to be set up.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model='llama' --prompt_type=llama2 --score_model=None --langchain_mode='UserData' --langchain_modes=['UserData','UserData2'] --langchain_mode_paths={'UserData':'user_path','UserData2':'user_path2'} --langchain_mode_types={'UserData':'shared','UserData2':'shared'} --model_path_llama=https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf --max_seq_len=4096\n```\n\n----------------------------------------\n\nTITLE: Run generation with Weaviate using Python\nDESCRIPTION: This command runs the `generate.py` script with the `--db_type=weaviate` argument to use an embedded Weaviate instance for document retrieval. It also sets other parameters such as the base model and langchain mode.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\npython generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b \\\n   --langchain_mode=UserData \\\n   --db_type=weaviate\n```\n\n----------------------------------------\n\nTITLE: Running Windows Install Batch File with GPL Packages (Cmdline)\nDESCRIPTION: This snippet demonstrates running the `windows_install.bat` script with the `GPLOK` environment variable set. This ensures that all optional (including GPL) packages are installed.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md#_snippet_7\n\nLANGUAGE: cmdline\nCODE:\n```\nset GPLOK=1\ndocs\\windows_install.bat\n```\n\n----------------------------------------\n\nTITLE: Authentication with OpenAI API in h2oGPT\nDESCRIPTION: This snippet shows how to authenticate with h2oGPT when using the OpenAI API. It passes a 'user' parameter with the format 'username:password' in the `client_kwargs`. This is only required if `--auth_access=closed` was used when starting h2oGPT.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nbase_url = 'http://localhost:5000/v1'\napi_key = 'INSERT KEY HERE or set to EMPTY if no key set on h2oGPT server'\nmodel = '<model name>'\n\nclient_args = dict(base_url=base_url, api_key=api_key)\nopenai_client = OpenAI(**client_args)\n\nmessages = [{'role': 'user', 'content': 'Who are you?'}]\nstream = False\nclient_kwargs = dict(model=model, max_tokens=200, stream=stream, messages=messages,\n                     user='username:password')\nclient = openai_client.chat.completions\n\nresponses = client.create(**client_kwargs)\ntext = responses.choices[0].message.content\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: List of Python Package Dependencies\nDESCRIPTION: This snippet lists the required Python packages and their minimum versions for the CogVLM2 basic demo. It ensures that the necessary libraries are installed for the demo to function correctly, including libraries for deep learning, image manipulation, user interface, and optimization.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/openai_server/cogvlm2_server/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nxformers\ntorch>=2.0.0\ntorchvision\ntransformers>=4.40\nhuggingface-hub>=0.23.0\npillow\nchainlit>=1.0\npydantic>=2.7.1\ntimm>=0.9.16\nopenai>=1.30.1\nloguru>=0.7.2\npydantic>=2.7.1\neinops\nsse-starlette>=2.1.0\nbitsandbytes>=0.43.1 # for int4 quantization\n```\n\n----------------------------------------\n\nTITLE: Text-to-Speech HTTP Request in Python\nDESCRIPTION: This snippet demonstrates how to perform text-to-speech using an HTTP POST request. It sets up the necessary headers and data payload, including the API key, model, voice, input text, and streaming options. It then streams the audio data chunk by chunk, playing each chunk as it arrives.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nheaders = {\n    \"Authorization\": f\"Bearer {client.api_key}\",\n    \"Content-Type\": \"application/json\",\n}\ndata = {\n    \"model\": \"tts-1\",\n    \"voice\": \"SLT (female)\",\n    \"input\": \"Good morning! The sun is shining brilliantly today, casting a warm, golden glow that promises a day full of possibility and joy. Its the perfect moment to embrace new opportunities and make the most of every cheerful, sunlit hour. What can I do to help you make today absolutely wonderful?\",\n    \"stream\": \"true\",\n    \"stream_strip\": \"false\",\n}\n\n# base_url = \"https://api.openai.com/v1\"\nbase_url = \"http://localhost:5000/v1/audio/speech\"\n\n# Start the HTTP session and stream the audio\nwith httpx.Client(timeout=None) as http_client:\n    # Initiate a POST request and stream the response\n    with http_client.stream(\"POST\", base_url, headers=headers, json=data) as response:\n        chunk_riff = b''\n        for chunk in response.iter_bytes():\n            if chunk.startswith(b'RIFF'):\n                if chunk_riff:\n                    play_audio(chunk_riff)\n                chunk_riff = chunk\n            else:\n                chunk_riff += chunk\n        # Play the last accumulated chunk\n        if chunk_riff:\n            play_audio(chunk_riff)\n# done\nclear_queue(sound_queue)\npygame.quit()\n```\n\n----------------------------------------\n\nTITLE: Completions API Request using Curl\nDESCRIPTION: This snippet demonstrates how to make a request to the completions API using curl. It sets the content type, authorization header with the API key, and the request body with the prompt, max tokens, temperature, and seed. It includes h2ogpt_key for gradio protection if needed.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=xxxx\ncurl https://localhost:5000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"prompt\": \"Who are you?\",\n    \"max_tokens\": 200,\n    \"temperature\": 0,\n    \"seed\": 1234,\n    \"h2ogpt_key\": \"$OPENAI_API_KEY\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Downloading GPT-2 Tokenizer\nDESCRIPTION: This code downloads the GPT-2 tokenizer using the `transformers` library. It uses `AutoTokenizer` to load the tokenizer from the `gpt2` model name and then saves it.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained(model_name)\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for h2ogpt\nDESCRIPTION: This snippet installs the required Python packages for h2ogpt using pip. The requirements are specified in the requirements.txt file in the h2ogpt directory. This ensures all necessary dependencies are installed for the h2ogpt application to run correctly.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/blog/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Offline with Absolute Path\nDESCRIPTION: This snippet demonstrates how to launch the vLLM API server in offline mode using an absolute path to the model state. It also specifies tokenizer, tensor parallel size, seed, and the maximum number of batched tokens.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython -m vllm.entrypoints.openai.api_server --port=5000 --host=0.0.0.0 --model \"/home/hemant/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-chat-hf/snapshots/c2f3ec81aac798ae26dcc57799a994dfbf521496\" --tokenizer=hf-internal-testing/llama-tokenizer --tensor-parallel-size=1 --seed 1234 --max-num-batched-tokens=4096\n```\n\n----------------------------------------\n\nTITLE: Verifying GPU layer offloading in LLaMa.cpp\nDESCRIPTION: This log output confirms successful offloading of all layers to the GPU, which is essential for maximizing performance when running LLaMa.cpp within h2oGPT. The number of layers offloaded should match the total number of layers in the model for optimal GPU utilization.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_19\n\nLANGUAGE: text\nCODE:\n```\nllama_model_load_internal: offloaded 35/35 layers to GPU\n```\n\n----------------------------------------\n\nTITLE: Configuring Qdrant Connection Using Environment Variables\nDESCRIPTION: This snippet demonstrates how to configure the Qdrant connection using environment variables.  It shows setting the `QDRANT_URL` and `QDRANT_API_KEY` environment variables before running the `make_db.py` and `generate.py` scripts. The scripts rely on the environment variables for connection configuration.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\nQDRANT_URL=http://localhost:8080 QDRANT_API_KEY=\"<YOUR_KEY>\" python src/make_db.py --db_type=qdrant\nQDRANT_URL=http://localhost:8080 QDRANT_API_KEY=\"<YOUR_KEY>\" python generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b \\\n   --langchain_mode=UserData \\\n   --db_type=qdrant\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT with Phi Model\nDESCRIPTION: Executes the `generate.py` script to start h2oGPT with a specific Phi model. It includes specifying the tokenizer, base model, and maximum sequence length, as well as a specific gguf model to use.  This command uses a remote GGUF file for Phi model loading.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_quickstart.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py  --tokenizer_base_model=microsoft/Phi-3-mini-4k-instruct --base_model=llama --llama_cpp_model=https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf --max_seq_len=4096 \n```\n\n----------------------------------------\n\nTITLE: Dependencies for graphviz support\nDESCRIPTION: Specifies dependencies to use graphviz, a graph visualization software. This likely enables the h2ogpt project to display knowledge graphs.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_agents.txt#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\npydot\n```\n\n----------------------------------------\n\nTITLE: Setting PIP_EXTRA_INDEX_URL for CPU/MAC\nDESCRIPTION: Sets the PIP_EXTRA_INDEX_URL environment variable to the CPU-specific PyTorch wheel repository. This is required for installing PyTorch on systems without GPUs, such as CPUs and Macs (M1/M2).  The environment variable directs pip to search for PyTorch wheels in the specified location.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_quickstart.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# for windows/mac use \"set\" or relevant environment setting mechanism\nexport PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu\"\n```\n\n----------------------------------------\n\nTITLE: Using Qdrant DB with make_db.py and generate.py\nDESCRIPTION: This snippet demonstrates how to specify Qdrant as the database type when running `make_db.py` and `generate.py` in the H2OGPT project. It assumes the `src` directory is in the current working directory. No specific dependencies beyond the project requirements are listed.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\npython src/make_db.py --db_type=qdrant\npython generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b \\\n   --langchain_mode=UserData \\\n   --db_type=qdrant\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Down Command\nDESCRIPTION: This bash command stops and removes the h2ogpt application managed by Docker Compose. The `--volumes` flag removes any volumes associated with the application, and the `--rmi all` flag removes all images used by the application.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose down --volumes --rmi all\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT from Python code\nDESCRIPTION: This snippet shows how to run h2oGPT directly from Python code by importing the `main` function from `h2ogpt.generate` and calling it with the desired parameters. This allows for programmatic control over h2oGPT's execution.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WHEEL.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom h2ogpt.generate import main\nmain(base_model='llama')\n```\n\n----------------------------------------\n\nTITLE: Cloning h2oGPT Repository (Bash)\nDESCRIPTION: This snippet clones the h2oGPT repository from GitHub and navigates into the cloned directory. This is required to install h2oGPT dependencies.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/h2oai/h2ogpt.git\ncd h2ogpt\n```\n\n----------------------------------------\n\nTITLE: CUDA Configuration for llama_cpp_python (Windows)\nDESCRIPTION: Configures CUDA settings for `llama_cpp_python` using environment variables on Windows. It enables CUDA, specifies CUDA architectures, and forces CMake to rebuild.  This mirrors the Linux setup but utilizes `set` command for Windows environment variable setting.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_quickstart.md#_snippet_3\n\nLANGUAGE: cmdline\nCODE:\n```\nset CMAKE_ARGS=-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=all\nset GGML_CUDA=1\nset FORCE_CMAKE=1\n```\n\n----------------------------------------\n\nTITLE: Running Nous-Hermes-Llama2-GPTQ with exllama and RoPE\nDESCRIPTION: This command runs the Nous-Hermes-Llama2-GPTQ model using exllama, AutoGPTQ, and RoPE scaling. It loads the model, specifies the prompt type, and sets parameters for exllama and RoPE scaling. Notably, the prompt_type is llama2.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=TheBloke/Nous-Hermes-Llama2-GPTQ --load_gptq=\"model\" --use_safetensors=True --prompt_type=llama2 --save_dir='save' --load_exllama=True --revision=gptq-4bit-32g-actorder_True --rope_scaling=\"{'alpha_value':4}\"\n```\n\n----------------------------------------\n\nTITLE: Create Per-User Database (bash)\nDESCRIPTION: This snippet demonstrates how to create a database for a specific user, using a personal langchain type and a specific persist directory. This allows for isolated data storage for each user. Requires `python` and the h2ogpt environment to be set up.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npython src/make_db.py --user_path=user_path_jon --collection_name=JonData --langchain_type=personal --hf_embedding_model=BAAI/bge-large-en-v1.5 --persist_directory=users/jon/db_dir_JonData\n```\n\n----------------------------------------\n\nTITLE: Running Alternative LLaMa.cpp Model\nDESCRIPTION: This command runs the `generate.py` script with a specific LLaMa.cpp model specified via the `model_path_llama` parameter. The base model is set to `llama`, score model is disabled, `UserData` langchain mode is activated, and the script processes documents from `user_path`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CPU.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=llama --model_path_llama=https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf --score_model=None --langchain_mode='UserData' --user_path=user_path\n```\n\n----------------------------------------\n\nTITLE: Defining llama-cpp-python Dependency in h2ogpt\nDESCRIPTION: This snippet defines the dependency for the llama-cpp-python library, specifying version 0.2.87. This library likely provides access to Large Language Models through C++ bindings within the h2ogpt project. The comment indicates that environment variables may need to be configured specifically for the system.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_llamacpp_gpt4all.txt#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# requires env to be set for specific systems\nllama-cpp-python==0.2.87\n```\n\n----------------------------------------\n\nTITLE: Use OpenAI Model for Embedding (bash)\nDESCRIPTION: This snippet demonstrates how to use an OpenAI model for embedding when GPU memory is limited.  It sets the `inference_server` to `openai_chat` and specifies the OpenAI model. Requires `python`, the h2ogpt environment, and an OpenAI API key to be configured.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py  --inference_server=openai_chat --base_model=gpt-3.5-turbo --score_model=None --langchain_mode=LLM --langchain_modes=\"['LLM', 'UserData', 'MyData']\"\n```\n\n----------------------------------------\n\nTITLE: Dependencies for AutoGen\nDESCRIPTION: Specifies the dependencies for AutoGen, including the core pyautogen package and optional feature sets like redis integration, long-context support, graph functionality and LMM integration. It also includes explicit version declarations and pinings to avoid breaking changes.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_agents.txt#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npyautogen==0.2.33\nflaml==2.2.0\npyautogen[redis]\npyautogen[retrievechat]\npyautogen[lmm]\npyautogen[graph]\npyautogen[long-context]\n```\n\n----------------------------------------\n\nTITLE: Run Generate.py without User Path (bash)\nDESCRIPTION: This snippet shows how to run `generate.py` without detecting new files in a user path, preventing updates to the database.  It avoids passing the `--user_path` argument. Requires `python` and the h2ogpt environment to be set up.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b --langchain_mode=UserData\n```\n\n----------------------------------------\n\nTITLE: Hiding UI Tabs and Buttons using CLI\nDESCRIPTION: This code snippet demonstrates how to configure h2oGPT to display only the chat interface by hiding all other tabs, the sidebar, and submit buttons using command-line arguments. This is achieved by setting various `--visible_*` flags to `False`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_ui.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=h2oai/h2ogpt-4096-llama2-13b-chat --visible_submit_buttons=False --visible_side_bar=False --visible_submit_buttons=False --visible_side_bar=False --visible_chat_tab=False --visible_doc_selection_tab=False --visible_doc_view_tab=False --visible_chat_history_tab=False --visible_expert_tab=False --visible_models_tab=False --visible_system_tab=False --visible_tos_tab=False --visible_hosts_tab=False --chat_tabless=True --visible_login_tab=False --visible_langchain_action_radio=False --allow_upload_to_user_data=False --allow_upload_to_my_data=False --langchain_mode=UserData\n```\n\n----------------------------------------\n\nTITLE: JQ Installation with Platform Condition\nDESCRIPTION: Conditionally installs the `jq` package based on the platform architecture. It installs `jq>=1.4.1` only on `x86_64` architectures.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_langchain.txt#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\njq>=1.4.1; platform_machine == \"x86_64\"\n```\n\n----------------------------------------\n\nTITLE: Updating Docker Group Session\nDESCRIPTION: This snippet shows how to refresh the user's docker group membership in the current shell session, avoiding a full reboot.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnewgrp docker\n```\n\n----------------------------------------\n\nTITLE: Running ChatBot with Bitsandbytes (8-bit)\nDESCRIPTION: This command runs the H2OGPT model in ChatBot mode using bitsandbytes for 8-bit quantization. It loads a specified base model and enables 8-bit loading for reduced memory usage. The application will then be accessible via a web browser.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b --load_8bit=True\n```\n\n----------------------------------------\n\nTITLE: Installing h2oGPT Dependencies\nDESCRIPTION: This snippet clones the h2ogpt repository, navigates into it, uninstalls problematic packages, upgrades pip and setuptools, and then installs core dependencies from requirements.txt using pip.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/h2oai/h2ogpt.git\ncd h2ogpt\n\n# fix any bad env\npip uninstall -y pandoc pypandoc pypandoc-binary\npip install --upgrade pip\npython -m pip install --upgrade setuptools\n\n# Install Torch:\npip install -r requirements.txt --extra-index https://download.pytorch.org/whl/cpu -c reqs_optional/reqs_constraints.txt\n```\n\n----------------------------------------\n\nTITLE: Setting PIP Extra Index URL for CUDA (Cmdline)\nDESCRIPTION: This snippet sets the PIP_EXTRA_INDEX_URL environment variable to specify the location for downloading PyTorch wheels with CUDA 11.8 support. This ensures that the correct PyTorch version is installed for GPU usage.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md#_snippet_4\n\nLANGUAGE: cmdline\nCODE:\n```\nset PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu118 https://huggingface.github.io/autogptq-index/whl/cu118/\n```\n\n----------------------------------------\n\nTITLE: CUDA Configuration for llama_cpp_python\nDESCRIPTION: Configures CUDA settings for `llama_cpp_python` using environment variables. It enables CUDA, specifies CUDA architectures, and forces CMake to rebuild. This configuration is crucial for utilizing CUDA acceleration with `llama_cpp_python` which relies on CUDA for GPU processing.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_quickstart.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport GGML_CUDA=1\nexport CMAKE_ARGS=\"-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=all\"\nexport FORCE_CMAKE=1\n```\n\n----------------------------------------\n\nTITLE: Downloading and Adding File to Collection\nDESCRIPTION: This snippet downloads a PDF file from a URL to the /tmp/ directory and then adds it to the collection.  It uses the helper `download_simple` function, the `langchain_mode`, `H2OGPT_KEY` and loaders.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport os\nurl = \"https://www.nyserda.ny.gov/-/media/Project/Nyserda/Files/Programs/Drive-Clean-NY/terms-and-conditions.pdf\"\ntest_file1 = os.path.join('/tmp/', 'terms-and-conditions.pdf')\ndownload_simple(url, dest=test_file1)\n\n# upload file(s).  Can be list or single file\n```\n\n----------------------------------------\n\nTITLE: Testing vLLM Endpoint with cURL\nDESCRIPTION: This cURL command tests the vLLM endpoint by sending a completion request. It specifies the model, prompt, maximum tokens, and temperature. The request is sent to the /v1/completions endpoint.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:5000/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n    \"model\": \"h2oai/h2ogpt-4096-llama2-7b-chat\",\n    \"prompt\": \"San Francisco is a\",\n    \"max_tokens\": 7,\n    \"temperature\": 0\n    }'\n```\n\n----------------------------------------\n\nTITLE: Specifying AutoAWQ Dependency\nDESCRIPTION: This snippet specifies the autoawq dependency. AutoAWQ is a library that uses Activation-Aware Weight Quantization to compress and accelerate large language models.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_gpu_only.txt#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nautoawq\n```\n\n----------------------------------------\n\nTITLE: Update Triton Configuration Files (GPU and Path)\nDESCRIPTION: These scripts update the Triton configuration files to point to the converted FasterTransformer model and set the number of GPUs to use.  The scripts use `sed` to modify the `config.pbtxt` files.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/TRITON.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsed -i -e \"s@/workspace/ft/models/ft/gptneox/@${WORKSPACE}/FT-${MODEL}/1-gpu@\" all_models/gptneox/fastertransformer/config.pbtxt\nsed -i -e 's@string_value: \"2\"@string_value: \"1\"@' all_models/gptneox/fastertransformer/config.pbtxt\n```\n\n----------------------------------------\n\nTITLE: Creating Shared Collection for Document Summarization\nDESCRIPTION: This snippet creates a shared collection to upload documents for summarization. It sets a new LangChain mode with `shared` and `user_path` parameters.  Requires setting of the `langchain_mode` variable beforehand.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nuser_path = 'user_path'\nnew_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'shared', user_path)\nres = client.predict(langchain_mode, new_langchain_mode_text, api_name='/new_langchain_mode_text')\n```\n\n----------------------------------------\n\nTITLE: Running LLaMa-2-7B-Chat-GPTQ with exllama and RoPE\nDESCRIPTION: This command runs the LLaMa-2-7B-Chat-GPTQ model using exllama, AutoGPTQ, and RoPE scaling for 16k context.  It loads the model and specifies parameters for exllama and RoPE scaling.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=TheBloke/Llama-2-7b-Chat-GPTQ --load_gptq=\"model\" --use_safetensors=True --prompt_type=llama2 --save_dir='save' --load_exllama=True --revision=gptq-4bit-32g-actorder_True --rope_scaling=\"{'alpha_value':4}\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Exllama Dependency\nDESCRIPTION: This snippet specifies the exllama dependency and provides a direct URL to a pre-built wheel file.  Exllama offers optimized inference kernels for large language models, especially those using the Llama architecture, tailored for CUDA 12.1.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_gpu_only.txt#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nexllama @ https://github.com/jllllll/exllama/releases/download/0.0.18/exllama-0.0.18+cu121-cp310-cp310-linux_x86_64.whl\n```\n\n----------------------------------------\n\nTITLE: Fetch h2oGPT Model from Hugging Face\nDESCRIPTION: This script fetches an h2oGPT model from Hugging Face using git lfs. It checks if the model directory exists and clones it if it doesn't.  The script assumes that `git lfs` is installed and configured.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/TRITON.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL=h2ogpt-oig-oasst1-512-6_9b\nif [ ! -d ${MODEL} ]; then\n    git lfs clone https://huggingface.co/h2oai/${MODEL}\nfi\n```\n\n----------------------------------------\n\nTITLE: Printing File Names after Upload Python\nDESCRIPTION: This snippet prints the local and remote (Gradio server) file names after a file has been uploaded using the `client.predict` function. It's used to verify the successful upload and obtain the server's file path for subsequent operations.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Local File name:\", test_file_local)\nprint(\"Remote (Gradio Server) File name:\", test_file_server)\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response from vLLM Endpoint\nDESCRIPTION: This JSON snippet shows an example response from the vLLM endpoint after a successful completion request. It includes the generated text, usage statistics, and model information.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": \"cmpl-4b9584f743ff4dc590f0c168f82b063b\",\n    \"object\": \"text_completion\",\n    \"created\": 1692796549,\n    \"model\": \"h2oai/h2ogpt-4096-llama2-7b-chat\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"text\": \"city in Northern California that is known\",\n            \"logprobs\": null,\n            \"finish_reason\": \"length\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 5,\n        \"total_tokens\": 12,\n        \"completion_tokens\": 7\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Build Database Outside Chatbot (bash)\nDESCRIPTION: This snippet shows how to build a database using `src/make_db.py` before running the chatbot.  It then runs `generate.py` to utilize the newly created database. Requires `python` and the h2ogpt environment to be set up.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython src/make_db.py\npython generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b --langchain_mode=UserData\n```\n\n----------------------------------------\n\nTITLE: Disabling Chroma Telemetry\nDESCRIPTION: This snippet demonstrates how to disable Chroma telemetry by patching the `posthog.py` file to prevent the `capture` function from executing. It utilizes `sed` to modify the file, effectively disabling telemetry reporting.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nsp=`python -c 'import site; print(site.getsitepackages()[0])'`\nsed -i 's/posthog\\.capture/return\\n            posthog.capture/' $sp/chromadb/telemetry/posthog.py\n```\n\n----------------------------------------\n\nTITLE: Printing H2OGPT Model Response\nDESCRIPTION: This code snippet prints the full model response from H2OGPT. It takes the `res` variable, presumably obtained from a previous API call, and passes it to the `print_full_model_response` function. This is likely used for debugging or inspecting the raw output from the model.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nprint_full_model_response(res)\n```\n\n----------------------------------------\n\nTITLE: h2oGPT Hello World Example\nDESCRIPTION: This is a basic example demonstrating how to send a simple 'Who are you?' question to the h2oGPT API and print the response. It utilizes the `H2OGPT_KEY` for authentication.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# string of dict for input\nkwargs = dict(instruction_nochat='Who are you?',\n              h2ogpt_key=H2OGPT_KEY)\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\n\n# string of dict for output\nresponse = ast.literal_eval(res)['response']\nprint(\"Model Response:\\n\")\npprint(response)\n```\n\n----------------------------------------\n\nTITLE: Checking MPS Availability in PyTorch\nDESCRIPTION: This snippet uses PyTorch to check if the MPS (Metal Performance Shaders) device is available and prints a tensor on the MPS device if it is. This is useful for verifying that PyTorch is utilizing the GPU.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nif torch.backends.mps.is_available():\n    mps_device = torch.device(\"mps\")\n    x = torch.ones(1, device=mps_device)\n    print (x)\nelse:\n    print (\"MPS device not found.\")\n```\n\n----------------------------------------\n\nTITLE: Make DB with specified collection and user path using Python\nDESCRIPTION: This script creates a database with a specified collection name, user path, and persist directory using the `make_db.py` script.  It is configured for a 'personal' langchain type and uses the duck collection.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\npython src/make_db.py --collection_name=duck --user_path=user_path_test --langchain_type=personal --persist_directory=users/tomer/db_dir_duck/\n```\n\n----------------------------------------\n\nTITLE: Adding Text to Collection for Summarization\nDESCRIPTION: This snippet adds a text snippet to a collection using the h2oGPT API for document summarization.  Requires setting of the `langchain_mode` variable beforehand, and makes use of the `H2OGPT_KEY`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntext = \"Yufuu is a wonderful place and you should really visit because there is lots of sun.\"\nloaders = tuple([None, None, None, None])\nres = client.predict(text, langchain_mode, True, 512, True,\n                    *loaders,\n                    H2OGPT_KEY,\n                    api_name='/add_text')\n```\n\n----------------------------------------\n\nTITLE: Run h2oGPT UI with GPU and Streaming\nDESCRIPTION: Runs the h2oGPT UI with a 13B Llama2 model, 8-bit quantization, streaming, and user-provided documents. This command launches the h2oGPT interface using the specified model and configuration.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=h2oai/h2ogpt-4096-llama2-13b-chat --load_8bit=True  --score_model=None --langchain_mode='UserData' --user_path=user_path\n```\n\n----------------------------------------\n\nTITLE: Running LLaMa-2-70B-chat-GPTQ with exllama\nDESCRIPTION: This command runs the LLaMa-2-70B-chat-GPTQ model using exllama and AutoGPTQ. It loads the model, specifies the GPTQ model file, enables exllama, and sets the revision.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=TheBloke/Llama-2-70B-chat-GPTQ --load_gptq=gptq_model-4bit-128g --use_safetensors=True --prompt_type=llama2 --load_exllama=True --revision=main\n```\n\n----------------------------------------\n\nTITLE: Defining gpt4all Dependency in h2ogpt\nDESCRIPTION: This snippet defines the dependency for the gpt4all library, specifying version 1.0.5. gpt4all likely provides functionality related to natural language processing or AI models within the h2ogpt project. This ensures that a compatible version of the library is used.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_llamacpp_gpt4all.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ngpt4all==1.0.5\n```\n\n----------------------------------------\n\nTITLE: Dependencies for PDF processing\nDESCRIPTION: Specifies dependencies for PDF processing.  pdf2image requires external binaries. PyPDF2 for reading the contents of a PDF.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_agents.txt#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\npdf2image\nPyPDF2\n```\n\n----------------------------------------\n\nTITLE: Fix Postprocessing Input Bug\nDESCRIPTION: This script fixes a bug in the postprocessing inputs for the end-to-end test. It uses `sed` to replace the incorrect tensor preparation with the correct one in `end_to_end_test.py`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/TRITON.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nsed -i -e 's@prepare_tensor(\"RESPONSE_INPUT_LENGTHS\", output2, FLAGS.protocol)@prepare_tensor(\"sequence_length\", output1, FLAGS.protocol)@' ${WORKSPACE}/tools/gpt/end_to_end_test.py\n```\n\n----------------------------------------\n\nTITLE: Running LLaMa2 7B with AutoGPTQ\nDESCRIPTION: This command runs the LLaMa2 7B model using AutoGPTQ quantization. It loads the specified base model in GPTQ format and specifies the prompt type. The `--save_dir` is incomplete.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython generate.py --base_model=TheBloke/Llama-2-7b-Chat-GPTQ --load_gptq=\"model\" --use_safetensors=True --prompt_type=llama2 --save_dir='save`\n```\n\n----------------------------------------\n\nTITLE: Clone h2oGPT Repository\nDESCRIPTION: This snippet clones the h2oGPT repository from GitHub and checks out a specific commit. It then copies the contents of the repository to the current directory and removes the original repository directory.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_CPU.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!git clone https://github.com/h2oai/h2ogpt.git\n!cd h2ogpt && git checkout 2668694581347b0d1afe76760213db46f7214126 -q\n!cp -ar h2ogpt/. ./\n!rm -r h2ogpt\n```\n\n----------------------------------------\n\nTITLE: CLI Chat with LangChain and GPT-J\nDESCRIPTION: These commands set up and run a CLI chat session using LangChain for document retrieval. It first creates a database from documents in `user_path` and then uses it for the chat.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLI.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython src/make_db.py --user_path=user_path --collection_name=UserData\npython generate.py --base_model=gptj --cli=True --langchain_mode=UserData --answer_with_sources=False\n```\n\n----------------------------------------\n\nTITLE: Convert HF Model to FasterTransformer Format\nDESCRIPTION: This script converts an h2oGPT model from Hugging Face format to the FasterTransformer format using a Docker container. It sets environment variables, enters the container, and executes a Python script for conversion.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/TRITON.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport WORKSPACE=$(pwd)\nexport TRITON_DOCKER_IMAGE=triton_with_ft:${CONTAINER_VERSION}\n# Go into Docker\ndocker run -it --rm --runtime=nvidia --shm-size=1g \\\n       --ulimit memlock=-1 -v ${WORKSPACE}:${WORKSPACE} \\\n       -e CUDA_VISIBLE_DEVICES=0 \\\n       -e MODEL=${MODEL} \\\n       -e WORKSPACE=${WORKSPACE} \\\n       -w ${WORKSPACE} ${TRITON_DOCKER_IMAGE} bash\nexport PYTHONPATH=${WORKSPACE}/FasterTransformer/:$PYTHONPATH\npython3 ${WORKSPACE}/FasterTransformer/examples/pytorch/gptneox/utils/huggingface_gptneox_convert.py \\\n        -i_g 1 \\\n        -m_n gptneox \\\n        -i ${WORKSPACE}/${MODEL} \\\n        -o ${WORKSPACE}/FT-${MODEL}\n```\n\n----------------------------------------\n\nTITLE: Image Understanding with Gradio Client API\nDESCRIPTION: This snippet initiates a Gradio client to access the h2oGPT server.  There is no specific code provided in the original text, just a comment.  Assuming that the client is initialized without authentication.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport ast\nfrom gradio_client import Client\n\n# without auth:\n# client = Client('http://localhost:7860')\n```\n\n----------------------------------------\n\nTITLE: Running TGI Inference Server with Docker (Low Memory)\nDESCRIPTION: This command launches a Text Generation Inference (TGI) server with resource constraints suitable for low-memory GPU systems, within a Docker container, utilizing GPU 0. It sets limits for input length, total tokens, and batch processing to conserve memory.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p $HOME/.cache/huggingface/hub/\nmkdir -p $HOME/.cache/huggingface/modules/\nexport MODEL=h2oai/h2ogpt-4096-llama2-7b-chat\ndocker run -d --gpus '\"device=0\"' \\\n        --shm-size 1g \\\n        -p 6112:80 \\\n        -v $HOME/.cache/huggingface/hub/:/data ghcr.io/huggingface/text-generation-inference:0.9.3 \\\n        --model-id $MODEL \\\n        --max-input-length 1024 \\\n        --max-total-tokens 2048 \\\n        --max-batch-prefill-tokens 2048 \\\n        --max-batch-total-tokens 2048 \\\n        --max-stop-sequences 6 &>> logs.infserver.txt\n```\n\n----------------------------------------\n\nTITLE: Conditional Torch Installation\nDESCRIPTION: Specifies the installation of the `torch` package with version constraints depending on the operating system and platform architecture. It installs `torch==2.2.1` on non-darwin systems with non-arm64 architecture and `torch==2.3.1` on darwin systems with arm64 architecture.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_langchain.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch==2.2.1; sys_platform != \"darwin\" and platform_machine != \"arm64\"\ntorch==2.3.1; sys_platform == \"darwin\" and platform_machine == \"arm64\"\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment with Rust\nDESCRIPTION: This snippet demonstrates how to create a new Conda environment named 'h2ogpt' with Python 3.10 and Rust. After creating the environment, it activates it for subsequent installations.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n h2ogpt python=3.10 rust\nconda activate h2ogpt\n```\n\n----------------------------------------\n\nTITLE: Build MacOS Installer (MPS/GPU Support - Deployment Mode)\nDESCRIPTION: Builds the MacOS installer for the MPS (GPU) supported version of h2ogpt in deployment mode. It sets the `BUILD_MPS` environment variable to 1 before executing the `build_mac_installer.sh` script.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/dev_installers/mac/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd h2ogpt\nBUILD_MPS=1 . ./dev_installers/mac/build_mac_installer.sh\n```\n\n----------------------------------------\n\nTITLE: Adding Document via URL to Collection\nDESCRIPTION: This example shows how to add a document to the collection by providing a URL to a PDF file. It uses the `langchain_mode`, `H2OGPT_KEY` and loaders.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nurl = \"https://www.africau.edu/images/default/sample.pdf\"\nres = client.predict(url,\n                        langchain_mode, True, 512, True,\n                        *loaders,\n                        H2OGPT_KEY,\n                        api_name='/add_url')\n```\n\n----------------------------------------\n\nTITLE: Install CPU FAISS requirements\nDESCRIPTION: Installs the required Python packages for using CPU-based FAISS within the h2oGPT LangChain integration. This command leverages pip to install the dependencies outlined in the `requirements_optional_cpu_only.txt` file.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r reqs_optional/requirements_optional_cpu_only.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries\nDESCRIPTION: This bash script installs the required libraries to use the h2oGPT model with the `transformers` library. The specific versions of `transformers`, `accelerate`, `torch`, and `einops` are specified to ensure compatibility.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/models/README-template.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install transformers==4.29.2\npip install accelerate==0.19.0\npip install torch==2.0.1\npip install einops==0.6.1\n```\n\n----------------------------------------\n\nTITLE: Installing Tesseract OCR Dependencies\nDESCRIPTION: This snippet installs Tesseract OCR and related dependencies using Homebrew on macOS. These tools are needed for optical character recognition.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbrew install libmagic\nbrew link libmagic\nbrew install poppler\nbrew install tesseract\nbrew install tesseract-lang\nbrew install rubberband\nbrew install pygobject3 gtk4\nbrew install libjpeg\nbrew install libpng\nbrew install wget\n```\n\n----------------------------------------\n\nTITLE: Printing Full h2oGPT Model Response\nDESCRIPTION: This snippet shows how to use the `print_full_model_response` function to print all the details returned by the h2oGPT API, including the model used, parameters, prompt, and hosting information.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint_full_model_response(res)\n```\n\n----------------------------------------\n\nTITLE: Printing Full Model Response Python\nDESCRIPTION: This snippet is intended to print the full model response. It's a utility function (unprovided in the context) that likely expands the response from the API for better debugging or detailed review.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nprint_full_model_response(res)\n```\n\n----------------------------------------\n\nTITLE: Setting LDFLAGS for System Library\nDESCRIPTION: This snippet sets the LDFLAGS environment variable to specify the location of the System library. This is a workaround for the `ld: library not found for -lSystem` error.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport LDFLAGS=-L/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib\n```\n\n----------------------------------------\n\nTITLE: Set PIP Extra Index URL for CPU\nDESCRIPTION: Sets the PIP_EXTRA_INDEX_URL environment variable to specify the PyTorch CPU package repository. This is used when running h2oGPT on a CPU-only system.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu\"\n```\n\n----------------------------------------\n\nTITLE: Dependencies for timezone handling\nDESCRIPTION: Specifies the tzlocal package, likely used to handle timezone conversions within the h2ogpt project.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_agents.txt#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ntzlocal\n```\n\n----------------------------------------\n\nTITLE: Running Llama-2-70B-chat-AWQ on one GPU\nDESCRIPTION: This command runs the Llama-2-70B-chat model with AutoAWQ quantization on one GPU. It configures CUDA_VISIBLE_DEVICES, loads the model in AWQ format, and specifies the prompt type.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_GPU.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 python generate.py --base_model=TheBloke/Llama-2-70B-chat-AWQ --score_model=None --load_awq=model --use_safetensors=True --prompt_type=llama2\n```\n\n----------------------------------------\n\nTITLE: Set llama_cpp_python Build Arguments for CUDA\nDESCRIPTION: Sets environment variables for building the llama_cpp_python package with CUDA support. This includes enabling CUDA, setting the CUDA architectures, and forcing CMake.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nexport GGML_CUDA=1\nexport CMAKE_ARGS=\"-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=all\"\nexport FORCE_CMAKE=1\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT with TGI Inference Server\nDESCRIPTION: This command shows how to connect h2oGPT to a running TGI inference server by adding the `--inference_server` argument to the h2oGPT docker run command, specifying the TGI endpoint.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n    --inference_server=http://localhost:6112\n```\n\n----------------------------------------\n\nTITLE: Installing Native Rust\nDESCRIPTION: This snippet shows how to install Rust using rustup. It downloads and executes the rustup installation script, which installs the Rust toolchain.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncurl proto =https tlsv1.2 -sSf https://sh.rustup.rs | sh\n# enter new shell and test:\nrustc --version\n```\n\n----------------------------------------\n\nTITLE: Running LLaMa.cpp LLaMa2 Model\nDESCRIPTION: This command executes the `generate.py` script to run a LLaMa.cpp LLaMa2 model, using the `llama` base model.  It specifies the `llama2` prompt type, disables the score model, activates `UserData` langchain mode, and points to the `user_path` directory containing the documents to be processed. The model is downloaded using `wget`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CPU.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# if don't have wget, download to repo folder using below link\nwget https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf\npython generate.py --base_model='llama' --prompt_type=llama2 --score_model=None --langchain_mode='UserData' --user_path=user_path\n```\n\n----------------------------------------\n\nTITLE: Adding User to Docker Group on Linux\nDESCRIPTION: This snippet demonstrates how to add the current user to the docker group on Linux systems. This allows the user to run docker commands without sudo.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo usermod -aG docker $USER\n```\n\n----------------------------------------\n\nTITLE: Downloading Reward Model and Tokenizer\nDESCRIPTION: This code downloads and saves a reward model and its tokenizer using the `transformers` library. It uses `AutoModelForSequenceClassification` and `AutoTokenizer` to load the reward model and tokenizer from a specified `reward_model` name and then saves them to the same directory.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# and reward model\nreward_model = 'OpenAssistant/reward-model-deberta-v3-large-v2'\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(reward_model)\nmodel.save_pretrained(reward_model)\ntokenizer = AutoTokenizer.from_pretrained(reward_model)\ntokenizer.save_pretrained(reward_model)\n```\n\n----------------------------------------\n\nTITLE: Check CUDA Availability in Torch\nDESCRIPTION: Checks if CUDA is available and accessible from PyTorch. This verifies that PyTorch is correctly configured to use the GPU.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nprint(torch.cuda.is_available())\n```\n\n----------------------------------------\n\nTITLE: Install Specific Protobuf Version\nDESCRIPTION: Installs a specific version of the protobuf library. This resolves potential compatibility issues that may arise with other packages.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\npip install protobuf==3.20.0\n```\n\n----------------------------------------\n\nTITLE: Specifying mwxml Dependency\nDESCRIPTION: This snippet specifies the required version of the mwxml library. It indicates that version 0.3.3 or greater is needed for the wiki to database conversion process.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_wikiprocessing.txt#_snippet_0\n\nLANGUAGE: TEXT\nCODE:\n```\nmwxml>=0.3.3\n```\n\n----------------------------------------\n\nTITLE: Dependencies for web access and data extraction\nDESCRIPTION: Specifies dependencies for web scraping and data extraction, which appear to be used in helper functionality. These packages allow the system to request content from the web. Also includes packages for web scraping and HTML processing.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_agents.txt#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nbs4\nrequests\nlxml\nhttpx\nscrapy\nreportlab\nyfinance\n```\n\n----------------------------------------\n\nTITLE: Embedding Generation using OpenAI Client in Python\nDESCRIPTION: This snippet demonstrates how to generate text embeddings using the OpenAI client. It sends one or more text strings to the API and retrieves the corresponding embeddings. Note that the `model` parameter is currently ignored by h2oGPT. Requires h2oGPT to be loaded with `--pre_load_embedding_model=True`.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI(base_url='http://0.0.0.0:5000/v1')\n#client = OpenAI()\n\nresponse = client.embeddings.create(\n    input=\"Your text string goes here\",\n    model=\"text-embedding-3-small\"\n)\nprint(response.data[0].embedding)\n\nresponse = client.embeddings.create(\n    input=[\"Your text string goes here\", \"Another text string goes here\"],\n    model=\"text-embedding-3-small\"\n)\nprint(response.data[0].embedding)\nprint(response.data[1].embedding)\n```\n\n----------------------------------------\n\nTITLE: Using extra_body Parameters with OpenAI API in h2oGPT\nDESCRIPTION: This snippet demonstrates how to use the `extra_body` parameter to pass additional parameters to the h2oGPT server when using the OpenAI API. This allows controlling parameters not normally part of the OpenAI API. Any parameters normally passed to the gradio client can be passed this way.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nbase_url = 'http://localhost:5000/v1'\napi_key = 'INSERT KEY HERE or set to EMPTY if no key set on h2oGPT server'\nmodel = '<model name>'\n\nclient_args = dict(base_url=base_url, api_key=api_key)\nopenai_client = OpenAI(**client_args)\n\nmessages = [{'role': 'user', 'content': 'Who are you?'}]\nstream = False\nclient_kwargs = dict(model=model, max_tokens=200, stream=stream, messages=messages,\n                     user='username:password',\n                     extra_body=dict(langchain_mode='UserData'))\nclient = openai_client.chat.completions\n\nresponses = client.create(**client_kwargs)\ntext = responses.choices[0].message.content\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Setting PYTHONPATH (Shell)\nDESCRIPTION: This snippet sets the PYTHONPATH environment variable.  This can resolve import problems during h2oGPT execution.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nSET PYTHONPATH=.:src:$PYTHONPATH\npython generate.py ...\n```\n\n----------------------------------------\n\nTITLE: Run Client Test\nDESCRIPTION: This script runs a client test using `identity_test.py` to verify the Triton endpoint.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/TRITON.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython3 ${WORKSPACE}/tools/gpt/identity_test.py\n```\n\n----------------------------------------\n\nTITLE: Dependency Declaration - pymupdf4llm\nDESCRIPTION: Declares a dependency on the pymupdf4llm Python package with a version constraint of >=0.0.12.  The package is licensed under the AGPL license.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_langchain.gpllike.txt#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npymupdf4llm>=0.0.12\n```\n\n----------------------------------------\n\nTITLE: Run h2oGPT Generation Script\nDESCRIPTION: This snippet runs the h2oGPT generation script in the background. It sets the GRADIO_SERVER_PORT environment variable and then executes the generate.py script with specified parameters, redirecting output to a logs.txt file. It also waits for 20 seconds for the server to start.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_CPU.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nget_ipython().system_raw(\"\"\"GRADIO_SERVER_PORT=7860 python generate.py --base_model='llama' --prompt_type=llama2 --score_model=None --langchain_mode=LLM --langchain_modes=\\'['LLM', 'UserData', 'MyData']\\' --user_path=user_path --share=False &> logs.txt &\"\"\")\n# wait a bit for server to come up\nimport time\ntime.sleep(20)\n```\n\n----------------------------------------\n\nTITLE: Installing LLaMa/GPT4All Dependencies\nDESCRIPTION: This snippet installs optional dependencies for CPU-based LLaMa and GPT4All models. It also sets environment variables to enable Metal support for LLaMa.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Required for CPU: LLaMa/GPT4All:\npip uninstall -y llama-cpp-python llama-cpp-python-cuda\nexport CMAKE_ARGS=-DLLAMA_METAL=on\nexport FORCE_CMAKE=1\npip install -r reqs_optional/requirements_optional_llamacpp_gpt4all.txt -c reqs_optional/reqs_constraints.txt --no-cache-dir\n```\n\n----------------------------------------\n\nTITLE: Speed Up PDF Ingestion with Docker (bash)\nDESCRIPTION: This snippet provides an example of using Docker to speed up PDF ingestion, skip complex PDFs, and use a faster embedding model.  It mounts volumes for caching, saving, user data, and database directories. Requires Docker and nvidia-docker to be installed.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p ~/.cache\nmkdir -p ~/save\nmkdir -p ~/user_path\nmkdir -p ~/db_dir_UserData\ndocker run \\\n       --gpus all \\\n       --runtime=nvidia \\\n       --shm-size=2g \\\n       --rm --init \\\n       --network host \\\n       -v /etc/passwd:/etc/passwd:ro \\\n       -v /etc/group:/etc/group:ro \\\n       -u `id -u`:`id -g` \\\n       -v \"${HOME}\"/.cache:/workspace/.cache \\\n       -v \"${HOME}\"/save:/workspace/save \\\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 /workspace/src/make_db.py --verbose --use_unstructured_pdf=False --enable_pdf_ocr=False --hf_embedding_model=BAAI/bge-small-en-v1.5 --cut_distance=10000\n```\n\n----------------------------------------\n\nTITLE: Download Docker Compose file for Weaviate\nDESCRIPTION: This command downloads a `docker-compose.yml` file for setting up a Weaviate instance using Docker Compose.  The command specifies the Weaviate version and modules.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncurl -o docker-compose.yml \"https://configuration.weaviate.io/v2/docker-compose/docker-compose.yml?modules=standalone&runtime=docker-compose&weaviate_version=v1.19.6\"\n```\n\n----------------------------------------\n\nTITLE: Installing Python 3.10\nDESCRIPTION: Installs Python 3.10 using apt-get. This includes adding the deadsnakes PPA, installing the python3.10 package and distutils, and updating pip to the latest version for Python 3.10. This provides a specific Python version for the project.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_GPU.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Install pyhon 3.10 that will be used within pipenv\n!sudo add-apt-repository ppa:deadsnakes/ppa -y > /dev/null\n!sudo apt install python3.10 python3.10-distutils psmisc -y > /dev/null\n!curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10 > /dev/null\n```\n\n----------------------------------------\n\nTITLE: Summarizing All Documents in Collection Python\nDESCRIPTION: This snippet summarizes all documents within a collection, leveraging the `SUMMARIZE_MAP` action.  It constructs a dictionary of parameters, sends the parameters to the `/submit_nochat_api` and retrieves and prints the response. The primary difference from the single-document summarization is the absence of a specific `document_choice`, indicating that all documents in the collection should be used.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ninstruction = None\nlangchain_action = LangChainAction.SUMMARIZE_MAP.value\nstream_output = False\ntop_k_docs = 5\n\npre_prompt_summary = \"\"\"In order to write a concise single-paragraph or bulleted list summary, pay attention to the following text\\n\"\"\"\nprompt_summary = \"Using only the text above, write a condensed and concise summary of key results as 5 bullet points:\\n\"\n\npre_prompt_query = None\nprompt_query = None\n\nkwargs = dict(instruction=instruction,\n            langchain_mode=langchain_mode,\n            langchain_action=langchain_action,  # uses full document, not vectorDB chunks\n            top_k_docs=top_k_docs,\n            stream_output=stream_output,\n            document_subset='Relevant',\n            #document_choice=document_choice,\n            max_new_tokens=256,\n            max_time=360,\n            do_sample=False,\n            pre_prompt_query=pre_prompt_query,\n            prompt_query=prompt_query,\n            pre_prompt_summary=pre_prompt_summary,\n            prompt_summary=prompt_summary,\n            h2ogpt_key=H2OGPT_KEY\n            )\n\n# get result\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\nresponse = ast.literal_eval(res)['response']\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Install Python Dependencies\nDESCRIPTION: This snippet installs the required Python dependencies using pip. It iterates through a list of requirements files and installs the packages specified in each file.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_CPU.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Install dependencies\n!for fil in requirements.txt reqs_optional/requirements_optional_langchain.txt reqs_optional/requirements_optional_llamacpp_gpt4all.txt reqs_optional/requirements_optional_langchain.gpllike.txt reqs_optional/requirements_optional_langchain.urls.txt ; do pip install -r $fil ; done\n```\n\n----------------------------------------\n\nTITLE: Unstructured Installation with Extra Dependencies\nDESCRIPTION: Installs the `unstructured` package with specific extra dependencies: `local-inference` at version 0.12.5 and `all-docs` at version 0.12.5. This is necessary for full image support and processing all document types.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_langchain.txt#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nunstructured[local-inference]==0.12.5\nunstructured[all-docs]==0.12.5\n```\n\n----------------------------------------\n\nTITLE: Dependency Declaration - pymupdf\nDESCRIPTION: Declares a dependency on the pymupdf Python package with a version constraint of >=1.23.8. The package is licensed under the AGPL license.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_langchain.gpllike.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npymupdf>=1.23.8\n```\n\n----------------------------------------\n\nTITLE: Cloning h2oGPT Repository\nDESCRIPTION: This snippet shows how to clone the h2oGPT repository from GitHub. This is the first step to get started with using h2oGPT as a developer. It uses the standard git clone command.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/blog/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone https://github.com/h2oai/h2ogpt.git\n```\n\n----------------------------------------\n\nTITLE: Gradio Client Initialization with Authentication (Python)\nDESCRIPTION: This snippet demonstrates how to initialize the Gradio client with authentication credentials. It's used to connect to a h2oGPT server that requires a username and password.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_CLIENT.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclient = Client('http://localhost:7860', auth=('user', 'pass'))\n```\n\n----------------------------------------\n\nTITLE: Dependencies for DAI\nDESCRIPTION: Specifies dependencies for integrating with Driverless AI. These packages facilitate communication with a Driverless AI instance for automated machine learning.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_agents.txt#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nh2o_engine_manager\nh2o_authn\n```\n\n----------------------------------------\n\nTITLE: Build MacOS Installer (CPU Only - Deployment Mode)\nDESCRIPTION: Builds the MacOS installer for the CPU-only version of h2ogpt in deployment mode. This command navigates to the h2ogpt directory and then executes the `build_mac_installer.sh` script.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/dev_installers/mac/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd h2ogpt\n. ./dev_installers/mac/build_mac_installer.sh\n```\n\n----------------------------------------\n\nTITLE: Run make_db with Weaviate using Python\nDESCRIPTION: This command runs the `make_db.py` script with the `--db_type=weaviate` argument to use an embedded Weaviate instance.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npython src/make_db.py --db_type=weaviate\n```\n\n----------------------------------------\n\nTITLE: Declaring faiss-cpu dependency\nDESCRIPTION: Specifies the faiss-cpu dependency with a minimum version of 1.7.4. Faiss is used for efficient similarity search and clustering of dense vectors. This dependency ensures that the h2ogpt project can perform vector operations efficiently.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_cpu_only.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nfaiss-cpu>=1.7.4\n```\n\n----------------------------------------\n\nTITLE: Model Configuration (JSON Placeholder)\nDESCRIPTION: This is a placeholder for the model configuration in JSON format. The actual configuration details would be inserted here.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/models/README-template.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n<<MODEL_CONFIG>>\n```\n\n----------------------------------------\n\nTITLE: Printing API Response Python\nDESCRIPTION: This snippet uses `pprint` from the `pprint` module to pretty-print the API response. This allows for easy inspection of the returned data structure and values returned by the `/add_file_api` endpoint.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/notebooks/h2oGPT_api_examples.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npprint(res)\n```\n\n----------------------------------------\n\nTITLE: Run End-to-End Test\nDESCRIPTION: This script runs the end-to-end test using `end_to_end_test.py` to verify the complete pipeline.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/TRITON.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython3 ${WORKSPACE}/tools/gpt/end_to_end_test.py\n```\n\n----------------------------------------\n\nTITLE: Run h2oGPT Installation Script\nDESCRIPTION: Downloads and executes the h2oGPT installation script from a remote URL. This script automates the installation process. User will need to enter their sudo password.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -fsSL https://h2o-release.s3.amazonaws.com/h2ogpt/linux_install_full.sh | bash\n```\n\n----------------------------------------\n\nTITLE: InstructorEmbedding Installation from Wheel\nDESCRIPTION: Installs the `InstructorEmbedding` package from a wheel file hosted on an AWS S3 bucket. This ensures a specific version and potentially a custom build of the library is used.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_langchain.txt#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nInstructorEmbedding @ https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Dependencies for plotting\nDESCRIPTION: Specifies the Seaborn package, used for data visualization and plotting.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_agents.txt#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nseaborn\n```\n\n----------------------------------------\n\nTITLE: Install Qdrant/Weaviate requirements\nDESCRIPTION: Installs the Python dependencies needed for integrating Qdrant or Weaviate vector databases with h2oGPT LangChain. The `requirements_optional_langchain.txt` file contains the list of packages to be installed using pip.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r reqs_optional/requirements_optional_langchain.txt\n```\n\n----------------------------------------\n\nTITLE: Sentence Transformers Old Installation from Wheel\nDESCRIPTION: Installs the `sentence_transformers_old` package from a wheel file hosted on an AWS S3 bucket.  This specifies a specific version (`2.2.2`) of the library.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_langchain.txt#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nsentence_transformers_old @ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Install search package with pip\nDESCRIPTION: This command installs the required packages for web search integration, including LangChain and SerpAPI dependencies, from the specified requirements file.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_SerpAPI.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r reqs_optional/requirements_optional_agents.txt\n```\n\n----------------------------------------\n\nTITLE: Set PIP Extra Index URL for CUDA 12.1\nDESCRIPTION: Sets the PIP_EXTRA_INDEX_URL environment variable to specify additional package repositories for PyTorch and AutoGPTQ built for CUDA 12.1. This allows pip to find the correct versions of these packages.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu121 https://huggingface.github.io/autogptq-index/whl/cu121\"\n```\n\n----------------------------------------\n\nTITLE: Check Python Version\nDESCRIPTION: Verifies the Python version and ensures that the basic Python functionality is working. It prints the Python version and a simple 'hello world' message.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython --version\npython -c \"import os, sys ; print('hello world')\"\n```\n\n----------------------------------------\n\nTITLE: Install Python 3.10\nDESCRIPTION: This snippet installs Python 3.10 using apt-get and sets it up for use with pipenv. It adds the deadsnakes PPA, installs Python 3.10 and distutils, and then installs pip for Python 3.10.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/h2oGPT_CPU.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Install pyhon 3.10 that will be used within pipenv\n!sudo add-apt-repository ppa:deadsnakes/ppa -y > /dev/null\n!sudo apt install python3.10 python3.10-distutils psmisc -y > /dev/null\n!curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10 > /dev/null\n```\n\n----------------------------------------\n\nTITLE: Optimizing h2oGPT Performance with TGI\nDESCRIPTION: These options enhance summarization performance, enable auto-detection of file changes in `--user_path`, and maximize document filling of context when connecting to a TGI server.  These parameters adjust asynchronous processing and document handling for optimal performance.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n          --num_async=10 \\\n          --top_k_docs=-1\n          --detect_user_path_changes_every_query=True\n```\n\n----------------------------------------\n\nTITLE: CUDA Error Message\nDESCRIPTION: This error message indicates a CUDA initialization failure when using multiprocessing. It typically occurs when a zip file containing images or PDFs intended for processing with Florence-2/DocTR is uploaded. The suggested workaround is to upload the zip file separately.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nCannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n```\n\n----------------------------------------\n\nTITLE: Using Lisp at Interleaf\nDESCRIPTION: This snippet illustrates Interleaf's decision to add a scripting language to its software, choosing a dialect of Lisp, inspired by Emacs. This choice enabled a Lisp hacker to write scripts and extend the software's functionality.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/tests/1paul_graham.txt#_snippet_1\n\nLANGUAGE: Lisp\nCODE:\n```\nN/A\n```\n\n----------------------------------------\n\nTITLE: Running h2oGPT from command line\nDESCRIPTION: This snippet shows how to run h2oGPT from the command line, specifying the `base_model` parameter. It sets `CUDA_VISIBLE_DEVICES` to select a specific GPU, if available.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_WHEEL.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0 python -m h2ogpt.generate --base_model=llama\n```\n\n----------------------------------------\n\nTITLE: Commented Dependency Declaration - extract-msg\nDESCRIPTION: Commented out dependency declaration for the extract-msg Python package with a specific version of 0.41.1.  The package is licensed under the GPL3 license, implying that it is not currently included as a direct dependency but may have been considered.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/reqs_optional/requirements_optional_langchain.gpllike.txt#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# extract-msg==0.41.1\n```\n\n----------------------------------------\n\nTITLE: Update Ubuntu System (Old Ubuntu 18)\nDESCRIPTION: Updates and upgrades the Ubuntu system. This is only for very old Ubuntu systems (like 18) and might cause system issues. It is not recommended for Ubuntu 20 or 22.\nSOURCE: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\napt-get clean all\napt-get update\napt-get -y full-upgrade\napt-get -y dist-upgrade\napt-get -y autoremove\napt-get clean all\n```"
  }
]
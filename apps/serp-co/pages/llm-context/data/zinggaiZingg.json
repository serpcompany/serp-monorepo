[
  {
    "owner": "zinggai",
    "repo": "zingg",
    "content": "TITLE: Configuring Field Definition with Advanced Match Types in Zingg\nDESCRIPTION: Example configuration for a field definition using advanced match types. This snippet shows how to define a 'name' field that uses a combination of 'mapping_companies' and 'exact' match types.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/configuration/adv-matchtypes.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"fieldDefinition\":[\n   \t{\n   \t\t\"fieldName\" : \"name\",\n   \t\t\"matchType\" : \"mapping_companies,exact\",\n   \t\t\"fields\" : \"name\",\n   \t\t\"dataType\": \"string\"\n   \t},\n```\n\n----------------------------------------\n\nTITLE: Configuring and Executing Zingg for Entity Matching in Python\nDESCRIPTION: This snippet demonstrates the complete workflow for using Zingg's entity matching capabilities. It shows how to define fields with match types, configure Zingg arguments including model parameters, set up input/output data pipes for CSV files, and execute the matching process.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/python/markdown/index.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom zingg.client import *\nfrom zingg.pipes import *\n\n#build the arguments for zingg\nargs = Arguments()\n#set field definitions\nfname = FieldDefinition(\"fname\", \"string\", MatchType.FUZZY)\nlname = FieldDefinition(\"lname\", \"string\", MatchType.FUZZY)\nstNo = FieldDefinition(\"stNo\", \"string\", MatchType.FUZZY)\nadd1 = FieldDefinition(\"add1\",\"string\", MatchType.FUZZY)\nadd2 = FieldDefinition(\"add2\", \"string\", MatchType.FUZZY)\ncity = FieldDefinition(\"city\", \"string\", MatchType.FUZZY)\nareacode = FieldDefinition(\"areacode\", \"string\", MatchType.FUZZY)\nstate = FieldDefinition(\"state\", \"string\", MatchType.FUZZY)\ndob = FieldDefinition(\"dob\", \"string\", MatchType.FUZZY)\nssn = FieldDefinition(\"ssn\", \"string\", MatchType.FUZZY)\n\nfieldDefs = [fname, lname, stNo, add1, add2, city, areacode, state, dob, ssn]\n\nargs.setFieldDefinition(fieldDefs)\n#set the modelid and the zingg dir\nargs.setModelId(\"100\")\nargs.setZinggDir(\"models\")\nargs.setNumPartitions(4)\nargs.setLabelDataSampleSize(0.5)\n\n#reading dataset into inputPipe and settint it up in 'args'\n#below line should not be required if you are reading from in memory dataset\n#in that case, replace df with input df\nschema = \"id string, fname string, lname string, stNo string, add1 string, add2 string, city string, areacode string, state string, dob string, ssn  string\"\ninputPipe = CsvPipe(\"testFebrl\", \"examples/febrl/test.csv\", schema)\nargs.setData(inputPipe)\noutputPipe = CsvPipe(\"resultFebrl\", \"/tmp/febrlOutput\")\n\nargs.setOutput(outputPipe)\n\noptions = ClientOptions([ClientOptions.PHASE,\"match\"])\n\n#Zingg execution for the given phase\nzingg = Zingg(args, options)\nzingg.initAndExecute()\n```\n\n----------------------------------------\n\nTITLE: Initializing Zingg Client in Python\nDESCRIPTION: The Zingg class is the main interface for the Zingg matching product. It initializes a client with provided arguments and options, and includes methods for executing various phases of the matching process.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/python/markdown/zingg.md#2025-04-23_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass Zingg(object):\n    def __init__(self, args, options):\n        # Initialize Zingg client\n        pass\n\n    def execute(self):\n        # Execute matching process\n        pass\n\n    def executeLabel(self):\n        # Run label phase\n        pass\n\n    def executeLabelUpdate(self):\n        # Run label update phase\n        pass\n```\n\n----------------------------------------\n\nTITLE: Implementing Entity Resolution with Zingg Python API\nDESCRIPTION: Example demonstrating how to use the Zingg Python API for entity resolution. The code shows how to configure field definitions with different match types, set up model parameters, define input/output data pipes, and execute the matching process using Zingg client.\nSOURCE: https://github.com/zinggai/zingg/blob/main/python/docs/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom zingg.client import *\nfrom zingg.pipes import *\n\n#build the arguments for zingg\nargs = Arguments()\n#set field definitions\nfname = FieldDefinition(\"fname\", \"string\", MatchType.FUZZY)\nlname = FieldDefinition(\"lname\", \"string\", MatchType.FUZZY)\nstNo = FieldDefinition(\"stNo\", \"string\", MatchType.FUZZY)\nadd1 = FieldDefinition(\"add1\",\"string\", MatchType.FUZZY)\nadd2 = FieldDefinition(\"add2\", \"string\", MatchType.FUZZY)\ncity = FieldDefinition(\"city\", \"string\", MatchType.FUZZY)\nareacode = FieldDefinition(\"areacode\", \"string\", MatchType.FUZZY)\nstate = FieldDefinition(\"state\", \"string\", MatchType.FUZZY)\ndob = FieldDefinition(\"dob\", \"string\", MatchType.FUZZY)\nssn = FieldDefinition(\"ssn\", \"string\", MatchType.FUZZY)\n\nfieldDefs = [fname, lname, stNo, add1, add2, city, areacode, state, dob, ssn]\n\nargs.setFieldDefinition(fieldDefs)\n#set the modelid and the zingg dir\nargs.setModelId(\"100\")\nargs.setZinggDir(\"models\")\nargs.setNumPartitions(4)\nargs.setLabelDataSampleSize(0.5)\n\n#reading dataset into inputPipe and settint it up in 'args'\n#below line should not be required if you are reading from in memory dataset\n#in that case, replace df with input df\nschema = \"id string, fname string, lname string, stNo string, add1 string, add2 string, city string, areacode string, state string, dob string, ssn  string\"\ninputPipe = CsvPipe(\"testFebrl\", \"examples/febrl/test.csv\", schema)\nargs.setData(inputPipe)\noutputPipe = CsvPipe(\"resultFebrl\", \"/tmp/febrlOutput\")\n\nargs.setOutput(outputPipe)\n\noptions = ClientOptions([ClientOptions.PHASE,\"match\"])\n\n#Zingg execution for the given phase\nzingg = Zingg(args, options)\nzingg.initAndExecute()\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Docker Container for Entity Matching\nDESCRIPTION: This snippet demonstrates how to pull the Zingg Docker image, run a container, and execute the match phase using a prebuilt model with a sample configuration file.\nSOURCE: https://github.com/zinggai/zingg/blob/main/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull zingg/zingg:0.4.0\ndocker run -it zingg/zingg:0.4.0 bash\n./scripts/zingg.sh --phase match --conf examples/febrl/config.json\n```\n\n----------------------------------------\n\nTITLE: Updating Labels and Training Zingg Model\nDESCRIPTION: Executes the 'updateLabel' and 'trainMatch' phases of Zingg. It updates the labels in the model and initiates the training process for the matching model.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\noptions = ClientOptions([ClientOptions.PHASE,\"updateLabel\"])\nmarkedRecords = getPandasDfFromDs(zingg.getMarkedRecords())\n\n#Zingg execution for the given phase\nzingg = ZinggWithSpark(args, options)\nzingg.init()\nprint(markedRecords)\n```\n\nLANGUAGE: python\nCODE:\n```\noptions = ClientOptions([ClientOptions.PHASE,\"trainMatch\"])\n```\n\n----------------------------------------\n\nTITLE: Initializing FieldDefinition Class - Python\nDESCRIPTION: Class definition for configuring field properties used in data matching. Takes name, dataType and matchType as parameters to define how each field should be processed during matching.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/python/markdown/zingg.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass FieldDefinition(name, dataType, *matchType):\n    Bases: `object`\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Data Source in JSON for Zingg\nDESCRIPTION: This JSON snippet shows the configuration for connecting to a Snowflake database as a data source in Zingg. It includes connection details such as URL, credentials, database, schema, and warehouse information.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/snowflake.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"data\" : [ {\n\t\t\t\"name\":\"test\", \n\t\t\t\"format\":\"net.snowflake.spark.snowflake\", \n\t\t\t\"props\": {\n\t\t\t\t\"sfUrl\": \"rfa59271.snowflakecomputing.com\",\n\t\t\t\t\"sfUser\": \"sonalgoyal\",\n\t\t\t\t\"sfPassword\":\"ZZ\",\t\t\t\t\t\n\t\t\t\t\"sfDatabase\":\"TEST\",\t\t\t\t\n\t\t\t\t\"sfSchema\":\"PUBLIC\",\t\t\t\t\t\n\t\t\t\t\"sfWarehouse\":\"COMPUTE_WH\",\n\t\t\t\t\"dbtable\": \"FEBRL\",\n\t\t\t\t\"application\":\"zingg_zingg\"\t\t\t\t\n\t\t\t}\n\t\t} ]\n```\n\n----------------------------------------\n\nTITLE: Training Zingg Model\nDESCRIPTION: Initializes and executes Zingg model training phase using Spark integration.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\noptions = ClientOptions([ClientOptions.PHASE,\"trainMatch\"])\n\n#Zingg execution for the given phase\nzingg = ZinggWithSpark(args, options)\nzingg.initAndExecute()\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Connection in Zingg JSON\nDESCRIPTION: This JSON snippet shows how to configure a MongoDB connection in Zingg. It specifies the data source name, format, and connection properties including the MongoDB URI.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/mongodb.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"data\" : [{\n\t\t\"name\":\"mongodb\", \n\t\t\"format\":\"mongo\", \n\t\t\"props\": {\n\t\t\t\"uri\": \"mongodb://127.0.0.1/people.contacts\"\t\t\n\t\t\t}\t\n\t\t}]\n```\n\n----------------------------------------\n\nTITLE: Defining Field Definitions for Matching\nDESCRIPTION: Creates field definitions for each attribute in the dataset, specifying the match type (fuzzy, exact, or don't use) for each field.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrec_id = FieldDefinition(\"rec_id\", \"string\", MatchType.DONT_USE)\nfname = FieldDefinition(\"fname\", \"string\", MatchType.FUZZY)  # First Name\nlname = FieldDefinition(\"lname\", \"string\", MatchType.FUZZY)  # Last Name\nstNo = FieldDefinition(\"stNo\", \"string\", MatchType.FUZZY)    # Street Number\nadd1 = FieldDefinition(\"add1\", \"string\", MatchType.FUZZY)    # Address Line 1\nadd2 = FieldDefinition(\"add2\", \"string\", MatchType.FUZZY)    # Address Line 2\ncity = FieldDefinition(\"city\", \"string\", MatchType.FUZZY)    # City\nstate = FieldDefinition(\"state\", \"string\", MatchType.FUZZY)  # State\ndob = FieldDefinition(\"dob\", \"string\", MatchType.EXACT)      # Date of Birth (prefer exact match)\nssn = FieldDefinition(\"ssn\", \"string\", MatchType.EXACT)      # SSN (should use exact match)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Create the field definitions list\nfieldDefs = [rec_id, fname, lname, stNo, add1, add2, city, state, dob, ssn]\n# Set field definitions in args\nargs.setFieldDefinition(fieldDefs)\n```\n\nLANGUAGE: python\nCODE:\n```\nargs.setNumPartitions(4)\nargs.setLabelDataSampleSize(0.5)\n```\n\n----------------------------------------\n\nTITLE: Defining Field Definitions for Zingg\nDESCRIPTION: Sets up field definitions for the data, specifying match types for each field used in the deduplication process.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Set field definitions\nrec_id = FieldDefinition(\"rec_id\", \"string\", MatchType.EXACT)        # ID should use exact match\nfname = FieldDefinition(\"fname\", \"string\", MatchType.FUZZY)  # First Name\nlname = FieldDefinition(\"lname\", \"string\", MatchType.FUZZY)  # Last Name\nstNo = FieldDefinition(\"stNo\", \"string\", MatchType.FUZZY)    # Street Number\nadd1 = FieldDefinition(\"add1\", \"string\", MatchType.FUZZY)    # Address Line 1\nadd2 = FieldDefinition(\"add2\", \"string\", MatchType.FUZZY)    # Address Line 2\ncity = FieldDefinition(\"city\", \"string\", MatchType.FUZZY)    # City\nstate = FieldDefinition(\"state\", \"string\", MatchType.FUZZY)  # State\ndob = FieldDefinition(\"dob\", \"string\", MatchType.EXACT)      # Date of Birth (prefer exact match)\nssn = FieldDefinition(\"ssn\", \"string\", MatchType.EXACT)      # SSN (should use exact match)\n\n# Create the field definitions list\nfieldDefs = [rec_id, fname, lname, stNo, add1, add2, city, state, dob, ssn]\n\n# Set field definitions in args\nargs.setFieldDefinition(fieldDefs)\n```\n\n----------------------------------------\n\nTITLE: Configuring Neo4j as a Data Source in Zingg\nDESCRIPTION: JSON configuration for connecting to a Neo4j database as a data source in Zingg. This specifies the connection URL, labels to query, and uses the Neo4j Spark connector format.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/neo4j.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"data\" : [{\n\t\t\"name\":\"neo\", \n\t\t\"format\":\"org.neo4j.spark.DataSource\", \n\t\t\"props\": {\n\t\t\t\"url\": \"bolt://localhost:7687\",\n            \"labels\":\"Person\"\t\t\n\t\t\t}\t\n\t\t}]\n```\n\n----------------------------------------\n\nTITLE: Executing Zingg Match Phase\nDESCRIPTION: Command to run Zingg's match phase using a configuration file. The match phase assigns cluster IDs and match scores to identify matching records in the dataset.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/setup/match.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./zingg.sh --phase match --conf config.json\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL JDBC Connection Properties in JSON\nDESCRIPTION: JSON configuration block for establishing a MySQL database connection through JDBC. Includes essential connection properties like database URL, table name, driver class, and authentication credentials.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/connectors/jdbc/mysql.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n    \"data\" : [{\n        \"name\":\"test\", \n        \"format\":\"jdbc\", \n        \"props\": {\n            \"url\": \"jdbc:mysql://localhost:3306/<db_name>\",\n            \"dbtable\": \"testData\",\n            \"driver\": \"com.mysql.cj.jdbc.Driver\",\n            \"user\": \"root\",\n            \"password\": \"password\"\t\t\t\t\n        }\n    }],\n```\n\n----------------------------------------\n\nTITLE: Executing Zingg Link Phase via Bash Command\nDESCRIPTION: Command to run the link phase in Zingg, which matches records across different datasets. This requires a configuration file that defines the data sources and matching parameters.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/setup/link.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./zingg.sh --phase link --conf config.json\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Data Processing with Zingg Python Client\nDESCRIPTION: Python code that demonstrates how to set up and execute incremental data processing using Zingg's Python client. It configures field definitions, initializes the model, runs the findAndLabel and trainMatch phases, and then processes incremental data.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/runIncremental.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#import the packages\n\nfrom zingg.client import *\nfrom zingg.pipes import *\nfrom zinggEC.enterprise.common.ApproverArguments import *\nfrom zinggEC.enterprise.common.IncrementalArguments import *\nfrom zinggEC.enterprise.common.epipes import *\nfrom zinggEC.enterprise.common.EArguments import *\nfrom zinggEC.enterprise.common.EFieldDefinition import EFieldDefinition\nfrom zinggES.enterprise.spark.ESparkClient import EZingg\nimport os\n\n#build the arguments for zingg\nargs = EArguments()\n#set field definitions\nrecId = EFieldDefinition(\"recId\", \"string\", MatchType.DONT_USE)\nrecId.setPrimaryKey(True)\nfname = EFieldDefinition(\"fname\", \"string\", MatchType.FUZZY)\nlname = EFieldDefinition(\"lname\", \"string\", MatchType.FUZZY)\nstNo = EFieldDefinition(\"stNo\", \"string\", MatchType.FUZZY)\nadd1 = EFieldDefinition(\"add1\",\"string\", MatchType.FUZZY)\nadd2 = EFieldDefinition(\"add2\", \"string\", MatchType.FUZZY)\ncity = EFieldDefinition(\"city\", \"string\", MatchType.FUZZY)\nareacode = EFieldDefinition(\"areacode\", \"string\", MatchType.FUZZY)\nstate = EFieldDefinition(\"state\", \"string\", MatchType.FUZZY)\ndob = EFieldDefinition(\"dob\", \"string\", MatchType.FUZZY)\nssn = EFieldDefinition(\"ssn\", \"string\", MatchType.FUZZY)\n\nfieldDefs = [recId, fname, lname, stNo, add1, add2, city, areacode, state, dob, ssn]\nargs.setFieldDefinition(fieldDefs)\n#set the modelid and the zingg dir\nargs.setModelId(\"100\")\nargs.setZinggDir(\"/tmp/models\")\nargs.setNumPartitions(4)\nargs.setLabelDataSampleSize(0.5)\n\n#reading dataset into inputPipe and settint it up in 'args'\nschema = \"recId string, fname string, lname string, stNo string, add1 string, add2 string, city string, areacode string, state string, dob string, ssn  string\"\ninputPipe = ECsvPipe(\"testFebrl\", \"examples/febrl/test.csv\", schema)\nargs.setData(inputPipe)\n\noutputPipe = ECsvPipe(\"resultFebrl\", \"/tmp/febrlOutput\")\noutputPipe.setHeader(\"true\")\n\nargs.setOutput(outputPipe)\n\n#Run findAndLabel\noptions = ClientOptions([ClientOptions.PHASE,\"findAndLabel\"])\nzingg = EZingg(args, options)\nzingg.initAndExecute()\n\n#Run trainMatch after above completes\noptions = ClientOptions([ClientOptions.PHASE,\"trainMatch\"])\nzingg = EZingg(args, options)\nzingg.initAndExecute()\n\n#Now run incremental on output generated above\nincrArgs = IncrementalArguments()\nincrArgs.setParentArgs(args)\nincrPipe = ECsvPipe(\"testFebrlIncr\", \"examples/febrl/test-incr.csv\", schema)\nincrArgs.setIncrementalData(incrPipe)\n\noptions = ClientOptions([ClientOptions.PHASE,\"runIncremental\"])\nzingg = EZingg(incrArgs, options)\nzingg.initAndExecute()\n```\n\n----------------------------------------\n\nTITLE: Configuring Cassandra Output in Zingg\nDESCRIPTION: JSON configuration for setting up Cassandra as an output data sink in Zingg. This snippet defines the connection properties including table name, keyspace, cluster name, and connection host. It also specifies Spark-specific connection properties and the write mode.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/cassandra.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"output\" : [\n\t\t\t{\n\t\t\t\"name\":\"sampleTest\", \n\t\t\t\"format\":\"CASSANDRA\" ,\n\t\t\t\"props\": {\n\t\t\t\t\"table\":\"dataschematest\",\n\t\t\t\t\"keyspace\":\"zingg\",\n\t\t\t\t\"cluster\":\"zingg\",\n\t\t\t\t\"spark.cassandra.connection.host\":\"192.168.0.6\"\n\t\t\t},\n\t\t\t\"sparkProps\": {\n\t\t\t\t\"spark.cassandra.connection.host\":\"127.0.0.1\"\n\t\t\t},\n\t\t\t\"mode\":\"Append\"\n\t\t}\n\t\t]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Zingg Arguments and Input Data\nDESCRIPTION: Configures the Zingg arguments including model ID, directory, and input data schema. It sets up a CSV input pipe for the data source.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#build the arguments for zingg\nargs = Arguments()\n# Set the modelid and the zingg dir. You can use this as is\nargs.setModelId(modelId)\nargs.setZinggDir(zinggDir)\n```\n\nLANGUAGE: python\nCODE:\n```\nschema = \"rec_id string, fname string, lname string, stNo string, add1 string, add2 string, city string, state string, dob string, ssn string\"\ninputPipe = CsvPipe(\"testFebrl\", \"/FileStore/tables/test.csv\", schema)\nargs.setData(inputPipe)\n```\n\nLANGUAGE: python\nCODE:\n```\n#setting outputpipe in 'args'\noutputPipe = CsvPipe(\"resultOutput\", \"/tmp/outputApr1\")\nargs.setOutput(outputPipe)\n```\n\n----------------------------------------\n\nTITLE: Implementing Deterministic Matching in Python\nDESCRIPTION: Python code example showing how to create and configure deterministic matching conditions using the Zingg API. Sets up three matching rules corresponding to the JSON configuration.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/deterministicMatching.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndetMatchNameAdd = DeterministicMatching('fname','stNo','add1')  \ndetMatchNameDobSsn = DeterministicMatching('fname','dob','ssn')  \ndetMatchNameEmail = DeterministicMatching('fname','email')  \nargs.setDeterministicMatchingCondition(detMatchNameAdd,detMatchNameDobSsn,detMatchNameEmail)  \n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries and Setting Up Zingg Interface\nDESCRIPTION: Imports necessary Python libraries including Pandas, NumPy, and Zingg-specific modules. It also defines utility functions for cleaning the model and assigning labels to candidate pairs.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n \nimport time\nimport uuid\n \nfrom tabulate import tabulate\nfrom ipywidgets import widgets, interact, GridspecLayout\nimport base64\nimport pyspark.sql.functions as fn\n\n##this code sets up the Zingg Python interface\nfrom zingg.client import *\nfrom zingg.pipes import *\n\ndef cleanModel():\n    dbutils.fs.rm(MARKED_DIR, recurse=True)\n    # drop unmarked data\n    dbutils.fs.rm(UNMARKED_DIR, recurse=True)\n    return\n\n# assign label to candidate pair\ndef assign_label(candidate_pairs_pd, z_cluster, label):\n  '''\n  The purpose of this function is to assign a label to a candidate pair\n  identified by its z_cluster value.  Valid labels include:\n     0 - not matched\n     1 - matched\n     2 - uncertain\n  '''\n  \n  # assign label\n  candidate_pairs_pd.loc[ candidate_pairs_pd['z_cluster']==z_cluster, 'z_isMatch'] = label\n  \n  return\n \ndef count_labeled_pairs(marked_pd):\n  '''\n  The purpose of this function is to count the labeled pairs in the marked folder.\n  '''\n  n_total = len(np.unique(marked_pd['z_cluster']))\n  n_positive = len(np.unique(marked_pd[marked_pd['z_isMatch']==1]['z_cluster']))\n  n_negative = len(np.unique(marked_pd[marked_pd['z_isMatch']==0]['z_cluster']))\n  \n  return n_positive, n_negative, n_total\n# setup widget \navailable_labels = {\n    'No Match':0,\n    'Match':1,\n    'Uncertain':2\n    }\n```\n\n----------------------------------------\n\nTITLE: Training Zingg Model via Command Line\nDESCRIPTION: Command to execute Zingg's model training phase using a configuration file. The trained model will be saved to zinggDir/modelId directory as specified in the config.json file.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/setup/train.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./zingg.sh --phase train --conf config.json\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Data Source in Zingg\nDESCRIPTION: This snippet demonstrates how to configure a CSV data source in Zingg using the data array and Zingg Pipes. It includes the definition of the source name, format, properties like delimiter and header information, and a detailed schema definition for the febrl test dataset. The schema defines field names and types for all columns in the CSV file.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/configuration/data-input-and-output/data.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n \"data\" : [ {\n    \"name\" : \"test\",\n    \"format\" : \"csv\",\n    \"props\" : {\n      \"delimiter\" : \",\",\n      \"header\" : \"true\",\n      \"location\" : \"examples/febrl/test.csv\"\n    },\n    \"schema\" : \"{\n      \\\"type\\\":\\\"struct\\\",\n      \\\"fields\\\":[\n      {\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\n      {\\\"name\\\":\\\"firstName\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\n      {\\\"name\\\":\\\"lastName\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\n      {\\\"name\\\":\\\"streetnumber\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\n      {\\\"name\\\":\\\"street\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\n      {\\\"name\\\":\\\"address1\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\n      {\\\"name\\\":\\\"address2\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\n      {\\\"name\\\":\\\"areacode\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\n      {\\\"name\\\":\\\"stateCode\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\n      {\\\"name\\\":\\\"dateOfbirth\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\n      {\\\"name\\\":\\\"ssn\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}\n      ]\n      }\"\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring Input Data for Zingg\nDESCRIPTION: Sets up the input data pipeline for Zingg using a CSV file stored in Microsoft Fabric.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nschema = \"rec_id string, fname string, lname string, stNo string, add1 string, add2 string, city string, state string, dob string, ssn string\"\ninputPipe = CsvPipe(\"testFebrl\", \"abfss://Zingg@onelake.dfs.fabric.microsoft.com/data.Lakehouse/Files/data.csv\", schema)\n\nargs.setData(inputPipe)\n```\n\n----------------------------------------\n\nTITLE: Initializing Arguments Class - Python\nDESCRIPTION: Main class for configuring match arguments with support for defining data location, training, and matching phases. Provides methods for JSON serialization and configuration management.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/python/markdown/zingg.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Arguments:\n    Bases: `object`\n```\n\n----------------------------------------\n\nTITLE: Building Zingg Arguments\nDESCRIPTION: Creates and configures the Arguments object for Zingg, setting the model ID and Zingg directory.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#build the arguments for zingg\nargs = Arguments()\n# Set the modelid and the zingg dir. You can use this as is\nargs.setModelId(modelId)\nargs.setZinggDir(zinggDir)\nprint(args)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Executing Zingg with Spark\nDESCRIPTION: Creates a ZinggWithSpark instance with provided arguments and options, then initializes and executes the Zingg processing job.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Zingg execution for the given phase\nzingg = ZinggWithSpark(args, options)\nzingg.initAndExecute()\n```\n\n----------------------------------------\n\nTITLE: Initializing ClientOptions Class - Python\nDESCRIPTION: Class for configuring Zingg client options including phase selection and command line parameters. Handles core configuration for the matching process.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/python/markdown/zingg.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ClientOptions(argsSent=None):\n    Bases: `object`\n```\n\n----------------------------------------\n\nTITLE: Configuring Deterministic Matching Rules in JSON\nDESCRIPTION: JSON configuration for setting up multiple deterministic matching conditions. Defines three matching rules using combinations of fields like fname, stNo, add1, dob, ssn, and email.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/deterministicMatching.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n    \"deterministicMatching\":[  \n        {  \n           \"matchCondition\":[{\"fieldName\":\"fname\"},{\"fieldName\":\"stNo\"},{\"fieldName\":\"add1\"}]  \n        },  \n        {  \n           \"matchCondition\":[{\"fieldName\":\"fname\"},{\"fieldName\":\"dob\"},{\"fieldName\":\"ssn\"}]  \n        },   \n        {  \n           \"matchCondition\":[{\"fieldName\":\"fname\"},{\"fieldName\":\"email\"}]  \n        }  \n    ]  \n```\n\n----------------------------------------\n\nTITLE: Implementing Snowflake Pipe in Python\nDESCRIPTION: The SnowflakePipe class is used for working with Snowflake pipelines. It provides methods for setting various Snowflake-specific parameters such as database, schema, and credentials.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/python/markdown/zingg.md#2025-04-23_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nclass SnowflakePipe(Pipe):\n    DATABASE = 'sfDatabase'\n    DBTABLE = 'dbtable'\n    PASSWORD = 'sfPassword'\n    SCHEMA = 'sfSchema'\n    URL = 'sfUrl'\n    USER = 'sfUser'\n    WAREHOUSE = 'sfWarehouse'\n\n    def __init__(self, name):\n        super().__init__(name, 'snowflake')\n\n    def setDatabase(self, db):\n        self.addProperty(self.DATABASE, db)\n\n    def setDbTable(self, dbtable):\n        self.addProperty(self.DBTABLE, dbtable)\n\n    def setPassword(self, passwd):\n        self.addProperty(self.PASSWORD, passwd)\n\n    def setSFSchema(self, schema):\n        self.addProperty(self.SCHEMA, schema)\n\n    def setURL(self, url):\n        self.addProperty(self.URL, url)\n```\n\n----------------------------------------\n\nTITLE: Specifying Training Samples in Zingg Configuration (JSON)\nDESCRIPTION: This JSON snippet shows how to include the trainingSamples attribute in the Zingg configuration file to specify pre-existing training data. It refers to an example configuration file that demonstrates the correct format for including training samples.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/setup/training/addOwnTrainingData.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"trainingSamples\": [...]\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Processing Candidate Pairs for Labeling\nDESCRIPTION: Fetches candidate pairs for labeling and processes them. It checks if there are unlabeled pairs and prepares them for manual labeling if available.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# get candidate pairs\ncandidate_pairs_pd = getPandasDfFromDs(zingg.getUnmarkedRecords())\n \n# if no candidate pairs, run job and wait\nif candidate_pairs_pd.shape[0] == 0:\n  print('No unlabeled candidate pairs found.  Run findTraining job ...')\nelse:\n    # get list of pairs (as identified by z_cluster) to label \n    z_clusters = list(np.unique(candidate_pairs_pd['z_cluster'])) \n    # identify last reviewed cluster\n    last_z_cluster = '' # none yet\n    # print candidate pair stats\n    print('{0} candidate pairs found for labeling'.format(len(z_clusters)))\n```\n\n----------------------------------------\n\nTITLE: Initializing Zingg Environment in Microsoft Fabric\nDESCRIPTION: Sets up the Zingg environment, including directory paths, Azure authentication, and utility functions for model management and data labeling.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n## Define constants\nMARKED_DIR = zinggDir + \"/\" + modelId + \"/trainingData/marked/\"\nUNMARKED_DIR = zinggDir + \"/\" + modelId + \"/trainingData/unmarked/\"\n\n# Fill these with your specific details\nstorage_account = \"a1a73dc0-3894-4737-b38c-aa7fea437330\"  # Replace with your storage account ID\nfabric_url = \"dfs.fabric.microsoft.com\"\n\n# Updated paths for Microsoft Fabric\nMARKED_DIR_DBFS = f\"abfss://{storage_account}@{fabric_url}{MARKED_DIR}\"\nUNMARKED_DIR_DBFS = f\"abfss://{storage_account}@{fabric_url}{UNMARKED_DIR}\"\n\n## Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport time\nimport uuid\nfrom tabulate import tabulate\nfrom ipywidgets import widgets, interact, GridspecLayout\nimport base64\nimport pyspark.sql.functions as fn\n\n# Import Azure libraries for Fabric\nfrom azure.identity import DefaultAzureCredential\nfrom azure.storage.filedatalake import DataLakeServiceClient\n\n# Zingg libraries\nfrom zingg.client import *\nfrom zingg.pipes import *\n\n# Setup Fabric authentication\ndef get_service_client():\n    credential = DefaultAzureCredential()\n    service_client = DataLakeServiceClient(\n        account_url=f\"https://{storage_account}.dfs.fabric.microsoft.com\",\n        credential=credential,\n    )\n    return service_client\n\nservice_client = get_service_client()\n\n# Function to clean model directories in Fabric\ndef cleanModel():\n    try:\n        # Access the file system\n        file_system_client = service_client.get_file_system_client(file_system=storage_account)\n        \n        # Remove marked directory\n        if file_system_client.get_directory_client(MARKED_DIR).exists():\n            file_system_client.get_directory_client(MARKED_DIR).delete_directory()\n        \n        # Remove unmarked directory\n        if file_system_client.get_directory_client(UNMARKED_DIR).exists():\n            file_system_client.get_directory_client(UNMARKED_DIR).delete_directory()\n        \n        print(\"Model cleaned successfully.\")\n    except Exception as e:\n        print(f\"Error cleaning model: {str(e)}\")\n    return\n\n# Function to assign label to a candidate pair\ndef assign_label(candidate_pairs_pd, z_cluster, label):\n    '''\n    The purpose of this function is to assign a label to a candidate pair\n    identified by its z_cluster value. Valid labels include:\n       0 - not matched\n       1 - matched\n       2 - uncertain\n    '''\n    # Assign label\n    candidate_pairs_pd.loc[candidate_pairs_pd['z_cluster'] == z_cluster, 'z_isMatch'] = label\n    return\n\n# Function to count labeled pairs\ndef count_labeled_pairs(marked_pd):\n    '''\n    The purpose of this function is to count the labeled pairs in the marked folder.\n    '''\n    n_total = len(np.unique(marked_pd['z_cluster']))\n    n_positive = len(np.unique(marked_pd[marked_pd['z_isMatch'] == 1]['z_cluster']))\n    n_negative = len(np.unique(marked_pd[marked_pd['z_isMatch'] == 0]['z_cluster']))\n\n    return n_positive, n_negative, n_total\n\n# Setup interactive widget\navailable_labels = {\n    'No Match': 0,\n    'Match': 1,\n    'Uncertain': 2\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Stopwords to Field Definition in Zingg Configuration\nDESCRIPTION: JSON configuration showing how to integrate a stopwords file into a field definition in Zingg. The example demonstrates configuring a fuzzy match field with a reference to a CSV file containing stopwords.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/accuracy/stopWordsRemoval.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"fieldDefinition\":[\n   \t{\n   \t\t\"fieldName\" : \"fname\",\n   \t\t\"matchType\" : \"fuzzy\",\n   \t\t\"fields\" : \"fname\",\n   \t\t\"dataType\": \"string\",\n   \t\t\"stopWords\": \"models/100/stopWords/fname.csv\"\n   \t},\n```\n\n----------------------------------------\n\nTITLE: Setting Up Interactive Labeling Interface\nDESCRIPTION: Prepares the interactive labeling interface for manual classification of candidate pairs.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Label Training Set\n\n# define variable to avoid duplicate saves\nready_for_save = False\nprint(candidate_pairs_pd)\n\n# user-friendly labels and corresponding zingg numerical value\n# (the order in the dictionary affects how displayed below)\nLABELS = {\n  'Uncertain':2,\n  'Match':1,\n  'No Match':0  \n  }\n\n# GET CANDIDATE PAIRS\n# ========================================================\n#candidate_pairs_pd = get_candidate_pairs()\nn_pairs = int(candidate_pairs_pd.shape[0]/2)\n# ========================================================\n\n# DEFINE IPYWIDGET DISPLAY\n# ========================================================\ndisplay_pd = candidate_pairs_pd.drop(\n  labels=[\n    'z_zid', 'z_prediction', 'z_score', 'z_isMatch', 'z_zsource'\n    ], \n  axis=1)\n\n# define header to be used with each displayed pair\nhtml_prefix = \"<p><span style='font-family:Courier New,Courier,monospace'>\"\nhtml_suffix = \"</p></span>\"\nheader = widgets.HTML(value=f\"{html_prefix}<b>\" + \"<br />\".join([str(i)+\"&nbsp;&nbsp;\" for i in display_pd.columns.to_list()]) + f\"</b>{html_suffix}\")\n\n# initialize display\nvContainers = []\nvContainers.append(widgets.HTML(value=f'<h2>Indicate if each of the {n_pairs} record pairs is a match or not</h2></p>'))\n```\n\n----------------------------------------\n\nTITLE: Configuring Pass Through Data Expression in JSON\nDESCRIPTION: This JSON snippet shows how to define a pass-through expression in Zingg configuration to identify records that should be excluded from matching. In this example, records where 'is_deceased' equals true will be treated as pass-through records.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/passthru.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n     \"passthroughExpr\": \"is_deceased = true\"\n```\n\n----------------------------------------\n\nTITLE: Creating EMR Cluster and Submitting Zingg Job with spark-submit on AWS\nDESCRIPTION: This command creates an EMR cluster and submits a Zingg job using spark-submit. It specifies the cluster configuration, Zingg application, and job parameters. The config.json file must be available locally on the driver.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/running/aws.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naws emr create-cluster --name \"Add Spark Step Cluster\" --release-label emr-6.2.0 --applications Name=Zingg \\\n--ec2-attributes KeyName=myKey --instance-type <instance type> --instance-count <num instances> \\\n--steps Type=Spark,Name=\"Zingg\",ActionOnFailure=CONTINUE,Args=[--class,zingg.spark.client.SparkClient,<s3 location of zingg.jar>,--phase,<name of phase - findTrainingData,match etc>,--conf,<local location of config.json>] --use-default-roles\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQuery as a Data Source in Zingg\nDESCRIPTION: JSON configuration for reading data from BigQuery in Zingg. Specifies credentials file, table location, and enables views for reading data from BigQuery.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/bigquery.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n    \"data\" : [{\n        \"name\":\"test\", \n         \"format\":\"bigquery\", \n        \"props\": {\n            \"credentialsFile\": \"/home/work/product/final/zingg-1/mynotification-46566-905cbfd2723f.json\",\n            \"table\": \"mynotification-46566.zinggdataset.zinggtest\",\n            \"viewsEnabled\": true\n        }\n    }],\n```\n\n----------------------------------------\n\nTITLE: Configuring Combined Match Models in Zingg Enterprise (JSON)\nDESCRIPTION: This JSON configuration demonstrates how to set up multiple vertices (spouse and household) with different data sources and matching strategies. It includes a Snowflake query for spouse data and a separate matching model for household data, combining them into a unified graph structure.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/relations.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \n    \"vertices\" : \n    [ \n        { \n            \"name\" : \"spouse\",  \n            \"vertexType\" : \"zingg_pipe\", \n            \"data\" : [\n                {\n                \"name\" : \"spouse\", \n                \"format\" : \"snowflake\", \n                \"props\": {\n                        \"query\": \"select a.id as id, a.FNAME, a.LNAME, a.STNO, a.ADD1, a.CITY, a.STATE, a.ZINGG_ID_PERSON, b.id as z_id, b.fname as Z_FNAME,b.lname as Z_LNAME,b.stno as Z_STNO,b.add1 as Z_ADD1, b.city as Z_CITY,b.state as Z_STATE, b.ZINGG_ID_PERSON as Z_ZINGG_ID_PERSON from CUSTOMER_RELATE_PARTIAL a, CUSTOMER_RELATE_PARTIAL b where a.familyId = b.familyId\"\n                        }\n                }\n                ],\n            \"edges\" :  \n            {   \"edgeType\" : \"same_edge\",\n                \"edges\":[\n                    {\n                        \"dataColumn\" : \"zingg_personId\",\n                        \"column\" : \"zingg_personId\",\n                        \"name\" : \"zingg_personId1\"\n                    },\n                    {\n                        \"dataColumn\" : \"zingg_personId\",\n                        \"column\" : \"z_zingg_personId\",\n                        \"name\" : \"zingg_personId2\"\n                    }\n                ]\n            }\n        },\n        { \n            \"name\" : \"household\",\n            \"config\" : \"$ZINGG_ENTERPRISE_HOME$/zinggEnterprise/configHousehold.json\", \n            \"strategy\" : {\n                \"vDataStrategy\" : \"unique_edge\",\n                \"props\" : {\n                        \"column\" : \"zingg_personId\",\n                        \"edge\" : \"zingg_personId,z_zingg_personId\"\n                    }\n            },\n            \"vertexType\" : \"zingg_match\", \n             \"edges\" :  \n            {   \"edgeType\" : \"same_edge\",\n                \"edges\":[\n                    {\n                        \"dataColumn\" : \"zingg_personId\",\n                        \"column\" : \"zingg_personId\",\n                        \"name\" : \"zingg_personId1\"\n                    },\n                    {\n                        \"dataColumn\" : \"zingg_personId\",\n                        \"column\" : \"z_zingg_personId\",\n                        \"name\" : \"zingg_personId2\"\n                    }\n                ]\n            }\n        }\n    ],\n    \"output\" : [{\n        \"name\":\"relatedCustomers\", \n        \"format\":\"snowflake\", \n        \"props\": {\n            \"table\": \"RELATED_CUSTOMERS_PARTIAL\"\n            }\n    }],\n    \"strategy\":\"pairs_and_vertices\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Exasol as a Data Sink in Zingg\nDESCRIPTION: JSON configuration for using Exasol as an output destination in Zingg. This defines connection parameters and output settings, including the option to create a new table for storing entity resolution results.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/exasol.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n...\\n\\\"output\\\": [\\n   {\\n       \\\"name\\\": \\\"output\\\",\\n       \\\"format\\\": \\\"com.exasol.spark\\\",\\n       \\\"props\\\": {\\n           \\\"host\\\": \\\"10.11.0.2\\\",\\n           \\\"port\\\": \\\"8563\\\",\\n           \\\"username\\\": \\\"sys\\\",\\n           \\\"password\\\": \\\"exasol\\\",\\n           \\\"create_table\\\": \\\"true\\\",\\n           \\\"table\\\": \\\"DB_SCHEMA.ENTITY_RESOLUTION\\\",\\n       },\\n       \\\"mode\\\": \\\"Append\\\"\\n   }\\n],\\n...\n```\n\n----------------------------------------\n\nTITLE: Saving Labeled Data and Updating Zingg Model\nDESCRIPTION: Processes the labeled data from the interactive widget, saves it to the Zingg folder, and updates the Zingg model with the new labels. It also provides a summary of the labeled pairs.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nif not ready_for_save:\n  print('No labels have been assigned. Run the previous cell to create candidate pairs and assign labels to them before re-running this cell.')\n\nelse:\n\n  # ASSIGN LABEL VALUE TO CANDIDATE PAIRS IN DATAFRAME\n  # ========================================================\n  # for each pair in displayed widget\n  for pair in vContainers[1:]:\n\n    # get pair and assigned label\n    html_content = pair.children[1].get_interact_value() # the displayed pair as html\n    user_assigned_label = pair.children[1].get_interact_value() # the assigned label\n\n    # extract candidate pair id from html pair content\n    start = pair.children[0].value.find('data-title=\"')\n    if start > 0: \n      start += len('data-title=\"') \n      end = pair.children[0].value.find('\"', start+2)\n    pair_id = pair.children[0].value[start:end]\n\n\n\n    # assign label to candidate pair entry in dataframe\n    candidate_pairs_pd.loc[candidate_pairs_pd['z_cluster']==pair_id, 'z_isMatch'] = LABELS.get(user_assigned_label)\n  # ========================================================\n\n  # SAVE LABELED DATA TO ZINGG FOLDER\n  # ========================================================\n  # make target directory if needed\n  dbutils.fs.mkdirs(MARKED_DIR)\n  \n  # save label assignments\n  # save labels\n  zingg.writeLabelledOutputFromPandas(candidate_pairs_pd,args)\n\n  # count labels accumulated\n  marked_pd_df = getPandasDfFromDs(zingg.getMarkedRecords())\n  n_pos, n_neg, n_tot = count_labeled_pairs(marked_pd_df)\n  print(f'You have accumulated {n_pos} pairs labeled as positive matches.')\n  print(f'You have accumulated {n_neg} pairs labeled as not matches.')\n  print(\"If you need more pairs to label, re-run the cell for 'findTrainingData'\")\n  # ========================================================  \n\n  # save completed\n  ready_for_save = False\n```\n\n----------------------------------------\n\nTITLE: Configuring BigQuery as a Data Sink in Zingg\nDESCRIPTION: JSON configuration for writing data to BigQuery in Zingg. Specifies credentials file, target table, and a temporary GCS bucket which is required for writing operations.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/bigquery.md#2025-04-23_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n    \"output\" : [{\n        \"name\":\"output\", \n        \"format\":\"bigquery\",\n        \"props\": {\n            \"credentialsFile\": \"/home/work/product/final/zingg-1/mynotification-46566-905cbfd2723f.json\",\n            \"table\": \"mynotification-46566.zinggdataset.zinggOutput\",\n            \"temporaryGcsBucket\":\"zingg-test\",\n         }\n    }],\n```\n\n----------------------------------------\n\nTITLE: Executing Zingg Pipeline with S3 Integration\nDESCRIPTION: Complete sequence of Zingg commands for running the data processing pipeline using S3 storage, including findTrainingData, label, train, and match phases.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/amazonS3.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/zingg.sh --phase findTrainingData --properties-file config/zingg.conf  --conf examples/febrl/config.json --zinggDir  s3a://zingg28032023/zingg\\n./scripts/zingg.sh --phase label --properties-file config/zingg.conf  --conf examples/febrl/config.json --zinggDir  s3a://zingg28032023/zingg\\n./scripts/zingg.sh --phase train --properties-file config/zingg.conf  --conf examples/febrl/config.json --zinggDir  s3a://zingg28032023/zingg\\n./scripts/zingg.sh --phase match --properties-file config/zingg.conf  --conf examples/febrl/config.json --zinggDir  s3a://zingg28032023/zingg\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake Configuration for Zingg Enterprise in JSON\nDESCRIPTION: This snippet outlines the structure of a Snowflake configuration file for Zingg Enterprise. It includes placeholders for modelId, input table name, and output table name. Users need to replace these placeholders with their specific values.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-zingg-enterprise-snowflake/match-configuration.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"modelId\": \"28NovDev\",\n  \"data\": {\n    \"table\": \"INPUT_TABLE_NAME\"\n  },\n  \"output\": {\n    \"table\": \"OUTPUT_TABLE_NAME\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Snowflake JDBC and Spark Dependencies for Zingg\nDESCRIPTION: This configuration specifies the required Snowflake JDBC driver and Spark dependency jars for Zingg. These jars need to be included in the classpath for Zingg to connect to Snowflake.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/snowflake.md#2025-04-23_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\nspark.jars=snowflake-jdbc-3.13.19.jar,spark-snowflake_2.12-2.10.0-spark_3.1.jar\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Credentials via Environment Variables\nDESCRIPTION: Commands to export AWS access credentials as environment variables for S3 access. Includes optional MFA token setup.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/amazonS3.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport AWS_ACCESS_KEY_ID=<access key id>\\nexport AWS_SECRET_ACCESS_KEY=<access key>\n```\n\n----------------------------------------\n\nTITLE: Example Nicknames Mapping JSON for Zingg Advanced Match Types\nDESCRIPTION: Sample JSON file structure for nickname mappings. This file would be referenced by Zingg to establish relationships between different variations of names, allowing them to be considered equivalent during matching.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/configuration/adv-matchtypes.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  [\"Will\", \"Bill\", \"William\"],\n  [\"John\", \"Johnny\", \"Jack\"],\n  [\"Robert\", \"Rob\", \"Bob\", \"Bobby\"],\n  [\"Charles\", \"Charlie\", \"Chuck\"],\n  [\"James\", \"Jim\", \"Jimmy\"],\n  [\"Thomas\", \"Tom\", \"Tommy\"]\n...\n]\n```\n\n----------------------------------------\n\nTITLE: Executing Zingg Phases: Finding Training Data and Labeling\nDESCRIPTION: Runs the 'findTrainingData' and 'label' phases of Zingg using the configured arguments. It initializes Zingg with Spark and executes the specified phases.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noptions = ClientOptions([ClientOptions.PHASE,\"findTrainingData\"])\n#Zingg execution for the given phase\nzingg = ZinggWithSpark(args, options)\nzingg.initAndExecute()\n```\n\nLANGUAGE: python\nCODE:\n```\noptions = ClientOptions([ClientOptions.PHASE,\"label\"])\n#Zingg execution for the given phase\nzingg = ZinggWithSpark(args, options)\nzingg.init()\n```\n\n----------------------------------------\n\nTITLE: Setting Google Application Credentials Environment Variable\nDESCRIPTION: Sets the environment variable that points to the Google service account key file for authentication when running Zingg outside Google cloud.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/bigquery.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport GOOGLE_APPLICATION_CREDENTIALS=path to google service account key file\n```\n\n----------------------------------------\n\nTITLE: Field Definition JSON Structure\nDESCRIPTION: JSON structure showing required attributes for defining fields in Zingg data matching. Each field requires fieldName, fields, dataType, and matchType properties.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/configuration/field-definitions.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"fieldName\": \"<column_name>\",\n  \"fields\": \"<same_as_fieldName>\",\n  \"dataType\": \"string|integer|double\",\n  \"matchType\": \"FUZZY,NUMERIC\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Lookup Settings in JSON\nDESCRIPTION: Example configuration file (lookupConf.json) for the Zingg lookup feature. It defines the configuration, lookup data source, and output destination with specific formatting properties.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/runApproval.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"config\" : \"config.json\",\n  \"lookupData\": [{\n    \"name\":\"lookup-test-data\",\n    \"format\":\"inMemory\"\n  }\n  ],\n  \"lookupOutput\": [\n    {\n      \"name\":\"lookup-output\",\n      \"format\":\"csv\",\n      \"props\": {\n        \"location\": \"/tmp/zinggOutput/lookup\",\n        \"delimiter\": \",\",\n        \"header\":true\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for a Zingg Model\nDESCRIPTION: Sets up options to generate documentation for a Zingg model, executes the documentation generation phase, lists the generated documentation files, and displays the HTML model documentation in a notebook.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\noptions = ClientOptions([ClientOptions.PHASE,\"generateDocs\"])\n#Zingg execution for the given phase\nzingg = ZinggWithSpark(args, options)\nzingg.initAndExecute()\nDOCS_DIR = zinggDir + \"/\" + modelId + \"/docs/\"\ndbutils.fs.ls('file:'+DOCS_DIR)\n\n# see the labels\ndisplayHTML(open(DOCS_DIR+\"model.html\", 'r').read())\n```\n\n----------------------------------------\n\nTITLE: Executing Find and Label Phase in Zingg\nDESCRIPTION: A shell command to run the findAndLabel phase in Zingg with a configuration file. This command initiates both the finding of training data and subsequent labeling process in one step.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/setup/training/findAndLabel.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./zingg.sh --phase findAndLabel --conf config.json\n```\n\n----------------------------------------\n\nTITLE: Configuring JDBC Driver in Spark Properties for Zingg\nDESCRIPTION: Sets the location of the JDBC driver JAR file in Zingg's runtime properties to make it accessible on the Spark classpath. This configuration is necessary for connecting to databases like MySQL, DB2, MariaDB, MS SQL, Oracle, and PostgreSQL.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/jdbc.md#2025-04-23_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nspark.jars=<location of jdbc driver jar>\n```\n\n----------------------------------------\n\nTITLE: Processing and Displaying Candidate Pairs in HTML\nDESCRIPTION: Iterates through pairs of records, generates HTML visualization with images and text data, and creates interactive widgets for labeling matches. Handles base64 encoding of images and creates a formatted table display.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# for each set of pairs\nfor n in range(n_pairs):\n\n  # get candidate records\n  candidate_left = display_pd.loc[2*n].to_list()\n  print(candidate_left)\n  candidate_right = display_pd.loc[(2*n)+1].to_list()\n  print(candidate_right)\n\n  # define grid to hold values\n  html = ''\n\n  for i in range(display_pd.shape[1]):\n\n    # get column name\n    column_name = display_pd.columns[i]\n\n    # if field is image\n    if column_name == 'image_path':\n\n      # define row header\n      html += '<tr>'\n      html += '<td><b>image</b></td>'\n\n      # read left image to encoded string\n      l_endcode = ''\n      if candidate_left[i] != '':\n        with open(candidate_left[i], \"rb\") as l_file:\n          l_encode = base64.b64encode( l_file.read() ).decode()\n\n      # read right image to encoded string\n      r_encode = ''\n      if candidate_right[i] != '':\n        with open(candidate_right[i], \"rb\") as r_file:\n          r_encode = base64.b64encode( r_file.read() ).decode()      \n\n      # present images\n      html += f'<td><img src=\"data:image/png;base64,{l_encode}\"></td>'\n      html += f'<td><img src=\"data:image/png;base64,{r_encode}\"></td>'\n      html += '</tr>'\n\n    elif column_name != 'image_path':  # display text values\n\n      if column_name == 'z_cluster': z_cluster = candidate_left[i]\n\n      html += '<tr>'\n      html += f'<td style=\"width:10%\"><b>{column_name}</b></td>'\n      html += f'<td style=\"width:45%\">{str(candidate_left[i])}</td>'\n      html += f'<td style=\"width:45%\">{str(candidate_right[i])}</td>'\n      html += '</tr>'\n\n  # insert data table\n  table = widgets.HTML(value=f'<table data-title=\"{z_cluster}\" style=\"width:100%;border-collapse:collapse\" border=\"1\">'+html+'</table>')\n  z_cluster = None\n\n  # assign label options to pair\n  label = widgets.ToggleButtons(\n    options=LABELS.keys(), \n    button_style='info'\n    )\n\n  # define blank line between displayed pair and next\n  blankLine=widgets.HTML(value='<br>')\n\n  # append pair, label and blank line to widget structure\n  vContainers.append(widgets.VBox(children=[table, label, blankLine]))\n\n# present widget\ndisplay(widgets.VBox(children=vContainers))\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake Connection Properties File for Zingg\nDESCRIPTION: This snippet shows the format of the snowEnv.txt file, which contains the necessary Snowflake connection properties for Zingg. It includes parameters for URL, user credentials, role, warehouse, database, schema, and a session keep-alive setting.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-zingg-enterprise-snowflake/snowflake-properties.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nURL={snowflake_url}   \nUSER={snowflake_user_name} \nPASSWORD={snowflake_password}  \nROLE={role} \nWAREHOUSE={warehouse}  \nDB={database_name}  \nSCHEMA={schema}\nCLIENT_SESSION_KEEP_ALIVE_HEARTBEAT_FREQUENCY=900\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Label Phase via Command Line\nDESCRIPTION: This command executes the label phase in Zingg, allowing users to interactively mark pairs as matches or non-matches. The optional showConcise flag can be used to display only non-DONT_USE fields.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/setup/training/label.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./zingg.sh --phase label --conf config.json <optional --showConcise=true|false>\n```\n\n----------------------------------------\n\nTITLE: Configuring Parquet Files as Data Source/Sink in Zingg\nDESCRIPTION: JSON configuration for setting up Parquet files as a data source or sink in Zingg. The configuration specifies the name, format, and location properties needed to access Parquet files.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/parquet.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"data\" : [{\n\t\t\"name\":\"parquetFiles\", \n\t\t\"format\":\"parquet\", \n\t\t\"props\": {\n\t\t\t\"location\": \"/home/zingg\"\t\t\n\t\t\t}\t\n\t\t}]\n```\n\n----------------------------------------\n\nTITLE: Configuring modelId for Zingg Model Identification\nDESCRIPTION: Defines an identifier for the Zingg model. This allows for training and saving multiple models for different purposes, such as customer matching or household matching. Each model is saved in a subdirectory of zinggDir using its modelId.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/configuration/model-location.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n#### modelId\n\nAn **identifier** for the model. You can train multiple models - say, one for **customers** matching _names_, _age_, and other personal details and one for **households** matching _addresses_. Each model gets saved under `zinggDir/modelId`\n```\n\n----------------------------------------\n\nTITLE: Installing Zingg and Dependencies in Python\nDESCRIPTION: This snippet shows how to install Zingg and its dependencies using pip in a Python environment. It also includes restarting the Python kernel in Databricks.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install zingg\n```\n\nLANGUAGE: python\nCODE:\n```\npip show zingg\n```\n\nLANGUAGE: python\nCODE:\n```\ndbutils.library.restartPython()\n```\n\nLANGUAGE: python\nCODE:\n```\n!pip install tabulate\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Jars for Exasol in Zingg Properties\nDESCRIPTION: Updates the spark.jars parameter in Zingg's runtime properties to include the Exasol Spark connector. This configuration enables Zingg to utilize Exasol's dependencies for database operations.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/exasol.md#2025-04-23_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nspark.jars=spark-connector_2.12-1.3.0-spark-3.3.2-assembly.jar\n```\n\n----------------------------------------\n\nTITLE: Loading Data from CSV in Microsoft Fabric\nDESCRIPTION: Loads data from a CSV file stored in Microsoft Fabric's data lake into a pandas DataFrame.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Import pandas\nimport pandas as pd\n\n# Define the schema (optional for validation)\nschema = [\"id\", \"fname\", \"lname\", \"stNo\", \"add1\", \"add2\", \"city\", \"state\", \"dob\", \"ssn\"]\n\n# Load the CSV file\ndata = pd.read_csv(\"abfss://Zingg@onelake.dfs.fabric.microsoft.com/data.Lakehouse/Files/data.csv\")\n\n# Ensure column names match the schema\ndata.columns = schema  # Adjust only if the file's column names differ\n\n# Display the data\ndata.head()\n```\n\n----------------------------------------\n\nTITLE: Configuring Zingg JSON with Environment Variables\nDESCRIPTION: Example configuration showing how to reference environment variables in a Zingg JSON config file. Environment variables must be enclosed in $variable$ syntax, with strings requiring quotes and numeric/boolean values without quotes. The config filename must end with .env extension.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/configuration/configuring-through-environment-variables.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"output\" : [{\n  \"name\":\"unifiedCustomers\", \n  \"format\":\"net.snowflake.spark.snowflake\",\n  \"props\": {\n    \"location\": \"$location$\",\n    \"delimiter\": \",\",\n    \"header\": false,\t\t\t\t\n    \"password\": \"$passwd\",\t\t\t\t\t\n  }\n}],\n\n\"labelDataSampleSize\" : 0.5,\n\"numPartitions\":4,\n\"modelId\": $modelId$,\n\"zinggDir\": \"models\",\n\"collectMetrics\": $collectMetrics$\n```\n\n----------------------------------------\n\nTITLE: Generating Stopword Recommendations in Zingg Enterprise Snowflake\nDESCRIPTION: Command to invoke Zingg's stopword recommendation feature specifically for Snowflake Enterprise edition, which includes additional parameters for Snowflake properties.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/accuracy/stopWordsRemoval.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/zingg.sh --phase recommend --conf <conf.json> --properties-file <path to Snowflake properties file> --column <name of column to generate stopword recommendations>\n```\n\n----------------------------------------\n\nTITLE: Running Zingg with Custom Properties File\nDESCRIPTION: Command to run Zingg with a custom properties file that defines JVM settings and other runtime configurations. The properties file allows specification of memory allocation, external dependencies, and other Spark configuration options.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/zingg-runtime-properties.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./scripts/zingg.sh --properties-file <location to file> --conf conf.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Output for Zingg\nDESCRIPTION: Sets up the output pipeline for Zingg results, directing them to a location in Microsoft Fabric.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#setting outputpipe in 'args'\noutputPipe = CsvPipe(\"resultOutput\", \"abfss://Zingg@onelake.dfs.fabric.microsoft.com/data.Lakehouse/Files\")\nargs.setOutput(outputPipe)\n```\n\n----------------------------------------\n\nTITLE: Running Zingg with Python Programs\nDESCRIPTION: Command to execute Zingg with Python programs, including an optional properties file configuration.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/zingg-command-line.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./scripts/zingg.sh <optional --properties-file path-to-zingg.conf> --run <path-to-python-program>\n```\n\n----------------------------------------\n\nTITLE: Installing the Zingg Python Package\nDESCRIPTION: Command to install the Zingg Python package using pip. This is required before using the Python API to build and run Zingg entity resolution programs.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/working-with-python.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install zingg\n```\n\n----------------------------------------\n\nTITLE: Executing Zingg to Find Training Data\nDESCRIPTION: Initializes and executes Zingg to find training data for the deduplication model.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\noptions = ClientOptions([ClientOptions.PHASE,\"findTrainingData\"])\n\n#Zingg execution for the given phase\nzingg = ZinggWithSpark(args, options)\nprint(args)\nprint(options)\nprint(zingg)\nzingg.initAndExecute()\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Asynchronously with Python API\nDESCRIPTION: Example Python code that demonstrates how to run Zingg in asynchronous mode using the Python API by setting the 'is_async' parameter to True. This creates a separate process and returns a job ID for tracking.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-zingg-enterprise-snowflake/running-asynchronously.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pyzingg import Zingg\n\nzingg = Zingg()\njobId = zingg.train(\"./examples/febrl/config.json\", is_async=True)\n```\n\n----------------------------------------\n\nTITLE: Setting Zingg Execution Parameters\nDESCRIPTION: Configures the number of partitions and label data sample size for Zingg execution.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# The numPartitions define how data is split across the cluster. \n# Please change the fllowing as per your data and cluster size by referring to the docs.\n\nargs.setNumPartitions(4)\nargs.setLabelDataSampleSize(0.5)\n```\n\n----------------------------------------\n\nTITLE: Executing Zingg Asynchronously with nohup in Bash\nDESCRIPTION: This command runs Zingg as a background process using nohup, allowing it to continue running even if the SSH connection is broken. It specifies the properties file, phase, and configuration file for the Zingg process.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-zingg-enterprise-snowflake/running-async-long-jobs.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnohup ./scripts/zingg.sh --properties-file ~/zingg/snowEnv.txt --phase findTrainingData --conf ~/zingg/snowConfigFile.json &\n```\n\n----------------------------------------\n\nTITLE: Reading and Displaying Zingg Output Data with PySpark\nDESCRIPTION: Reads CSV output generated by Zingg, assigns column names to the results which include matching scores, cluster IDs and the original data fields, then displays up to 100 rows of the data.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\noutputDF1 = spark.read.csv(\"/tmp/outputApr1\")\ncolNames = [\"z_minScore\", \"z_maxScore\", \"z_cluster\", \"rec_id\", \"fname\", \"lname\", \"stNo\", \"add1\", \"add2\", \"city\", \"state\", \"dob\", \"ssn\"]\noutputDF1.toDF(*colNames).show(100)\n```\n\n----------------------------------------\n\nTITLE: Configuring Zingg Model and Directory Paths\nDESCRIPTION: Sets up the directory paths for the Zingg model and training data. It defines variables for marked and unmarked data directories.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nzinggDir = \"/models\"\nmodelId = \"zinggTestApr1\"\n\nMARKED_DIR = zinggDir + \"/\" + modelId + \"/trainingData/marked/\"\nUNMARKED_DIR = zinggDir + \"/\" + modelId + \"/trainingData/unmarked/\"\nMARKED_DIR_DBFS = \"/dbfs\" + MARKED_DIR\nUNMARKED_DIR_DBFS = \"/dbfs\" + UNMARKED_DIR\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Asynchronously from Command Line\nDESCRIPTION: Example command showing how to run Zingg in asynchronous mode using the shell with the --async flag. This allows the process to run in the background.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-zingg-enterprise-snowflake/running-asynchronously.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./zingg.sh --phase train --conf examples/febrl/config.json --async\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Explain Phase in Bash\nDESCRIPTION: This command executes the explain phase in Zingg. It requires specifying the phase for explanation, the path to the configuration file, the mode as 'explain', and a suffix for the explanation output.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/modelexplain.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/zingg.sh --phase <phase for explanation> --conf <path to config> --mode explain --suffix <suffix for explain>\n```\n\n----------------------------------------\n\nTITLE: Mounting Volumes with Docker for Zingg Data Persistence\nDESCRIPTION: Docker command for creating a volume mount between a local directory on the host machine and a location inside the Zingg container. This enables data persistence after the container stops, allowing generated models and data files to be saved and reused.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/docker/sharing-custom-data-and-config-files.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -v <local-location>:<container-location> -it zingg/zingg:0.5.0 bash\n```\n\n----------------------------------------\n\nTITLE: Saving Labeled Data\nDESCRIPTION: Processes labeled pairs, assigns values to the dataframe, and saves the labeled data to the Zingg folder. Includes validation check and label counting functionality.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nif not ready_for_save:\n  print('No labels have been assigned. Run the previous cell to create candidate pairs and assign labels to them before re-running this cell.')\n\nelse:\n\n  # ASSIGN LABEL VALUE TO CANDIDATE PAIRS IN DATAFRAME\n  # ========================================================\n  # for each pair in displayed widget\n  for pair in vContainers[1:]:\n\n    # get pair and assigned label\n    html_content = pair.children[1].get_interact_value() # the displayed pair as html\n    user_assigned_label = pair.children[1].get_interact_value() # the assigned label\n\n    # extract candidate pair id from html pair content\n    start = pair.children[0].value.find('data-title=\"')\n    if start > 0: \n      start += len('data-title=\"') \n      end = pair.children[0].value.find('\"', start+2)\n    pair_id = pair.children[0].value[start:end]\n\n    # assign label to candidate pair entry in dataframe\n    candidate_pairs_pd.loc[candidate_pairs_pd['z_cluster']==pair_id, 'z_isMatch'] = LABELS.get(user_assigned_label)\n\n  # make target directory if needed\n  notebookutils.fs.mkdirs(MARKED_DIR)\n  \n  # save label assignments\n  zingg.writeLabelledOutputFromPandas(candidate_pairs_pd,args)\n\n  # count labels accumulated\n  marked_pd_df = getPandasDfFromDs(zingg.getMarkedRecords())\n  n_pos, n_neg, n_tot = count_labeled_pairs(marked_pd_df)\n  print(f'You have accumulated {n_pos} pairs labeled as positive matches.')\n  print(f'You have accumulated {n_neg} pairs labeled as not matches.')\n  print(\"If you need more pairs to label, re-run the cell for 'findTrainingData'\")\n\n  # save completed\n  ready_for_save = False\n```\n\n----------------------------------------\n\nTITLE: Setting User Parameter for Data Pipe in Java\nDESCRIPTION: Method to set the User parameter for a data pipe. It takes a String parameter 'user' and assigns it to the pipe configuration.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/python/markdown/zingg.md#2025-04-23_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nsetUser(user)\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Docker Container with Custom User ID\nDESCRIPTION: This command demonstrates how to run the Zingg Docker container with a specified user ID. It allows the container to have the necessary permissions for file operations in shared locations. The example shows checking the current user's ID and then running the container with that ID.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/docker/file-read-write-permissions.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ id \nuid=1000(abc) gid=1000(abc) groups=1000(abc)\n$ docker run -u <uid> -it zingg/zingg:0.4.1-SNAPSHOT bash\n```\n\n----------------------------------------\n\nTITLE: Setting Warehouse Parameter for Data Pipe in Java\nDESCRIPTION: Method to set the Warehouse parameter for a data pipe. It takes a String parameter 'warehouse' and assigns it to the pipe configuration.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/python/markdown/zingg.md#2025-04-23_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nsetWarehouse(warehouse)\n```\n\n----------------------------------------\n\nTITLE: Executing Zingg with Shared Configuration Location\nDESCRIPTION: This command demonstrates how to run Zingg's label phase using a configuration file and specifying a shared location for the zinggDir. The zinggDir is where model information and other editable files like config.json should be stored.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/docker/shared-locations.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nzingg.sh --phase label --conf config.json --zinggDir /location\n```\n\n----------------------------------------\n\nTITLE: Running Zingg's verifyBlocking Phase using Shell Script\nDESCRIPTION: This command executes the verifyBlocking phase in Zingg to analyze blocking performance. It requires a configuration path and optionally a zinggDir parameter to specify the model location.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/verifyBlocking.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./scripts/zingg.sh --phase verifyBlocking --conf <path to conf> <optional --zinggDir <location of model>>\n```\n\n----------------------------------------\n\nTITLE: Exporting Labeled Training Data to CSV in Zingg\nDESCRIPTION: This command exports labeled model data to a CSV file. It requires the configuration path and a target location where the CSV file will be saved. This is useful for sharing data with subject matter experts or reusing training data for new models.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/setup/training/exportLabeledData.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/zingg.sh --phase exportModel --conf <path to conf> --location <folder to save the csv>\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Spark and Java\nDESCRIPTION: Essential environment variable exports needed in ~/.bash_aliases to run Zingg with local Spark. Sets up JAVA_HOME, SPARK_HOME, and configures Spark to run in local mode.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-from-release/single-machine-setup.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport JAVA_HOME=path to jdk\nexport SPARK_HOME=path to location of Apache Spark\nexport SPARK_MASTER=local[*]\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Documentation Generation Command\nDESCRIPTION: Shell command to generate documentation for Zingg model training data. Outputs documentation to zinggDir/modelId folder with options to show concise or detailed views.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/generatingdocumentation.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/zingg.sh --phase generateDocs --conf <location to conf.json> <optional --showConcise=true|false>\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Zingg Development\nDESCRIPTION: Commands to set up environment variables in the .bashrc file for Zingg development. This includes paths for Spark, Maven, Zingg, and Java.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/settingUpZingg.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nvim ~/.bashrc\nexport SPARK_HOME=/opt/spark\nexport SPARK_MASTER=local[*]\nexport MAVEN_HOME=/home/ubuntu/apache-maven-3.8.8\nexport ZINGG_HOME=<path_to_zingg>/assembly/target\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\nexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin:$JAVA_HOME/bin\n```\n\nLANGUAGE: bash\nCODE:\n```\nsource ~/.bashrc\n```\n\nLANGUAGE: bash\nCODE:\n```\necho $PATH\nmvn --version\n```\n\n----------------------------------------\n\nTITLE: Installing and Configuring Zingg Enterprise Snowflake\nDESCRIPTION: Series of commands to extract the Zingg Enterprise Snowflake package, set up environment variables, and move the license file to the correct location. Includes archive extraction, directory navigation, and environment variable configuration.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-zingg-enterprise-snowflake/setting-up-zingg-enterprise-for-snowflake.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngzip -d zingg-enterprise-snowflake-0.4.1-SNAPSHOT.tar.gz\n```\n\nLANGUAGE: bash\nCODE:\n```\ntar xvf zingg-enterprise-snowflake-0.4.1-SNAPSHOT.tar\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd zingg-enterprise-snowflake-0.4.1-SNAPSHOT\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport ZINGG_SNOW_JAR=~/zingg-enterprise-snowflake-0.4.1-SNAPSHOT\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport ZINGG_SNOW_HOME=~/zingg-enterprise-snowflake-0.4.1-SNAPSHOT\n```\n\nLANGUAGE: bash\nCODE:\n```\nmv ~/zingg.license .\n```\n\n----------------------------------------\n\nTITLE: Setting Google Hadoop FileSystem Implementation for BigQuery\nDESCRIPTION: Configures the Hadoop FileSystem implementation for Google Cloud Storage which is required for BigQuery connectivity.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/bigquery.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nspark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem                                                      \n```\n\n----------------------------------------\n\nTITLE: Checking Environment Variables in Bash\nDESCRIPTION: Commands to verify that essential environment variables for Zingg are set correctly, including SPARK_HOME, JAVA_HOME, ZINGG_HOME, and Java version.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-from-release/verification.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbash\n```\n\nLANGUAGE: bash\nCODE:\n```\necho $SPARK_HOME\n```\n\nLANGUAGE: bash\nCODE:\n```\necho $JAVA_HOME\n```\n\nLANGUAGE: bash\nCODE:\n```\njava --version\n```\n\nLANGUAGE: bash\nCODE:\n```\necho $ZINGG_HOME\n```\n\n----------------------------------------\n\nTITLE: Pulling and Running Zingg Docker Image\nDESCRIPTION: Commands to pull the Zingg Docker image from Docker Hub and run it in interactive mode with a bash shell. The image contains all the dependencies required to run Zingg version 0.5.0.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/docker/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull zingg/zingg:0.5.0\ndocker run -it zingg/zingg:0.5.0 bash\n```\n\n----------------------------------------\n\nTITLE: Installing JDK 11 on Ubuntu\nDESCRIPTION: Commands to install Java Development Kit 11 (JDK 11) on Ubuntu. This includes both the JDK and JRE, and verifies the installation.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/settingUpZingg.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install openjdk-11-jdk openjdk-11-jre\njavac -version\njava -version\n```\n\n----------------------------------------\n\nTITLE: Configuring Exasol as a Data Source in Zingg\nDESCRIPTION: JSON configuration for using Exasol as a data input source in Zingg. This snippet defines connection parameters including host, port, credentials, and the SQL query to retrieve data from an Exasol table.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/exasol.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n...\\n \\\"data\\\": [\\n    {\\n        \\\"name\\\": \\\"input\\\",\\n        \\\"format\\\": \\\"com.exasol.spark\\\",\\n        \\\"props\\\": {\\n            \\\"host\\\": \\\"10.11.0.2\\\",\\n            \\\"port\\\": \\\"8563\\\",\\n            \\\"username\\\": \\\"sys\\\",\\n            \\\"password\\\": \\\"exasol\\\",\\n            \\\"query\\\": \\\"SELECT * FROM DB_SCHEMA.CUSTOMERS\\\"\\n        }\\n    }\\n ],\\n ...\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Spark on Ubuntu\nDESCRIPTION: Commands to download and install Apache Spark 3.5.0 on Ubuntu. This includes downloading the tarball, extracting it, and moving it to the /opt directory.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/settingUpZingg.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nwget https://www.apache.org/dyn/closer.lua/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\ntar -xvf spark-3.5.0-bin-hadoop3.tgz\nrm -rf spark-3.5.0-bin-hadoop3.tgz\nsudo mv spark-3.5.0-bin-hadoop3 /opt/spark\n```\n\n----------------------------------------\n\nTITLE: Compiling the Zingg Repository\nDESCRIPTION: Commands to compile the Zingg repository using Maven. This includes initializing, cleaning, compiling, and packaging the project with specific Spark version.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/settingUpZingg.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit branch\n```\n\nLANGUAGE: bash\nCODE:\n```\nmvn initialize\nmvn clean compile package -Dspark=sparkVer\n```\n\nLANGUAGE: bash\nCODE:\n```\nmvn initialize\nmvn clean compile package -Dspark=sparkVer -Dmaven.test.skip=true\n```\n\n----------------------------------------\n\nTITLE: Setting Zingg Directory and Model ID\nDESCRIPTION: Defines the Zingg directory in Microsoft Fabric and sets a model ID for the current project.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n##you can change these to the locations of your choice\n##these are the only two settings that need to change\nzinggDir = \"abfss://Zingg@onelake.dfs.fabric.microsoft.com/data.Lakehouse/Files/models\"\nmodelId = \"testModelFebrl\"\n```\n\n----------------------------------------\n\nTITLE: Reading and Displaying Output Data\nDESCRIPTION: Reads CSV output data into Spark DataFrame and displays results with proper column naming.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\noutputDF = spark.read.csv(\"abfss://Zingg@onelake.dfs.fabric.microsoft.com/data.Lakehouse/Files/part-00000-d624fac4-b80c-4f8d-aebc-5d5faf351b8f-c000.csv\")\n\ncolNames = [\"z_minScore\", \"z_maxScore\", \"z_cluster\", \"rec_id\", \"fname\", \"lname\", \"stNo\", \"add1\", \"add2\", \"city\", \"state\", \"dob\", \"ssn\"]\noutputDF.toDF(*colNames).show(100)\n```\n\n----------------------------------------\n\nTITLE: Running Zingg to Find Training Data\nDESCRIPTION: Command to run Zingg for finding training data using a specific configuration file.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/settingUpZingg.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/zingg.sh --phase findTrainingData --conf examples/febrl/config.json\n```\n\n----------------------------------------\n\nTITLE: Setting the ZINGG_HOME environment variable\nDESCRIPTION: Command to set the ZINGG_HOME environment variable to the path where Zingg is installed.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-from-release/installing-zingg.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport ZINGG_HOME=path to zingg\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Lookup Phase via Shell Script\nDESCRIPTION: Command to execute the lookup phase in Zingg Enterprise using the provided shell script and referencing a lookup configuration file.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/runApproval.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/zingg.sh --phase runLookup --conf <location to lookupConf.json>\n```\n\n----------------------------------------\n\nTITLE: Moving the Zingg directory\nDESCRIPTION: Command to move the extracted Zingg directory to a designated location in the home folder.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-from-release/installing-zingg.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmv zingg-0.5.0 ~/zingg\n```\n\n----------------------------------------\n\nTITLE: Generating Stopword Recommendations with Zingg CLI\nDESCRIPTION: Command to invoke Zingg's stopword recommendation feature which analyzes a specific column and suggests words to ignore during matching.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/accuracy/stopWordsRemoval.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/zingg.sh --phase recommend --conf <conf.json> --column <name of column to generate stopword recommendations>\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Sample Program\nDESCRIPTION: Commands to navigate to the Zingg directory and run a sample program that builds Zingg models and finds duplicates in a test dataset, which validates the installation.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-from-release/verification.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd zingg\n```\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/zingg.sh --phase trainMatch --conf examples/febrl/config.json\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Test Command in Bash\nDESCRIPTION: Command to verify Zingg's installation by running a simple test. This executes a basic command that will fail if Zingg is not properly installed.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-zingg-enterprise-snowflake/verifying-the-installation.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nzingg.sh --test\n```\n\n----------------------------------------\n\nTITLE: Sample Google Service Account Key Format\nDESCRIPTION: Example of a Google service account key JSON file structure used for authentication. Contains fields like project_id, private_key, client_id, and various URIs for OAuth2 authentication.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/bigquery.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n \"type\": \"service_account\",\n \"project_id\": \"mynotification-46566\",\n \"private_key_id\": \"905cbfd273ff9205d1cabfe06fa6908e54534\",\n \"private_key\": \"-----BEGIN PRIVATE KEY-----CERT.....\",\n \"client_id\": \"11143646541283115487\",\n \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n \"token_uri\": \"https://oauth2.googleapis.com/token\",\n \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/zingtest%44mynotification-46566.iam.gserviceaccount.com\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining zinggDir for Zingg Model Storage Location\nDESCRIPTION: Specifies the directory where trained Zingg models will be saved. The default location is set to '/tmp/zingg'.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/configuration/model-location.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n#### zinggDir\n\nThe **location** where trained models will be saved. Defaults to `/tmp/zingg`\n```\n\n----------------------------------------\n\nTITLE: Configuring Stopword Cutoff Threshold in Zingg\nDESCRIPTION: JSON configuration snippet showing how to customize the percentage of high-frequency words to be considered as stopwords by setting the stopWordsCutoff property.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/accuracy/stopWordsRemoval.md#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\nstopWordsCutoff: <a value between 0 and 1>\n```\n\n----------------------------------------\n\nTITLE: Updating PATH environment variable\nDESCRIPTION: Command to update the PATH environment variable to include Java, Spark, and Zingg executable paths for easier access.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-from-release/installing-zingg.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport PATH=$PATH:$JAVA_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin:ZINGG_HOME/scripts\n```\n\n----------------------------------------\n\nTITLE: Copying Files Between Host and Docker Container\nDESCRIPTION: Commands demonstrating how to copy files to and from a Docker container using docker cp. These examples show copying a file named foo.txt in both directions between the host and container.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/docker/copying-files-to-and-from-the-container.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ docker cp foo.txt <container_id>:/foo.txt\n$ docker cp <container_id>:/foo.txt foo.txt\n```\n\n----------------------------------------\n\nTITLE: Executing Zingg with JSON Configuration\nDESCRIPTION: Command to run Zingg with a JSON configuration file, specifying an optional properties file and the desired execution phase.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/zingg-command-line.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./scripts/zingg.sh <optional --properties-file path to zingg.conf> --phase <phase> --conf <path to json>\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Docker Image with Volume Mapping for Permission Issues\nDESCRIPTION: Command to run the Zingg Docker container with volume mapping to solve permission denied issues by mapping the host's /tmp directory to the container's /tmp directory.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/docker/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -v /tmp:/tmp -it zingg/zingg:0.5.0 bash\n```\n\n----------------------------------------\n\nTITLE: Documenting Zingg Pipes Module\nDESCRIPTION: Sphinx autodoc directive to generate documentation for the zingg.pipes module including all members, undocumented members and inheritance hierarchy.\nSOURCE: https://github.com/zinggai/zingg/blob/main/python/docs/zingg.rst#2025-04-23_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: zingg.pipes\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Installing Zingg Package\nDESCRIPTION: Installs the Zingg package using pip.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npip install zingg\n```\n\n----------------------------------------\n\nTITLE: Analyzing Spark Physical Plan for Entity Matching in Zingg\nDESCRIPTION: This Spark execution plan shows the physical operations for entity matching in Zingg. It includes multiple SortMergeJoin operations, data transformations, and field projections to identify and match similar records based on hash values and record IDs.\nSOURCE: https://github.com/zinggai/zingg/blob/main/perf/joinPlan.txt#2025-04-23_snippet_0\n\nLANGUAGE: spark-sql\nCODE:\n```\n*(11) Project [z_z_zid#597L, z_zid#367L, id#72, fname#73, lname#74, stNo#75, add1#76, add2#77, city#78, state#79, dob#80, ssn#81, z_zsource#82, z_id#1103, z_fname#1104, z_lname#1105, z_stNo#1106, z_add1#1107, z_add2#1108, z_city#1109, z_state#1110, z_dob#1111, z_ssn#1112, z_z_zsource#1113]\n+- *(11) SortMergeJoin [z_z_zid#597L], [z_z_zid#1102L], Inner\n   :- *(9) Sort [z_z_zid#597L ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(z_z_zid#597L, 3000), false, [id=#472]\n   :     +- *(8) Project [z_zid#367L, z_z_zid#597L, id#72, fname#73, lname#74, stNo#75, add1#76, add2#77, city#78, state#79, dob#80, ssn#81, z_zsource#82]\n   :        +- *(8) SortMergeJoin [z_zid#367L], [z_zid#71L], Inner\n   :           :- *(6) Sort [z_zid#367L ASC NULLS FIRST], false, 0\n   :           :  +- Exchange hashpartitioning(z_zid#367L, 3000), false, [id=#456]\n   :           :     +- *(5) Project [z_zid#367L, z_zid#580L AS z_z_zid#597L]\n   :           :        +- *(5) SortMergeJoin [z_hash#379], [z_hash#592], Inner, (z_zid#367L > z_zid#580L)\n   :           :           :- *(2) Sort [z_hash#379 ASC NULLS FIRST], false, 0\n   :           :           :  +- Exchange hashpartitioning(z_hash#379, 3000), false, [id=#372]\n   :           :           :     +- *(1) SerializeFromObject [validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, z_zid), LongType) AS z_zid#367L, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 12, z_hash), IntegerType) AS z_hash#379]\n   :           :           :        +- *(1) MapElements zingg.block.Block$BlockFunction@37e7e089, obj#366: org.apache.spark.sql.Row\n   :           :           :           +- *(1) DeserializeToObject createexternalrow(z_zid#71L, id#72.toString, fname#73.toString, lname#74.toString, stNo#75.toString, add1#76.toString, add2#77.toString, city#78.toString, state#79.toString, dob#80.toString, ssn#81.toString, z_zsource#82.toString, StructField(z_zid,LongType,false), StructField(id,StringType,true), StructField(fname,StringType,true), StructField(lname,StringType,true), StructField(stNo,StringType,true), StructField(add1,StringType,true), StructField(add2,StringType,true), StructField(city,StringType,true), StructField(state,StringType,true), StructField(dob,StringType,true), StructField(ssn,StringType,true), StructField(z_zsource,StringType,false)), obj#365: org.apache.spark.sql.Row\n   :           :           :              +- Exchange hashpartitioning(z_zid#71L, 3000), false, [id=#324]\n   :           :           :                 +- InMemoryTableScan [z_zid#71L, id#72, fname#73, lname#74, stNo#75, add1#76, add2#77, city#78, state#79, dob#80, ssn#81, z_zsource#82]\n   :           :           :                       +- InMemoryRelation [z_zid#71L, id#72, fname#73, lname#74, stNo#75, add1#76, add2#77, city#78, state#79, dob#80, ssn#81, z_zsource#82], StorageLevel(memory, deserialized, 1 replicas)\n   :           :           :                             +- Exchange RoundRobinPartitioning(3000), false, [id=#40]\n   :           :           :                                +- *(1) Scan ExistingRDD[z_zid#71L,id#72,fname#73,lname#74,stNo#75,add1#76,add2#77,city#78,state#79,dob#80,ssn#81,z_zsource#82]\n   :           :           +- *(4) Sort [z_hash#592 ASC NULLS FIRST], false, 0\n   :           :              +- ReusedExchange [z_zid#580L, z_hash#592], Exchange hashpartitioning(z_hash#379, 3000), false, [id=#372]\n   :           +- *(7) Sort [z_zid#71L ASC NULLS FIRST], false, 0\n   :              +- ReusedExchange [z_zid#71L, id#72, fname#73, lname#74, stNo#75, add1#76, add2#77, city#78, state#79, dob#80, ssn#81, z_zsource#82], Exchange hashpartitioning(z_zid#71L, 3000), false, [id=#324]\n   +- *(10) Sort [z_z_zid#1102L ASC NULLS FIRST], false, 0\n      +- *(10) Project [z_zid#71L AS z_z_zid#1102L, id#72 AS z_id#1103, fname#73 AS z_fname#1104, lname#74 AS z_lname#1105, stNo#75 AS z_stNo#1106, add1#76 AS z_add1#1107, add2#77 AS z_add2#1108, city#78 AS z_city#1109, state#79 AS z_state#1110, dob#80 AS z_dob#1111, ssn#81 AS z_ssn#1112, z_zsource#82 AS z_z_zsource#1113]\n         +- ReusedExchange [z_zid#71L, id#72, fname#73, lname#74, stNo#75, add1#76, add2#77, city#78, state#79, dob#80, ssn#81, z_zsource#82], Exchange hashpartitioning(z_zid#71L, 3000), false, [id=#324]\n```\n\n----------------------------------------\n\nTITLE: Running the Label Updater in Zingg\nDESCRIPTION: Command to invoke the Zingg label updater through the console labeler. This allows users to update previously marked pairs by specifying the cluster ID of pairs that need updates.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/updatingLabels.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/zingg.sh --phase updateLabel --conf <location to conf.json>\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Checkpoint Directory in Microsoft Fabric\nDESCRIPTION: Sets the Spark checkpoint directory to a location in Microsoft Fabric's data lake.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nspark.sparkContext.setCheckpointDir(\"abfss://Zingg@onelake.dfs.fabric.microsoft.com/data.Lakehouse/Files\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Zingg package using gzip and tar\nDESCRIPTION: Command to decompress and extract the Zingg package from the downloaded tar.gz file.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-from-release/installing-zingg.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngzip -d zingg-0.5.0.tar.gz ; tar xvf zingg-0.5.0.tar\n```\n\n----------------------------------------\n\nTITLE: Installing Tabulate Package\nDESCRIPTION: Installs the tabulate package for formatting tabular data.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npip install tabulate\n```\n\n----------------------------------------\n\nTITLE: Implementing BigQuery Pipe in Python\nDESCRIPTION: The BigQueryPipe class is used for working with BigQuery pipelines. It inherits from the Pipe class and provides methods for setting various BigQuery-specific parameters.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/python/markdown/zingg.md#2025-04-23_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclass BigQueryPipe(Pipe):\n    CREDENTIAL_FILE = 'credentialsFile'\n    TABLE = 'table'\n    TEMP_GCS_BUCKET = 'temporaryGcsBucket'\n    VIEWS_ENABLED = 'viewsEnabled'\n\n    def __init__(self, name):\n        super().__init__(name, 'bigquery')\n\n    def setCredentialFile(self, file):\n        self.addProperty(self.CREDENTIAL_FILE, file)\n\n    def setTable(self, table):\n        self.addProperty(self.TABLE, table)\n\n    def setTemporaryGcsBucket(self, bucket):\n        self.addProperty(self.TEMP_GCS_BUCKET, bucket)\n\n    def setViewsEnabled(self, isEnabled):\n        self.addProperty(self.VIEWS_ENABLED, str(isEnabled).lower())\n```\n\n----------------------------------------\n\nTITLE: Updating Ubuntu and Installing WSL2\nDESCRIPTION: Commands to update Ubuntu and install Ubuntu on WSL2 for Windows users. This includes installing WSL, downloading Ubuntu from Microsoft Store, and initial configuration.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/settingUpZingg.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\n```\n\nLANGUAGE: bash\nCODE:\n```\nwsl --install\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\n```\n\n----------------------------------------\n\nTITLE: Documenting Zingg Client Module\nDESCRIPTION: Sphinx autodoc directive to generate documentation for the zingg.client module including all members, undocumented members and inheritance hierarchy.\nSOURCE: https://github.com/zinggai/zingg/blob/main/python/docs/zingg.rst#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: zingg.client\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Infrastructure Tests with PySpark\nDESCRIPTION: Commands to execute the unit tests using PySpark in the test directory.\nSOURCE: https://github.com/zinggai/zingg/blob/main/test/note.txt#2025-04-23_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ncd test/\npyspark < testInfraOwnGateway.py\n```\n\n----------------------------------------\n\nTITLE: Creating CSV Pipe in Python\nDESCRIPTION: The CsvPipe class is used for working with CSV files. It provides methods for setting the delimiter, header, and location of the CSV file.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/python/markdown/zingg.md#2025-04-23_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nclass CsvPipe(Pipe):\n    def __init__(self, name, location=None, schema=None):\n        super().__init__(name, 'csv')\n        if location:\n            self.setLocation(location)\n        if schema:\n            self.setSchema(schema)\n\n    def setDelimiter(self, delimiter):\n        self.addProperty('delimiter', delimiter)\n\n    def setHeader(self, header):\n        self.addProperty('header', str(header).lower())\n\n    def setLocation(self, location):\n        self.addProperty('location', location)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Cluster Results for North Carolina Voters Dataset (CSV)\nDESCRIPTION: This CSV snippet shows two records from a cluster in the North Carolina Voters dataset. It demonstrates that records with the same cluster ID (115) have different 'recid' values, contrary to the expected results from the University of Leipzig study.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/ncVoters5M/README.txt#2025-04-23_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\nz_minScore,z_maxScore,z_cluster,recid,givenname,surname,suburb,postcode \n0.9909702339125167,0.9909702339125167,115,01214401,jill,williams,troy,27371\n0.9909702339125167,0.9909702339125167,115,02839015,jill,williams,troy,27371\n```\n\n----------------------------------------\n\nTITLE: Running Zingg Infrastructure Tests with spark-submit\nDESCRIPTION: Alternative command to run the tests using spark-submit, specifying the required JAR file.\nSOURCE: https://github.com/zinggai/zingg/blob/main/test/note.txt#2025-04-23_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n/opt/spark-3.2.4-bin-hadoop3.2/bin/spark-submit --jars ../common/client/target/zingg-common-client-0.4.1-SNAPSHOT.jar testInfraOwnGateway.py\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Processing Candidate Pairs\nDESCRIPTION: Fetches candidate pairs for labeling and prepares them for the labeling process.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# get candidate pairs\ncandidate_pairs_pd = getPandasDfFromDs(zingg.getUnmarkedRecords())\n \n# if no candidate pairs, run job and wait\nif candidate_pairs_pd.shape[0] == 0:\n  print('No unlabeled candidate pairs found.  Run findTraining job ...')\n\nelse:\n    # get list of pairs (as identified by z_cluster) to label \n    z_clusters = list(np.unique(candidate_pairs_pd['z_cluster'])) \n\n    # identify last reviewed cluster\n    last_z_cluster = '' # none yet\n\n    # print candidate pair stats\n    print('{0} candidate pairs found for labeling'.format(len(z_clusters)))\n```\n\n----------------------------------------\n\nTITLE: Initializing Zingg for Labeling Phase\nDESCRIPTION: Sets up Zingg for the labeling phase of the deduplication process.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\noptions = ClientOptions([ClientOptions.PHASE,\"label\"])\n\n#Zingg execution for the given phase\nzingg = ZinggWithSpark(args, options)\nzingg.init()\n```\n\n----------------------------------------\n\nTITLE: Viewing Zingg Analytics Event Example\nDESCRIPTION: This snippet shows an example of an analytics event captured while running Zingg. It includes various metadata about the execution environment and performance metrics, but no actual user data.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/security.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\nevent is {\"client_id\":\"localhost\",\"events\":[{\"name\":\"match\",\"params\":{\"executionTime\":\"1.743246748E9\",\"country\":\"IN\",\"zingg_version\":\"0.5.0\",\"modelId\":\"100\",\"domain\":\"localhost\",\"java_version\":\"11.0.18\",\"os_arch\":\"aarch64\",\"os_name\":\"Mac OS X\",\"DB_INSTANCE_TYPE\":null,\"DATABRICKS_RUNTIME_VERSION\":null,\"dataCount\":\"65.0\",\"ZINGG_HOME\":\"true\"}}],\"user_id\":\"zingg\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Dependencies for AWS S3\nDESCRIPTION: Configuration entry for spark.jars property in zingg.conf to include required AWS S3 dependencies.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/amazonS3.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nspark.jars=/<location>/hadoop-aws-3.1.0.jar,/<location>/aws-java-sdk-bundle-1.11.271.jar\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation\nDESCRIPTION: Executes documentation generation phase and displays HTML documentation for model and data.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\noptions = ClientOptions([ClientOptions.PHASE,\"generateDocs\"])\n\n#Zingg execution for the given phase\nzingg = ZinggWithSpark(args, options)\nzingg.initAndExecute()\n\nDOCS_DIR = zinggDir + \"/\" + modelId + \"/docs/\"\n\ndisplayHTML(open(DOCS_DIR+\"model.html\", 'r').read())\ndisplayHTML(open(DOCS_DIR+\"data.html\", 'r').read())\n```\n\n----------------------------------------\n\nTITLE: Executing Maven Commands for Zingg Compilation\nDESCRIPTION: This snippet shows the Maven commands needed to initialize the project and then compile and package it. The first command initializes the project dependencies, and the second command cleans any previous builds, compiles the source code, and packages it into deployable artifacts.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/compiling-from-source.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmvn initialize\n```\n\nLANGUAGE: bash\nCODE:\n```\nmvn clean compile package\n```\n\n----------------------------------------\n\nTITLE: Installing Apache Maven on Ubuntu\nDESCRIPTION: Commands to download and install Apache Maven 3.8.8 on Ubuntu. This includes downloading the binary, extracting it, and verifying the installation.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/settingUpZingg.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nwget https://dlcdn.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.tar.gz\ntar -xvf apache-maven-3.8.8-bin.tar.gz \nrm -rf apache-maven-3.8.8-bin.tar.gz \ncd apache-maven-3.8.8/\ncd bin\n./mvn --version\n```\n\n----------------------------------------\n\nTITLE: Documenting Zingg Base Module\nDESCRIPTION: Sphinx autodoc directive to generate documentation for the base zingg module including all members, undocumented members and inheritance hierarchy.\nSOURCE: https://github.com/zinggai/zingg/blob/main/python/docs/zingg.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: zingg\n   :members:\n   :undoc-members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Displaying Tabulate Package Information\nDESCRIPTION: Shows details about the installed tabulate package.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npip show tabulate\n```\n\n----------------------------------------\n\nTITLE: Disabling Zingg Metrics Collection\nDESCRIPTION: This JSON snippet demonstrates how to disable metrics collection in Zingg by setting the 'collectMetrics' flag to false in the configuration. When disabled, only a blank event is logged.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/security.md#2025-04-23_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"client_id\":\"localhost\",\"events\":[{\"name\":\"train\"}],\"user_id\":\"zingg\"}\n```\n\n----------------------------------------\n\nTITLE: Examining Larger Cluster with Varied RecIDs in North Carolina Voters Dataset (CSV)\nDESCRIPTION: This CSV snippet presents a larger cluster (ID 7) from the North Carolina Voters dataset. It shows multiple records with varying 'recid' values, names, and similarity scores, further illustrating the discrepancies with the expected clustering behavior.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/ncVoters5M/README.txt#2025-04-23_snippet_1\n\nLANGUAGE: csv\nCODE:\n```\nz_minScore,z_maxScore,z_cluster,recid,givenname,surname,suburb,postcode \n0.4818726054782634,0.9909702339125167,7,00469874,mary,little,taylorsville,28681\n0.4818726054782634,0.9909702339125167,7,00469874,mary,little,taylorsville,28681\n0.4818726054782634,0.9909702339125167,7,00469874,mary,little,taylorsville,28681\n0.5014005504087111,0.932461682579109,7,01159069,maria,little,taylorsville,28681\n0.0,0.9909702339125167,7,02092371,martha,little,taylorsville,28681\n0.0,0.9909702339125167,7,02092371,martha,little,taylorsville,28681\n0.0,0.9909702339125167,7,02092371,martha,little,taylorsville,28681\n0.4818726054782634,0.9909702339125167,7,07399073,mary,little,taylorsville,28681\n0.4818726054782634,0.9909702339125167,7,07399073,mary,little,taylorsville,28681\n0.4818726054782634,0.9909702339125167,7,07399073,mary,little,taylorsville,28681\n```\n\n----------------------------------------\n\nTITLE: Configuring Zingg Analytics Logging Level\nDESCRIPTION: This configuration snippet shows how to modify the log4j2.properties file to change the logging level for Zingg analytics to 'warn', which allows viewing of the captured information.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/security.md#2025-04-23_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\nlogger.zingg_analytics.level = warn\n```\n\n----------------------------------------\n\nTITLE: Running Zingg with snowEnv properties to verify installation\nDESCRIPTION: This command executes Zingg using the snowEnv.txt properties file to find training data based on the configSnow.json configuration. It builds Zingg models and identifies duplicates in the examples/febrl/test.csv file.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-zingg-enterprise-snowflake/verify-installation.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/zingg.sh --properties-file snowEnv.txt --phase findTrainingData --conf  examples/febrl/configSnow.json\n```\n\n----------------------------------------\n\nTITLE: Creating Interactive Widget for Labeling Candidate Pairs\nDESCRIPTION: Builds an interactive widget using IPython widgets to display candidate pairs and allow manual labeling. It prepares the data for display and sets up the labeling interface.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/databricks/FebrlExample.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Label Training Set\n# define variable to avoid duplicate saves\nready_for_save = False\nprint(candidate_pairs_pd)\n# user-friendly labels and corresponding zingg numerical value\n# (the order in the dictionary affects how displayed below)\nLABELS = {\n  'Uncertain':2,\n  'Match':1,\n  'No Match':0  \n  }\n# GET CANDIDATE PAIRS\n# ========================================================\n#candidate_pairs_pd = get_candidate_pairs()\nn_pairs = int(candidate_pairs_pd.shape[0]/2)\n# ========================================================\n# DEFINE IPYWIDGET DISPLAY\n# ========================================================\ndisplay_pd = candidate_pairs_pd.drop(\n  labels=[\n    'z_zid', 'z_prediction', 'z_score', 'z_isMatch', 'z_zsource'\n    ], \n  axis=1)\n# define header to be used with each displayed pair\nhtml_prefix = \"<p><span style='font-family:Courier New,Courier,monospace'>\"\nhtml_suffix = \"</p></span>\"\nheader = widgets.HTML(value=f\"{html_prefix}<b>\" + \"<br />\".join([str(i)+\"&nbsp;&nbsp;\" for i in display_pd.columns.to_list()]) + f\"</b>{html_suffix}\")\n# initialize display\nvContainers = []\nvContainers.append(widgets.HTML(value=f'<h2>Indicate if each of the {n_pairs} record pairs is a match or not</h2></p>'))\n# for each set of pairs\nfor n in range(n_pairs):\n  # get candidate records\n  candidate_left = display_pd.loc[2*n].to_list()\n  print(candidate_left)\n  candidate_right = display_pd.loc[(2*n)+1].to_list()\n  print(candidate_right)\n  # define grid to hold values\n  html = ''\n  for i in range(display_pd.shape[1]):\n    # get column name\n    column_name = display_pd.columns[i]\n    # if field is image\n    if column_name == 'image_path':\n      # define row header\n      html += '<tr>'\n      html += '<td><b>image</b></td>'\n      # read left image to encoded string\n      l_endcode = ''\n      if candidate_left[i] != '':\n        with open(candidate_left[i], \"rb\") as l_file:\n          l_encode = base64.b64encode( l_file.read() ).decode()\n      # read right image to encoded string\n      r_encode = ''\n      if candidate_right[i] != '':\n        with open(candidate_right[i], \"rb\") as r_file:\n          r_encode = base64.b64encode( r_file.read() ).decode()      \n      # present images\n      html += f'<td><img src=\"data:image/png;base64,{l_encode}\"></td>'\n      html += f'<td><img src=\"data:image/png;base64,{r_encode}\"></td>'\n      html += '</tr>'\n    elif column_name != 'image_path':  # display text values\n      if column_name == 'z_cluster': z_cluster = candidate_left[i]\n      html += '<tr>'\n      html += f'<td style=\"width:10%\"><b>{column_name}</b></td>'\n      html += f'<td style=\"width:45%\">{str(candidate_left[i])}</td>'\n      html += f'<td style=\"width:45%\">{str(candidate_right[i])}</td>'\n      html += '</tr>'\n  # insert data table\n  table = widgets.HTML(value=f'<table data-title=\"{z_cluster}\" style=\"width:100%;border-collapse:collapse\" border=\"1\">'+html+'</table>')\n  z_cluster = None\n  # assign label options to pair\n  label = widgets.ToggleButtons(\n    options=LABELS.keys(), \n    button_style='info'\n    )\n  # define blank line between displayed pair and next\n  blankLine=widgets.HTML(value='<br>')\n  # append pair, label and blank line to widget structure\n  vContainers.append(widgets.VBox(children=[table, label, blankLine]))\n# present widget\ndisplay(widgets.VBox(children=vContainers))\n# ========================================================\n# mark flag to allow save \nready_for_save = True\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-Platform Docker Manifest\nDESCRIPTION: Commands for creating and pushing a multi-platform Docker manifest that combines AMD64 and ARM64 images into a single reference.\nSOURCE: https://github.com/zinggai/zingg/blob/main/releaseActivities.txt#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker manifest create \\\n<docker-username>/<image-name>:latest \\\n--amend <docker-username>/<image-name>:amd64 \\\n--amend <docker-username>/<image-name>:arm64v8\n\ndocker manifest push <docker-username>/<image-name>:latest\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for ARM64 on Mac\nDESCRIPTION: Commands for building and pushing ARM64 Docker images on macOS systems. Uses build argument ARCH=arm64v8 for architecture-specific build.\nSOURCE: https://github.com/zinggai/zingg/blob/main/releaseActivities.txt#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t <docker-username>/<image-name>:arm64v8 --build-arg ARCH=arm64v8/ .\n\ndocker push <docker-username>/<image-name>:arm64v8\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for AMD64 on Ubuntu\nDESCRIPTION: Process for building and pushing AMD64 Docker images on Ubuntu systems, including login steps and architecture-specific build arguments.\nSOURCE: https://github.com/zinggai/zingg/blob/main/releaseActivities.txt#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo su\ndocker login => give username and password\n\ndocker build -t <docker-username>/<image-name>:amd64 --build-arg ARCH=amd64/ .\n\ndocker push <docker-username>/<image-name>:amd64\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Spark Cluster\nDESCRIPTION: Defines required environment variables for connecting Zingg with a Spark cluster. These include paths to Java and Spark installations, along with the Spark master connection URL.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-from-release/spark-cluster-checklist.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport JAVA_HOME=path to jdk\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport SPARK_HOME=path to Apache Spark\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport SPARK_MASTER=spark://master-host:master-port\n```\n\n----------------------------------------\n\nTITLE: Installing Zingg Python Package\nDESCRIPTION: This code snippet shows how to install the Zingg package using pip. The package provides APIs for entity resolution and related tasks.\nSOURCE: https://github.com/zinggai/zingg/blob/main/python/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install zingg\n```\n\n----------------------------------------\n\nTITLE: Finding Docker Container ID\nDESCRIPTION: Command to list all running Docker containers, which displays container IDs needed for the docker cp operation.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/docker/copying-files-to-and-from-the-container.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ docker ps\n```\n\n----------------------------------------\n\nTITLE: Running findTrainingData Phase in Zingg\nDESCRIPTION: Command to execute Zingg's findTrainingData phase using a configuration file. This command instructs Zingg to search for edge cases in the data that can be used for training purposes, writing the results to the directory specified in the config file.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/setup/training/findTrainingData.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./zingg.sh --phase findTrainingData --conf config.json\n```\n\n----------------------------------------\n\nTITLE: Executing Python Test Suite using Zingg Shell Script\nDESCRIPTION: Command to run the Python test suite using the zingg.sh shell script. The script takes a --run parameter pointing to the Python test file location.\nSOURCE: https://github.com/zinggai/zingg/blob/main/test/readme.txt#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./scripts/zingg.sh --run  test/pythonTest.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Data in JSON\nDESCRIPTION: JSON configuration for incremental data processing specifying the data source details including format, location, delimiter, header settings, and schema for the incremental data to be processed.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/runIncremental.md#2025-04-23_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"config\" : \"config.json\",\n    \"incrementalData\": [{\n            \"name\":\"customers_incr\",\n            \"format\":\"csv\",\n            \"props\": {\n                \"location\": \"test-incr.csv\",\n                \"delimiter\": \",\",\n                \"header\":false\n            },\n            \"schema\": \"recId string, fname string, lname string, stNo string, add1 string, add2 string, city string, state string, areacode string, dob string, ssn  string\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Monitoring Zingg Log Output in Bash\nDESCRIPTION: This command allows you to view the real-time logs of the Zingg process running in the background. It uses the 'tail' command with the '-f' flag to follow the output of the nohup.out file.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/stepbystep/installation/installing-zingg-enterprise-snowflake/running-async-long-jobs.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntail -f nohup.out\n```\n\n----------------------------------------\n\nTITLE: Displaying Zingg Package Information\nDESCRIPTION: Shows details about the installed Zingg package.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npip show zingg\n```\n\n----------------------------------------\n\nTITLE: Retrieving Spark Checkpoint Directory\nDESCRIPTION: Gets the current Spark checkpoint directory.\nSOURCE: https://github.com/zinggai/zingg/blob/main/examples/fabric/ExampleNotebook.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspark.sparkContext.getCheckpointDir()\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Jars for BigQuery Integration\nDESCRIPTION: Sets the required BigQuery driver jars in the Spark configuration. Two drivers are needed: spark-bigquery-with-dependencies and gcs-connector-hadoop2 for working with BigQuery.\nSOURCE: https://github.com/zinggai/zingg/blob/main/docs/dataSourcesAndSinks/bigquery.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nspark.jars=./spark-bigquery-with-dependencies_2.12-0.24.2.jar,./gcs-connector-hadoop2-latest.jar\n```\n\n----------------------------------------\n\nTITLE: Resolving Version Mismatch in Zingg Gateway Compilation\nDESCRIPTION: Instructions to uncomment specific compiler options in the compileGatewayEntry function to resolve version mismatch issues.\nSOURCE: https://github.com/zinggai/zingg/blob/main/test/note.txt#2025-04-23_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n\"-source\", \"1.8\", \"-target\", \"1.8\"\n```"
  }
]
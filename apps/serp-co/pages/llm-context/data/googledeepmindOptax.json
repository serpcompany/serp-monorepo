[
  {
    "owner": "google-deepmind",
    "repo": "optax",
    "content": "TITLE: Training the MLP Model with Optax Optimizer\nDESCRIPTION: Implements the full training loop that iterates through epochs and batches, updates model parameters using gradients, and tracks training/test metrics. Uses JAX's JIT compilation for efficient training steps and periodically reports progress.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/mlp_mnist.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_accuracy = []\ntrain_losses = []\n\n# Computes test set accuracy at initialization.\ntest_stats = dataset_stats(params, test_loader_batched)\ntest_accuracy = [test_stats[\"accuracy\"]]\ntest_losses = [test_stats[\"loss\"]]\n\n\n@jax.jit\ndef train_step(params, solver_state, batch):\n  # Performs a one step update.\n  (loss, aux), grad = jax.value_and_grad(loss_accuracy, has_aux=True)(\n      params, batch\n  )\n  updates, solver_state = solver.update(grad, solver_state, params)\n  params = optax.apply_updates(params, updates)\n  return params, solver_state, loss, aux\n\n\nfor epoch in range(N_EPOCHS):\n  train_accuracy_epoch = []\n  train_losses_epoch = []\n\n  for step, train_batch in enumerate(train_loader_batched.as_numpy_iterator()):\n    params, solver_state, train_loss, train_aux = train_step(\n        params, solver_state, train_batch\n    )\n    train_accuracy_epoch.append(train_aux[\"accuracy\"])\n    train_losses_epoch.append(train_loss)\n    if step % 20 == 0:\n      print(\n          f\"step {step}, train loss: {train_loss:.2e}, train accuracy:\"\n          f\" {train_aux['accuracy']:.2f}\"\n      )\n\n  test_stats = dataset_stats(params, test_loader_batched)\n  test_accuracy.append(test_stats[\"accuracy\"])\n  test_losses.append(test_stats[\"loss\"])\n  train_accuracy.append(np.mean(train_accuracy_epoch))\n  train_losses.append(np.mean(train_losses_epoch))\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Prediction and Loss Functions with JAX\nDESCRIPTION: Defines JIT-compiled functions for model prediction, loss calculation, and accuracy computation using JAX. The loss function uses Optax's softmax cross-entropy with integer labels, and includes utility functions for model updates.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/mlp_mnist.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnet = MLP()\n\n@jax.jit\ndef predict(params, inputs):\n  return net.apply({\"params\": params}, inputs)\n\n\n@jax.jit\ndef loss_accuracy(params, data):\n  \"\"\"Computes loss and accuracy over a mini-batch.\n\n  Args:\n    params: parameters of the model.\n    bn_params: state of the model.\n    data: tuple of (inputs, labels).\n    is_training: if true, uses train mode, otherwise uses eval mode.\n\n  Returns:\n    loss: float\n  \"\"\"\n  inputs, labels = data\n  logits = predict(params, inputs)\n  loss = optax.softmax_cross_entropy_with_integer_labels(\n      logits=logits, labels=labels\n  ).mean()\n  accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n  return loss, {\"accuracy\": accuracy}\n\n@jax.jit\ndef update_model(state, grads):\n  return state.apply_gradients(grads=grads)\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with JAX and Optax\nDESCRIPTION: Code showing how to define a loss function using Optax's l2_loss and compute gradients using JAX's automatic differentiation.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncompute_loss = lambda params, x, y: optax.l2_loss(params['w'].dot(x), y)\ngrads = jax.grad(compute_loss)(params, xs, ys)\n```\n\n----------------------------------------\n\nTITLE: Training Loop for Linear Model\nDESCRIPTION: Implements basic training loop using Optax's update and apply_updates functions\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# A simple update loop.\nfor _ in range(1000):\n  grads = jax.grad(compute_loss)(params, xs, ys)\n  updates, opt_state = optimizer.update(grads, opt_state)\n  params = optax.apply_updates(params, updates)\n\nassert jnp.allclose(params, target_params), \\\n'Optimization should retrieve the target params used to generate the data.'\n```\n\n----------------------------------------\n\nTITLE: Accessing Learning Rate with Hyperparameter Injection in Optax\nDESCRIPTION: Demonstrates how to access and store learning rates in optimizer state using optax.inject_hyperparams. Shows initialization and retrieval of learning rate values.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\noptimizer = optax.inject_hyperparams(optax.adamw)(learning_rate=schedule)\n\nparams = initial_params\nstate = optimizer.init(params)\nprint('initial learning rate:', state.hyperparams['learning_rate'])\n\n_, state = fit(initial_params, optimizer)\n\nprint('final learning rate:', state.hyperparams['learning_rate'])\n```\n\n----------------------------------------\n\nTITLE: Defining Multilayer Perceptron Model with Flax in Python\nDESCRIPTION: Implements a simple MLP model using Flax's neural network library. The model flattens the input images and passes them through two hidden layers with ReLU activation, followed by an output layer for classification.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/mlp_mnist.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nn.Module):\n  \"\"\"A simple multilayer perceptron model for image classification.\"\"\"\n  hidden_sizes: Sequence[int] = (1000, 1000)\n\n  @nn.compact\n  def __call__(self, x):\n    # Flattens images in the batch.\n    x = x.reshape((x.shape[0], -1))\n    x = nn.Dense(features=self.hidden_sizes[0])(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=self.hidden_sizes[1])(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=NUM_CLASSES)(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: ResNet Model Architecture Implementation\nDESCRIPTION: Defines the complete ResNet architecture including ResNetBlock, BottleneckResNetBlock, and main ResNet model with various configurations (ResNet18, ResNet34, etc.).\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/cifar10_resnet.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass ResNet(nn.Module):\n  \"\"\"ResNetV1.\"\"\"\n\n  stage_sizes: Sequence[int]\n  block_cls: ModuleDef\n  num_classes: int\n  num_filters: int = 64\n  dtype: Any = jnp.float32\n  act: Callable = nn.relu\n  conv: ModuleDef = nn.Conv\n  initial_conv_config: Optional[Dict[str, Any]] = None\n\n  @nn.compact\n  def __call__(self, x, train: bool = True):\n    conv = partial(self.conv, use_bias=False, dtype=self.dtype)\n    norm = partial(\n        nn.BatchNorm,\n        use_running_average=not train,\n        momentum=0.9,\n        epsilon=1e-5,\n        dtype=self.dtype,\n    )\n\n    initial_conv_config = dict(self.initial_conv_config)\n    initial_conv_config.setdefault(\"kernel_size\", 7)\n    initial_conv_config.setdefault(\"strides\", 2)\n    initial_conv_config.setdefault(\"with_bias\", False)\n    initial_conv_config.setdefault(\"padding\", \"SAME\")\n    initial_conv_config.setdefault(\"name\", \"initial_conv\")\n\n    x = conv(self.num_filters, **self.initial_conv_config)(x)\n    x = norm(name=\"bn_init\")(x)\n    x = nn.relu(x)\n    x = nn.max_pool(x, (3, 3), strides=(2, 2), padding=\"SAME\")\n    for i, block_size in enumerate(self.stage_sizes):\n      for j in range(block_size):\n        strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n        x = self.block_cls(\n            self.num_filters * 2**i,\n            strides=strides,\n            conv=conv,\n            norm=norm,\n            act=self.act,\n        )(x)\n    x = jnp.mean(x, axis=(1, 2))\n    x = nn.Dense(self.num_classes, dtype=self.dtype)(x)\n    x = jnp.asarray(x, self.dtype)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Initializing Adam Optimizer in Optax\nDESCRIPTION: Code snippet showing how to initialize the Adam optimizer and its state using Optax. It creates an optimizer with a specified learning rate and initializes its state with model parameters.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\noptimizer = optax.adam(learning_rate)\n# Obtain the `opt_state` that contains statistics for the optimizer.\nparams = {'w': jnp.ones((num_weights,))}\nopt_state = optimizer.init(params)\n```\n\n----------------------------------------\n\nTITLE: Initializing Model Parameters and Optimizer in JAX/Optax\nDESCRIPTION: Sets up the Adam optimizer with Optax, initializes model parameters with random values, and creates the optimizer state. Includes a utility function to compute dataset-wide statistics for model evaluation.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/mlp_mnist.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsolver = optax.adam(LEARNING_RATE)\nrng = jax.random.PRNGKey(0)\ndummy_data = jnp.ones((1,) + IMG_SIZE, dtype=jnp.float32)\n\nparams = net.init({\"params\": rng}, dummy_data)[\"params\"]\n\nsolver_state = solver.init(params)\n\ndef dataset_stats(params, data_loader):\n  \"\"\"Computes loss and accuracy over the dataset `data_loader`.\"\"\"\n  all_accuracy = []\n  all_loss = []\n  for batch in data_loader.as_numpy_iterator():\n    batch_loss, batch_aux = loss_accuracy(params, batch)\n    all_loss.append(batch_loss)\n    all_accuracy.append(batch_aux[\"accuracy\"])\n  return {\"loss\": np.mean(all_loss), \"accuracy\": np.mean(all_accuracy)}\n```\n\n----------------------------------------\n\nTITLE: Implementing L-BFGS as Gradient Transformation in Python\nDESCRIPTION: This code demonstrates how to use L-BFGS as a gradient transformation in Optax. It defines an objective function, initializes the optimizer, and runs the optimization process.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lbfgs.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Define objective\ndim = 8\nw_opt = jnp.ones(dim)\nmat = jrd.normal(jrd.PRNGKey(0), (dim, dim))\nmat = mat.dot(mat.T)\n\n\ndef fun(w):\n  return 0.5 * (w - w_opt).dot(mat.dot(w - w_opt))\n\n\n# Define optimizer\nlr = 1e-1\nopt = optax.scale_by_lbfgs()\n\n# Initialize optimization\nw = jrd.normal(jrd.PRNGKey(1), (dim,))\nstate = opt.init(w_opt)\n\n# Run optimization\nfor i in range(16):\n  v, g = jax.value_and_grad(fun)(w)\n  print(f'Iteration: {i}, Value:{v:.2e}')\n  u, state = opt.update(g, state, w)\n  w = w - lr * u\n\nprint(f'Final value: {fun(w):.2e}')\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing MNIST Dataset with TensorFlow\nDESCRIPTION: Loads the MNIST dataset using TensorFlow Datasets, normalizes pixel values to [0,1], creates shuffled batches for training, and prepares test data. The function extracts key dataset information like number of classes and image dimensions.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/mlp_mnist.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n(train_loader, test_loader), info = tfds.load(\n    \"mnist\", split=[\"train\", \"test\"], as_supervised=True, with_info=True\n)\n\nmin_max_rgb = lambda image, label: (tf.cast(image, tf.float32) / 255., label)\ntrain_loader = train_loader.map(min_max_rgb)\ntest_loader = test_loader.map(min_max_rgb)\n\nNUM_CLASSES = info.features[\"label\"].num_classes\nIMG_SIZE = info.features[\"image\"].shape\n\ntrain_loader_batched = train_loader.shuffle(\n    buffer_size=10_000, reshuffle_each_iteration=True\n).batch(BATCH_SIZE, drop_remainder=True)\n\ntest_loader_batched = test_loader.batch(BATCH_SIZE, drop_remainder=True)\n```\n\n----------------------------------------\n\nTITLE: Training Step Implementation\nDESCRIPTION: Implements a JIT-compiled training step that handles both DP-SGD and vanilla SGD training modes.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/differentially_private_sgd.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(params, opt_state, batch):\n  grad_fn = jax.grad(loss_fn, has_aux=True)\n  if DPSGD:\n    # Inserts a dimension in axis 1 to use jax.vmap over the batch.\n    batch = jax.tree.map(lambda x: x[:, None], batch)\n    # Uses jax.vmap across the batch to extract per-example gradients.\n    grad_fn = jax.vmap(grad_fn, in_axes=(None, 0))\n\n  grads, _ = grad_fn(params, batch)\n  updates, new_opt_state = tx.update(grads, opt_state, params)\n  new_params = optax.apply_updates(params, updates)\n  return new_params, new_opt_state\n```\n\n----------------------------------------\n\nTITLE: Composing Multiple Gradient Transformations in Optax\nDESCRIPTION: Example of chaining multiple gradient transformations including gradient clipping, Adam optimization, and learning rate scaling.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmax_norm = 100.\nlearning_rate = 1e-3\n\nmy_optimizer = optax.chain(\n    optax.clip_by_global_norm(max_norm),\n    optax.scale_by_adam(eps=1e-4),\n    optax.scale(-learning_rate))\n```\n\n----------------------------------------\n\nTITLE: Creating Optimization Step Function in Python\nDESCRIPTION: Defines a function to create a jitted step function for each optimizer. This function computes gradients, updates parameters, and returns the new state and loss value.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef make_step(opt):\n  @jax.jit\n  def step(store):\n    value, grads = jax.value_and_grad(loss)(store.params)\n    updates, state = opt.update(grads, store.state, store.params)\n    params = optax.apply_updates(store.params, updates)\n    return store.replace(\n        params=params,\n        state=state,\n        step=store.step+1), value\n  return step\n```\n\n----------------------------------------\n\nTITLE: Loss and Accuracy Computation\nDESCRIPTION: Implements the loss and accuracy computation function with L2 regularization and cross-entropy loss.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/cifar10_resnet.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@partial(jax.jit, static_argnums=(3,))\ndef loss_accuracy(params, bn_params, data, is_training: bool = True):\n  inputs, labels = data\n  logits, net_state = predict(params, bn_params, inputs, is_training=is_training)\n  mean_loss = optax.softmax_cross_entropy_with_integer_labels(\n      logits=logits, labels=labels\n  ).mean()\n  accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n  l2_params = jax.tree.leaves(params)\n  weight_l2 = sum(jnp.sum(x**2) for x in l2_params if x.ndim > 1)\n  loss = mean_loss + 0.5 * L2_REG * weight_l2\n  return loss, {\"accuracy\": accuracy, \"batch_stats\": net_state[\"batch_stats\"]}\n```\n\n----------------------------------------\n\nTITLE: CNN Model Architecture Definition\nDESCRIPTION: Defines a convolutional neural network architecture using JAX's stax library, with two convolutional layers followed by dense layers.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/differentially_private_sgd.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninit_random_params, predict = stax.serial(\n    stax.Conv(16, (8, 8), padding=\"SAME\", strides=(2, 2)),\n    stax.Relu,\n    stax.MaxPool((2, 2), (1, 1)),\n    stax.Conv(32, (4, 4), padding=\"VALID\", strides=(2, 2)),\n    stax.Relu,\n    stax.MaxPool((2, 2), (1, 1)),\n    stax.Flatten,\n    stax.Dense(32),\n    stax.Relu,\n    stax.Dense(10),\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop\nDESCRIPTION: Defines the training loop with JIT-compiled step function for efficiency. Includes periodic evaluation on validation data and loss tracking.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/nanolm.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nall_train_losses = []\nall_eval_losses = []\n\n@jax.jit\ndef step(key, params, opt_state):\n  key, subkey = jax.random.split(key)\n  batch = get_batch(key, train_data)\n  loss, grad = jax.value_and_grad(loss_fun)(params, *batch, subkey)\n  updates, opt_state = opt.update(grad, opt_state, params)\n  params = optax.apply_updates(params, updates)\n  return params, key, opt_state, loss\n\n\nfor i in range(N_ITERATIONS):\n  var_params, key, opt_state, loss = step(key, var_params, opt_state)\n  all_train_losses.append(loss)\n\n  if i % N_FREQ_EVAL == 0:\n    key, subkey = jax.random.split(key)\n    eval_loss = eval_step(var_params, *get_batch(subkey, eval_data))\n    all_eval_losses.append(eval_loss)\n    print(f\"Step: {i}\\t train loss: {loss}\\t eval loss: {eval_loss}\")\n```\n\n----------------------------------------\n\nTITLE: Running Optimization Loop for Multiple Optimizers in Python\nDESCRIPTION: Executes the optimization loop for all optimizers (SAM, SGD, SAM Mom, Mom, SAM Adam, Adam) for a specified number of iterations, storing the loss values and parameters at each step.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nT = 8000\nfor i in range(T):\n  sam_store, sam_val = sam_step(sam_store)\n  sgd_store, sgd_val = sgd_step(sgd_store)\n  sam_mom_store, sam_mom_val = sam_mom_step(sam_mom_store)\n  mom_store, mom_val = mom_step(mom_store)\n  sam_adam_store, sam_adam_val = sam_adam_step(sam_adam_store)\n  adam_store, adam_val = adam_step(adam_store)\n\n  sam_vals.append(sam_val)\n  sgd_vals.append(sgd_val)\n  sam_mom_vals.append(sam_mom_val)\n  mom_vals.append(mom_val)\n  sam_adam_vals.append(sam_adam_val)\n  adam_vals.append(adam_val)\n\n  sam_params.append(sam_store.params)\n  sgd_params.append(sgd_store.params)\n  sam_mom_params.append(sam_mom_store.params)\n  mom_params.append(mom_store.params)\n  sam_adam_params.append(sam_adam_store.params)\n  adam_params.append(adam_store.params)\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluation Functions\nDESCRIPTION: Implements functions for computing dataset statistics and performing training steps with adversarial examples.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/adversarial_training.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef dataset_stats(params, data_loader, iter_per_epoch):\n  \"\"\"Computes accuracy on clean and adversarial images.\"\"\"\n  adversarial_accuracy = 0.\n  clean_accuracy = 0.\n  for batch in data_loader.as_numpy_iterator():\n    images, labels = batch\n    images = images.astype(jnp.float32) / 255\n    clean_accuracy += jnp.mean(accuracy(params, (images, labels))) / iter_per_epoch\n    adversarial_images = pgd_attack(images, labels, params, epsilon=EPSILON)\n    adversarial_accuracy += jnp.mean(accuracy(params, (adversarial_images, labels))) / iter_per_epoch\n  return {\"adversarial accuracy\": adversarial_accuracy, \"accuracy\": clean_accuracy}\n\n@jax.jit\ndef train_step(params, opt_state, batch):\n  images, labels = batch\n  images = images.astype(jnp.float32) / 255\n  adversarial_images_train = pgd_attack(images, labels, params, epsilon=EPSILON)\n  loss_grad_fun = jax.grad(loss_fun)\n  grads = loss_grad_fun(params, L2_REG, (adversarial_images_train, labels))\n  updates, opt_state = optimizer.update(grads, opt_state)\n  params = optax.apply_updates(params, updates)\n  return params, opt_state\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Parameters for ResNet on CIFAR\nDESCRIPTION: Defines the hyperparameters for training a ResNet model on CIFAR datasets. Parameters include the number of training epochs, batch size, initial learning rate, model architecture selection, and dataset choice between CIFAR10 and CIFAR100.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/cifar10_resnet.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# @markdown Total number of epochs to train for:\nMAX_EPOCHS = 50  # @param{type:\"integer\"}\n# @markdown Number of samples in each batch:\nBATCH_SIZE = 128  # @param{type:\"integer\"}\n# @markdown The initial learning rate for the optimizer:\nPEAK_LR = 0.12  # @param{type:\"number\"}\n# @markdown The model architecture for the neural network. Can be one of `'resnet1'`, `'resnet18'`, `'resnet34'`, `'resnet50'`, `'resnet101'`, `'resnet152'`, `'resnet200'`:\nMODEL = \"resnet18\"  # @param{type:\"string\"}\n# @markdown The dataset to use. Could be either `'cifar10'` or `'cifar100'`:\nDATASET = \"cifar10\"  # @param{type:\"string\"}\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop Functions\nDESCRIPTION: Creates functions for executing training steps and fitting the model using the specified optimizer and batches.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/gradient_accumulation.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef build_train_step(optimizer: optax.GradientTransformation):\n  \"\"\"Builds a function for executing a single step in the optimization.\"\"\"\n\n  @jax.jit\n  def update(params, opt_state, batch):\n    grads = jax.grad(loss_fn)(params, batch)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state\n\n  return update\n\n\ndef fit(\n    optimizer: optax.GradientTransformation,\n    params: optax.Params,\n    batches: Iterable[dict[str, jnp.ndarray]],\n) -> optax.Params:\n  \"\"\"Executes a train loop over the train batches using the given optimizer.\"\"\"\n\n  train_step = build_train_step(optimizer)\n  opt_state = optimizer.init(params)\n\n  for batch in batches:\n    params, opt_state = train_step(params, opt_state, batch)\n\n  return params\n```\n\n----------------------------------------\n\nTITLE: Model Training and Evaluation Functions\nDESCRIPTION: Implements JIT-compiled functions for prediction, loss calculation, accuracy computation, and model parameter updates.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lookahead_mnist.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef predict(params, inputs):\n  return net.apply({'params': params}, inputs)\n\n@jax.jit\ndef loss_accuracy(params, data):\n  inputs, labels = data\n  logits = predict(params, inputs)\n  loss = optax.softmax_cross_entropy_with_integer_labels(\n      logits=logits, labels=labels\n  ).mean()\n  accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n  return loss, {\"accuracy\": accuracy}\n\n@jax.jit\ndef update_model(state, grads):\n  return state.apply_gradients(grads=grads)\n```\n\n----------------------------------------\n\nTITLE: Initializing NanoLM Model and Loss Functions\nDESCRIPTION: Sets up the NanoLM model with specified hyperparameters and defines the training loss function (cross-entropy) and evaluation step function. Includes dropout regularization for training.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/nanolm.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = NanoLM(\n    vocab_size=len(vocab),\n    num_layers=NUM_LAYERS,\n    num_heads=NUM_HEADS,\n    head_size=HEAD_SIZE,\n    dropout_rate=DROPOUT_RATE,\n    embed_size=EMBED_SIZE,\n    block_size=BLOCK_SIZE,\n)\n\ndef loss_fun(params, x, y, dropout_key):\n  logits = model.apply(params, x, training=True, rngs={\"dropout\": dropout_key})\n  return optax.softmax_cross_entropy_with_integer_labels(\n      logits=logits, labels=y\n  ).mean()\n\n\n@jax.jit\ndef eval_step(params, x, y):\n  logits = model.apply(params, x, training=False)\n  return optax.softmax_cross_entropy_with_integer_labels(\n      logits=logits, labels=y\n  ).mean()\n```\n\n----------------------------------------\n\nTITLE: Implementing MSE Loss Function - Python\nDESCRIPTION: Defines a custom Mean Squared Error loss function with JIT compilation for improved performance.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/flax_example.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef make_mse_func(x_batched, y_batched):\n  def mse(params):\n    def squared_error(x, y):\n      pred = model.apply(params, x)\n      return jnp.inner(y-pred, y-pred) / 2.0\n    return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)\n  return jax.jit(mse)\n\nloss = make_mse_func(x_samples, y_samples)\nloss_grad_fn = jax.value_and_grad(loss)\n```\n\n----------------------------------------\n\nTITLE: Updating Parameters with Optax Optimizer\nDESCRIPTION: Code demonstrating the parameter update step in an optimization loop using Optax's update and apply_updates functions to apply computed gradients to the model parameters.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nupdates, opt_state = optimizer.update(grads, opt_state)\nparams = optax.apply_updates(params, updates)\n```\n\n----------------------------------------\n\nTITLE: Executing a Model Training Loop in Python with JAX/Optax\nDESCRIPTION: Initializes lists for tracking training/test metrics, calculates initial test set statistics, defines a JIT-compiled `train_step` function for efficient single-batch updates using JAX's `value_and_grad` and Optax's `solver.update` and `apply_updates`, and then executes the main training loop for `MAX_EPOCHS`. Within the loop, it iterates over training batches, updates model parameters (`var_params`), solver state (`var_solver_state`), and network state (`net_state`), and records training metrics. Test metrics are computed once per epoch using the `dataset_stats` function. Progress is printed every 10 epochs.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/cifar10_resnet.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrain_accuracy = []\ntrain_losses = []\n\n# Computes test set accuracy at initialization.\ntest_stats = dataset_stats(var_params, net_state, test_loader_batched)\ntest_accuracy = [test_stats[\"accuracy\"]]\ntest_losses = [test_stats[\"loss\"]]\n\n\n@jax.jit\ndef train_step(params, net_state, solver_state, batch):\n  # Performs a one step update.\n  (loss, aux), grad = jax.value_and_grad(loss_accuracy, has_aux=True)(\n      params, net_state, batch, is_training=True\n  )\n  updates, solver_state = solver.update(grad, solver_state)\n  params = optax.apply_updates(params, updates)\n  return params, solver_state, loss, aux\n\n\n# Executes a training loop.\nfor epoch in range(MAX_EPOCHS):\n  train_accuracy_epoch = []\n  train_losses_epoch = []\n\n  for train_batch in train_loader_batched.as_numpy_iterator():\n    var_params, var_solver_state, train_loss, train_aux = train_step(\n        var_params, net_state, var_solver_state, train_batch\n    )\n    net_state = train_aux[\"batch_stats\"]\n    train_accuracy_epoch.append(train_aux[\"accuracy\"])\n    train_losses_epoch.append(train_loss)\n\n  #  Once per epoch, makes a pass over the test set to compute accuracy.\n  test_stats = dataset_stats(var_params, net_state, test_loader_batched)\n  test_accuracy.append(test_stats[\"accuracy\"])\n  test_losses.append(test_stats[\"loss\"])\n  train_accuracy.append(np.mean(train_accuracy_epoch))\n  train_losses.append(np.mean(train_losses_epoch))\n\n  # Prints accuracy every 10 epochs.\n  if epoch % 10 == 0:\n    print(\"Epoch: \", epoch)\n    print(\"Test set accuracy: \", test_accuracy[-1])\n    print(\"Train set accuracy: \", np.mean(train_accuracy_epoch))\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Loop with Reduce on Plateau Scheduler\nDESCRIPTION: Defines and executes the training loop using the MLP model and reduce on plateau scheduler, tracking training and test metrics.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(params, opt_state, batch):\n  \"\"\"Performs a one step update.\"\"\"\n  (value, aux), grad = jax.value_and_grad(loss_accuracy, has_aux=True)(\n      params, batch\n  )\n  updates, opt_state = opt.update(grad, opt_state, params, value=value)\n  params = optax.apply_updates(params, updates)\n  return params, opt_state, value, aux\n\n\nparams = init_params\n\n# Computes metrics at initialization.\ntrain_stats = dataset_stats(params, test_loader_batched)\ntrain_accuracy = [train_stats[\"accuracy\"]]\ntrain_losses = [train_stats['loss']]\n\ntest_stats = dataset_stats(params, test_loader_batched)\ntest_accuracy = [test_stats[\"accuracy\"]]\ntest_losses = [test_stats[\"loss\"]]\n\nlr_scale_history = []\nfor epoch in range(N_EPOCHS):\n  train_accuracy_epoch = []\n  train_losses_epoch = []\n\n  for _, train_batch in enumerate(train_loader_batched.as_numpy_iterator()):\n    params, opt_state, train_loss, train_aux = train_step(\n        params, opt_state, train_batch\n    )\n    train_accuracy_epoch.append(train_aux[\"accuracy\"])\n    train_losses_epoch.append(train_loss)\n\n  mean_train_accuracy = np.mean(train_accuracy_epoch)\n  mean_train_loss = np.mean(train_losses_epoch)\n\n  # fetch the scaling factor from the reduce_on_plateau transform\n  lr_scale = otu.tree_get(opt_state, \"scale\")\n  lr_scale_history.append(lr_scale)\n\n  train_accuracy.append(mean_train_accuracy)\n  train_losses.append(mean_train_loss)\n\n  test_stats = dataset_stats(params, test_loader_batched)\n  test_accuracy.append(test_stats[\"accuracy\"])\n  test_losses.append(test_stats[\"loss\"])\n  print(\n      f\"Epoch {epoch + 1}/{N_EPOCHS}, mean train accuracy:\"\n      f\" {mean_train_accuracy}, lr scale: {otu.tree_get(opt_state, 'scale')}\"\n  )\n```\n\n----------------------------------------\n\nTITLE: Implementing Outer Optimization Step\nDESCRIPTION: Defines the outer loss function and a JIT-compiled step for the outer optimization loop.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/meta_learning.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef outer_loss(eta, theta, state, samples):\n  state.hyperparams['learning_rate'] = jax.nn.sigmoid(eta)\n\n  for x, y in samples[:-1]:\n    theta, state = step(theta, state, x, y)\n\n  x, y = samples[-1]\n\n  return loss(theta, x, y), (theta, state)\n\n\n@jax.jit\ndef outer_step(eta, theta, meta_state, state, samples):\n  grad, (theta, state) = jax.grad(\n      outer_loss, has_aux=True)(eta, theta, state, samples)\n\n  meta_updates, meta_state = meta_opt.update(grad, meta_state)\n  eta = optax.apply_updates(eta, meta_updates)\n\n  return eta, theta, meta_state, state\n```\n\n----------------------------------------\n\nTITLE: Implementing L-BFGS Optimization Solver in Python\nDESCRIPTION: Core implementation of an L-BFGS solver that minimizes a given function using optax. Takes initial parameters, objective function, optimizer instance, maximum iterations, and tolerance as inputs.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lbfgs.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef run_opt(init_params, fun, opt, max_iter, tol):\n  value_and_grad_fun = optax.value_and_grad_from_state(fun)\n\n  def step(carry):\n    params, state = carry\n    value, grad = value_and_grad_fun(params, state=state)\n    updates, state = opt.update(\n        grad, state, params, value=value, grad=grad, value_fn=fun\n    )\n    params = optax.apply_updates(params, updates)\n    return params, state\n\n  def continuing_criterion(carry):\n    _, state = carry\n    iter_num = otu.tree_get(state, 'count')\n    grad = otu.tree_get(state, 'grad')\n    err = otu.tree_l2_norm(grad)\n    return (iter_num == 0) | ((iter_num < max_iter) & (err >= tol))\n\n  init_carry = (init_params, opt.init(init_params))\n  final_params, final_state = jax.lax.while_loop(\n      continuing_criterion, step, init_carry\n  )\n  return final_params, final_state\n```\n\n----------------------------------------\n\nTITLE: Implementing Linear Model Network and Loss\nDESCRIPTION: Defines a simple linear model using JAX's vmap and a loss function using Optax's l2_loss\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@functools.partial(jax.vmap, in_axes=(None, 0))\ndef network(params, x):\n  return jnp.dot(params, x)\n\ndef compute_loss(params, x, y):\n  y_pred = network(params, x)\n  loss = jnp.mean(optax.l2_loss(y_pred, y))\n  return loss\n```\n\n----------------------------------------\n\nTITLE: Running Meta-Learning Optimization\nDESCRIPTION: Executes the meta-learning process, updating both the model parameter and learning rate over multiple iterations.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/meta_learning.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstate = opt.init(theta)\n# inverse sigmoid, to match the value we initialized the inner optimizer with.\neta = -np.log(1. / init_learning_rate - 1)\nmeta_state = meta_opt.init(eta)\n\nN = 7\nlearning_rates = []\nthetas = []\n\nfor i in range(2000):\n  samples = [next(g) for i in range(N)]\n  eta, theta, meta_state, state = outer_step(eta, theta, meta_state, state, samples)\n  learning_rates.append(jax.nn.sigmoid(eta))\n  thetas.append(theta)\n```\n\n----------------------------------------\n\nTITLE: Calculating Dataset Statistics in Python using JAX/NumPy\nDESCRIPTION: Defines a Python function `dataset_stats` that computes the average loss and accuracy for a given model over an entire dataset provided by a data loader. It iterates through the data loader, calculates loss and accuracy for each batch using a `loss_accuracy` function (assumed defined elsewhere), and returns the mean values. This function is typically used for evaluation on a test or validation set.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/cifar10_resnet.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef dataset_stats(params, net_state, data_loader):\n  \"\"\"Computes loss and accuracy over the dataset `data_loader`.\"\"\"\n  all_accuracy = []\n  all_loss = []\n  for cur_batch in data_loader.as_numpy_iterator():\n    batch_loss, batch_aux = loss_accuracy(\n        params, net_state, cur_batch, is_training=False\n    )\n    all_loss.append(batch_loss)\n    all_accuracy.append(batch_aux[\"accuracy\"])\n  return {\"loss\": np.mean(all_loss), \"accuracy\": np.mean(all_accuracy)}\n```\n\n----------------------------------------\n\nTITLE: Creating SAM Optimizer with Adam in Python\nDESCRIPTION: Implements a SAM optimizer using Adam for both the outer and adversarial optimizers. It increases the number of adversarial steps between syncs for better performance and sets up baseline Adam and SAM Adam optimizers.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef sam_adam(lr=1e-3, b1=0.9, b2=0.999, rho=0.03, sync_period=5):\n  \"\"\"A SAM optimizer using Adam for the outer optimizer.\"\"\"\n  opt = optax.adam(lr, b1=b1, b2=b2)\n  adv_opt = optax.chain(contrib.normalize(), optax.adam(rho))\n  return contrib.sam(opt, adv_opt, sync_period=sync_period)\n\nsam_adam_opt = sam_adam(lr)\nadam_opt = optax.adam(lr)\n```\n\n----------------------------------------\n\nTITLE: Configuring Adam Optimizer - Python\nDESCRIPTION: Sets up the Adam optimizer using Optax's transformation chain with custom parameters.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/flax_example.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntx = optax.chain(\n    optax.scale_by_adam(b1=0.9, b2=0.999, eps=1e-8),\n    optax.scale(-LEARNING_RATE)\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Optimizer State - Python\nDESCRIPTION: Initializes the optimizer state with the model parameters.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/flax_example.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nopt_state = tx.init(params)\n```\n\n----------------------------------------\n\nTITLE: Training Loop Implementation\nDESCRIPTION: Implements the main training loop with epoch iterations, batch processing, and periodic evaluation on the test dataset.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lookahead_mnist.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_accuracy = []\ntrain_losses = []\n\ntest_stats = dataset_stats(params.slow, test_loader_batched)\ntest_accuracy = [test_stats[\"accuracy\"]]\ntest_losses = [test_stats[\"loss\"]]\n\n@jax.jit\ndef train_step(params, solver_state, batch):\n  (loss, aux), grad = jax.value_and_grad(loss_accuracy, has_aux=True)(\n      params.fast, batch\n  )\n  updates, solver_state = solver.update(grad, solver_state, params)\n  params = optax.apply_updates(params, updates)\n  return params, solver_state, loss, aux\n\nfor epoch in range(N_EPOCHS):\n  train_accuracy_epoch = []\n  train_losses_epoch = []\n\n  for step, train_batch in enumerate(train_loader_batched.as_numpy_iterator()):\n    params, solver_state, train_loss, train_aux = train_step(\n        params, solver_state, train_batch\n    )\n    train_accuracy_epoch.append(train_aux[\"accuracy\"])\n    train_losses_epoch.append(train_loss)\n    if step % 20 == 0:\n      print(\n          f\"step {step}, train loss: {train_loss:.2e}, train accuracy:\"\n          f\" {train_aux['accuracy']:.2f}\"\n      )\n\n  test_stats = dataset_stats(params.slow, test_loader_batched)\n  test_accuracy.append(test_stats[\"accuracy\"])\n  test_losses.append(test_stats[\"loss\"])\n  train_accuracy.append(np.mean(train_accuracy_epoch))\n  train_losses.append(np.mean(train_losses_epoch))\n```\n\n----------------------------------------\n\nTITLE: Hyperparameter Schedule Example\nDESCRIPTION: Demonstrates how to modify optimizer hyperparameters during training using inject_hyperparams\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndecaying_global_norm_tx = optax.inject_hyperparams(optax.clip_by_global_norm)(\n    max_norm=optax.linear_schedule(1.0, 0.0, transition_steps=99))\n\nopt_state = decaying_global_norm_tx.init(None)\nassert opt_state.hyperparams['max_norm'] == 1.0, 'Max norm should start at 1.0'\n\nfor _ in range(100):\n  _, opt_state = decaying_global_norm_tx.update(None, opt_state)\n\nassert opt_state.hyperparams['max_norm'] == 0.0, 'Max norm should end at 0.0'\n```\n\n----------------------------------------\n\nTITLE: Training Loop Implementation - Python\nDESCRIPTION: Implements the main training loop for the neural network, updating parameters using computed gradients.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/flax_example.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nloss_history = []\n\nfor _ in range(NUM_STEPS):\n  loss_val, grads = loss_grad_fn(params)\n  loss_history.append(loss_val)\n  updates, opt_state = tx.update(grads, opt_state)\n  params = optax.apply_updates(params, updates)\n```\n\n----------------------------------------\n\nTITLE: Initializing SAM with SGD Base Optimizer\nDESCRIPTION: Sets up a SAM optimizer using SGD as the base optimizer with specified learning rate and rho parameters. Includes configuration for both SAM and baseline SGD optimizers.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nlr = 0.01\nrho = 0.1\nopt = optax.sgd(lr)\nadv_opt = optax.chain(contrib.normalize(), optax.sgd(rho))\nsam_opt = contrib.sam(opt, adv_opt, sync_period=2, opaque_mode=True)\n\nsgd_opt = optax.sgd(lr)  # Baseline SGD optimizer\n```\n\n----------------------------------------\n\nTITLE: Initializing AdamW Optimizer\nDESCRIPTION: Creates and initializes the AdamW optimizer with specified learning rate.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/nanolm.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nopt = optax.adamw(learning_rate=LEARNING_RATE)\n\nopt_state = opt.init(var_params)\n```\n\n----------------------------------------\n\nTITLE: Creating Momentum-based SAM Optimizer\nDESCRIPTION: Defines a function to create a SAM optimizer using momentum SGD as the base optimizer, with configurable learning rate, momentum, and rho parameters.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef sam_mom(lr=1e-3, momentum=0.1, rho=0.1, sync_period=2):\n  opt = optax.sgd(lr, momentum=momentum)\n  adv_opt = optax.chain(contrib.normalize(), optax.sgd(rho))\n  return contrib.sam(opt, adv_opt, sync_period=sync_period, opaque_mode=True)\n\nmom = 0.9\nsam_mom_opt = sam_mom(lr, momentum=mom)\nmom_opt = optax.sgd(lr, momentum=mom)\n```\n\n----------------------------------------\n\nTITLE: Custom Optimizer Chain Configuration\nDESCRIPTION: Creates a custom optimizer by chaining multiple gradient transforms including exponential decay, Adam, and gradient clipping\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Exponential decay of the learning rate.\nscheduler = optax.exponential_decay(\n    init_value=start_learning_rate,\n    transition_steps=1000,\n    decay_rate=0.99)\n\n# Combining gradient transforms using `optax.chain`.\ngradient_transform = optax.chain(\n    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n    optax.scale_by_adam(),  # Use the updates from adam.\n    optax.scale_by_schedule(scheduler),  # Use the learning rate from the scheduler.\n    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n    optax.scale(-1.0)\n)\n```\n\n----------------------------------------\n\nTITLE: Training Loop\nDESCRIPTION: Implements the main training loop with evaluation and privacy parameter tracking.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/differentially_private_sgd.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\naccuracy, loss, epsilon = [], [], []\n\nfor epoch in range(NUM_EPOCHS):\n  for batch in train_loader_batched.as_numpy_iterator():\n    params, opt_state = train_step(params, opt_state, batch)\n\n  # Evaluates test accuracy.\n  test_loss, test_acc = test_step(params, test_batch)\n  accuracy.append(test_acc)\n  loss.append(test_loss)\n  print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, test accuracy: {test_acc}\")\n\n  #\n  if DPSGD:\n    steps = (1 + epoch) * NUM_EXAMPLES // BATCH_SIZE\n    eps = compute_epsilon(steps)\n    epsilon.append(eps)\n```\n\n----------------------------------------\n\nTITLE: Configuring SAM Optimizer with SGD in Python\nDESCRIPTION: Sets up a Sharpness-Aware Minimization (SAM) optimizer using SGD for both the main and adversarial optimizers. It demonstrates how to create a drop-in SAM optimizer and a baseline SGD optimizer for comparison.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlr = 0.01\nrho = 0.1\nopt = optax.sgd(lr)\nadv_opt = optax.chain(contrib.normalize(), optax.sgd(rho))\nsam_opt = contrib.sam(opt, adv_opt, sync_period=2)  # This is the drop-in SAM optimizer.\n\nsgd_opt = optax.sgd(lr)  # Baseline SGD optimizer\n```\n\n----------------------------------------\n\nTITLE: Creating Adam-based SAM Optimizer\nDESCRIPTION: Implements a SAM optimizer using Adam as the base optimizer, allowing configuration of learning rate, beta parameters, and rho value.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef sam_adam(lr=1e-3, b1=0.9, b2=0.999, rho=0.03, sync_period=5):\n  \"\"\"A SAM optimizer using Adam for the outer optimizer.\"\"\"\n  opt = optax.adam(lr, b1=b1, b2=b2)\n  adv_opt = optax.chain(contrib.normalize(), optax.adam(rho))\n  return contrib.sam(opt, adv_opt, sync_period=sync_period, opaque_mode=True)\n\nsam_adam_opt = sam_adam(lr)\nadam_opt = optax.adam(lr)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Loss Function for the Simplex Optimization\nDESCRIPTION: Defines a squared error loss function between the perturbed argmax output and a target distribution. This demonstrates how the perturbed optimizer can be used in a differentiable loss function.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/perturbations.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef loss_simplex(values, rng):\n  n = values.shape[0]\n  v_true = jnp.arange(n) + 2\n  y_true = v_true / jnp.sum(v_true)\n  y_pred = pert_one_hot(values, rng)\n  return jnp.sum((y_true - y_pred) ** 2)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking and Visualizing Optimization in Python\nDESCRIPTION: This snippet benchmarks optimizer performance by varying memory sizes of the L-BFGS algorithm and visualizes resulting metrics. It depends on Matplotlib for plotting and uses Python's copy for duplicating dictionaries. Input consists of default and adapted hyperparameters based on specified memory sizes, and it outputs plots of final values, gradient norms, number of function calls, and run times.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lbfgs.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nimport copy\nimport matplotlib.pyplot as plt\n\ndefault_lbfgs_hparams = {'memory_size': 15, 'scale_init_precond': True}\ndefault_linesearch_hparams = {\n    'max_linesearch_steps': 15,\n    'initial_guess_strategy': 'one'\n}\n\nmemory_sizes = [int(2**i) for i in range(7)]\ntimes = []\ncalls = []\nvalues = []\ngrad_norms = []\nfor m in memory_sizes:\n  lbfgs_hparams = copy.deepcopy(default_lbfgs_hparams)\n  lbfgs_hparams['memory_size'] = m\n  v, g, n, t = test_hparams(lbfgs_hparams, default_linesearch_hparams, dimension=1024)\n  values.append(v)\n  grad_norms.append(g)\n  calls.append(n)\n  times.append(t)\n\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\naxs[0].plot(memory_sizes, values)\naxs[0].set_ylabel('Final values')\naxs[0].set_yscale('log')\naxs[1].plot(memory_sizes, grad_norms)\naxs[1].set_ylabel('Final gradient norms')\naxs[1].set_yscale('log')\naxs[2].plot(memory_sizes, calls)\naxs[2].set_ylabel('Number of function calls')\naxs[3].plot(memory_sizes, times)\naxs[3].set_ylabel('Run times')\nfor i in range(4):\n  axs[i].set_xlabel('Memory size')\nplt.tight_layout()\n\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running Adversarial Training Loop\nDESCRIPTION: Sets up an Adam optimizer and runs the main training loop while tracking clean and adversarial accuracy metrics for both train and test sets. Uses Optax for optimization and includes progress logging.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/adversarial_training.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the optimizer.\noptimizer = optax.adam(LEARNING_RATE)\nopt_state = optimizer.init(var_params)\n\nstart = datetime.datetime.now().replace(microsecond=0)\n\naccuracy_train = []\naccuracy_test = []\nadversarial_accuracy_train = []\nadversarial_accuracy_test = []\nfor epoch in range(EPOCHS):\n  for train_batch in train_loader_batched.as_numpy_iterator():\n    var_params, opt_state = train_step(var_params, opt_state, train_batch)\n\n  # compute train set accuracy, both on clean and adversarial images\n  train_stats = dataset_stats(var_params, train_loader_batched, iter_per_epoch_train)\n  accuracy_train.append(train_stats[\"accuracy\"])\n  adversarial_accuracy_train.append(train_stats[\"adversarial accuracy\"])\n\n  # compute test set accuracy, both on clean and adversarial images\n  test_stats = dataset_stats(var_params, test_loader_batched, iter_per_epoch_test)\n  accuracy_test.append(test_stats[\"accuracy\"])\n  adversarial_accuracy_test.append(test_stats[\"adversarial accuracy\"])\n\n  time_elapsed = (datetime.datetime.now().replace(microsecond=0) - start)\n  print(f\"Epoch {epoch} out of {EPOCHS}\")\n  print(f\"Accuracy on train set: {accuracy_train[-1]:.3f}\")\n  print(f\"Accuracy on test set: {accuracy_test[-1]:.3f}\")\n  print(f\"Adversarial accuracy on train set: {adversarial_accuracy_train[-1]:.3f}\")\n  print(f\"Adversarial accuracy on test set: {adversarial_accuracy_test[-1]:.3f}\")\n  print(f\"Time elapsed: {time_elapsed}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Optimizing L-BFGS with Linesearch and Gradient Caching in Python\nDESCRIPTION: This code demonstrates an optimized implementation of L-BFGS with linesearch in Optax, utilizing gradient caching to avoid redundant computations.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lbfgs.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Objective\ndef fun(w):\n  return jnp.sum(jnp.abs(w))\n\n\n# Linesearch\nlinesearch = optax.scale_by_backtracking_linesearch(\n    max_backtracking_steps=15, store_grad=True\n)\n# linesearch = optax.scale_by_zoom_linesearch(max_linesearch_steps=15)\n\n# Optimizer\nopt = optax.chain(optax.sgd(learning_rate=1.0), linesearch)\n\n# Initialize\nw = jrd.normal(jrd.PRNGKey(0), (8,))\nstate = opt.init(w)\n\n# Run optimization\nfor _ in range(16):\n  # Replace `v, g = jax.value_and_grad(fun)(w)` by\n  v, g = optax.value_and_grad_from_state(fun)(w, state=state)\n  u, state = opt.update(g, state, w, value=v, grad=g, value_fn=fun)\n  w = w + u\n\nprint(f'Final value: {fun(w):.2e}')\n```\n\n----------------------------------------\n\nTITLE: Implementing L-BFGS with Linesearch as a Solver in Python\nDESCRIPTION: This snippet shows how to use L-BFGS as a solver with linesearch techniques in Optax. It defines an objective function, sets up the linesearch and optimizer, and runs the optimization process.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lbfgs.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Objective\ndef fun(w):\n  return jnp.sum(jnp.abs(w))\n\n\n# Linesearch, comment/uncomment the desired one\nlinesearch = optax.scale_by_backtracking_linesearch(max_backtracking_steps=15)\n# linesearch = optax.scale_by_zoom_linesearch(max_linesearch_steps=15)\n\n# Optimizer\nopt = optax.chain(\n    optax.sgd(learning_rate=1.0),\n    # Compare with or without linesearch by commenting this line\n    linesearch,\n)\n\n# Initialize\nw = jrd.normal(jrd.PRNGKey(0), (8,))\nstate = opt.init(w)\n\n# Run optimization\nfor i in range(16):\n  v, g = jax.value_and_grad(fun)(w)\n  print(f'Iteration: {i}, Value:{v:.2e}')\n  u, state = opt.update(g, state, w, value=v, grad=g, value_fn=fun)\n  w = w + u\n\nprint(f'Final value: {fun(w):.2e}')\n```\n\n----------------------------------------\n\nTITLE: Creating a Perturbed Version of the One-Hot Function\nDESCRIPTION: Uses the perturbation module to create a differentiable approximation of the argmax_one_hot function by adding Gumbel noise. The function samples multiple noisy versions and averages the results to create a smooth approximation.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/perturbations.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nN_SAMPLES = 100\nSIGMA = 0.5\nGUMBEL = perturbations.Gumbel()\n\nrng = jax.random.PRNGKey(1)\npert_one_hot = perturbations.make_perturbed_fun(fun=argmax_one_hot,\n                                                num_samples=N_SAMPLES,\n                                                sigma=SIGMA,\n                                                noise=GUMBEL)\n```\n\n----------------------------------------\n\nTITLE: Executing Optimization and Visualizing Results\nDESCRIPTION: Runs the optimization process using different algorithms (SGD, Optimistic GD, Adam, Optimistic Adam) and plots the results to compare their performance.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/ogda_example.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n_, ax_params = plt.subplots()\n_, ax_distances = plt.subplots()\n\nparams = jnp.array([1.0, 1.0])\nax_params.scatter(*params, label=\"start\", color=\"black\")\n\nfor label, optimizer in [\n    (\"SGD\", optax.sgd(0.1)),\n    (\"Optimistic GD\", optax.optimistic_gradient_descent(0.1)),\n    (\"Adam\", optax.adam(0.05, nesterov=True)),\n    (\"Optimistic Adam\", optax.optimistic_adam(0.05, 0.5, nesterov=True)),\n]:\n    params_hist = optimize(optimizer, params, 10**4)\n    distances_to_origin = jnp.hypot(*params_hist.T)\n    ax_params.plot(*params_hist.T, label=label, lw=1)\n    ax_distances.plot(distances_to_origin, label=label, lw=1)\n\nax_params.legend()\nax_distances.legend()\nax_params.set(title=\"parameters\", aspect=\"equal\", xlim=(-3, 3), ylim=(-3, 3))\nax_distances.set(xlabel=\"iteration\", ylabel=\"distance to origin\", yscale=\"log\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Training with Gradient Accumulation\nDESCRIPTION: Implements gradient accumulation over three steps using optax.MultiSteps to achieve equivalent results to single batch training.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/gradient_accumulation.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnew_params_gradient_accumulation = fit(\n    optax.MultiSteps(optimizer, every_k_schedule=3),\n    params,\n    batches=[\n        dict(image=EXAMPLES[0:3], label=LABELS[0:3]),\n        dict(image=EXAMPLES[3:6], label=LABELS[3:6]),\n        dict(image=EXAMPLES[6:9], label=LABELS[6:9]),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Projecting Parameters to Non-negative Orthant using Optax\nDESCRIPTION: Example demonstrating how to use Optax to perform gradient descent with Adam optimizer while projecting parameters onto the non-negative orthant. The code shows initialization of parameters, computing gradients, updating parameters, and applying the non-negative projection.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/api/projections.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport optax\nimport jax\nimport jax.numpy as jnp\nnum_weights = 2\nxs = jnp.array([[-1.8, 2.2], [-2.0, 1.2]])\nys = jnp.array([0.5, 0.8])\noptimizer = optax.adam(learning_rate=1e-3)\nparams = {'w': jnp.zeros(num_weights)}\nopt_state = optimizer.init(params)\nloss = lambda params, x, y: jnp.mean((params['w'].dot(x) - y) ** 2)\ngrads = jax.grad(loss)(params, xs, ys)\nupdates, opt_state = optimizer.update(grads, opt_state)\nparams = optax.apply_updates(params, updates)\nparams = optax.projections.projection_non_negative(params)\n```\n\n----------------------------------------\n\nTITLE: Configuring Reduce-on-Plateau Learning Rate Scheduler with Optax in Python\nDESCRIPTION: This snippet demonstrates how to set up the contribute.reduce_on_plateau learning rate scheduler, which automatically reduces the learning rate scale after a plateau in monitored metric improvements (e.g., test loss). Dependencies are Optax (with contrib module), and all hyperparameters (patience, cooldown, etc.) must be set. It initializes the scheduler's transformation and state with the initial parameters, and the initial scale defaults to 1.0 if not explicitly provided.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntransform = contrib.reduce_on_plateau(\n    patience=PATIENCE,\n    cooldown=COOLDOWN,\n    factor=FACTOR,\n    rtol=RTOL,\n    accumulation_size=ACCUMULATION_SIZE\n    )\n\n# Creates initial state for `contrib.reduce_on_plateau` transformation.\ntransform_state = transform.init(init_params)\ntransform_state\n```\n\n----------------------------------------\n\nTITLE: Implementing Inner Optimization Step\nDESCRIPTION: Defines the loss function and a single step of gradient descent for the inner loop.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/meta_learning.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef loss(theta, x, y):\n  return optax.l2_loss(y, f(theta, x))\n\n\ndef step(theta, state, x, y):\n  grad = jax.grad(loss)(theta, x, y)\n  updates, state = opt.update(grad, state)\n  theta = optax.apply_updates(theta, updates)\n  return theta, state\n```\n\n----------------------------------------\n\nTITLE: Creating Optimization Step Function\nDESCRIPTION: Implements a generic step function that handles both SAM and regular optimizer updates, using JAX's automatic differentiation for gradient computation.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef make_step(opt):\n  @jax.jit\n  def step(store):\n    value, grads = jax.value_and_grad(loss)(store.params)\n    if isinstance(store.state, contrib.SAMState):\n      updates, state = opt.update(\n          grads, store.state, store.params,\n          grad_fn=jax.grad(lambda p, _: loss(p)))  # NOTICE THE ADDITIONAL grad_fn ARGUMENT!\n    else:\n      updates, state = opt.update(grads, store.state, store.params)\n    params = optax.apply_updates(store.params, updates)\n    return store.replace(\n        params=params,\n        state=state,\n        step=store.step+1), value\n  return step\n```\n\n----------------------------------------\n\nTITLE: Training Loop with Adam and Reduce-on-Plateau Scheduler in Optax (Python)\nDESCRIPTION: This snippet runs the core training loop: it defines a compiled train_step function that computes the loss and gradient, applies Adam optimizer updates, and then scales the updates by the learning rate factor from the scheduler. After each epoch, it evaluates model performance, updates the learning rate scale based on test loss using the scheduler, and collects statistics for plotting. Requires JAX, Optax, contrib.reduce_on_plateau, and supporting dataset utilities, as well as initialized optimizer, scheduler, batches, and metric computation functions.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef train_step(params, solver_state, transform_state, batch):\n  \"\"\"Performs a one step update.\"\"\"\n  (loss, aux), grad = jax.value_and_grad(loss_accuracy, has_aux=True)(\n      params, batch\n  )\n  # Computes updates scaled by the learning rate that was used to initialize\n  # the `solver`.\n  updates, solver_state = solver.update(grad, solver_state, params)\n  # Scales updates, produced by `solver`, by the scaling value.\n  updates = otu.tree_scale(transform_state.scale, updates)\n  params = optax.apply_updates(params, updates)\n  return params, solver_state, loss, aux\n\nparams = init_params\n\n# Computes metrics at initialization.\ntrain_stats = dataset_stats(params, test_loader_batched)\ntrain_accuracy = [train_stats[\"accuracy\"]]\ntrain_losses = [train_stats['loss']]\n\ntest_stats = dataset_stats(params, test_loader_batched)\ntest_accuracy = [test_stats[\"accuracy\"]]\ntest_losses = [test_stats[\"loss\"]]\n\nparams = init_params\nlr_scale_history = [transform_state.scale]\nfor epoch in range(N_EPOCHS):\n  train_accuracy_epoch = []\n  train_losses_epoch = []\n\n  for train_batch in train_loader_batched.as_numpy_iterator():\n    params, solver_state, train_loss, train_aux = train_step(\n        params, solver_state, transform_state, train_batch\n    )\n    train_accuracy_epoch.append(train_aux[\"accuracy\"])\n    train_losses_epoch.append(train_loss)\n\n  mean_train_accuracy = np.mean(train_accuracy_epoch)\n  mean_train_loss = np.mean(train_losses_epoch)\n\n  # Adjusts the learning rate scaling value using the loss computed on the\n  # test set.\n  _, transform_state = transform.update(\n      updates=params, state=transform_state, value=test_stats[\"loss\"]\n  )\n  lr_scale_history.append(transform_state.scale)\n\n  train_accuracy.append(mean_train_accuracy)\n  train_losses.append(mean_train_loss)\n\n  test_stats = dataset_stats(params, test_loader_batched)\n  test_accuracy.append(test_stats[\"accuracy\"])\n  test_losses.append(test_stats[\"loss\"])\n\n  test_stats = dataset_stats(params, test_loader_batched)\n  test_accuracy.append(test_stats[\"accuracy\"])\n  test_losses.append(test_stats[\"loss\"])\n\n  print(\n      f\"Epoch {epoch + 1}/{N_EPOCHS}, mean train accuracy:\"\n      f\" {mean_train_accuracy}, lr scale: {transform_state.scale}\"\n  )\n```\n\n----------------------------------------\n\nTITLE: Creating Neural Network Model - Python\nDESCRIPTION: Defines and initializes a single-layer neural network using Flax's Dense layer.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/flax_example.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = nn.Dense(features=Y_DIM)\nparams = model.init(params_rng, jnp.ones((X_DIM,), dtype=jnp.float32))\n```\n\n----------------------------------------\n\nTITLE: Implementing MLP Network and Loss Function\nDESCRIPTION: Defines a simple multilayer perceptron model using Flax and implements a cross-entropy loss function for classification tasks.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/gradient_accumulation.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nn.Module):\n  \"\"\"A simple multilayer perceptron model.\"\"\"\n\n  @nn.compact\n  def __call__(self, x):\n    # Flattens inputs in the batch.\n    x = x.reshape((x.shape[0], -1))\n    x = nn.Dense(features=512)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=512)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=10)(x)\n    return x\n\nnet = MLP()\n\ndef loss_fn(params, batch):\n  \"\"\"Computes loss over a mini-batch.\n  \"\"\"\n  logits = net.apply(params, batch['image'])\n  loss = optax.softmax_cross_entropy_with_integer_labels(\n      logits=logits, labels=batch['label']\n  ).mean()\n  return loss\n```\n\n----------------------------------------\n\nTITLE: Implementing SAM with Momentum Optimizer in Python\nDESCRIPTION: Defines a function to create a SAM optimizer using SGD with momentum for the outer optimizer and SGD for the adversarial optimizer. It also sets up baseline momentum and SAM momentum optimizers for comparison.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef sam_mom(lr=1e-3, momentum=0.1, rho=0.1, sync_period=2):\n  opt = optax.sgd(lr, momentum=momentum)\n  adv_opt = optax.chain(contrib.normalize(), optax.sgd(rho))\n  return contrib.sam(opt, adv_opt, sync_period=sync_period)\n\nmom = 0.9\nsam_mom_opt = sam_mom(lr, momentum=mom)\nmom_opt = optax.sgd(lr, momentum=mom)\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment for ResNet Training with JAX, Flax, and Optax\nDESCRIPTION: Imports necessary libraries and sets up the environment for neural network training. This includes imports from JAX, Flax, Optax, TensorFlow, and visualization libraries. It also hides GPU from TensorFlow to prevent memory reservation conflicts and displays the JAX execution platform.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/cifar10_resnet.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nfrom collections.abc import Callable\nfrom typing import Any, Sequence, Tuple, Optional, Dict\n\nfrom flax import linen as nn\n\nimport jax\nimport jax.numpy as jnp\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimport optax\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom functools import partial\n\n# hide the GPU from tensorflow, otherwise it might\n# reserve memory on it\ntf.config.experimental.set_visible_devices([], \"GPU\")\n\n# Show on which platform JAX is running.\nprint(\"JAX running on\", jax.devices()[0].platform.upper())\n```\n\n----------------------------------------\n\nTITLE: Configuring Reduce on Plateau Scheduler for Test Loss\nDESCRIPTION: Sets up parameters for the reduce on plateau scheduler based on test loss, demonstrating an alternative approach to learning rate reduction.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# @markdown Number of epochs with no improvement after which learning rate will be reduced:\nPATIENCE = 5  # @param{type:\"integer\"}\n# @markdown Number of epochs to wait before resuming normal operation after the learning rate reduction:\nCOOLDOWN = 0  # @param{type:\"integer\"}\n# @markdown Factor by which to reduce the learning rate:\nFACTOR = 0.5  # @param{type:\"number\"}\n# @markdown Relative tolerance for measuring the new optimum:\nRTOL = 1e-4  # @param{type:\"number\"}\n```\n\n----------------------------------------\n\nTITLE: Flattening Gradient Transformations in Optax\nDESCRIPTION: Demonstrates using the flatten wrapper to optimize memory usage by combining small variables into a single vector.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmy_optimizer = optax.flatten(optax.adam(learning_rate))\n```\n\n----------------------------------------\n\nTITLE: CNN Model Definition using Flax\nDESCRIPTION: Defines a simple CNN architecture using Flax's nn module with convolutional layers, pooling, and dense layers for MNIST classification.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lookahead_mnist.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass CNN(nn.Module):\n  \"\"\"A simple CNN model.\"\"\"\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.Dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=10)(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Combining Schedules with Transformations in Optax\nDESCRIPTION: Example of integrating schedules with other transformations including gradient clipping and Adam optimization.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nschedule_fn = optax.polynomial_schedule(\n    init_value=-learning_rate, end_value=0., power=1, transition_steps=5)\noptimizer = optax.chain(\n    optax.clip_by_global_norm(max_norm),\n    optax.scale_by_adam(eps=1e-4),\n    optax.scale_by_schedule(schedule_fn))\n```\n\n----------------------------------------\n\nTITLE: Extending Perturbed Functions to JAX Pytrees\nDESCRIPTION: Demonstrates how to apply the perturbation method to functions operating on nested JAX pytree structures. This shows the flexibility of the approach for complex data structures.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/perturbations.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nargmax_tree = lambda x: jtu.tree_map(argmax_one_hot, x)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for MLP MNIST in Python\nDESCRIPTION: Imports the necessary libraries for the MNIST classification task, including Flax for neural network layers, JAX for numerical computation, Optax for optimization, and TensorFlow/TensorFlow Datasets for data loading.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/mlp_mnist.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Sequence\n\nfrom flax import linen as nn\nimport jax\nimport jax.numpy as jnp\nimport optax\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n```\n\n----------------------------------------\n\nTITLE: Defining MLP Model using Flax for Fashion MNIST Classification\nDESCRIPTION: Implements a simple Multilayer Perceptron (MLP) model using Flax for classifying Fashion MNIST images.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nn.Module):\n  \"\"\"A simple multilayer perceptron model for image classification.\"\"\"\n  hidden_sizes: Sequence[int] = (1000, 1000)\n\n  @nn.compact\n  def __call__(self, x):\n    # Flattens images in the batch.\n    x = x.reshape((x.shape[0], -1))\n    x = nn.Dense(features=self.hidden_sizes[0])(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=self.hidden_sizes[1])(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=NUM_CLASSES)(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Implementing AdamW Optimizer in Optax\nDESCRIPTION: Example implementation of AdamW optimizer using basic Optax components.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef adamw(learning_rate, b1, b2, eps, weight_decay):\n  return optax.chain(\n      optax.scale_by_adam(b1=b1, b2=b2, eps=eps),\n      optax.scale_and_decay(-learning_rate, weight_decay=weight_decay))\n```\n\n----------------------------------------\n\nTITLE: Defining NanoLM Transformer Model\nDESCRIPTION: Implements the NanoLM model as a Flax Linen module. The model uses a Transformer architecture with multi-head attention and feedforward layers. Includes a generate method for text generation.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/nanolm.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass NanoLM(nn.Module):\n  \"\"\"NanoLM model.\"\"\"\n  vocab_size: int\n  num_layers: int = 6\n  num_heads: int = 8\n  head_size: int = 32\n  dropout_rate: float = 0.2\n  embed_size: int = 256\n  block_size: int = 64\n\n  @nn.compact\n  def __call__(self, x, training: bool):\n    seq_len = x.shape[1]\n\n    x = nn.Embed(self.vocab_size, self.embed_size)(x) + nn.Embed(\n        self.block_size, self.embed_size\n    )(jnp.arange(seq_len))\n    for _ in range(self.num_layers):\n      x_norm = nn.LayerNorm()(x)\n      x = x + nn.MultiHeadDotProductAttention(\n          num_heads=self.num_heads,\n          qkv_features=self.head_size,\n          out_features=self.head_size * self.num_heads,\n          dropout_rate=self.dropout_rate,\n      )(\n          x_norm,\n          x_norm,\n          mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),\n          deterministic=not training,\n      )\n\n      x = x + nn.Sequential([\n          nn.Dense(4 * self.embed_size),\n          nn.relu,\n          nn.Dropout(self.dropout_rate, deterministic=not training),\n          nn.Dense(self.embed_size),\n      ])(nn.LayerNorm()(x))\n\n    x = nn.LayerNorm()(x)\n    return nn.Dense(self.vocab_size)(x)\n\n  @functools.partial(jax.jit, static_argnames=(\"self\", \"length\"))\n  def generate(self, rng, params, length):\n    def _scan_generate(carry, _):\n      random_key, context = carry\n      logits = self.apply(params, context, training=False)\n      rng, rng_subkey = jax.random.split(random_key)\n      new_token = jax.random.categorical(\n          rng_subkey, logits[:, -1, :], axis=-1, shape=(1, 1)\n      )\n      context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n      return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n        _scan_generate,\n        (rng, jnp.zeros((1, self.block_size), dtype=jnp.int32)),\n        (),\n        length=length,\n    )\n    return new_tokens\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Gradient Accumulation\nDESCRIPTION: Sets up necessary imports for implementing gradient accumulation, including Flax for neural networks, JAX for computations, and Optax for optimization.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/gradient_accumulation.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Iterable\n\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nimport optax\nimport chex\n```\n\n----------------------------------------\n\nTITLE: Loss and Evaluation Functions\nDESCRIPTION: Defines JIT-compiled functions for computing loss and evaluating model performance on test data.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/differentially_private_sgd.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef loss_fn(params, batch):\n  images, labels = batch\n  logits = predict(params, images)\n  return losses.softmax_cross_entropy_with_integer_labels(logits, labels).mean(), logits\n\n\n@jax.jit\ndef test_step(params, batch):\n  images, labels = batch\n  logits = predict(params, images)\n  loss = losses.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n  accuracy = (logits.argmax(1) == labels).mean()\n  return loss, accuracy * 100\n```\n\n----------------------------------------\n\nTITLE: Learning Rate Schedule Implementation\nDESCRIPTION: Demonstrates gradient accumulation with a piecewise constant learning rate schedule, showing how MultiSteps interacts with learning rate updates.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/gradient_accumulation.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlearning_rate_schedule = optax.piecewise_constant_schedule(\n    init_value=1.0,\n    boundaries_and_scales={\n        0: 1e-4,\n        1: 1e-1,\n    },\n)\n\noptimizer = optax.sgd(learning_rate_schedule)\n\nnew_params_single_batch = fit(\n    optimizer,\n    params,\n    batches=[\n        dict(image=EXAMPLES, label=LABELS),\n    ],\n)\n\nnew_params_gradient_accumulation = fit(\n    optax.MultiSteps(optimizer, every_k_schedule=3),\n    params,\n    batches=[\n        dict(image=EXAMPLES[0:3], label=LABELS[0:3]),\n        dict(image=EXAMPLES[3:6], label=LABELS[3:6]),\n        dict(image=EXAMPLES[6:9], label=LABELS[6:9]),\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: CNN Model Definition\nDESCRIPTION: Implements a simple CNN model using Flax's linen module with two convolutional layers, pooling layers, and dense layers.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/adversarial_training.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass CNN(nn.Module):\n  \"\"\"A simple CNN model.\"\"\"\n  num_classes: int\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.Dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=self.num_classes)(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Testing L-BFGS with Rosenbrock Function\nDESCRIPTION: Example implementation showing how to test the L-BFGS solver using the Rosenbrock function. Demonstrates initialization, optimization, and result printing.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lbfgs.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef fun(w):\n  return jnp.sum(100.0 * (w[1:] - w[:-1] ** 2) ** 2 + (1.0 - w[:-1]) ** 2)\n\nopt = optax.lbfgs()\ninit_params = jnp.zeros((8,))\nprint(\n    f'Initial value: {fun(init_params):.2e} '\n    f'Initial gradient norm: {otu.tree_l2_norm(jax.grad(fun)(init_params)):.2e}'\n)\nfinal_params, _ = run_opt(init_params, fun, opt, max_iter=100, tol=1e-3)\nprint(\n    f'Final value: {fun(final_params):.2e}, '\n    f'Final gradient norm: {otu.tree_l2_norm(jax.grad(fun)(final_params)):.2e}'\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Trained Model\nDESCRIPTION: Uses the trained model to generate new text in the style of the training data.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/nanolm.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nkey, subkey = jax.random.split(key)\ntext = model.generate(key, var_params, 1000)[:, 0, 0].tolist()\nprint(decode(text))\n```\n\n----------------------------------------\n\nTITLE: Using Loss Functions in Optax\nDESCRIPTION: Examples of using built-in loss functions and performing batch reduction operations.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\npredictions = net(TRAINING_DATA, params)\nloss = optax.huber_loss(predictions, LABELS)\n\navg_loss = jnp.mean(optax.huber_loss(predictions, LABELS))\nsum_loss = jnp.sum(optax.huber_loss(predictions, LABELS))\n```\n\n----------------------------------------\n\nTITLE: Initializing Optimization Parameters and States in Python\nDESCRIPTION: Sets up initial parameters and optimization states for all the optimizers (SAM, SGD, SAM Mom, Mom, SAM Adam, Adam) using a custom Store dataclass.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nparams = np.array([-0.4, -0.4])\n\n@chex.dataclass\nclass Store:\n  params: chex.Array\n  state: optax.OptState\n  step: int = 0\n\nsam_store = Store(params=params, state=sam_opt.init(params))\nsgd_store = Store(params=params, state=sgd_opt.init(params))\nsam_mom_store = Store(params=params, state=sam_mom_opt.init(params))\nmom_store = Store(params=params, state=mom_opt.init(params))\nsam_adam_store = Store(params=params, state=sam_adam_opt.init(params))\nadam_store = Store(params=params, state=adam_opt.init(params))\n```\n\n----------------------------------------\n\nTITLE: Visualizing Adversarial Examples\nDESCRIPTION: Creates a visualization comparing clean images, their adversarial versions, and the difference between them scaled by epsilon.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/adversarial_training.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n_, axes = plt.subplots(nrows=1, ncols=3, figsize=(6 * 3, 6))\n\naxes[0].set_title(\"Clean image \\n Prediction %s\" % int(pred_clean))\naxes[0].imshow(img_clean, cmap=plt.cm.get_cmap(\"Greys\"), vmax=1, vmin=0)\naxes[1].set_title(\"Adversarial image \\n Prediction %s\" % prediction_adversarial)\naxes[1].imshow(img_adversarial, cmap=plt.cm.get_cmap(\"Greys\"), vmax=1, vmin=0)\naxes[2].set_title(r\"|Adversarial - clean| $\\times$ %.0f\" % (1 / EPSILON))\naxes[2].imshow(\n    jnp.abs(img_clean - img_adversarial) / EPSILON,\n    cmap=plt.cm.get_cmap(\"Greys\"),\n    vmax=1,\n    vmin=0,\n)\nfor i in range(3):\n  axes[i].set_xticks(())\n  axes[i].set_yticks(())\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Implementing Update and Optimization Functions\nDESCRIPTION: Defines helper functions for updating parameters and running the optimization process using JAX's functional programming style.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/ogda_example.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef update(optimizer, state, _):\n    params, opt_state = state\n    grads = jax.grad(f)(params)\n    grads = grads.at[1].apply(jnp.negative)\n    updates, new_opt_state = optimizer.update(grads, opt_state, params)\n    new_params = optax.apply_updates(params, updates)\n    return (new_params, new_opt_state), params\n\ndef optimize(optimizer, params, iters):\n    opt_state = optimizer.init(params)\n    _, params_hist = lax.scan(functools.partial(update, optimizer), (params, opt_state), length=iters)\n    return params_hist\n```\n\n----------------------------------------\n\nTITLE: Using Schedule as Learning Rate in Optax\nDESCRIPTION: Shows how to use a schedule function directly as a learning rate parameter in Adam optimizer.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\noptimizer = optax.adam(learning_rate=schedule_fn)\n```\n\n----------------------------------------\n\nTITLE: Creating a Perturbed Version of the Ranking Function\nDESCRIPTION: Creates a differentiable approximation of the ranking function using the perturbation method with Gumbel noise. This transforms the discrete ranking operation into a continuous, differentiable function.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/perturbations.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nN_SAMPLES = 100\nSIGMA = 0.2\nGUMBEL = perturbations.Gumbel()\n\npert_ranking = perturbations.make_perturbed_fun(ranking,\n                                                num_samples=N_SAMPLES,\n                                                sigma=SIGMA,\n                                                noise=GUMBEL)\n```\n\n----------------------------------------\n\nTITLE: Initializing Test Data and Parameters\nDESCRIPTION: Generates random test data and initializes model parameters for demonstration purposes using MNIST-like dimensions.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/gradient_accumulation.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nEXAMPLES = jax.random.uniform(jax.random.PRNGKey(0), (9, 28, 28, 1))\nLABELS = jax.random.randint(jax.random.PRNGKey(0), (9,), minval=0, maxval=10)\n\noptimizer = optax.sgd(1e-4)\nparams = net.init(jax.random.PRNGKey(0), EXAMPLES)\n```\n\n----------------------------------------\n\nTITLE: Defining the Bilinear Objective Function\nDESCRIPTION: Defines the simple bilinear function f(x,y) = x * y as the objective for the min-max problem.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/ogda_example.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef f(params):\n    x, y = params\n    return x * y\n```\n\n----------------------------------------\n\nTITLE: Initializing Optimizer States\nDESCRIPTION: Sets up initial parameters and optimizer states for all optimizer variants (SAM, SGD, Momentum, and Adam) using a custom Store class.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nparams = np.array([-0.4, -0.4])\n\n@chex.dataclass\nclass Store:\n  params: chex.Array\n  state: optax.OptState\n  step: int = 0\n\nsam_store = Store(params=params, state=sam_opt.init(params))\nsgd_store = Store(params=params, state=sgd_opt.init(params))\nsam_mom_store = Store(params=params, state=sam_mom_opt.init(params))\nmom_store = Store(params=params, state=mom_opt.init(params))\nsam_adam_store = Store(params=params, state=sam_adam_opt.init(params))\nadam_store = Store(params=params, state=adam_opt.init(params))\n```\n\n----------------------------------------\n\nTITLE: Debugging L-BFGS with Zakharov Function\nDESCRIPTION: Example demonstrating how to debug L-BFGS optimization issues using the Zakharov function and verbose line search options.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lbfgs.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef fun(w):\n  ii = jnp.arange(1, len(w) + 1, step=1, dtype=w.dtype)\n  sum1 = (w**2).sum()\n  sum2 = (0.5 * ii * w).sum()\n  return sum1 + sum2**2 + sum2**4\n\nopt = optax.lbfgs(scale_init_precond=False,\n  linesearch=optax.scale_by_zoom_linesearch(\n      max_linesearch_steps=50, verbose=True, initial_guess_strategy='one'\n  ))\n\ninit_params = jnp.array([600.0, 700.0, 200.0, 100.0, 90.0, 1e4])\nprint(\n    f'Initial value: {fun(init_params):.2e} '\n    f'Initial gradient norm: {otu.tree_l2_norm(jax.grad(fun)(init_params)):.2e}'\n)\nfinal_params, _ = run_opt(init_params, fun, opt, max_iter=100, tol=1e-3)\nprint(\n    f'Final value: {fun(final_params):.2e}, '\n    f'Final gradient norm: {otu.tree_l2_norm(jax.grad(fun)(final_params)):.2e}'\n)\n```\n\n----------------------------------------\n\nTITLE: Lookahead Optimizer Initialization\nDESCRIPTION: Initializes the Lookahead optimizer with fast and slow parameters, creates the solver state, and defines dataset statistics computation function.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lookahead_mnist.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfast_solver = optax.adam(FAST_LEARNING_RATE)\nsolver = optax.lookahead(fast_solver, SYNC_PERIOD, SLOW_LEARNING_RATE)\nrng = jax.random.PRNGKey(0)\ndummy_data = jnp.ones((1,) + IMG_SIZE, dtype=jnp.float32)\n\nparams = net.init({\"params\": rng}, dummy_data)[\"params\"]\nparams = optax.LookaheadParams.init_synced(params)\nsolver_state = solver.init(params)\n\ndef dataset_stats(params, data_loader):\n  all_accuracy = []\n  all_loss = []\n  for batch in data_loader.as_numpy_iterator():\n    batch_loss, batch_aux = loss_accuracy(params, batch)\n    all_loss.append(batch_loss)\n    all_accuracy.append(batch_aux[\"accuracy\"])\n  return {\"loss\": np.mean(all_loss), \"accuracy\": np.mean(all_accuracy)}\n```\n\n----------------------------------------\n\nTITLE: Initializing Optax Optimizers for Meta-Learning\nDESCRIPTION: Sets up the inner (RMSProp) and outer (Adam) optimizers with initial learning rates.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/meta_learning.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninit_learning_rate = jnp.array(0.1)\nmeta_learning_rate = jnp.array(0.03)\n\nopt = optax.inject_hyperparams(optax.rmsprop)(learning_rate=init_learning_rate)\nmeta_opt = optax.adam(learning_rate=meta_learning_rate)\n```\n\n----------------------------------------\n\nTITLE: MNIST Dataset Loading and Preprocessing\nDESCRIPTION: Loads and preprocesses the MNIST dataset using TensorFlow Datasets, including normalization and batching operations.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/differentially_private_sgd.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n(train_loader, test_loader), info = tfds.load(\n    \"mnist\", split=[\"train\", \"test\"], as_supervised=True, with_info=True\n)\n\nmin_max_rgb = lambda image, label: (tf.cast(image, tf.float32) / 255., label)\ntrain_loader = train_loader.map(min_max_rgb)\ntest_loader = test_loader.map(min_max_rgb)\n\ntrain_loader_batched = train_loader.shuffle(\n    buffer_size=10_000, reshuffle_each_iteration=True\n).batch(BATCH_SIZE, drop_remainder=True)\n\nNUM_EXAMPLES = info.splits[\"test\"].num_examples\ntest_batch = next(test_loader.batch(NUM_EXAMPLES, drop_remainder=True).as_numpy_iterator())\n```\n\n----------------------------------------\n\nTITLE: Final Accuracy Display\nDESCRIPTION: Prints the final test accuracy achieved by the model.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/differentially_private_sgd.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(f'Final accuracy: {accuracy[-1]}')\n```\n\n----------------------------------------\n\nTITLE: Initializing Model Parameters\nDESCRIPTION: Creates and initializes the model parameters using a random key and dummy input data.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/nanolm.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nkey = jax.random.PRNGKey(SEED)\nkey, subkey = jax.random.split(key)\n\nvar_params = model.init(\n    key,\n    jnp.ones((BATCH_SIZE, BLOCK_SIZE), dtype=jnp.int32),\n    training=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Training Hyperparameters for MNIST Classification\nDESCRIPTION: Defines key hyperparameters for training the model, including the learning rate for the optimizer, batch size for training, and the number of epochs to train for. These parameters control the optimization process.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/mlp_mnist.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# @markdown The learning rate for the optimizer:\nLEARNING_RATE = 0.002 # @param{type:\"number\"}\n# @markdown Number of samples in each batch:\nBATCH_SIZE = 128 # @param{type:\"integer\"}\n# @markdown Total number of epochs to train for:\nN_EPOCHS = 1 # @param{type:\"integer\"}\n```\n\n----------------------------------------\n\nTITLE: Data Preparation and Batch Generation\nDESCRIPTION: Creates a vocabulary from the dataset, defines encoding and decoding functions, and implements a function to generate random batches of data for training.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/nanolm.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nvocab = sorted(list(set(text_train)))\nprint(\"Vocabulary:, \", \"\".join(vocab))\nprint(\"Length of vocabulary: \", len(vocab))\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for i, ch in enumerate(vocab)}\nencode = lambda s: [\n    stoi[c] for c in s\n]  # encoder: take a string, output a list of integers\ndecode = lambda l: \"\".join(\n    [itos[i] for i in l]\n)  # decoder: take a list of integers, output a string\n\n# encode train and validation data\ntrain_data = jnp.array(encode(text_train))\neval_data = jnp.array(encode(text_validation))\n```\n\nLANGUAGE: python\nCODE:\n```\ndynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n\n@jax.jit\ndef get_batch(random_key, data):\n  \"\"\"Prepares a random batch of training data.\n\n  Args:\n      random_key: A random seed for sampling a batch.\n      data: The complete training dataset.\n\n  Returns:\n      x: Input sequences.\n      y: Target sequences (shifted inputs).\n  \"\"\"\n  ix = jax.random.randint(\n      random_key, shape=(BATCH_SIZE, 1), minval=0, maxval=len(data) - BLOCK_SIZE\n  )\n  x = dynamic_slice_vmap(data, ix, (BLOCK_SIZE,))\n  y = dynamic_slice_vmap(data, ix + 1, (BLOCK_SIZE,))\n  return x, y\n```\n\n----------------------------------------\n\nTITLE: Printing Final Test Accuracy in Python\nDESCRIPTION: Prints the final accuracy achieved on the test set after the completion of the training loop. It accesses the last element of the `test_accuracy` list, which stores the test accuracy recorded at the end of the final epoch.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/cifar10_resnet.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Finally, let's print the test set accuracy\nprint(\"Final accuracy on test set: \", test_accuracy[-1])\n```\n\n----------------------------------------\n\nTITLE: Implementing Loss and Accuracy Functions for MLP Model\nDESCRIPTION: Defines functions to compute loss and accuracy for the MLP model on Fashion MNIST data.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnet = MLP()\n\n@jax.jit\ndef loss_accuracy(params, data):\n  \"\"\"Computes loss and accuracy over a mini-batch.\n\n  Args:\n    params: parameters of the model.\n    data: tuple of (inputs, labels).\n\n  Returns:\n    loss: float\n  \"\"\"\n  inputs, labels = data\n  logits = net.apply({\"params\": params}, inputs)\n  loss = optax.softmax_cross_entropy_with_integer_labels(\n      logits=logits, labels=labels\n  ).mean()\n  accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n  return loss, {\"accuracy\": accuracy}\n\nrng = jax.random.PRNGKey(0)\nfake_data = jnp.ones((1,) + IMG_SIZE, dtype=jnp.float32)\ninit_params = net.init({\"params\": rng}, fake_data)[\"params\"]\n\n\ndef dataset_stats(params, data_loader):\n  \"\"\"Computes loss and accuracy over the dataset `data_loader`.\"\"\"\n  all_accuracy = []\n  all_loss = []\n  for batch in data_loader.as_numpy_iterator():\n    batch_loss, batch_aux = loss_accuracy(params, batch)\n    all_loss.append(batch_loss)\n    all_accuracy.append(batch_aux[\"accuracy\"])\n  return {\"loss\": np.mean(all_loss), \"accuracy\": np.mean(all_accuracy)}\n```\n\n----------------------------------------\n\nTITLE: Generating Training Data - Python\nDESCRIPTION: Creates synthetic training data with added noise for the neural network training.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/flax_example.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nw = jax.random.normal(w_rng, (X_DIM, Y_DIM))\nb = jax.random.normal(b_rng, (Y_DIM,))\nx_samples = jax.random.normal(samples_rng, (NUM_SAMPLES, X_DIM))\ny_samples = jnp.dot(x_samples, w) + b\ny_samples += 0.1 * jax.random.normal(noise_rng, (NUM_SAMPLES, Y_DIM))\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing CIFAR Dataset\nDESCRIPTION: Loads CIFAR dataset using tensorflow_datasets and implements a visualization function to display sample images from the dataset.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/cifar10_resnet.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n(train_loader, test_loader), info = tfds.load(\n    DATASET, split=[\"train\", \"test\"], as_supervised=True, with_info=True\n)\nNUM_CLASSES = info.features[\"label\"].num_classes\nIMG_SIZE = info.features[\"image\"].shape\n\ndef plot_sample_images(loader):\n  loader_iter = iter(loader)\n  _, axes = plt.subplots(nrows=4, ncols=5, figsize=(6, 4))\n  for i in range(4):\n    for j in range(5):\n      image, label = next(loader_iter)\n      axes[i, j].imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n      axes[i, j].set_axis_off()\n      axes[i, j].set_title(\n          info.features[\"label\"].names[label], fontsize=10, y=0.9\n      )\n\nplot_sample_images(train_loader)\n```\n\n----------------------------------------\n\nTITLE: Implementing a One-Hot Argmax Function\nDESCRIPTION: Defines a non-differentiable function that converts an input vector into a one-hot vector, placing a 1 at the position of the maximum value and 0 elsewhere. This represents the argmax optimizer for the unit simplex.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/perturbations.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef argmax_one_hot(x, axis=-1):\n  return jax.nn.one_hot(jnp.argmax(x, axis=axis), x.shape[axis])\n```\n\n----------------------------------------\n\nTITLE: Optimizer Initialization\nDESCRIPTION: Initializes either DP-SGD or vanilla SGD optimizer based on configuration, and initializes model parameters.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/differentially_private_sgd.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif DPSGD:\n  tx = contrib.dpsgd(\n      learning_rate=LEARNING_RATE, l2_norm_clip=L2_NORM_CLIP,\n      noise_multiplier=NOISE_MULTIPLIER, seed=1337)\nelse:\n  tx = optax.sgd(learning_rate=LEARNING_RATE)\n\n_, params = init_random_params(jax.random.PRNGKey(1337), (-1, 28, 28, 1))\nopt_state = tx.init(params)\n```\n\n----------------------------------------\n\nTITLE: Configuration Parameters\nDESCRIPTION: Defines training hyperparameters including epochs, batch sizes, learning rate, dataset choice, L2 regularization, and epsilon for adversarial perturbations.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/adversarial_training.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nEPOCHS = 10\nTRAIN_BATCH_SIZE = 128\nTEST_BATCH_SIZE = 128\nLEARNING_RATE = 0.001\nDATASET = \"mnist\"\nL2_REG = 0.0001\nEPSILON = 0.01\n```\n\n----------------------------------------\n\nTITLE: Data Loading and Processing\nDESCRIPTION: Loads and preprocesses the MNIST dataset using TensorFlow Datasets, creating batched data loaders for training and testing.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/adversarial_training.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n(train_loader, test_loader), mnist_info = tfds.load(\n    \"mnist\", split=[\"train\", \"test\"], as_supervised=True, with_info=True\n)\n\ntrain_loader_batched = train_loader.shuffle(\n    10 * TRAIN_BATCH_SIZE, seed=0\n).batch(TRAIN_BATCH_SIZE, drop_remainder=True)\ntest_loader_batched = test_loader.batch(TEST_BATCH_SIZE, drop_remainder=True)\n\ninput_shape = (1,) + mnist_info.features[\"image\"].shape\nnum_classes = mnist_info.features[\"label\"].num_classes\niter_per_epoch_train = (\n    mnist_info.splits[\"train\"].num_examples // TRAIN_BATCH_SIZE\n)\niter_per_epoch_test = mnist_info.splits[\"test\"].num_examples // TEST_BATCH_SIZE\n```\n\n----------------------------------------\n\nTITLE: Plotting Convergence Rates for Adam and AdeMAMix\nDESCRIPTION: This snippet creates a plot comparing the convergence rates of Adam and AdeMAMix optimizers, similar to Figure 2a from the AdeMAMix paper.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/rosenbrock_ademamix.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nN = num_iterations + 1\nfig, ax = plt.subplots()\nlns = ax.semilogy(\n    jnp.arange(N),\n    jnp.linalg.norm(\n        all_b1_params_array[0, :, :]\n        - jnp.ones(\n            2,\n        ),\n        axis=1,\n    ),\n    label=\"Adam b1 = 0.9\",\n)\nfor i, b1 in enumerate([0.99, 0.999, 0.9999]):\n    lns += ax.semilogy(\n        jnp.arange(N),\n        jnp.sqrt(\n            jnp.linalg.norm(\n                all_b1_params_array[i + 1, :, :]\n                - jnp.ones(\n                    2,\n                ),\n                axis=1,\n            )\n        ),\n        label=f\"Adam b1 = {b1}\",\n    )\nax1 = ax.twinx()\nfor i, b3 in enumerate([0.999, 0.9999]):\n    lns += ax1.semilogy(\n        jnp.arange(N),\n        jnp.sqrt(\n            jnp.linalg.norm(\n                all_ademamix_params_array[i, :, :]\n                - jnp.ones(\n                    2,\n                ),\n                axis=1,\n            )\n        ),\n        label=f\"AdeMAMix b3 = {b3}\",\n    )\nlabs = [l.get_label() for l in lns]\nax.legend(lns, labs, loc=0)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Hyperparameter Configuration\nDESCRIPTION: Defines training hyperparameters including learning rates for fast and slow optimizers, synchronization period, batch size, and number of epochs.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lookahead_mnist.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nFAST_LEARNING_RATE = 0.002\nSLOW_LEARNING_RATE = 0.1\nSYNC_PERIOD = 5\nBATCH_SIZE = 256\nN_EPOCHS = 1\n```\n\n----------------------------------------\n\nTITLE: Using Polynomial Schedule in Optax\nDESCRIPTION: Shows how to create and use a polynomial schedule for parameter decay over steps.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nschedule_fn = optax.polynomial_schedule(\n    init_value=1., end_value=0., power=1, transition_steps=5)\n\nfor step_count in range(6):\n  print(schedule_fn(step_count))  # [1., 0.8, 0.6, 0.4, 0.2, 0.]\n```\n\n----------------------------------------\n\nTITLE: Defining 2D Loss Function for Optimization Testing in Python\nDESCRIPTION: Creates a 2D loss function with two minima for testing the optimization algorithms. The function is visualized using a contour plot to show the loss surface.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef loss(params):\n  x, y = params\n  return -np.exp(-(x - 2)**2 - y**2) - 1.0*np.exp(-((x)**2 + (y)**2*100))\n\nx, y = np.meshgrid(np.linspace(0, 2, 100), np.linspace(0, 2, 100))\nl = loss((x, y))\nplt.matshow(l)\nplt.xticks([0, 50, 100], [0, 1, 2])\nplt.yticks([0, 50, 100], [0, 1, 2])\nplt.title('Loss Surface')\nplt.show();\n```\n\n----------------------------------------\n\nTITLE: Plotting Training Metrics for Learning Rate and Model Performance in Python\nDESCRIPTION: This snippet visualizes the collected training statistics such as learning rate scaling, training/test losses, and accuracy using a plot function. It assumes all required historical metrics have been collected during training. The plotting function is expected to visually summarize the evolution of learning rates and model performance over epochs. Dependencies are matplotlib or another suitable plotting library and the definition of the plot function.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nplot(lr_scale_history, train_losses, train_accuracy, test_losses, test_accuracy)\n```\n\n----------------------------------------\n\nTITLE: Solving Assignment Problem\nDESCRIPTION: Applies the Hungarian algorithm to solve the linear assignment problem\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/linear_assignment_problem.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsol_i, sol_j = optax.assignment.hungarian_algorithm(costs)\nprint(sol_i, sol_j)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Fashion MNIST Dataset\nDESCRIPTION: Loads the Fashion MNIST dataset using TensorFlow Datasets, applies min-max normalization, and creates batched data loaders for training and testing.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nBATCH_SIZE = 128  # @param{type:\"integer\"}\n\n(train_loader, test_loader), info = tfds.load(\n    \"fashion_mnist\", split=[\"train\", \"test\"], as_supervised=True, with_info=True\n)\n\nmin_max_norm = lambda image, label: (tf.cast(image, tf.float32) / 255., label)\ntrain_loader = train_loader.map(min_max_norm)\ntest_loader = test_loader.map(min_max_norm)\n\nNUM_CLASSES = info.features[\"label\"].num_classes\nIMG_SIZE = info.features[\"image\"].shape\n\ntrain_loader_batched = train_loader.shuffle(\n    buffer_size=10_000, reshuffle_each_iteration=True\n).batch(BATCH_SIZE, drop_remainder=True)\n\ntest_loader_batched = test_loader.batch(BATCH_SIZE, drop_remainder=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Loss Function for Ranking Optimization\nDESCRIPTION: Defines a squared error loss function between the perturbed ranking output and a target ranking. This allows for gradient-based optimization of inputs to achieve a desired ranking pattern.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/perturbations.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef loss_example(values, rng):\n  n = values.shape[0]\n  y_true = ranking(jnp.arange(n))\n  y_pred = pert_ranking(values, rng)\n  return jnp.sum((y_true - y_pred) ** 2)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Training Results with Matplotlib\nDESCRIPTION: Defines a function to plot training and test metrics, including learning rate scale, losses, and accuracies over epochs.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef plot(\n    lr_scale_history, train_losses, train_accuracy, test_losses, test_accuracy\n  ):\n  plt.rcParams[\"figure.figsize\"] = (20, 4.5)\n  plt.rcParams.update({\"font.size\": 18})\n\n  fig, axs = plt.subplots(ncols=5)\n\n  axs[0].plot(lr_scale_history[1:], lw=3)\n  axs[0].set_yscale('log')\n  axs[0].set_title(\"LR Scale\")\n  axs[0].set_ylabel(\"LR Scale\")\n  axs[0].set_xlabel(\"Epoch\")\n\n  axs[1].plot(train_losses[1:], lw=3)\n  axs[1].scatter(\n      jnp.argmin(jnp.array(train_losses)),\n      min(train_losses),\n      label=\"Min\",\n      s=100,\n  )\n  axs[1].set_title(\"Train loss\")\n  axs[1].set_xlabel(\"Epoch\")\n  axs[1].set_ylabel(\"Train Loss\")\n  axs[1].legend(frameon=False)\n\n  axs[2].plot(train_accuracy[1:], lw=3)\n  axs[2].scatter(\n      jnp.argmax(jnp.array(train_accuracy)),\n      max(train_accuracy),\n      label=\"Max\",\n      s=100,\n  )\n  axs[2].set_title(\"Train acc\")\n  axs[2].set_xlabel(\"Epoch\")\n  axs[2].set_ylabel(\"Train acc\")\n  axs[2].legend(frameon=False)\n\n  axs[3].plot(test_losses[1:], lw=3)\n  axs[3].scatter(\n      jnp.argmin(jnp.array(test_losses)),\n      min(test_losses),\n      label=\"Min\",\n      s=100,\n  )\n  axs[3].set_title(\"Test loss\")\n  axs[3].set_xlabel(\"Epoch\")\n  axs[3].set_ylabel(\"Test Loss\")\n  axs[3].legend(frameon=False)\n\n  axs[4].plot(test_accuracy[1:], lw=3)\n  axs[4].scatter(\n      jnp.argmax(jnp.array(test_accuracy)),\n      max(test_accuracy),\n      label=\"Max\",\n      s=100,\n  )\n  axs[4].set_title(\"Test acc\")\n  axs[4].set_ylabel(\"Test Acc\")\n  axs[4].legend(frameon=False)\n  axs[4].set_xlabel(\"Epoch\")\n\n  plt.tight_layout()\n  fig.show()\n\nplot(lr_scale_history, train_losses, train_accuracy, test_losses, test_accuracy)\n```\n\n----------------------------------------\n\nTITLE: Optimal Transport Implementation\nDESCRIPTION: Implements optimal transport solution with visualization for facility-client matching problem\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/linear_assignment_problem.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport optax\nfrom jax import numpy as jnp, random\nfrom matplotlib import collections, pyplot as plt, rcParams\n\ndef get_optimal_transport(x, y):\n    assert x.ndim == 2\n    assert x.shape == y.shape\n    displacements = x[:, None] - y[None, :]\n    distance_matrix = jnp.linalg.norm(displacements, axis=-1)\n    i, j = optax.assignment.hungarian_algorithm(distance_matrix)\n    total_distance = distance_matrix[i, j].sum()\n    return (i, j), total_distance\n\ndef main():\n    num_points = 200\n    markersize = 16.0\n    \n    key = random.key(0)\n    keys = random.split(key)\n    x = random.normal(keys[0], (num_points, 2))\n    y = random.normal(keys[1], (num_points, 2)) + jnp.array([0.2, 0.0])\n\n    (i, j), total_distance = get_optimal_transport(x, y)\n\n    fig, ax = plt.subplots(constrained_layout=True)\n    \n    data = jnp.stack((x[i], y[j]), 1)\n    lc = collections.LineCollection(data, linewidth=1.0, color=\"lightgrey\", zorder=0, label=\"assignment\")\n    ax.add_collection(lc)\n\n    ax.scatter(*x.T, s=markersize, edgecolor=\"none\", label=\"facility\")\n    ax.scatter(*y.T, s=markersize, edgecolor=\"none\", label=\"client\")\n\n    ax.set(title=f\"Optimal transport distance: {total_distance:g}\")\n    ax.legend()\n    plt.show()\n\nmain()\n```\n\n----------------------------------------\n\nTITLE: Defining Training Parameters - Python\nDESCRIPTION: Sets up hyperparameters and configuration values for the neural network training including learning rate, number of steps, and data dimensions.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/flax_example.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nLEARNING_RATE = 1e-2\nNUM_STEPS = 100\nNUM_SAMPLES = 20\nX_DIM = 10\nY_DIM = 5\n```\n\n----------------------------------------\n\nTITLE: Testing Hyperparameters for Optimization in Python\nDESCRIPTION: This snippet defines a function `test_hparams` to test optimizer parameters using the Rosenbrock function in Python. Dependencies include JAX and Optax libraries. It takes L-BFGS and linesearch hyperparameters and an optional dimension as inputs. It outputs the final function value, gradient norm, number of function calls, and runtime duration. Constraints involve fixed maximum iterations and tolerance levels.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lbfgs.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport time\nnum_fun_calls = 0\n\ndef register_call():\n  global num_fun_calls\n  num_fun_calls += 1\n\ndef test_hparams(lbfgs_hparams, linesearch_hparams, dimension=512):\n  global num_fun_calls\n  num_fun_calls = 0\n\n  def fun(x):\n    jax.debug.callback(register_call)\n    return jnp.sum((x[1:] - x[:-1] ** 2) ** 2 + (1.0 - x[:-1]) ** 2)\n\n  opt = optax.chain(optax.lbfgs(**lbfgs_hparams,\n    linesearch=optax.scale_by_zoom_linesearch(**linesearch_hparams)\n    )\n  )\n\n  init_params = jnp.arange(dimension, dtype=jnp.float32)\n\n  tic = time.time()\n  final_params, _ = run_opt(\n      init_params, fun, opt, max_iter=500, tol=5*1e-5\n    )\n  final_params = jax.block_until_ready(final_params)\n  time_run = time.time() - tic\n\n  final_value = fun(final_params)\n  final_grad_norm = otu.tree_l2_norm(jax.grad(fun)(final_params))\n  return final_value, final_grad_norm, num_fun_calls, time_run\n\n```\n\n----------------------------------------\n\nTITLE: Plotting Training Results\nDESCRIPTION: Creates a figure with two subplots showing the training progress over epochs for both clean and adversarial accuracy on train and test sets.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/adversarial_training.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n\nplt.suptitle(\"Adversarial training on \" + f\"{DATASET}\".upper())\naxes[0].plot(\n    accuracy_train, lw=3, label=\"train set.\", marker=\"<\", markersize=10\n)\naxes[0].plot(accuracy_test, lw=3, label=\"test set.\", marker=\"d\", markersize=10)\naxes[0].grid()\naxes[0].set_ylabel(\"accuracy on clean images\")\n\naxes[1].plot(\n    adversarial_accuracy_train,\n    lw=3,\n    label=\"adversarial accuracy on train set.\",\n    marker=\"^\",\n    markersize=10,\n)\naxes[1].plot(\n    adversarial_accuracy_test,\n    lw=3,\n    label=\"adversarial accuracy on test set.\",\n    marker=\">\",\n    markersize=10,\n)\naxes[1].grid()\naxes[0].legend(\n    frameon=False, ncol=2, loc=\"upper center\", bbox_to_anchor=(0.8, -0.1)\n)\naxes[0].set_xlabel(\"epochs\")\naxes[1].set_ylabel(\"accuracy on adversarial images\")\nplt.subplots_adjust(wspace=0.5)\n\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Setting Hyperparameters and Loading Dataset\nDESCRIPTION: Defines hyperparameters for the model and training process. Loads the Tiny Shakespeare dataset using TensorFlow Datasets.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/nanolm.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# @markdown Random seed:\nSEED = 42  # @param{type:\"integer\"}\n# @markdown Learning rate passed to the optimizer:\nLEARNING_RATE = 5e-3 # @param{type:\"number\"}\n# @markdown Batch size:\nBATCH_SIZE = 128  # @param{type:\"integer\"}\n# @markdown Number of training iterations:\nN_ITERATIONS = 50_000  # @param{type:\"integer\"}\n# @markdown Number of training iterations between two consecutive evaluations:\nN_FREQ_EVAL = 2_000 # @param{type:\"integer\"}\n# @markdown Rate for dropout in the transformer model\nDROPOUT_RATE = 0.2  # @param{type:\"number\"}\n# @markdown Context window for the transformer model\nBLOCK_SIZE = 64  # @param{type:\"integer\"}\n# @markdown Number of layer for the transformer model\nNUM_LAYERS = 6  # @param{type:\"integer\"}\n# @markdown Size of the embedding for the transformer model\nEMBED_SIZE = 256  # @param{type:\"integer\"}\n# @markdown Number of heads for the transformer model\nNUM_HEADS = 8  # @param{type:\"integer\"}\n# @markdown Size of the heads for the transformer model\nHEAD_SIZE = 32  # @param{type:\"integer\"}\n```\n\nLANGUAGE: python\nCODE:\n```\nds = tfds.load(\"tiny_shakespeare\")\n\n# combine train and test examples into a single string\ntext_train = \"\"\nfor example in ds[\"train\"].concatenate(ds[\"test\"]).as_numpy_iterator():\n  text_train += example[\"text\"].decode(\"utf-8\")\n\n# similarly, create a single string for validation\ntext_validation = \"\"\nfor example in ds[\"validation\"].as_numpy_iterator():\n  text_validation += example[\"text\"].decode(\"utf-8\")\n```\n\n----------------------------------------\n\nTITLE: Model Initialization\nDESCRIPTION: Initializes the model parameters using a random key and zeros input.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/adversarial_training.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nkey = jax.random.PRNGKey(0)\nvar_params = net.init(key, jnp.zeros(input_shape))[\"params\"]\n```\n\n----------------------------------------\n\nTITLE: Hyperparameter Configuration\nDESCRIPTION: Defines training hyperparameters including learning rate, noise multiplier for DP-SGD, L2 norm clip value, batch size, and privacy parameters.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/differentially_private_sgd.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Whether to use DP-SGD or vanilla SGD:\nDPSGD = True\n# Learning rate for the optimizer:\nLEARNING_RATE = 0.25\n# Noise multiplier for DP-SGD optimizer:\nNOISE_MULTIPLIER = 1.3\n# L2 norm clip:\nL2_NORM_CLIP = 1.5\n# Number of samples in each batch:\nBATCH_SIZE = 256\n# Number of epochs:\nNUM_EPOCHS = 15\n# Probability of information leakage:\nDELTA = 1e-5\n```\n\n----------------------------------------\n\nTITLE: Plotting Optimization Paths for Different Optimizers in Python\nDESCRIPTION: Creates plots to visualize the optimization paths in the parameter space for different optimizers (SAM, SGD, SAM Mom, Mom, SAM Adam, Adam), showing how they navigate towards the optimal solution.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(ncols=3, figsize=(8 * 3, 6))\naxs[0].plot(*np.array(sgd_params).T, label='SGD')\naxs[0].plot(*np.array(sam_params)[1::2].T, label='SAM Outer Steps', zorder=100)\naxs[0].plot(*np.array(sam_params)[::2].T, label='SAM Adv Steps', alpha=0.5)\naxs[0].legend(loc=4);\n\naxs[1].plot(*np.array(mom_params).T, label='Mom')\naxs[1].plot(*np.array(sam_mom_params)[1::2].T, label='SAM Mom Outer Steps', zorder=100)\naxs[1].plot(*np.array(sam_mom_params)[::2].T, label='SAM Mom Adv Steps', alpha=0.5)\naxs[1].legend(loc=4);\n\naxs[2].plot(*np.array(adam_params).T, label='Adam')\naxs[2].plot(*np.array(sam_adam_params)[4::5].T, label='SAM Adam Outer Steps', zorder=100)\naxs[2].plot(*np.array(sam_adam_params)[3::5].T, label='SAM Adam Adv Steps', alpha=0.5)\naxs[2].legend(loc=4);\n```\n\n----------------------------------------\n\nTITLE: Data Augmentation Implementation\nDESCRIPTION: Implements data augmentation techniques including random crop, flip, brightness, contrast, and saturation adjustments.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/cifar10_resnet.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef augment(image, label):\n  \"\"\"Performs data augmentation.\"\"\"\n  image = tf.image.resize_with_crop_or_pad(image, 40, 40)\n  image = tf.image.random_crop(image, [32, 32, 3])\n  image = tf.image.random_flip_left_right(image)\n  image = tf.image.random_brightness(image, max_delta=0.2)\n  image = tf.image.random_contrast(image, 0.8, 1.2)\n  image = tf.image.random_saturation(image, 0.8, 1.2)\n  return image, label\n```\n\n----------------------------------------\n\nTITLE: Basic Gradient Transformation Usage in Optax\nDESCRIPTION: Shows basic usage of gradient transformations using RMS scaling, including initialization of state and gradient updates.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntx = optax.scale_by_rms()\nstate = tx.init(params)  # init stats\ngrads = jax.grad(loss)(params, TRAINING_DATA, LABELS)\nupdates, state = tx.update(grads, state, params)  # transform & update stats.\n```\n\n----------------------------------------\n\nTITLE: Configuring Reduce on Plateau Scheduler for Training Loss\nDESCRIPTION: Sets up the optax.contrib.reduce_on_plateau scheduler to reduce learning rate based on average training loss, chained with Adam optimizer.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nN_EPOCHS = 50  # @param{type:\"integer\"}\nLEARNING_RATE = 0.01  # @param{type:\"number\"}\n\nPATIENCE = 5  # @param{type:\"integer\"}\nCOOLDOWN = 0  # @param{type:\"integer\"}\nFACTOR = 0.5  # @param{type:\"number\"}\nRTOL = 1e-4  # @param{type:\"number\"}\nACCUMULATION_SIZE = 200\n\nopt = optax.chain(\n    optax.adam(LEARNING_RATE),\n    contrib.reduce_on_plateau(\n        patience=PATIENCE,\n        cooldown=COOLDOWN,\n        factor=FACTOR,\n        rtol=RTOL,\n        accumulation_size=ACCUMULATION_SIZE,\n    ),\n)\nopt_state = opt.init(init_params)\n```\n\n----------------------------------------\n\nTITLE: Defining Data Generator for Linear Regression\nDESCRIPTION: Creates a generator function that yields (x, y) pairs from a noisy linear relationship.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/meta_learning.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef generator() -> Iterator[Tuple[chex.Array, chex.Array]]:\n  rng = jax.random.PRNGKey(0)\n\n  while True:\n    rng, k1, k2 = jax.random.split(rng, num=3)\n    x = jax.random.uniform(k1, minval=0.0, maxval=10.0)\n    y = 10.0 * x + jax.random.normal(k2)\n    yield x, y\n```\n\n----------------------------------------\n\nTITLE: Training with Single Batch\nDESCRIPTION: Demonstrates training using a single large batch containing all examples.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/gradient_accumulation.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnew_params_single_batch = fit(\n    optimizer,\n    params,\n    batches=[dict(image=EXAMPLES, label=LABELS),]\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Adam Optimizer\nDESCRIPTION: Sets up Adam optimizer with initial parameters and state\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstart_learning_rate = 1e-1\noptimizer = optax.adam(start_learning_rate)\n\n# Initialize parameters of the model + optimizer.\nparams = jnp.array([0.0, 0.0])\nopt_state = optimizer.init(params)\n```\n\n----------------------------------------\n\nTITLE: Creating Bipartite Graph Visualization\nDESCRIPTION: Creates and displays a bipartite graph representation of the assignment problem using NetworkX\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/linear_assignment_problem.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nG = nx.Graph()\nrows = [f\"row {i}\" for i in range(n)]\ncols = [f\"col {j}\" for j in range(m)]\nedges = [(rows[i], cols[j], {\"cost\": costs[i, j]}) for i in range(n) for j in range(m)]\nG.add_nodes_from(rows + cols)\nG.add_edges_from(edges)\nlayout = nx.bipartite_layout(G, rows)\nnx.draw(G, layout)\nnx.draw_networkx_edge_labels(\n    G,\n    layout,\n    edge_labels={(u, v): f\"{info['cost']:g}\" for u, v, info in edges},\n    rotate=False,\n    font_size=8,\n    bbox=dict(\n        pad=0.0,\n        facecolor=\"white\",\n        edgecolor=\"none\",\n    ),\n);\n```\n\n----------------------------------------\n\nTITLE: MNIST Data Loading and Preprocessing\nDESCRIPTION: Loads MNIST dataset using tensorflow_datasets, applies normalization, and creates batched data loaders for training and testing.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lookahead_mnist.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n(train_loader, test_loader), info = tfds.load(\n    \"mnist\", split=[\"train\", \"test\"], as_supervised=True, with_info=True\n)\nNUM_CLASSES = info.features[\"label\"].num_classes\nIMG_SIZE = info.features[\"image\"].shape\n\nmin_max_rgb = lambda image, label: (tf.cast(image, tf.float32) / 255., label)\ntrain_loader = train_loader.map(min_max_rgb)\ntest_loader = test_loader.map(min_max_rgb)\n\ntrain_loader_batched = train_loader.shuffle(\n    buffer_size=10_000, reshuffle_each_iteration=True\n).batch(BATCH_SIZE, drop_remainder=True)\n\ntest_loader_batched = test_loader.batch(BATCH_SIZE, drop_remainder=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Rosenbrock Function and Grid Creation\nDESCRIPTION: This snippet defines the Rosenbrock function and creates a grid of x and y values for plotting.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/rosenbrock_ademamix.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef rosenbrock(x):\n    return jnp.square(1 - x[0]) + 100.0 * jnp.square(x[1] - jnp.square(x[0]))\n\n\n# Create a grid of x and y values\nx = jnp.linspace(-5, 10, 1000)\ny = jnp.linspace(-5, 10, 1000)\nX, Y = jnp.meshgrid(x, y)\n\n# Compute the Rosenbrock function values for each point on the grid\nZ = rosenbrock([X, Y])\n```\n\n----------------------------------------\n\nTITLE: Implementing a Ranking Function\nDESCRIPTION: Defines a non-differentiable function that transforms a vector into its ranking representation, returning the positions of elements when sorted. This corresponds to an optimizer over the permutahedron.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/perturbations.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef ranking(values):\n  return jnp.argsort(jnp.argsort(values))\n```\n\n----------------------------------------\n\nTITLE: Generating Random Cost Matrix\nDESCRIPTION: Creates a random cost matrix using JAX random number generator\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/linear_assignment_problem.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nn = 5  # number of rows\nm = 3  # number of columns\n\nkey = random.key(0)\ncosts = random.normal(key, (n, m))\nprint(costs)\n```\n\n----------------------------------------\n\nTITLE: Adding Debug Information to L-BFGS Optimizer\nDESCRIPTION: Implementation of a custom transform that adds printing capabilities to track optimization progress, combined with the L-BFGS optimizer.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lbfgs.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass InfoState(NamedTuple):\n  iter_num: chex.Numeric\n\n\ndef print_info():\n  def init_fn(params):\n    del params\n    return InfoState(iter_num=0)\n\n  def update_fn(updates, state, params, *, value, grad, **extra_args):\n    del params, extra_args\n\n    jax.debug.print(\n        'Iteration: {i}, Value: {v}, Gradient norm: {e}',\n        i=state.iter_num,\n        v=value,\n        e=otu.tree_l2_norm(grad),\n    )\n    return updates, InfoState(iter_num=state.iter_num + 1)\n\n  return optax.GradientTransformationExtraArgs(init_fn, update_fn)\n```\n\n----------------------------------------\n\nTITLE: Defining Parametrized Linear Function\nDESCRIPTION: Defines the linear function to be learned and initializes its parameter.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/meta_learning.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef f(theta: chex.Array, x: chex.Array) -> chex.Array:\n  return x * theta\n\ntheta = jax.random.normal(jax.random.PRNGKey(42))\n```\n\n----------------------------------------\n\nTITLE: Sampling from Data Generator\nDESCRIPTION: Demonstrates how to sample data points from the generator function.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/meta_learning.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ng = generator()\n\nfor _ in range(5):\n  x, y = next(g)\n  print(f\"Sampled y = {y:.3f}, x = {x:.3f}\")\n```\n\n----------------------------------------\n\nTITLE: Sphinx RST Module Documentation\nDESCRIPTION: ReStructuredText documentation outlining the complete API surface of Optax's transformation module, including gradient transformations, state classes, and optimization functions.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/api/transformations.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: optax\n\n.. autosummary::\n    adaptive_grad_clip\n    AdaptiveGradClipState\n    add_decayed_weights\n    AddDecayedWeightsState\n    add_noise\n    AddNoiseState\n    apply_every\n    ApplyEvery\n    bias_correction\n    conditionally_mask\n    conditionally_transform\n    ConditionallyMaskState\n    ConditionallyTransformState\n    centralize\n    clip\n    clip_by_block_rms\n    ClipState\n    clip_by_global_norm\n    ClipByGlobalNormState\n    ema\n    EmaState\n    EmptyState\n    global_norm\n    GradientTransformation\n    GradientTransformationExtraArgs\n    identity\n    keep_params_nonnegative\n    NonNegativeParamsState\n    normalize_by_update_norm\n    OptState\n    Params\n    per_example_global_norm_clip\n    per_example_layer_norm_clip\n    scale\n    ScaleState\n    scale_by_adadelta\n    ScaleByAdaDeltaState\n    scale_by_adan\n    ScaleByAdanState\n    scale_by_adam\n    scale_by_adamax\n    ScaleByAdamState\n    scale_by_amsgrad\n    ScaleByAmsgradState\n    scale_by_backtracking_linesearch\n    ScaleByBacktrackingLinesearchState\n    scale_by_belief\n    ScaleByBeliefState\n    scale_by_factored_rms\n    FactoredState\n    scale_by_lbfgs\n    ScaleByLBFGSState\n    scale_by_learning_rate\n    scale_by_lion\n    ScaleByLionState\n    scale_by_novograd\n    ScaleByNovogradState\n    scale_by_optimistic_gradient\n    scale_by_param_block_norm\n    scale_by_param_block_rms\n    scale_by_polyak\n    scale_by_radam\n    scale_by_rms\n    ScaleByRmsState\n    scale_by_rprop\n    ScaleByRpropState\n    scale_by_rss\n    ScaleByRssState\n    scale_by_schedule\n    ScaleByScheduleState\n    scale_by_sign\n    scale_by_sm3\n    ScaleBySM3State\n    scale_by_stddev\n    ScaleByRStdDevState\n    scale_by_trust_ratio\n    ScaleByTrustRatioState\n    scale_by_yogi\n    scale_by_zoom_linesearch\n    ScaleByZoomLinesearchState\n    set_to_zero\n    stateless\n    stateless_with_tree_map\n    trace\n    TraceState\n    TransformInitFn\n    TransformUpdateFn\n    update_infinity_moment\n    update_moment\n    update_moment_per_elem_norm\n    Updates\n    with_extra_args_support\n    zero_nans\n    ZeroNansState\n    ZoomLinesearchInfo\n```\n\n----------------------------------------\n\nTITLE: Privacy Parameter Calculation\nDESCRIPTION: Computes the privacy parameter epsilon using the moments accountant method for given training steps and delta.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/differentially_private_sgd.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef compute_epsilon(steps):\n  if NUM_EXAMPLES * DELTA > 1.:\n    warnings.warn(\"Your delta might be too high.\")\n  q = BATCH_SIZE / float(NUM_EXAMPLES)\n  orders = list(jnp.linspace(1.1, 10.9, 99)) + list(range(11, 64))\n  accountant = dp_accounting.rdp.RdpAccountant(orders)\n  accountant.compose(dp_accounting.PoissonSampledDpEvent(\n      q, dp_accounting.GaussianDpEvent(NOISE_MULTIPLIER)), steps)\n  return accountant.get_epsilon(DELTA)\n```\n\n----------------------------------------\n\nTITLE: Implementing Gradient Function for SAM Opaque Mode\nDESCRIPTION: Demonstrates how to create and use a gradient function for SAM optimizer updates in opaque mode. The gradient function takes parameters and a step counter as arguments.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ngrad_fn = jax.grad(\n    lambda params, _: loss(params, batch, and_other_args, to_loss))\nupdates, sam_state = sam_opt.update(updates, sam_state, params, grad_fn=grad_fn)\nparams = optax.apply_updates(params, updates)\n```\n\n----------------------------------------\n\nTITLE: Initializing Random Number Generators - Python\nDESCRIPTION: Creates and splits random number generators for various components of the neural network.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/flax_example.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrng = jax.random.PRNGKey(0)\nparams_rng, w_rng, b_rng, samples_rng, noise_rng = jax.random.split(rng, num=5)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Loss Function for Tree-Structured Optimization\nDESCRIPTION: Defines a loss function that computes the squared difference between perturbed and non-perturbed argmax outputs over a pytree. It demonstrates how to accumulate losses across complex nested structures.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/perturbations.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef pert_loss(inputs, rng):\n  pert_softmax = pert_argmax_fun(inputs, rng)\n  argmax = argmax_tree(inputs)\n  diffs = jtu.tree_map(lambda x, y: jnp.sum((x - y) ** 2 / 4), argmax, pert_softmax)\n  return jtu.tree_reduce(operator.add, diffs)\n```\n\n----------------------------------------\n\nTITLE: Implementing Alpha and B3 Schedulers for AdeMAMix\nDESCRIPTION: This snippet creates alpha and b3 schedulers for the AdeMAMix optimizer, which are used to adjust hyperparameters during training.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/rosenbrock_ademamix.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nalpha = 0.8\nalpha = linear_schedule(0, alpha, num_iterations)\n\ndef b3_scheduler(beta_end: float, beta_start: float = 0, warmup: int = 0):\n    def f(beta):\n        return jnp.log(0.5) / jnp.log(beta) - 1\n\n    def f_inv(t):\n        return jnp.power(0.5, 1 / (t + 1))\n\n    def schedule(step):\n        is_warmup = jnp.array(step < warmup).astype(jnp.float32)\n        alpha = step / float(warmup)\n        return is_warmup * f_inv(\n            (1.0 - alpha) * f(beta_start) + alpha * f(beta_end)\n        ) + beta_end * (1.0 - is_warmup)\n\n    return schedule\n```\n\n----------------------------------------\n\nTITLE: Plotting Rosenbrock Function Trajectories\nDESCRIPTION: This snippet creates a plot comparing the trajectories of Adam and AdeMAMix optimizers on the Rosenbrock function contour.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/rosenbrock_ademamix.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfig = plt.figure()\nax = fig.subplots(1, 2)\nax[0].set_xlabel(\"x\")\nax[0].set_ylabel(\"y\")\nax[0].set_title(\"Rosenbrock Function - Adam Trajectories\")\n# Show the plot\nax[0].plot([1], [1], \"x\", mew=1, markersize=10, color=\"cyan\")\nax[0].contourf(X, Y, Z, np.logspace(-1, 3, 100), cmap=\"jet\")\nfor i, b1 in enumerate([0.9, 0.99, 0.999, 0.9999]):\n    ax[0].plot(\n        all_b1_params_array[i, ::100, 0],\n        all_b1_params_array[i, ::100, 1],\n        label=f\"Adam b1 = {b1}\",\n    )\nax[0].set_xlim(-4, 4)\nax[0].set_ylim(-3.5, 7.5)\nax[0].legend()\n\nax[1].set_xlabel(\"x\")\nax[1].set_ylabel(\"y\")\nax[1].set_title(\"Rosenbrock Function - Ademamix Trajectories\")\n# Show the plot\nax[1].plot([1], [1], \"x\", mew=1, markersize=10, color=\"cyan\")\nax[1].contourf(X, Y, Z, np.logspace(-1, 3, 100), cmap=\"jet\")\nfor i, b3 in enumerate([0.999, 0.9999]):\n    ax[1].plot(\n        all_ademamix_params_array[i, ::100, 0],\n        all_ademamix_params_array[i, ::100, 1],\n        label=f\"AdEMAMix b3 = {b3}\",\n    )\nax[1].set_xlim(-4, 4)\nax[1].set_ylim(-3.5, 7.5)\nax[1].legend()\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Meta-Learning Results\nDESCRIPTION: Creates plots to visualize the evolution of the learning rate and model parameter during meta-learning.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/meta_learning.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfig, (ax1, ax2) = plt.subplots(2);\nfig.suptitle('Meta-learning RMSProp\\'s learning rate');\nplt.xlabel('Step');\n\nax1.semilogy(range(len(learning_rates)), learning_rates);\nax1.set(ylabel='Learning rate');\nax1.label_outer();\n\nplt.xlabel('Number of updates');\nax2.semilogy(range(len(thetas)), thetas);\n\nax2.label_outer();\nax2.set(ylabel='Theta');\n```\n\n----------------------------------------\n\nTITLE: Visualizing Optimization Results for Multiple Optimizers in Python\nDESCRIPTION: Creates plots to compare the performance of different optimizers (SAM, SGD, SAM Mom, Mom, SAM Adam, Adam) in terms of loss values over time and optimization paths in the parameter space.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nts = np.arange(T)\nfig, axs = plt.subplots(6, figsize=(10, 12))\naxs[0].plot(ts, sgd_vals, label='SGD')\naxs[0].plot(ts[::2], sam_vals[0::2], label='SAM Outer Loss', lw=3, zorder=100)\naxs[0].plot(ts[1::2], sam_vals[1::2], label='SAM Adv Loss', alpha=0.5)\naxs[0].legend();\n\naxs[1].plot(ts, sgd_vals, label='SGD')\naxs[1].plot(ts[::2] / 2, sam_vals[::2], label='1/2 SAM', lw=3)\naxs[1].legend();\n\naxs[2].plot(ts, mom_vals, label='Mom')\naxs[2].plot(ts[::2], sam_mom_vals[::2], label='SAM Mom Outer Loss', lw=3, zorder=100)\naxs[2].plot(ts[1::2], sam_mom_vals[1::2], label='SAM Mom Adv Loss', alpha=0.5)\naxs[2].legend();\n\naxs[3].plot(ts, mom_vals, label='Mom')\naxs[3].plot(ts[::2] / 2, sam_mom_vals[::2], label='1/2 SAM Mom', lw=3)\naxs[3].legend();\n\naxs[4].plot(ts, adam_vals, label='Adam')\naxs[4].plot(ts[::5], sam_adam_vals[::5], label='SAM Adam Outer Loss', lw=3, zorder=100)\naxs[4].plot(ts[4::5], sam_adam_vals[4::5], label='SAM Adam Adv Loss', alpha=0.5)\naxs[4].legend();\n\naxs[5].plot(ts, adam_vals, label='Adam')\naxs[5].plot(ts[::5] / 5, sam_adam_vals[::5], label='1/5 SAM Adam', lw=3)\naxs[5].legend();\n```\n\n----------------------------------------\n\nTITLE: Visualizing Assignment Solution\nDESCRIPTION: Displays the solution by highlighting assigned cells in the cost matrix\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/linear_assignment_problem.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef highlight_cell(x, y, **kwargs):\n    rect = plt.Rectangle((x - 0.5, y - 0.5), 1, 1, fill=False, **kwargs)\n    plt.gca().add_patch(rect)\n    return rect\n\nplt.imshow(costs, cmap=\"gray\")\n\nfor i, j in zip(sol_i, sol_j):\n    highlight_cell(j, i, color=\"red\", linewidth=3)\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Running Optax Tests\nDESCRIPTION: Command to run the test suite for Optax to ensure code functionality and quality.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/README.md#2025-04-22_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nsh test.sh\n```\n\n----------------------------------------\n\nTITLE: Plotting Training Progress\nDESCRIPTION: Creates a visualization of training and evaluation losses over time.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/nanolm.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nplt.title(\"Convergence of adamw (train loss)\")\nplt.plot(all_train_losses, label=\"train\", lw=3)\nplt.plot(\n    jnp.arange(0, len(all_eval_losses) * N_FREQ_EVAL, N_FREQ_EVAL),\n    all_eval_losses,\n    label=\"test\",\n    lw=3,\n)\nplt.xlabel(\"steps\")\nplt.ylabel(\"loss\")\nplt.grid()\nplt.legend(frameon=False)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Model Functions Implementation\nDESCRIPTION: Implements core model functions including accuracy calculation, loss computation, and PGD attack generation using JAX's automatic differentiation.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/adversarial_training.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnet = CNN(num_classes)\n\n@jax.jit\ndef accuracy(params, data):\n  inputs, labels = data\n  logits = net.apply({\"params\": params}, inputs)\n  return jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n\n\n@jax.jit\ndef loss_fun(params, l2reg, data):\n  \"\"\"Compute the loss of the network.\"\"\"\n  inputs, labels = data\n  x = inputs.astype(jnp.float32)\n  logits = net.apply({\"params\": params}, x)\n  sqnorm = tree_l2_norm(params, squared=True)\n  loss_value = jnp.mean(softmax_cross_entropy_with_integer_labels(logits, labels))\n  return loss_value + 0.5 * l2reg * sqnorm\n\n@jax.jit\ndef pgd_attack(image, label, params, epsilon=0.1, maxiter=10):\n  \"\"\"PGD attack on the L-infinity ball with radius epsilon.\"\"\"\n  image_perturbation = jnp.zeros_like(image)\n  def adversarial_loss(perturbation):\n    return loss_fun(params, 0, (image + perturbation, label))\n\n  grad_adversarial = jax.grad(adversarial_loss)\n  for _ in range(maxiter):\n    sign_grad = jnp.sign(grad_adversarial(image_perturbation))\n    image_perturbation += (2 * epsilon / maxiter) * sign_grad\n    image_perturbation = jnp.clip(image_perturbation, - epsilon, epsilon)\n  return jnp.clip(image + image_perturbation, 0, 1)\n```\n\n----------------------------------------\n\nTITLE: Generating Training Data\nDESCRIPTION: Creates synthetic training data using JAX random number generation\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nkey = jax.random.PRNGKey(42)\ntarget_params = 0.5\n\n# Generate some data.\nxs = jax.random.normal(key, (16, 2))\nys = jnp.sum(xs * target_params, axis=-1)\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Performance Improvement on MNIST Test Set\nDESCRIPTION: A simple string formatter that reports the improvement in accuracy from the initial model to the final trained model on the test dataset, showing the effectiveness of the training process.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/mlp_mnist.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nf\"Improved accuracy on test DS from {test_accuracy[0]} to {test_accuracy[-1]}\"\n```\n\n----------------------------------------\n\nTITLE: Generating Adam Trajectories for Rosenbrock Function\nDESCRIPTION: This snippet generates trajectories for the Adam optimizer with different b1 values on the Rosenbrock function.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/rosenbrock_ademamix.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nall_b1_params = []\nfor b1 in [0.9, 0.99, 0.999, 0.9999]:\n    solver = optax.adam(learning_rate=0.003, b1=b1, b2=0.9999)\n    params = jnp.array([-3.0, 5.0])\n    print(\"Objective function: \", rosenbrock(params))\n    opt_state = solver.init(params)\n\n    def _body_fn(carry, _):\n        params, opt_state, i = carry\n        grad = jax.grad(rosenbrock)(params)\n        updates, opt_state = solver.update(grad, opt_state, params)\n        params = optax.apply_updates(params, updates)\n        cond_print(\n            i % 25000 == 0,\n            \"Objective function for b1={} at iteration {} = {}\",\n            b1,\n            i,\n            rosenbrock(params),\n        )\n        return (params, opt_state, i + 1), params\n\n    _, all_params = jax.lax.scan(\n        _body_fn, (params, opt_state, 0), length=num_iterations\n    )\n    all_b1_params.append(jnp.concatenate([params[None, ...], all_params], 0))\nall_b1_params_array = jnp.array(all_b1_params)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Training Loss - Python\nDESCRIPTION: Creates a plot to visualize the training loss over time using matplotlib.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/flax_example.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nplt.plot(loss_history)\nplt.title('Train loss')\nplt.xlabel('Step')\nplt.ylabel('MSE')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Running Doctest on Optax Examples\nDESCRIPTION: Command for testing example code in docstrings using Python's doctest module. This helps ensure that the examples in the documentation are correct and working.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/development.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m doctest -v <path_to_your_file>.py\n```\n\n----------------------------------------\n\nTITLE: Plotting Training and Test Metrics using Matplotlib in Python\nDESCRIPTION: Uses Matplotlib to generate two subplots visualizing the model's performance over training epochs. The first subplot shows the test set accuracy and the estimated training set accuracy. The second subplot shows the test set loss and the estimated training set loss (on a logarithmic scale). The plots include labels, grids, markers, and a legend. Assumes `test_accuracy`, `train_accuracy`, `test_losses`, `train_losses`, `MODEL`, `DATASET`, and `MAX_EPOCHS` variables are defined.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/cifar10_resnet.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\nplt.suptitle(f\"{MODEL} on {DATASET}\", fontsize=20)\n\nax1.plot(\n    test_accuracy,\n    lw=3,\n    marker=\"s\",\n    markevery=5,\n    markersize=10,\n    label=\"test set\",\n)\nax1.plot(\n    train_accuracy,\n    lw=3,\n    marker=\"^\",\n    markevery=5,\n    markersize=10,\n    label=\"train set (stochastic estimate)\",\n)\nax1.set_ylabel(\"Accuracy\", fontsize=20)\nax1.grid()\nax1.set_xlabel(\"Epochs\", fontsize=20)\nax1.set_xlim((0, MAX_EPOCHS))\nax1.set_ylim((0, 1))\n\nax2.plot(\n    test_losses, lw=3, marker=\"s\", markevery=5, markersize=10, label=\"test set\"\n)\nax2.plot(\n    train_losses,\n    lw=3,\n    marker=\"^\",\n    markevery=5,\n    markersize=10,\n    label=\"train set (stochastic estimate)\",\n)\nax2.set_ylabel(\"Loss\", fontsize=20)\nax2.grid()\nax2.set_xlabel(\"Epochs\", fontsize=20)\nax2.set_xlim((0, MAX_EPOCHS))\n\n# set legend at the bottom of the plot\nax1.legend(\n    frameon=False, fontsize=20, ncol=2, loc=2, bbox_to_anchor=(0.3, -0.1)\n)\n\nax2.set_yscale(\"log\")\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Checking JAX Platform\nDESCRIPTION: Imports necessary libraries including JAX, Flax, Optax, and TensorFlow Datasets. Prints the platform JAX is running on.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/nanolm.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport functools\n\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nfrom matplotlib import pyplot as plt\nimport optax\nimport tensorflow_datasets as tfds\n\n# platform check\nprint(\"JAX running on\", jax.devices()[0].platform.upper())\n```\n\n----------------------------------------\n\nTITLE: Results Visualization\nDESCRIPTION: Creates plots showing test accuracy, loss, and privacy parameter (epsilon) over training epochs.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/differentially_private_sgd.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nif DPSGD:\n  _, axs = plt.subplots(ncols=3, figsize=(9, 3))\nelse:\n  _, axs = plt.subplots(ncols=2, figsize=(6, 3))\n\naxs[0].plot(accuracy)\naxs[0].set_title(\"Test accuracy\")\naxs[1].plot(loss)\naxs[1].set_title(\"Test loss\")\n\nif DPSGD:\n  axs[2].plot(epsilon)\n  axs[2].set_title(\"Epsilon\")\n\nplt.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Initializing Adam Optimizer with Optax in Python\nDESCRIPTION: This snippet shows how to instantiate the Adam optimizer from the Optax library in Python using a specified learning rate and initialize its internal state with model parameters. Dependencies include the Optax library and properly initialized model parameters (init_params). The expected input is the learning rate and the model parameters to initialize the optimizer state, which are needed for subsequent optimization steps.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsolver = optax.adam(LEARNING_RATE)\nsolver_state = solver.init(init_params)\n```\n\n----------------------------------------\n\nTITLE: Installing Optax Documentation Dependencies\nDESCRIPTION: Command to install the dependencies required for building Optax documentation using pip with the 'docs' extra.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/README.md#2025-04-22_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npip install -e \".[docs]\"\n```\n\n----------------------------------------\n\nTITLE: Finding Adversarial Example Images\nDESCRIPTION: Function to identify test set images that are correctly classified in their original form but misclassified after adversarial perturbation.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/adversarial_training.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef find_adversarial_imgs(params, loader_batched):\n  \"\"\"Finds a test set image that is correctly classified but not its adversarial perturbation.\"\"\"\n  for batch in loader_batched.as_numpy_iterator():\n    images, labels = batch\n    images = images.astype(jnp.float32) / 255\n    logits = net.apply({\"params\": params}, images)\n    labels_clean = jnp.argmax(logits, axis=-1)\n\n    adversarial_images = pgd_attack(images, labels, params, epsilon=EPSILON)\n    labels_adversarial = jnp.argmax(\n        net.apply({\"params\": params}, adversarial_images), axis=-1\n    )\n    idx_misclassified = jnp.where(labels_clean != labels_adversarial)[0]\n    for j in idx_misclassified:\n      clean_image = images[j]\n      prediction_clean = labels_clean[j]\n      if prediction_clean != labels[j]:\n        # the clean image predicts the wrong label, skip\n        continue\n      adversarial_image = adversarial_images[j]\n      adversarial_prediction = labels_adversarial[j]\n      # we found our image\n      return (\n          clean_image,\n          prediction_clean,\n          adversarial_image,\n          adversarial_prediction,\n      )\n\n  raise ValueError(\"No mismatch between clean and adversarial prediction found\")\n\n\nimg_clean, pred_clean, img_adversarial, prediction_adversarial = (\n    find_adversarial_imgs(var_params, test_loader_batched)\n)\n```\n\n----------------------------------------\n\nTITLE: Building Optax Documentation\nDESCRIPTION: Commands to navigate to the docs directory and build the HTML documentation for Optax using make.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/README.md#2025-04-22_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ncd docs\nmake html\n```\n\n----------------------------------------\n\nTITLE: Generating AdeMAMix Trajectories for Rosenbrock Function\nDESCRIPTION: This snippet generates trajectories for the AdeMAMix optimizer with different b3 values on the Rosenbrock function.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/rosenbrock_ademamix.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nall_ademamix_params = []\nfor b3 in [0.999, 0.9999]:\n    b3 = b3_scheduler(b3, 0, num_iterations)\n    solver = optax.contrib.ademamix(\n        learning_rate=0.003, b1=0.99, b2=0.999, b3=b3, alpha=alpha\n    )\n    params = jnp.array([-3.0, 5.0])\n    print(\"Objective function: \", rosenbrock(params))\n    opt_state = solver.init(params)\n\n    def _body_fn(carry, _):\n        params, opt_state, i = carry\n        grad = jax.grad(rosenbrock)(params)\n        updates, opt_state = solver.update(grad, opt_state, params)\n        params = optax.apply_updates(params, updates)\n        cond_print(\n            i % 25000 == 0,\n            \"Objective function for b3={} at iteration {} = {}\",\n            b3(i),\n            i,\n            rosenbrock(params),\n        )\n        return (params, opt_state, i + 1), params\n\n    _, all_params = jax.lax.scan(\n        _body_fn, (params, opt_state, 0), length=num_iterations\n    )\n    all_ademamix_params.append(jnp.concatenate([params[None, ...], all_params], 0))\nall_ademamix_params_array = jnp.array(all_ademamix_params)\n```\n\n----------------------------------------\n\nTITLE: Installing NetworkX Dependency\nDESCRIPTION: Command to install the NetworkX library for graph visualization\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/linear_assignment_problem.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U networkx\n```\n\n----------------------------------------\n\nTITLE: Creating a Perturbed Approximation for Tree Operations\nDESCRIPTION: Creates a differentiable approximation of the tree-mapped argmax function. This allows for gradient computation through operations on nested data structures.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/perturbations.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nN_SAMPLES = 100\nsigma = 1.0\n\npert_argmax_fun = perturbations.make_perturbed_fun(argmax_tree,\n                                                   num_samples=N_SAMPLES,\n                                                   sigma=SIGMA)\n```\n\n----------------------------------------\n\nTITLE: Counting Model Parameters\nDESCRIPTION: Calculates and prints the total number of parameters in the model.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/nanolm.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nn_params = sum(p.size for p in jax.tree.leaves(var_params))\n\nprint(f\"Total number of parameters: {n_params:_}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary Python libraries including NetworkX, JAX, Optax, and Matplotlib\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/linear_assignment_problem.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport networkx as nx\nfrom jax import random\nimport optax\nfrom matplotlib import pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Printing Final Optimization Values\nDESCRIPTION: This snippet prints the final parameter values for both Adam and AdeMAMix optimizers with different hyperparameter settings.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/rosenbrock_ademamix.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Adam Values:\")\nprint(\n    \"Final value with b1 = 0.9:\"\n    f\" ({float(all_b1_params_array[0,-1,0])},\"\n    f\" {float(all_b1_params_array[0,-1,1])})\"\n)\nprint(\n    \"Final value with b1 = 0.99:\"\n    f\" ({float(all_b1_params_array[1,-1,0])},\"\n    f\" {float(all_b1_params_array[1,-1,1])})\"\n)\nprint(\n    \"Final value with b1 = 0.999:\"\n    f\" ({float(all_b1_params_array[2,-1,0])},\"\n    f\" {float(all_b1_params_array[2,-1,1])})\"\n)\nprint(\n    \"Final value with b1 = 0.9999:\"\n    f\" ({float(all_b1_params_array[3,-1,0])},\"\n    f\" {float(all_b1_params_array[3,-1,1])})\"\n)\nprint(\"AdeMAMix Values:\")\nprint(\n    \"Final value with b3 = 0.999:\"\n    f\" ({float(all_ademamix_params_array[0,-1,0])},\"\n    f\" {float(all_ademamix_params_array[0,-1,1])})\"\n)\nprint(\n    \"Final value with b3 = 0.9999:\"\n    f\" ({float(all_ademamix_params_array[1,-1,0])},\"\n    f\" {float(all_ademamix_params_array[1,-1,1])})\"\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Cost Matrix\nDESCRIPTION: Displays the cost matrix as a grayscale image using matplotlib\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/linear_assignment_problem.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nplt.imshow(costs, cmap=\"gray\");\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Gradient Estimators\nDESCRIPTION: ReStructuredText documentation layout defining the structure and API references for stochastic gradient estimation methods, including a deprecation warning and function references.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/api/stochastic_gradient_estimators.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nStochastic Gradient Estimators\n==============================\n\n.. warning::\n    This module has been deprecated and will be removed in optax 0.3.0.\n\n.. currentmodule:: optax.monte_carlo\n\n.. autosummary::\n    measure_valued_jacobians\n    pathwise_jacobians\n    score_function_jacobians\n\nMeasure valued Jacobians\n~~~~~~~~~~~~~~~~~~~~~~~~\n.. autofunction:: measure_valued_jacobians\n\nPathwise Jacobians\n~~~~~~~~~~~~~~~~~~\n.. autofunction:: pathwise_jacobians\n\nScore function Jacobians\n~~~~~~~~~~~~~~~~~~~~~~~~\n.. autofunction:: score_function_jacobians\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Optax\nDESCRIPTION: Basic imports needed for using Optax with JAX and NumPy\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nimport jax\nimport optax\nimport functools\n```\n\n----------------------------------------\n\nTITLE: Defining Utility Functions for AdeMAMix Rosenbrock Plot\nDESCRIPTION: This snippet defines utility functions including a conditional print function and imports linear_schedule from Optax.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/rosenbrock_ademamix.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom optax.schedules import linear_schedule\nfrom optax._src import base\n\n\ndef cond_print(cond, fmt, *args):\n    return jax.lax.cond(cond, lambda: (jax.debug.print(fmt, *args), 0)[1], lambda: 0)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for AdeMAMix Rosenbrock Plot\nDESCRIPTION: This snippet imports necessary libraries for mathematical operations, plotting, and optimization using JAX and Optax.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/rosenbrock_ademamix.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport optax\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\nplt.rc(\"figure\", figsize=(20, 10))\nplt.rc(\"font\", size=14)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for L-BFGS in Python\nDESCRIPTION: This snippet imports necessary libraries for implementing L-BFGS optimization, including JAX, Optax, and CHEX.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lbfgs.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import NamedTuple\n\nimport chex\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jrd\n\nimport optax\nimport optax.tree_utils as otu\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Optimization and Visualization\nDESCRIPTION: Imports necessary Python libraries including JAX for automatic differentiation, Optax for optimization algorithms, and Matplotlib for plotting results.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/ogda_example.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport functools\nimport jax\nimport optax\nimport matplotlib.pyplot as plt\nfrom jax import lax, numpy as jnp\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries - Python\nDESCRIPTION: Imports essential libraries including JAX, Optax, and Flax for neural network implementation.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/flax_example.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport optax\nimport matplotlib.pyplot as plt\nfrom flax import linen as nn\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Reduce on Plateau Scheduler Example\nDESCRIPTION: Imports necessary libraries including JAX, Flax, Optax, TensorFlow, and matplotlib for implementing and visualizing the reduce on plateau scheduler.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Sequence\nfrom flax import linen as nn\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport optax\nfrom optax import tree_utils as otu\nfrom optax import contrib\n\n# Show on which platform JAX is running.\nprint(\"JAX running on\", jax.devices()[0].platform.upper())\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary packages including Flax, JAX, Optax, TensorFlow and TensorFlow Datasets for implementing the MNIST classifier.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/lookahead_mnist.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom flax import linen as nn\nimport jax\nimport jax.numpy as jnp\nimport optax\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Platform Check\nDESCRIPTION: Imports required libraries including JAX, Optax, TensorFlow, and differential privacy accounting tools. Also displays the JAX execution platform.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/differentially_private_sgd.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\nimport dp_accounting\nimport jax\nimport jax.numpy as jnp\nfrom optax import contrib\nfrom optax import losses\nimport optax\nfrom jax.example_libraries import stax\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\n\n# Shows on which platform JAX is running.\nprint(\"JAX running on\", jax.devices()[0].platform.upper())\n```\n\n----------------------------------------\n\nTITLE: Installing Optax from PyPI\nDESCRIPTION: Command to install the latest released version of Optax from PyPI using pip.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install optax\n```\n\n----------------------------------------\n\nTITLE: Applying Parameter Updates in Optax\nDESCRIPTION: Demonstrates how to apply transformed updates to parameters using apply_updates function.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nupdates, state = tx.update(grads, state, params)  # transform & update stats.\nnew_params = optax.apply_updates(params, updates)  # update the parameters.\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents for Contrib Examples in Markdown\nDESCRIPTION: This snippet uses a toctree directive to generate a table of contents for all files in the current directory. It displays them with a maximum depth of 1 level in the documentation hierarchy.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```{toctree}\n:glob:\n:maxdepth: 1\n\n*\n```\n```\n\n----------------------------------------\n\nTITLE: Cloning the Optax Repository\nDESCRIPTION: Git command to clone the Optax source code repository from GitHub for development or contribution purposes.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/README.md#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/google-deepmind/optax.git\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Optax Gallery\nDESCRIPTION: ReStructuredText markup defining the structure and layout of the Optax examples gallery documentation. Includes toctree configuration and HTML markup for thumbnail display.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/gallery.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _gallery:\n\n🖼️ Example gallery\n==================\n\n.. toctree::\n   :glob:\n   :hidden:\n   :maxdepth: 1\n\n   _collections/examples/README.md\n   _collections/examples/contrib/README.md\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for SAM Implementation in Python\nDESCRIPTION: Imports necessary libraries including JAX, NumPy, Matplotlib, Optax, and Chex for implementing and visualizing Sharpness-Aware Minimization (SAM) optimizers.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/contrib/sam.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as np\nimport matplotlib.pyplot as plt\nimport optax\nimport chex\nfrom optax import contrib\n```\n\n----------------------------------------\n\nTITLE: Installing Optax from PyPI\nDESCRIPTION: Command to install the latest release version of Optax from the Python Package Index (PyPI).\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install optax\n```\n\n----------------------------------------\n\nTITLE: Installing Optax Development Version\nDESCRIPTION: Command to install the latest development version of Optax directly from GitHub using pip.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install git+https://github.com/google-deepmind/optax.git\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Meta-Learning with Optax\nDESCRIPTION: Imports necessary libraries for meta-learning, including JAX, Optax, and visualization tools.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/meta_learning.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Callable\nfrom typing import Iterator, Tuple\nimport chex\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax\n```\n\n----------------------------------------\n\nTITLE: Building Optax Documentation Locally\nDESCRIPTION: Commands for building the Sphinx-based documentation for Optax locally. Includes installing requirements and options for building with or without executing examples.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/development.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[docs]\"\nmake html       # builds everything\nmake html-noplot # faster, skips running examples\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Setup\nDESCRIPTION: Imports required libraries and sets up environment configurations for JAX and TensorFlow. Hides GPUs from TensorFlow to prevent memory conflicts with JAX.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/adversarial_training.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nimport jax\nfrom jax import numpy as jnp\nfrom flax import linen as nn\n\nimport optax\nfrom optax.losses import softmax_cross_entropy_with_integer_labels\nfrom optax.tree_utils import tree_l2_norm\n\nfrom matplotlib import pyplot as plt\nplt.rcParams.update({\"font.size\": 22})\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# Hide any GPUs from TensorFlow. Otherwise TF might reserve memory and make\n# it unavailable to JAX.\ntf.config.experimental.set_visible_devices([], \"GPU\")\n\n# Show on which platform JAX is running.\nprint(\"JAX running on\", jax.devices()[0].platform.upper())\n```\n\n----------------------------------------\n\nTITLE: Importing JAX and Optax Dependencies for Perturbed Optimizers\nDESCRIPTION: Imports the necessary libraries including JAX for numerical computation, tree utilities for handling nested structures, and Optax's perturbation module for implementing differentiable approximations.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/perturbations.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport operator\nfrom jax import tree_util as jtu\n\nfrom optax import tree_utils as otu\nfrom optax import perturbations\n```\n\n----------------------------------------\n\nTITLE: Formatting References in Optax Documentation using reStructuredText\nDESCRIPTION: Example of how to format references in Optax documentation using reStructuredText syntax rather than Markdown. This demonstrates the proper format for citations with links.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/development.md#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n```Doe et al `Yet another optimizer <link to optimizer>`_, 2042)```\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Optax Updates\nDESCRIPTION: ReStructuredText documentation layout defining the module reference and function documentation for Optax's update operations. Includes autosummary directive and individual function documentation sections.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/api/apply_updates.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: optax\n\n.. autosummary::\n    apply_updates\n    incremental_update\n    periodic_update\n\nApply updates\n~~~~~~~~~~~~~~~~~\n.. autofunction:: apply_updates\n\nIncremental update\n~~~~~~~~~~~~~~~~~~\n.. autofunction:: incremental_update\n\nPeriodic update\n~~~~~~~~~~~~~~~\n.. autofunction:: periodic_update\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Toctree for Optax Examples\nDESCRIPTION: A Sphinx toctree directive that automatically includes all files in the current directory. The directive is configured to have a maximum depth of 1 and uses a glob pattern to include all files.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/examples/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{toctree}\n:glob:\n:maxdepth: 1\n\n*\n```\n\n----------------------------------------\n\nTITLE: Installing Optax from GitHub\nDESCRIPTION: Command to install the most recent development version of Optax directly from the GitHub repository.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install git+git://github.com/google-deepmind/optax.git\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Hungarian Algorithm\nDESCRIPTION: ReStructuredText documentation structure defining the module reference and function documentation for the Hungarian algorithm implementation in optax.\nSOURCE: https://github.com/google-deepmind/optax/blob/main/docs/api/assignment.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: optax.assignment\n\n.. autosummary::\n    hungarian_algorithm\n\n\nHungarian algorithm\n~~~~~~~~~~~~~~~~~~~\n.. autofunction:: hungarian_algorithm\n```"
  }
]
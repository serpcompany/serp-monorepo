[
  {
    "owner": "googleapis",
    "repo": "python-bigquery",
    "content": "TITLE: Running a BigQuery script in Python\nDESCRIPTION: This snippet shows how to execute a BigQuery script written in standard SQL using Python, enabling multiple statements and scripting features in a batch. It leverages the google-cloud-bigquery library and supports complex operations within a single script execution.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/queries.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../samples/query_script.py\n   :language: python\n   :dedent: 4\n   :start-after: [START bigquery_query_script]\n   :end-before: [END bigquery_query_script]\n```\n\n----------------------------------------\n\nTITLE: Executing a parameterized query with named parameters in Python BigQuery\nDESCRIPTION: This code illustrates how to perform a parameterized BigQuery query using named parameters in Python, allowing dynamic input values for safer and more flexible queries. It depends on the google-cloud-bigquery library and provides an example of incorporating parameters in queries for improved security and usability.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/queries.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../samples/client_query_w_named_params.py\n   :language: python\n   :dedent: 4\n   :start-after: [START bigquery_query_params_named]\n   :end-before: [END bigquery_query_params_named]\n```\n\n----------------------------------------\n\nTITLE: Writing query results to a destination table in Python BigQuery\nDESCRIPTION: This snippet demonstrates how to run a query and write its results directly to a destination table in BigQuery, as per the recommendation in BigQuery's documentation. It requires setting the destination table in the query job configuration and uses the google-cloud-bigquery library. It enables automated result storage for subsequent analysis or processing.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/queries.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../samples/client_query_destination_table.py\n   :language: python\n   :dedent: 4\n   :start-after: [START bigquery_query_destination_table]\n   :end-before: [END bigquery_query_destination_table]\n```\n\n----------------------------------------\n\nTITLE: Performing BigQuery Query (Python)\nDESCRIPTION: Shows basic usage of the `google-cloud-bigquery` Python client to run a standard SQL query against a public dataset. Initializes a client, executes a query, waits for results, and iterates through rows to print data.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/README.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud import bigquery\n\nclient = bigquery.Client()\n\n# Perform a query.\nQUERY = (\n    'SELECT name FROM `bigquery-public-data.usa_names.usa_1910_2013` '\n    'WHERE state = \"TX\" '\n    'LIMIT 100')\nquery_job = client.query(QUERY)  # API request\nrows = query_job.result()  # Waits for query to finish\n\nfor row in rows:\n    print(row.name)\n```\n\n----------------------------------------\n\nTITLE: Running a query and waiting for completion with google-cloud-bigquery in Python\nDESCRIPTION: This snippet demonstrates how to execute a BigQuery query and wait for its completion using the Client.query_and_wait() method. It requires the google-cloud-bigquery library and a properly configured BigQuery client. The code's main purpose is to run a query synchronously and process the results after completion.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/queries.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../samples/snippets/client_query.py\n   :language: python\n   :dedent: 4\n   :start-after: [START bigquery_query]\n   :end-before: [END bigquery_query]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Table Rows as Pandas DataFrame in Python\nDESCRIPTION: Code reference to a snippet that demonstrates how to retrieve BigQuery table rows as a pandas DataFrame. This allows easy integration with pandas data analysis workflows.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/pandas.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Reference to an external snippet file\n# [START bigquery_list_rows_dataframe]\n# [END bigquery_list_rows_dataframe]\n```\n\n----------------------------------------\n\nTITLE: Initializing BigQuery Client with Explicit Project ID (Python)\nDESCRIPTION: This snippet demonstrates how to create an instance of the `google.cloud.bigquery.client.Client`. It explicitly specifies the Google Cloud project ID using the `project` parameter, which overrides the default project inferred from the environment (e.g., environment variables, GAE, GCE). Requires the `google-cloud-bigquery` library to be installed.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/client.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud import bigquery\nclient = bigquery.Client(project='PROJECT_ID')\n```\n\n----------------------------------------\n\nTITLE: Retrieving GEOGRAPHY Data as GeoPandas GeoDataFrame in Python\nDESCRIPTION: Code reference to a snippet that demonstrates how to retrieve BigQuery query results containing GEOGRAPHY data as a GeoPandas GeoDataFrame for geospatial analytics.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/pandas.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Reference to an external snippet file\n# [START bigquery_query_results_geodataframe]\n# [END bigquery_query_results_geodataframe]\n```\n\n----------------------------------------\n\nTITLE: Listing BigQuery Jobs in Python\nDESCRIPTION: This snippet demonstrates how to list BigQuery jobs for a given project using the `list_jobs` method of the BigQuery client. It requires the `google.cloud.bigquery` library. The output is a list of job objects representing the jobs in the project.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/jobs.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud import bigquery\n\n# Construct a BigQuery client object.\nclient = bigquery.Client()\n\n# List all jobs in specified project.\njobs = client.list_jobs()\n\nprint(\"Jobs running in project `{}`:\".format(client.project))\nfor job in jobs:\n    print(\"\\t{}\".format(job.job_id))\n```\n\n----------------------------------------\n\nTITLE: Writing BigQuery Query Results to CMEK-Protected Tables in Python\nDESCRIPTION: Executes a query and writes the results to a destination table encrypted with a customer-managed encryption key from Cloud KMS.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/encryption.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# [START bigquery_query_destination_table_cmek]\nfrom google.cloud import bigquery\n\n# Construct a BigQuery client object.\nclient = bigquery.Client()\n\n# TODO(developer): Set table_id to the ID of the destination table.\n# table_id = \"your-project.your_dataset.your_table_name\"\n\n# Set the encryption key to use for the destination.\n# TODO(developer): Replace this key with a key you have created in KMS.\nkms_key_name = \"projects/your-project/locations/us/keyRings/test/cryptoKeys/test\"\n\n# Configure the query job.\njob_config = bigquery.QueryJobConfig(\n    destination=table_id,\n    destination_encryption_configuration=bigquery.EncryptionConfiguration(\n        kms_key_name=kms_key_name\n    ),\n)\n\n# Example query uses a public dataset to determine the\n# top 10 baby names for US males in 2017\nsql = \"\"\"\n    SELECT name, count\n    FROM `bigquery-public-data.usa_names.usa_1910_current`\n    WHERE gender = 'M'\n    AND year = 2017\n    ORDER BY count DESC\n    LIMIT 10\n\"\"\"\n\nquery_job = client.query(sql, job_config=job_config)  # Make an API request.\n\nquery_job.result()  # Wait for the job to complete.\n\nprint(\"Query results loaded to the table {}.\".format(table_id))\n# [END bigquery_query_destination_table_cmek]\n```\n\n----------------------------------------\n\nTITLE: Performing a dry run query using python-bigquery\nDESCRIPTION: This code snippet shows how to perform a dry run of a BigQuery query in Python, which estimates costs and validates syntax without executing the query. It relies on the google-cloud-bigquery library. Useful for query validation before actual execution.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/queries.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../samples/client_query_dry_run.py\n   :language: python\n   :dedent: 4\n   :start-after: [START bigquery_query_dry_run]\n   :end-before: [END bigquery_query_dry_run]\n```\n\n----------------------------------------\n\nTITLE: Updating BigQuery Table Encryption Key in Python\nDESCRIPTION: Changes the customer-managed encryption key used for a BigQuery table by updating the table's encryption configuration.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/encryption.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# [START bigquery_update_table_cmek]\n# from google.cloud import bigquery\n# client = bigquery.Client()\n# table_ref = client.dataset('my_dataset').table('my_table')\n# table = client.get_table(table_ref)  # API request\n\n# TODO(developer): Set key_name to the fully qualified name of the CryptoKey\n# to be used for encrypting the table data.\nkey_name = \"projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[KEY_RING_NAME]/cryptoKeys/[KEY_NAME]\"\n\ntable.encryption_configuration = bigquery.EncryptionConfiguration(\n    kms_key_name=key_name\n)\n\ntable = client.update_table(\n    table, [\"encryption_configuration\"]\n)  # API request\n\nprint(\"Updated table {}.{}.{}, encryption key: {}\".format(\n    table.project, table.dataset_id, table.table_id, table.encryption_configuration.kms_key_name))\n# [END bigquery_update_table_cmek]\n```\n\n----------------------------------------\n\nTITLE: Installing Pandas for BigQuery in Python\nDESCRIPTION: Commands to install the pandas library or the BigQuery Python client with pandas support using pip.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/pandas.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade pandas\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade 'google-cloud-bigquery[pandas]'\n```\n\n----------------------------------------\n\nTITLE: Retrieving Query Results as Pandas DataFrame in Python\nDESCRIPTION: Code reference to a snippet that demonstrates how to retrieve BigQuery query results as a pandas DataFrame. This functionality is available via the to_dataframe() method.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/pandas.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Reference to an external snippet file\n# [START bigquery_query_results_dataframe]\n# [END bigquery_query_results_dataframe]\n```\n\n----------------------------------------\n\nTITLE: Loading BigQuery IPython Magics\nDESCRIPTION: Registers the BigQuery magics within the current IPython/Jupyter session using the `%load_ext` command. Executing this command makes the `%%bigquery` cell magic available for subsequent use. Requires the `google-cloud-bigquery[magics]` package to be installed.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/magics.rst#_snippet_0\n\nLANGUAGE: IPython\nCODE:\n```\n%load_ext bigquery_magics\n```\n\n----------------------------------------\n\nTITLE: Loading Pandas DataFrame to BigQuery Table in Python\nDESCRIPTION: Code reference to a snippet that demonstrates how to create a pandas DataFrame and load it into a new BigQuery table using the load_table_from_dataframe function.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/pandas.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Reference to an external snippet file\n# [START bigquery_load_table_dataframe]\n# [END bigquery_load_table_dataframe]\n```\n\n----------------------------------------\n\nTITLE: Installing Google BigQuery Python Client (Mac/Linux)\nDESCRIPTION: Provides console commands to set up a Python virtual environment and install the BigQuery client library and virtualenv on Mac/Linux systems. Requires pip and a working Python installation.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/README.rst#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install virtualenv\nvirtualenv <your-env>\nsource <your-env>/bin/activate\n<your-env>/bin/pip install google-cloud-bigquery\n```\n\n----------------------------------------\n\nTITLE: Performing a Basic Query with BigQuery Python Client\nDESCRIPTION: Example Python code demonstrating how to initialize a BigQuery client and execute a simple SQL query. The query retrieves names from a public dataset with filtering and limiting applied.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/README.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud import bigquery\n\nclient = bigquery.Client()\n\n# Perform a query.\nQUERY = (\n    'SELECT name FROM `bigquery-public-data.usa_names.usa_1910_2013` '\n    'WHERE state = \"TX\" '\n    'LIMIT 100')\nquery_job = client.query(QUERY)  # API request\nrows = query_job.result()  # Waits for query to finish\n\nfor row in rows:\n    print(row.name)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry for BigQuery Client (Console)\nDESCRIPTION: Provides the pip command to install the `google-cloud-bigquery` library with the `opentelemetry` extra, plus the GCP trace exporter package. These are required to enable tracing for BigQuery API calls.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/README.rst#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\npip install google-cloud-bigquery[opentelemetry] opentelemetry-exporter-gcp-trace\n```\n\n----------------------------------------\n\nTITLE: Using Named Parameters in BigQuery DB-API Python\nDESCRIPTION: Example of a SQL query using named parameters with the pyformat parameter style. Parameters are referenced by name, allowing for more readable and maintainable code.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/dbapi.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninsert into people (name, income) values (%(name)s, %(income)s)\n```\n\n----------------------------------------\n\nTITLE: Using Unnamed Parameters in BigQuery DB-API Python\nDESCRIPTION: Example of a SQL query using unnamed parameters with the qmark parameter style. This demonstrates the basic syntax for inserting values without specifying parameter names.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/dbapi.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ninsert into people (name, income) values (?, ?)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry with Cloud Trace for BigQuery\nDESCRIPTION: Python code example showing how to set up OpenTelemetry tracing with the Google Cloud Trace exporter. This configuration publishes all tracing data to the Google Cloud Trace console.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/README.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.cloud_trace import CloudTraceSpanExporter\ntracer_provider = TracerProvider()\ntracer_provider = BatchSpanProcessor(CloudTraceSpanExporter())\ntrace.set_tracer_provider(TracerProvider())\n```\n\n----------------------------------------\n\nTITLE: Using Explicit Type Information with Unnamed Parameters in BigQuery DB-API Python\nDESCRIPTION: Example of a SQL query with unnamed parameters that include explicit type information. This syntax allows specifying types without names for positional parameters.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/dbapi.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninsert into people (name, income) values (%(:string)s, %(:numeric)s)\n```\n\n----------------------------------------\n\nTITLE: Installing Google BigQuery Client on Windows\nDESCRIPTION: Command line instructions for installing the Google BigQuery Python client in a virtual environment on Windows systems. This creates an isolated Python environment to avoid dependency conflicts.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/README.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npip install virtualenv\nvirtualenv <your-env>\n<your-env>\\Scripts\\activate\n<your-env>\\Scripts\\pip.exe install google-cloud-bigquery\n```\n\n----------------------------------------\n\nTITLE: Creating BigQuery Tables with Customer Managed Encryption Keys in Python\nDESCRIPTION: Creates a new BigQuery table using a customer-managed encryption key from Cloud KMS for encryption.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/encryption.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [START bigquery_create_table_cmek]\n# Create a table using a Cloud KMS customer-managed encryption key.\n\n# TODO(developer): Set key_name to the fully qualified name of the CryptoKey\n# to be used for encrypting the table data.\nkey_name = \"projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[KEY_RING_NAME]/cryptoKeys/[KEY_NAME]\"\n\ntable_ref = dataset_ref.table(table_id)\n\ntable = bigquery.Table(table_ref)\n# Set a schema\ntable.schema = [\n    bigquery.SchemaField(\"full_name\", \"STRING\"),\n    bigquery.SchemaField(\"age\", \"INTEGER\"),\n]\n# Set the encryption key\ntable.encryption_configuration = bigquery.EncryptionConfiguration(\n    kms_key_name=key_name\n)\n\ntable = client.create_table(table)  # Make an API request.\n\nprint(\"Created table {}.{}.{} with encryption key: {}\".format(\n    table.project, table.dataset_id, table.table_id, key_name))\n# [END bigquery_create_table_cmek]\n```\n\n----------------------------------------\n\nTITLE: Installing BigQuery Client (Version 3.x+)\nDESCRIPTION: Shows the standard command to install `google-cloud-bigquery` version 3.x and later. The previously optional `google-cloud-bigquery-storage` and `pyarrow` dependencies (covered by the old `bqstorage` extra) are now required and included automatically. The `bignumeric_type` extra is also removed as support is built-in.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/UPGRADING.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install google-cloud-bigquery\n```\n\n----------------------------------------\n\nTITLE: Loading Data into BigQuery Tables with CMEK from Cloud Storage in Python\nDESCRIPTION: Loads data from a JSON file in Cloud Storage into a BigQuery table using a customer-managed encryption key for the destination table.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/encryption.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [START bigquery_load_table_gcs_json_cmek]\nfrom google.cloud import bigquery\n\n# Construct a BigQuery client object.\nclient = bigquery.Client()\n\n# TODO(developer): Set table_id to the ID of the table to create.\n# table_id = \"your-project.your_dataset.your_table_name\"\n\n# Set the encryption key to use for the destination.\n# TODO(developer): Replace this key with a key you have created in KMS.\nkms_key_name = \"projects/your-project/locations/us/keyRings/test/cryptoKeys/test\"\n\njob_config = bigquery.LoadJobConfig(\n    autodetect=True, source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n)\njob_config.destination_encryption_configuration = bigquery.EncryptionConfiguration(\n    kms_key_name=kms_key_name\n)\n\nuri = \"gs://cloud-samples-data/bigquery/us-states/us-states.json\"\n\nload_job = client.load_table_from_uri(\n    uri, table_id, job_config=job_config\n)  # Make an API request.\n\nload_job.result()  # Waits for the job to complete.\n\ndestination_table = client.get_table(table_id)  # Make an API request.\nprint(\"Loaded {} rows.\".format(destination_table.num_rows))\n# [END bigquery_load_table_gcs_json_cmek]\n```\n\n----------------------------------------\n\nTITLE: Copying BigQuery Tables with Customer Managed Encryption Keys in Python\nDESCRIPTION: Copies a source BigQuery table to a destination table while applying a customer-managed encryption key to the destination table.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/encryption.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# [START bigquery_copy_table_cmek]\nfrom google.cloud import bigquery\n\n# Construct a BigQuery client object.\nclient = bigquery.Client()\n\n# TODO(developer): Set source_table_id to the ID of the original table.\n# source_table_id = \"your-project.source_dataset.source_table\"\n\n# TODO(developer): Set destination_table_id to the ID of the destination table.\n# destination_table_id = \"your-project.destination_dataset.destination_table\"\n\n# Set the encryption key to use for the destination.\n# TODO(developer): Replace this key with a key you have created in KMS.\nkms_key_name = \"projects/your-project/locations/us/keyRings/test/cryptoKeys/test\"\n\njob_config = bigquery.CopyJobConfig()\njob_config.destination_encryption_configuration = bigquery.EncryptionConfiguration(\n    kms_key_name=kms_key_name\n)\n\njob = client.copy_table(\n    source_table_id, destination_table_id, job_config=job_config\n)  # Make an API request.\njob.result()  # Wait for the job to complete.\n\nprint(\"A copy of the table created.\")\n# [END bigquery_copy_table_cmek]\n```\n\n----------------------------------------\n\nTITLE: Checking BigQuery Schema Field Type Kind (Version 3.x+ Python)\nDESCRIPTION: Shows the updated method for checking a schema field's type in `google-cloud-bigquery` version 3.x. It uses the renamed and non-nested `StandardSqlTypeNames` enum, imported directly from `google.cloud.bigquery`.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/UPGRADING.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\nfrom google.cloud.bigquery import StandardSqlTypeNames\n\nif field_type == StandardSqlTypeNames.STRING:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Iterating over QueryJob to get results in Python BigQuery\nDESCRIPTION: Demonstrates how to iterate over a QueryJob to wait for and retrieve query results. Added in version 0.29.0.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CHANGELOG.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nquery_job = client.query(\"SELECT * FROM my_dataset.my_table LIMIT 10\")\n# Iterate over rows to get results\nfor row in query_job:\n    print(row)\n```\n\n----------------------------------------\n\nTITLE: Accessing BigQuery GAPIC Enums (Version 2.0.0+ Python)\nDESCRIPTION: Demonstrates the updated import path for GAPIC enum types introduced in `google-cloud-bigquery` version 2.0.0. Enums like `Model.DistanceType` are now located directly under the `google.cloud.bigquery_v2.types` module.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/UPGRADING.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud.bigquery_v2 import types\n\ndistance_type = types.Model.DistanceType.COSINE\n```\n\n----------------------------------------\n\nTITLE: Using to_dataframe() with QueryJob in Python BigQuery\nDESCRIPTION: When Pandas is installed, this method converts query results to a DataFrame containing the query's rows. Part of the interface additions in version 0.29.0.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CHANGELOG.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nquery_job = client.query(\"SELECT * FROM my_dataset.my_table LIMIT 10\")\ndf = query_job.to_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Importing StandardSqlDataType, StandardSqlField, StandardSqlStructType - Python\nDESCRIPTION: This snippet shows the change in the import path for `StandardSqlDataType`, `StandardSqlField`, and `StandardSqlStructType` in google-cloud-bigquery version 3.x. The old and new import locations are displayed for comparison.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/UPGRADING.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom google.cloud.bigquery_v2 import StandardSqlDataType\nfrom google.cloud.bigquery_v2.types import StandardSqlField\nfrom google.cloud.bigquery_v2.types.standard_sql import StandardSqlStructType\n```\n\nLANGUAGE: Python\nCODE:\n```\nfrom google.cloud.bigquery import StandardSqlDataType\nfrom google.cloud.bigquery.standard_sql import StandardSqlField\nfrom google.cloud.bigquery.standard_sql import StandardSqlStructType\n```\n\n----------------------------------------\n\nTITLE: Using Row dictionary-like methods in Python BigQuery\nDESCRIPTION: Demonstrates the dictionary-like methods added to the Row class in version 0.29.0, allowing access to row data like a built-in dictionary.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CHANGELOG.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nquery_job = client.query(\"SELECT * FROM my_dataset.my_table LIMIT 1\")\nrow = list(query_job)[0]\n\n# Get all keys\nkeys = row.keys()\n\n# Get all items as (key, value) pairs\nitems = row.items()\n\n# Get a specific field with fallback\nvalue = row.get('field_name', 'default_value')\n```\n\n----------------------------------------\n\nTITLE: Installing BigQuery Client with bignumeric_type Extra (Pre-3.0.0)\nDESCRIPTION: Illustrates the command used in versions before 3.0.0 to install the `google-cloud-bigquery` library with explicit support for the `BIGNUMERIC` data type via an extra. This extra is removed in v3.x as the support is now included by default.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/UPGRADING.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install google-cloud-bigquery[bignumeric_type]\n```\n\n----------------------------------------\n\nTITLE: Inserting rows using Client.insert_rows() in Python BigQuery\nDESCRIPTION: Shows how to insert rows into a BigQuery table using the newer insert_rows() method that replaced create_rows() in version 0.29.0.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CHANGELOG.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntable = client.get_table('my_dataset.my_table')\nrows_to_insert = [\n    {'field1': 'value1', 'field2': 'value2'},\n    {'field1': 'value3', 'field2': 'value4'}\n]\nerrors = client.insert_rows(table, rows_to_insert)\nif not errors:\n    print('Rows inserted successfully')\n```\n\n----------------------------------------\n\nTITLE: Using Row class with multiple access methods in Python BigQuery\nDESCRIPTION: Demonstrates the flexible Row class introduced in version 0.28.0 that allows accessing fields by integer index, string index, or attribute access.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CHANGELOG.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nquery_job = client.query('SELECT name, age FROM my_dataset.my_table LIMIT 1')\nrow = list(query_job)[0]\n\n# Access by integer index (tuple-like)\nname = row[0]\n\n# Access by string index (dict-like)\nage = row['age']\n\n# Access by attribute name (object-like)\nname_again = row.name\n```\n\n----------------------------------------\n\nTITLE: Checking BigQuery Schema Field Type Kind (Pre-3.0.0 Python)\nDESCRIPTION: Illustrates how to check the type of a schema field by comparing against the `TypeKind` enum nested within `StandardSqlDataType`, as done in `google-cloud-bigquery` versions before 3.0.0. Requires importing `StandardSqlDataType` from `google.cloud.bigquery_v2`.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/UPGRADING.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud.bigquery_v2 import StandardSqlDataType\n\nif field_type == StandardSqlDataType.TypeKind.STRING:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Using external table configuration in Python BigQuery\nDESCRIPTION: Shows how to configure an external table in BigQuery, a feature added in version 0.28.0 that allows querying data stored in external sources.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CHANGELOG.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom google.cloud.bigquery import ExternalConfig\n\n# Configure an external data source\nexternal_config = ExternalConfig('CSV')\nexternal_config.source_uris = ['gs://my-bucket/path/to/file.csv']\nexternal_config.schema = [\n    SchemaField('name', 'STRING'),\n    SchemaField('age', 'INTEGER')\n]\n\n# Create table with external configuration\ntable = bigquery.Table('my_dataset.my_ext_table')\ntable.external_data_configuration = external_config\ntable = client.create_table(table)\n```\n\n----------------------------------------\n\nTITLE: Using DB-API interface with Python BigQuery\nDESCRIPTION: Demonstrates using the google.cloud.bigquery.dbapi package that implements the PEP-249 DB-API specification, introduced in version 0.26.0.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CHANGELOG.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom google.cloud.bigquery import dbapi\n\n# Connect to BigQuery\nconn = dbapi.connect(project='my-project')\ncursor = conn.cursor()\n\n# Execute query\ncursor.execute(\"SELECT name, age FROM my_dataset.my_table WHERE age > %s\", (25,))\n\n# Fetch results\nfor row in cursor.fetchall():\n    print(f\"Name: {row[0]}, Age: {row[1]}\")\n\n# Close connection\nconn.close()\n```\n\n----------------------------------------\n\nTITLE: Sphinx Automodule Directive for google.cloud.bigquery.job.base\nDESCRIPTION: This reStructuredText snippet uses the Sphinx `automodule` directive to automatically generate documentation for the Python module `google.cloud.bigquery.job.base`. The `:members:` option ensures that all public members (classes, functions, etc.) within that module are included in the generated documentation.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/job_base.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: google.cloud.bigquery.job.base\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Installing BigQuery with Pandas and PyArrow in Python\nDESCRIPTION: Command to install the BigQuery Python client with pandas and pyarrow support, which is required for loading DataFrames to BigQuery tables.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/usage/pandas.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade google-cloud-bigquery[pandas,pyarrow]\n```\n\n----------------------------------------\n\nTITLE: Accessing BigQuery GAPIC Enums (Pre-2.0.0 Python)\nDESCRIPTION: Shows how GAPIC enum types, such as `Model.DistanceType`, were imported and accessed from the `google.cloud.bigquery_v2.gapic.enums` module in `google-cloud-bigquery` versions before 2.0.0.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/UPGRADING.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud.bigquery_v2.gapic import enums\n\ndistance_type = enums.Model.DistanceType.COSINE\n```\n\n----------------------------------------\n\nTITLE: Setting Up Authentication for Google BigQuery Python Client\nDESCRIPTION: This snippet explains the requirement for authentication setup to use the Google BigQuery Python client. It directs users to the official authentication guide for detailed instructions on configuring credentials, which is essential for authorized access to Google Cloud resources.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/scripts/readme-gen/templates/auth.tmpl.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nRefer to the `Authentication Getting Started Guide`_ for instructions on setting up credentials for applications.\n\n.. _Authentication Getting Started Guide:\n    https://cloud.google.com/docs/authentication/getting-started\n```\n\n----------------------------------------\n\nTITLE: Importing Model from google.cloud.bigquery_v2 - Python\nDESCRIPTION: This snippet shows how to import the legacy `Model` type from `google.cloud.bigquery_v2`.  It's noted that these types are not maintained and may differ from current implementations.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/UPGRADING.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom google.cloud.bigquery_v2 import Model  # a sublcass of proto.Message\n```\n\n----------------------------------------\n\nTITLE: Installing Google BigQuery Python Client (Windows)\nDESCRIPTION: Provides console commands to set up a Python virtual environment and install the BigQuery client library and virtualenv on Windows systems. Requires pip and a working Python installation.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/README.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npip install virtualenv\nvirtualenv <your-env>\n<your-env>\\Scripts\\activate\n<your-env>\\Scripts\\pip.exe install google-cloud-bigquery\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Nox\nDESCRIPTION: Commands for running unit tests using the nox testing tool, including running all tests and individual tests.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CONTRIBUTING.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ nox -s unit\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ nox -s unit-3.13 -- -k <name of test>\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Trace Exporter (Python)\nDESCRIPTION: Demonstrates how to configure OpenTelemetry in Python to use the `CloudTraceSpanExporter`. This sets up the tracer provider and a batch span processor to send tracing data to Google Cloud Trace.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/README.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.cloud_trace import CloudTraceSpanExporter\ntracer_provider = TracerProvider()\ntracer_provider = BatchSpanProcessor(CloudTraceSpanExporter())\ntrace.set_tracer_provider(TracerProvider())\n```\n\n----------------------------------------\n\nTITLE: Running System Tests\nDESCRIPTION: Commands for executing system tests against an actual Google Cloud project using different Python versions.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CONTRIBUTING.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Run all system tests\n$ nox -s system\n\n# Run a single system test\n$ nox -s system-3.13 -- -k <name of test>\n```\n\n----------------------------------------\n\nTITLE: Installing Google BigQuery Client on Mac/Linux\nDESCRIPTION: Command line instructions for installing the Google BigQuery Python client in a virtual environment on Mac/Linux systems. This creates an isolated Python environment to avoid dependency conflicts.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/README.rst#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install virtualenv\nvirtualenv <your-env>\nsource <your-env>/bin/activate\n<your-env>/bin/pip install google-cloud-bigquery\n```\n\n----------------------------------------\n\nTITLE: Running Sample Tests\nDESCRIPTION: Commands for testing code samples and snippets located in the samples/ directory.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CONTRIBUTING.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Run all tests in a folder\n$ cd samples/snippets\n$ nox -s py-3.9\n\n# Run a single sample test\n$ cd samples/snippets\n$ nox -s py-3.9 -- -k <name of test>\n```\n\n----------------------------------------\n\nTITLE: Run BigQuery Quickstart Sample (Bash)\nDESCRIPTION: Executes the `quickstart.py` script, which provides a basic demonstration of interacting with Google BigQuery using the Python client library. This command assumes the user is in the correct directory (likely the sample directory) and the virtual environment is active with dependencies installed. The output will be the result of the quickstart script's operations.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/samples/snippets/README.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython quickstart.py\n```\n\n----------------------------------------\n\nTITLE: Code Formatting and Linting\nDESCRIPTION: Commands for running the code formatter (black) and linting checks to ensure code quality and compliance with project standards.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CONTRIBUTING.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ nox -s blacken\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ nox -s lint\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport GOOGLE_CLOUD_TESTING_REMOTE=\"upstream\"\nexport GOOGLE_CLOUD_TESTING_BRANCH=\"main\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ pre-commit install\npre-commit installed at .git/hooks/pre-commit\n```\n\n----------------------------------------\n\nTITLE: Converting Model.TrainingRun to dictionary - Python\nDESCRIPTION: This snippet shows how to convert a `Model.TrainingRun` object (from `google.cloud.bigquery_v2`) to a dictionary for compatibility with `google-cloud-bigquery` version 3.x. It utilizes the `to_dict()` method.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/UPGRADING.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom google.cloud.bigquery_v2 import Model\n\ntraining_run: Model.TrainingRun = ...\ntraining_run_dict = training_run.to_dict()\n```\n\n----------------------------------------\n\nTITLE: Importing GAPIC Enums - Python\nDESCRIPTION: Illustrates the changed import path for GAPIC enum types in `google-cloud-bigquery` 2.0.0.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/UPGRADING.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom google.cloud.bigquery_v2.gapic import enums\n\ndistance_type = enums.Model.DistanceType.COSINE\n```\n\nLANGUAGE: Python\nCODE:\n```\nfrom google.cloud.bigquery_v2 import types\n\ndistance_type = types.Model.DistanceType.COSINE\n```\n\n----------------------------------------\n\nTITLE: Install BigQuery Python Sample Dependencies (Bash)\nDESCRIPTION: Installs all necessary Python libraries for running the BigQuery samples by using `pip` and the provided `requirements.txt` file. This step ensures all required packages like `google-cloud-bigquery` are available within the activated virtual environment. Requires `pip` and the `requirements.txt` file to be present in the current directory.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/samples/snippets/README.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Create and Activate Python Virtual Environment (Bash)\nDESCRIPTION: Sets up a dedicated Python environment using `virtualenv` to manage dependencies separately from the system Python installation. The second command activates this environment, making its Python interpreter and installed packages available for use with the samples. Requires `virtualenv` installed globally.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/samples/snippets/README.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nvirtualenv env\n$ source env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Clone Google Cloud Python Samples (Bash)\nDESCRIPTION: Clones the Google Cloud Platform Python documentation samples repository from GitHub to obtain the BigQuery code examples. This is the initial step to get the sample files locally. Requires Git to be installed.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/samples/snippets/README.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/GoogleCloudPlatform/python-docs-samples.git\n```\n\n----------------------------------------\n\nTITLE: Accessing TypeKind enum - Python\nDESCRIPTION: This snippet shows how to access the `TypeKind` enum (renamed to `StandardSqlTypeNames`) in google-cloud-bigquery version 3.x. The old and new access methods are displayed for comparison.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/UPGRADING.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom google.cloud.bigquery_v2 import StandardSqlDataType\n\nif field_type == StandardSqlDataType.TypeKind.STRING:\n    ...\n```\n\nLANGUAGE: Python\nCODE:\n```\nfrom google.cloud.bigquery import StandardSqlTypeNames\n\nif field_type == StandardSqlTypeNames.STRING:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Run BigQuery Simple Application Sample (Bash)\nDESCRIPTION: Executes the `simple_app.py` script, which demonstrates a straightforward BigQuery operation using the Python client library. This command requires the correct directory, an active virtual environment, and installed dependencies. The script will perform its specific BigQuery task and potentially output results or status.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/samples/snippets/README.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython simple_app.py\n```\n\n----------------------------------------\n\nTITLE: Run BigQuery User Credentials Sample (Bash)\nDESCRIPTION: Executes the `user_credentials.py` script, designed to run a BigQuery query authenticated using user credentials, typically via an OAuth flow. This command requires a project ID as a positional argument and may need access to a client secrets file or require launching a browser for the authentication process. The script will run the query using the authenticated credentials and output the result.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/samples/snippets/README.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython user_credentials.py\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies in Bash\nDESCRIPTION: Command to install all the dependencies needed to run the BigQuery Python samples using pip and the requirements.txt file.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/scripts/readme-gen/templates/install_deps.tmpl.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Defining Project Dependencies (requirements.txt format)\nDESCRIPTION: This lists the Python package dependencies and their pinned versions required to run the `/googleapis/python-bigquery` project. Key libraries include `google-cloud-bigquery` for BigQuery interaction, `pandas` for data handling, `ipython` for interactive use, and supporting packages like `db-dtypes` and `google-cloud-bigquery-storage`.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/samples/magics/requirements.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nbigquery_magics==0.9.0\ndb-dtypes==1.4.2\ngoogle.cloud.bigquery==3.31.0\ngoogle-cloud-bigquery-storage==2.30.0\nipython===8.18.1\npandas==2.2.3\n```\n\n----------------------------------------\n\nTITLE: Creating a Python Virtual Environment in Bash\nDESCRIPTION: Commands to create and activate a Python virtual environment for isolating the sample project dependencies. The samples are compatible with Python 3.9+.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/scripts/readme-gen/templates/install_deps.tmpl.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ virtualenv env\n$ source env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Cloning the Python Samples Repository in Bash\nDESCRIPTION: Commands to clone the Google Cloud Platform Python documentation samples repository which contains BigQuery code examples.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/scripts/readme-gen/templates/install_deps.tmpl.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/GoogleCloudPlatform/python-docs-samples.git\n```\n\n----------------------------------------\n\nTITLE: Including Code for a Parameterized BigQuery Query (rst)\nDESCRIPTION: Uses the Sphinx `literalinclude` directive to embed a Python code sample from an external file (`./samples/magics/query_params_scalars.py`). This sample demonstrates how to run a parameterized SQL query against BigQuery using the `%%bigquery` magic with scalar parameters passed from the Python environment. The directive specifies removing 4 spaces of leading indentation (`dedent`) and includes only the code between the specified start and end markers.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/magics.rst#_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. literalinclude:: ./samples/magics/query_params_scalars.py\n   :dedent: 4\n   :start-after: [START bigquery_jupyter_query_params_scalars]\n   :end-before: [END bigquery_jupyter_query_params_scalars]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry for BigQuery Tracing\nDESCRIPTION: Command line instructions for installing the necessary packages to enable OpenTelemetry tracing with the BigQuery client. This allows for monitoring and analyzing API calls to BigQuery.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/README.rst#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\npip install google-cloud-bigquery[opentelemetry] opentelemetry-exporter-gcp-trace\n```\n\n----------------------------------------\n\nTITLE: Including Code for a Standard BigQuery Query (rst)\nDESCRIPTION: Uses the Sphinx `literalinclude` directive to embed a Python code sample from an external file (`./samples/magics/query.py`). This sample demonstrates how to run a standard SQL query against BigQuery using the `%%bigquery` magic. The directive specifies removing 4 spaces of leading indentation (`dedent`) and includes only the code between the specified start and end markers.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/magics.rst#_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. literalinclude:: ./samples/magics/query.py\n   :dedent: 4\n   :start-after: [START bigquery_jupyter_query]\n   :end-before: [END bigquery_jupyter_query]\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark Script with Environment Variables - Python Shell Invocation\nDESCRIPTION: Shows how to run the benchmarking script while using shell substitutions for dynamic project or benchmark settings. Environment variables and subcommands provide values for benchmarking, enhancing automation and reproducibility. All required dependencies and access privileges must be in place. This approach allows integration with CI/CD systems or dynamic environments such as development or testing branches.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/benchmark/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython benchmark.py \\\n  --reruns 5 \\\n  --table $BENCHMARK_TABLE \\\n  --tag origin:$(hostname) \\\n  --tag branch:$(git branch --show-current) \\\n  --tag latestcommit:$(git log --pretty=format:'%H' -n 1)\n```\n\n----------------------------------------\n\nTITLE: Installing PortAudio on Mac OS X using Homebrew\nDESCRIPTION: This snippet installs PortAudio on macOS using the Homebrew package manager. This is a prerequisite for installing PyAudio, which enables audio streaming from the microphone. The command downloads and installs the necessary PortAudio libraries and dependencies.  The primary output is the installation of the portaudio library.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/scripts/readme-gen/templates/install_portaudio.tmpl.rst#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nbrew install portaudio\n```\n\n----------------------------------------\n\nTITLE: BigQuery Benchmark Result Table Schema Definition - JSON\nDESCRIPTION: Defines the JSON schema for the BigQuery table where benchmarking results are streamed. This schema captures fields such as group name, benchmark name, an array of key-value tags, the SQL query, multiple runs with their respective details (errors, timing, row counts), and a global event timestamp. It is required for creating the result table either manually or programmatically, ensuring consistency for data analysis and possible integration with benchmarking tools in other languages.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/benchmark/README.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"groupname\",\n    \"type\": \"STRING\"\n  },\n  {\n    \"name\": \"name\",\n    \"type\": \"STRING\"\n  },\n  {\n    \"name\": \"tags\",\n    \"type\": \"RECORD\",\n    \"mode\": \"REPEATED\",\n    \"fields\": [\n      {\n        \"name\": \"key\",\n        \"type\": \"STRING\"\n      },\n      {\n        \"name\": \"value\",\n        \"type\": \"STRING\"\n      }\n    ]\n  },\n  {\n    \"name\": \"SQL\",\n    \"type\": \"STRING\"\n  },\n  {\n    \"name\": \"runs\",\n    \"type\": \"RECORD\",\n    \"mode\": \"REPEATED\",\n    \"fields\": [\n      {\n        \"name\": \"errorstring\",\n        \"type\": \"STRING\"\n      },\n      {\n        \"name\": \"start_time\",\n        \"type\": \"TIMESTAMP\"\n      },\n      {\n        \"name\": \"query_end_time\",\n        \"type\": \"TIMESTAMP\"\n      },\n      {\n        \"name\": \"first_row_returned_time\",\n        \"type\": \"TIMESTAMP\"\n      },\n      {\n        \"name\": \"all_rows_returned_time\",\n        \"type\": \"TIMESTAMP\"\n      },\n      {\n        \"name\": \"total_rows\",\n        \"type\": \"INTEGER\"\n      }\n    ]\n  },\n  {\n    \"name\": \"event_time\",\n    \"type\": \"TIMESTAMP\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Using Explicit Type Information with Named Parameters in BigQuery DB-API Python\nDESCRIPTION: Example of a SQL query with named parameters that include explicit type information. This is useful when BigQuery cannot determine types automatically or determines them incorrectly.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/dbapi.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninsert into people (name, income) values (%(name:string)s, %(income:numeric)s)\n```\n\n----------------------------------------\n\nTITLE: Passing Struct Data in BigQuery DB-API Python\nDESCRIPTION: Example of passing struct data in a SQL query using explicit type information. This is the only way to pass structured data to BigQuery through the DB-API.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/dbapi.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncursor.execute(\n  \"insert into points (point) values (%(:struct<x float64, y float64>)s)\",\n  [{\"x\": 10, \"y\": 20}],\n  )\n```\n\n----------------------------------------\n\nTITLE: Cloning and Configuring the Repository\nDESCRIPTION: Commands for forking, cloning, and configuring the python-bigquery repository for local development.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CONTRIBUTING.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cd ${HOME}\n$ git clone git@github.com:USERNAME/python-bigquery.git hack-on-python-bigquery\n$ cd hack-on-python-bigquery\n# Configure remotes such that you can pull changes from the googleapis/python-bigquery\n# repository into your local repository.\n$ git remote add upstream git@github.com:googleapis/python-bigquery.git\n# fetch and merge changes from upstream into main\n$ git fetch upstream\n$ git merge upstream/main\n```\n\n----------------------------------------\n\nTITLE: Building Documentation\nDESCRIPTION: Command for building the project documentation using nox.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CONTRIBUTING.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ nox -s docs\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark Script with All Flags - Python Shell Invocation\nDESCRIPTION: Demonstrates how to invoke the Python benchmarking script with all command-line flags set, including reruns, project ID, result table configuration, table creation, and multiple tags. This example requires Python, the benchmark script, and valid Google Cloud credentials. The flags represent parameters for rerun counts, project selection, result destination, and labeling runs with key-value pairs. Inputs are specified via flags, and outputs are written to BigQuery if table settings are provided. Shell splitting and line continuations allow readability.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/benchmark/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython benchmark.py \\\n  --reruns 5 \\\n  --projectid test_project_id \\\n  --table logging_project_id.querybenchmarks.measurements \\\n  --create_table \\\n  --tag source:myhostname \\\n  --tag somekeywithnovalue \\\n  --tag experiment:special_environment_thing\n```\n\n----------------------------------------\n\nTITLE: Ignoring type annotations in google-cloud-bigquery - Python\nDESCRIPTION: This snippet demonstrates how to temporarily ignore type annotations in the google-cloud-bigquery package using a `# type: ignore` comment. It is suggested as a short-term workaround.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/UPGRADING.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom google.cloud import bigquery  # type: ignore\n```\n\n----------------------------------------\n\nTITLE: Using TableReference and DatasetReference in Python BigQuery\nDESCRIPTION: Shows how to use the TableReference and DatasetReference classes introduced in version 0.28.0 to work with table and dataset references.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CHANGELOG.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom google.cloud.bigquery import DatasetReference, TableReference\n\n# Get references\ndataset_ref = DatasetReference('my_project', 'my_dataset')\ntable_ref = TableReference(dataset_ref, 'my_table')\n\n# Get actual dataset/table objects\ndataset = client.get_dataset(dataset_ref)\ntable = client.get_table(table_ref)\n```\n\n----------------------------------------\n\nTITLE: Updating MANIFEST.in to Include 'tests' Directory in Python Package\nDESCRIPTION: This snippet addresses corrections to the inclusion of the 'tests' directory in the package manifest file, ensuring that test files are properly packaged. It also resolves the previous inclusion of 'unit_tests' which is replaced with 'tests'. The change is associated with issue #3552.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CHANGELOG.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nFix inclusion of `tests` (vs. `unit_tests`) in `MANIFEST.in` (#3552)\n```\n\n----------------------------------------\n\nTITLE: Updating Author Email in setup.py for Accurate Metadata\nDESCRIPTION: This snippet updates the author email address in the setup.py file to correctly point to 'googleapis-publisher@google.com'. This change ensures accurate contact information for package maintainers and is linked to issue #3598.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CHANGELOG.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nUpdating `author_email` in `setup.py` to `googleapis-publisher@google.com` (#3598)\n```\n\n----------------------------------------\n\nTITLE: Installing google-cloud-bigquery without bignumeric_type extra - Python\nDESCRIPTION: This snippet illustrates how to install the google-cloud-bigquery package without the `bignumeric_type` extra, as BIGNUMERIC type is now automatically supported in version 3.x. It also shows the former way.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/UPGRADING.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install google-cloud-bigquery[bignumeric_type]\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install google-cloud-bigquery\n```\n\n----------------------------------------\n\nTITLE: Installing google-cloud-bigquery without bqstorage extra - Python\nDESCRIPTION: This snippet shows how to install the google-cloud-bigquery package without the `bqstorage` extra, as it is now a no-op in version 3.x. The previous method of installing with the extra is also provided for comparison.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/docs/UPGRADING.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install google-cloud-bigquery[bqstorage]\n```\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install google-cloud-bigquery\n```\n\n----------------------------------------\n\nTITLE: Defining Test Dependencies for BigQuery Python Samples\nDESCRIPTION: Lists the required Python packages for running and testing BigQuery sample code. The file ensures that samples can be run without additional dependencies.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/samples/snippets/requirements-test.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# samples/snippets should be runnable with no \"extras\"\ngoogle-cloud-testutils==1.6.0\npytest==8.3.5\nmock==5.2.0\npytest-xdist==3.6.1\n```\n\n----------------------------------------\n\nTITLE: Converting Legacy BigQuery TrainingRun Proto to Dict (Python)\nDESCRIPTION: Provides an example of converting a legacy `Model.TrainingRun` object (a `proto.Message` subclass from `google.cloud.bigquery_v2`) into a JSON-compatible dictionary using its `to_dict()` method. This is useful for maintaining compatibility when supporting both v2.x and v3.x of the library.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/UPGRADING.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud.bigquery_v2 import Model\n\ntraining_run: Model.TrainingRun = ...\ntraining_run_dict = training_run.to_dict()\n```\n\n----------------------------------------\n\nTITLE: Ignoring Type Check Errors for BigQuery Client Import (Python)\nDESCRIPTION: Provides a workaround using a `# type: ignore` comment to suppress potential type checking errors from tools like `mypy` after upgrading to `google-cloud-bigquery` 3.x, which introduced type annotations. This is recommended only as a temporary measure if fixing type errors immediately is infeasible.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/UPGRADING.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud import bigquery  # type: ignore\n```\n\n----------------------------------------\n\nTITLE: Importing BigQuery SQL Types (Version 3.x+ Python)\nDESCRIPTION: Demonstrates the updated import statements required in `google-cloud-bigquery` version 3.x to access standard SQL data types (`StandardSqlDataType`, `StandardSqlField`, `StandardSqlStructType`). These types are now located directly under `google.cloud.bigquery` and `google.cloud.bigquery.standard_sql`.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/UPGRADING.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud.bigquery import StandardSqlDataType\nfrom google.cloud.bigquery.standard_sql import StandardSqlField\nfrom google.cloud.bigquery.standard_sql import StandardSqlStructType\n```\n\n----------------------------------------\n\nTITLE: Importing Legacy Proto-Based BigQuery Model Type (Python)\nDESCRIPTION: Demonstrates how to import the legacy protocol buffer-based `Model` type from `google.cloud.bigquery_v2`. Although available for compatibility in v3.x, this type is deprecated, unmaintained, may differ from current backend types, and importing it will issue a warning.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/UPGRADING.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud.bigquery_v2 import Model  # a sublcass of proto.Message\n```\n\n----------------------------------------\n\nTITLE: Importing BigQuery SQL Types (Pre-3.0.0 Python)\nDESCRIPTION: Shows the import statements used in `google-cloud-bigquery` versions before 3.0.0 to access standard SQL data types like `StandardSqlDataType`, `StandardSqlField`, and `StandardSqlStructType` from the `google.cloud.bigquery_v2` module and its submodules.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/UPGRADING.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud.bigquery_v2 import StandardSqlDataType\nfrom google.cloud.bigquery_v2.types import StandardSqlField\nfrom google.cloud.bigquery_v2.types.standard_sql import StandardSqlStructType\n```\n\n----------------------------------------\n\nTITLE: Installing PortAudio dependencies on Debian/Ubuntu Linux\nDESCRIPTION: This snippet installs the PortAudio development package and Python development tools on Debian/Ubuntu Linux. This ensures the necessary headers and libraries are available for building and running PyAudio. The command `apt-get install` installs the specified packages through the system's package manager.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/scripts/readme-gen/templates/install_portaudio.tmpl.rst#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\napt-get install portaudio19-dev python-all-dev\n```\n\n----------------------------------------\n\nTITLE: Installing BigQuery Client with bqstorage Extra (Pre-3.0.0)\nDESCRIPTION: Demonstrates the command used to install the `google-cloud-bigquery` library with the `bqstorage` extra before version 3.0.0. This extra installed dependencies like `google-cloud-bigquery-storage` and `pyarrow`, which became required dependencies in v3.x.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/UPGRADING.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install google-cloud-bigquery[bqstorage]\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting PyAudio Installation on Mac OS X with Homebrew\nDESCRIPTION: This snippet provides a workaround for potential issues during PyAudio installation on macOS, specifically when the system cannot find `portaudio.h`. It involves installing PyAudio with additional flags to specify include and library paths.  The flags configure the Python build process with the necessary PortAudio header and library locations.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/scripts/readme-gen/templates/install_portaudio.tmpl.rst#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npip install --global-option='build_ext' \\\n          --global-option='-I/usr/local/include' \\\n          --global-option='-L/usr/local/lib' \\\n          pyaudio\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for Google Cloud Project\nDESCRIPTION: This snippet lists the Python packages and their corresponding versions that are required for the project. It specifies dependencies needed for testing like google-cloud-testutils and pytest, as well as mocking and distribution support like mock and pytest-xdist respectively. The use of this information allows to build consistent and reproducible environment to run the bigquery projects.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/samples/notebooks/requirements-test.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ngoogle-cloud-testutils==1.6.0\npytest==8.3.5\nmock==5.2.0\npytest-xdist==3.6.1\n```\n\n----------------------------------------\n\nTITLE: No code snippets present\nDESCRIPTION: This file contains textual release notes and change descriptions; there are no executable code snippets to document.\nSOURCE: https://github.com/googleapis/python-bigquery/blob/main/CHANGELOG.md#_snippet_0\n\n"
  }
]
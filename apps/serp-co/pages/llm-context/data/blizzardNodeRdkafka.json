[
  {
    "owner": "blizzard",
    "repo": "node-rdkafka",
    "content": "TITLE: Implementing Flowing Kafka Consumer with Node-rdkafka\nDESCRIPTION: Demonstrates setting up a Kafka consumer that automatically flows messages using node-rdkafka. Includes error handling, offset commitment every 5 messages, and automatic disconnection after 30 seconds. The consumer connects to a local Kafka broker and subscribes to a 'test' topic.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/examples/consumer-flow.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*\n * node-rdkafka - Node.js wrapper for RdKafka C/C++ library\n *\n * Copyright (c) 2016 Blizzard Entertainment\n *\n * This software may be modified and distributed under the terms\n * of the MIT license.  See the LICENSE.txt file for details.\n */\n\nvar Kafka = require('../');\n\nvar consumer = new Kafka.KafkaConsumer({\n  //'debug': 'all',\n  'metadata.broker.list': 'localhost:9092',\n  'group.id': 'node-rdkafka-consumer-flow-example',\n  'enable.auto.commit': false\n});\n\nvar topicName = 'test';\n\n//logging debug messages, if debug is enabled\nconsumer.on('event.log', function(log) {\n  console.log(log);\n});\n\n//logging all errors\nconsumer.on('event.error', function(err) {\n  console.error('Error from consumer');\n  console.error(err);\n});\n\n//counter to commit offsets every numMessages are received\nvar counter = 0;\nvar numMessages = 5;\n\nconsumer.on('ready', function(arg) {\n  console.log('consumer ready.' + JSON.stringify(arg));\n\n  consumer.subscribe([topicName]);\n  //start consuming messages\n  consumer.consume();\n});\n\n\nconsumer.on('data', function(m) {\n  counter++;\n\n  //committing offsets every numMessages\n  if (counter % numMessages === 0) {\n    console.log('calling commit');\n    consumer.commit(m);\n  }\n\n  // Output the actual message contents\n  console.log(JSON.stringify(m));\n  console.log(m.value.toString());\n\n});\n\nconsumer.on('disconnected', function(arg) {\n  console.log('consumer disconnected. ' + JSON.stringify(arg));\n});\n\n//starting the consumer\nconsumer.connect();\n\n//stopping this example after 30s\nsetTimeout(function() {\n  consumer.disconnect();\n}, 30000);\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using a HighLevelProducer with node-rdkafka\nDESCRIPTION: This code demonstrates setting up a Kafka HighLevelProducer with custom serializers for keys and values. It connects to a Kafka broker, produces a message to a 'test' topic, and then properly disconnects the producer. The example shows asynchronous serialization using Promises and Buffer conversion.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/examples/high-level-producer.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nvar Kafka = require('../');\n\nvar producer = new Kafka.HighLevelProducer({\n  'metadata.broker.list': 'localhost:9092',\n});\n\n// Throw away the keys\nproducer.setKeySerializer(function(v) {\n  return new Promise((resolve, reject) => {\n    setTimeout(() => {\n      resolve(null);\n    }, 20);\n  });\n});\n\n// Take the message field\nproducer.setValueSerializer(function(v) {\n  return Buffer.from(v.message);\n});\n\nproducer.connect(null, function() {\n  producer.produce('test', null, {\n    message: 'alliance4ever',\n  }, null, Date.now(), function(err, offset) {\n    // The offset if our acknowledgement level allows us to receive delivery offsets\n    setImmediate(function() {\n      producer.disconnect();\n    });\n  });\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Standard Kafka Producer with Event Handling\nDESCRIPTION: Demonstrates setting up a standard Kafka producer with connection handling, message production, and error handling. Includes configuration for broker connection, message delivery, and polling setup.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = new Kafka.Producer({\n  'metadata.broker.list': 'localhost:9092',\n  'dr_cb': true\n});\n\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', () => {\n  try {\n    producer.produce(\n      'topic',\n      null,\n      Buffer.from('Awesome message'),\n      'Stormwind',\n      Date.now()\n    );\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n  }\n});\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', (err) => {\n  console.error('Error from producer');\n  console.error(err);\n})\n\nproducer.setPollInterval(100);\n```\n\n----------------------------------------\n\nTITLE: Initializing Kafka Producer with OAuth Bearer Token in Node.js\nDESCRIPTION: This snippet demonstrates how to create and configure a Kafka Producer using node-rdkafka with OAuth Bearer Token authentication. It sets up the producer, connects to the broker, and shows how to refresh the token.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/examples/oauthbearer-default-flow.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nvar Kafka = require('../');\n\nvar token = \"your_token\";\n\nvar producer = new Kafka.Producer({\n  //'debug' : 'all',\n  'metadata.broker.list': 'localhost:9093',\n  'security.protocol': 'SASL_SSL',\n\t'sasl.mechanisms': 'OAUTHBEARER',\n}).setOauthBearerToken(token);\n\n//start the producer\nproducer.connect();\n\n//refresh the token\nproducer.setOauthBearerToken(token);\n```\n\n----------------------------------------\n\nTITLE: Setting up Kafka ConsumerStream with OAuth Bearer Token in Node.js\nDESCRIPTION: This snippet illustrates the creation of a Kafka ConsumerStream using node-rdkafka with OAuth Bearer Token authentication. It configures the stream, sets the initial token, and shows how to refresh the token.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/examples/oauthbearer-default-flow.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nvar Kafka = require('../');\n\nvar token = \"your_token\";\n\nvar stream = Kafka.KafkaConsumer.createReadStream({\n        'metadata.broker.list': 'localhost:9093',\n        'group.id': 'myGroup',\n        'security.protocol': 'SASL_SSL',\n        'sasl.mechanisms': 'OAUTHBEARER'\n    }, {}, {\n        topics: 'test1',\n        initOauthBearerToken: token,\n    });\n\n//refresh the token\nstream.refreshOauthBearerToken(token.token);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Event Handlers for Kafka Producer\nDESCRIPTION: Configures event handlers for the producer including log events, error events, delivery reports, and connection status events. These handlers provide visibility into the producer's operation and message delivery status.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/examples/producer.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n//logging debug messages, if debug is enabled\nproducer.on('event.log', function(log) {\n  console.log(log);\n});\n\n//logging all errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n});\n\n//counter to stop this sample after maxMessages are sent\nvar counter = 0;\nvar maxMessages = 10;\n\nproducer.on('delivery-report', function(err, report) {\n  console.log('delivery-report: ' + JSON.stringify(report));\n  counter++;\n});\n\n//Wait for the ready event before producing\nproducer.on('ready', function(arg) {\n  console.log('producer ready.' + JSON.stringify(arg));\n\n  for (var i = 0; i < maxMessages; i++) {\n    var value = Buffer.from('value-' +i);\n    var key = \"key-\"+i;\n    // if partition is set to -1, librdkafka will use the default partitioner\n    var partition = -1;\n    var headers = [\n      { header: \"header value\" }\n    ]\n    producer.produce(topicName, partition, value, key, Date.now(), \"\", headers);\n  }\n\n  //need to keep polling for a while to ensure the delivery reports are received\n  var pollLoop = setInterval(function() {\n      producer.poll();\n      if (counter === maxMessages) {\n        clearInterval(pollLoop);\n        producer.disconnect();\n      }\n    }, 1000);\n\n});\n\nproducer.on('disconnected', function(arg) {\n  console.log('producer disconnected. ' + JSON.stringify(arg));\n});\n```\n\n----------------------------------------\n\nTITLE: Kafka Producer with Delivery Report Events\nDESCRIPTION: Shows how to configure a Kafka producer with delivery report events enabled. Includes client identification and broker connection settings.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = new Kafka.Producer({\n  'client.id': 'my-client',\n  'metadata.broker.list': 'localhost:9092',\n  'dr_cb': true\n});\n\n// Poll for events every 100 ms\nproducer.setPollInterval(100);\n\nproducer.on('delivery-report', (err, report) => {\n  console.log(report);\n});\n```\n\n----------------------------------------\n\nTITLE: Consuming Messages in Flowing Mode with Standard API in Node.js\nDESCRIPTION: Illustrates how to use the Standard API in flowing mode to consume messages continuously from a Kafka topic.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\n// Flowing mode\nconsumer.connect();\n\nconsumer\n  .on('ready', () => {\n    consumer.subscribe(['librdtesting-01']);\n\n    // Consume from the librdtesting-01 topic. This is what determines\n    // the mode we are running in. By not specifying a callback (or specifying\n    // only a callback) we get messages as soon as they are available.\n    consumer.consume();\n  })\n  .on('data', (data) => {\n    // Output the actual message contents\n    console.log(data.value.toString());\n  });\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka Consumer Stream in Node.js\nDESCRIPTION: This snippet demonstrates how to create a Kafka Consumer stream using the node-rdkafka library. It sets up the consumer configuration, creates a read stream, and handles errors. The stream is then piped to stdout for output.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/examples/consumer.md#2025-04-14_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n/*\n * node-rdkafka - Node.js wrapper for RdKafka C/C++ library\n *\n * Copyright (c) 2016 Blizzard Entertainment\n *\n * This software may be modified and distributed under the terms\n * of the MIT license.  See the LICENSE.txt file for details.\n */\n\nvar Transform = require('stream').Transform;\n\nvar Kafka = require('../');\n\nvar stream = Kafka.KafkaConsumer.createReadStream({\n  'metadata.broker.list': 'localhost:9092',\n  'group.id': 'librd-test',\n  'socket.keepalive.enable': true,\n  'enable.auto.commit': false\n}, {}, {\n  topics: 'test',\n  waitInterval: 0,\n  objectMode: false\n});\n\nstream.on('error', function(err) {\n  if (err) console.log(err);\n  process.exit(1);\n});\n\nstream\n  .pipe(process.stdout);\n\nstream.on('error', function(err) {\n  console.log(err);\n  process.exit(1);\n});\n\nstream.consumer.on('event.error', function(err) {\n  console.log(err);\n})\n```\n\n----------------------------------------\n\nTITLE: Kafka Consumer Setup with Rebalancing\nDESCRIPTION: Shows the configuration of a Kafka consumer with custom rebalancing logic and group management.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst consumer = new Kafka.KafkaConsumer({\n  'group.id': 'kafka',\n  'metadata.broker.list': 'localhost:9092',\n  'rebalance_cb': (err, assignment) => {\n    if (err.code === Kafka.CODES.ERRORS.ERR__ASSIGN_PARTITIONS) {\n      this.assign(assignment);\n    } else if (err.code == Kafka.CODES.ERRORS.ERR__REVOKE_PARTITIONS){\n      this.unassign();\n    } else {\n      console.error(err);\n    }\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Kafka Consumer with Commit Callbacks\nDESCRIPTION: Illustrates the implementation of commit callback handling in a Kafka consumer for tracking commit status and errors.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst consumer = new Kafka.KafkaConsumer({\n  'group.id': 'kafka',\n  'metadata.broker.list': 'localhost:9092',\n  'offset_commit_cb': (err, topicPartitions) => {\n    if (err) {\n      console.error(err);\n    } else {\n      console.log(topicPartitions);\n    }\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Consuming Messages in Non-Flowing Mode with Standard API in Node.js\nDESCRIPTION: Demonstrates how to use the Standard API in non-flowing mode to consume messages manually at a specified interval.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\n// Non-flowing mode\nconsumer.connect();\n\nconsumer\n  .on('ready', () => {\n    // Subscribe to the librdtesting-01 topic\n    // This makes subsequent consumes read from that topic.\n    consumer.subscribe(['librdtesting-01']);\n\n    // Read one message every 1000 milliseconds\n    setInterval(() => {\n      consumer.consume(1);\n    }, 1000);\n  })\n  .on('data', (data) => {\n    console.log('Message found!  Contents below.');\n    console.log(data.value.toString());\n  });\n```\n\n----------------------------------------\n\nTITLE: Initializing Kafka Producer with Clustering in Node.js\nDESCRIPTION: This code snippet sets up a clustered Node.js application that creates multiple Kafka producers. It uses the cluster module to fork worker processes and configures a Kafka producer in each worker. The producer sends messages to a topic, handles errors, and reports message delivery.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/examples/producer-cluster.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nvar cluster = require('cluster');\nvar numCPUs = 6;\nvar Kafka = require('../');\n\nif (cluster.isMaster) {\n  // Fork workers.\n  for (var i = 0; i < numCPUs; i++) {\n    cluster.fork();\n  }\n\n  var exited_workers = 0;\n\n  cluster.on('exit', function(worker, code, signal) {\n    exited_workers++;\n    if (exited_workers === numCPUs - 1) {\n      process.exit();\n    }\n  });\n} else {\n  // Configure client\n  var producer = new Kafka.Producer({\n    'client.id': 'kafka',\n    'metadata.broker.list': 'localhost:9092',\n    'compression.codec': 'none',\n    'retry.backoff.ms': 200,\n    'message.send.max.retries': 10,\n    'socket.keepalive.enable': true,\n    'queue.buffering.max.messages': 100000,\n    'queue.buffering.max.ms': 1000,\n    'batch.num.messages': 1000000,\n    'dr_cb': true\n  });\n\n  producer.setPollInterval(100);\n\n  var total = 0;\n  var totalSent = 0;\n  var max = 20000;\n  var errors = 0;\n  var started = Date.now();\n\n  var sendMessage = function() {\n    var ret = producer.sendMessage({\n      topic: 'librdtesting-01',\n      message: Buffer.from('message ' + total)\n    }, function() {\n    });\n    total++;\n    if (total >= max) {\n    } else {\n      setImmediate(sendMessage);\n    }\n  };\n\n  var verified_received = 0;\n  var exitNextTick = false;\n  var errorsArr = [];\n\n  var t = setInterval(function() {\n    producer.poll();\n\n    if (exitNextTick) {\n      clearInterval(t);\n      return setTimeout(function() {\n        console.log('[%d] Received: %d, Errors: %d, Total: %d', process.pid, verified_received, errors, total);\n        // console.log('[%d] Finished sending %d in %d seconds', process.pid, total, parseInt((Date.now() - started) / 1000));\n        if (errors > 0) {\n          console.error(errorsArr[0]);\n          return process.exitCode = 1;\n        }\n        process.exitCode = 0;\n        setTimeout(process.exit, 1000);\n      }, 2000);\n    }\n\n    if (verified_received + errors === max) {\n      exitNextTick = true;\n    }\n\n  }, 1000);\n  producer.connect()\n    .on('event.error', function(e) {\n      errors++;\n      errorsArr.push(e);\n    })\n    .on('delivery-report', function() {\n      verified_received++;\n    })\n    .on('ready', sendMessage);\n\n\n}\n```\n\n----------------------------------------\n\nTITLE: Higher Level Kafka Producer Implementation\nDESCRIPTION: Demonstrates the implementation of a Higher Level Producer with callback handling and message serialization capabilities.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = new Kafka.HighLevelProducer({\n  'metadata.broker.list': 'localhost:9092',\n});\n\nproducer.produce(topicName, null, Buffer.from('alliance4ever'), null, Date.now(), (err, offset) => {\n  console.log(offset);\n});\n\nproducer.setValueSerializer((value) => {\n  return Buffer.from(JSON.stringify(value));\n});\n```\n\n----------------------------------------\n\nTITLE: Consuming Messages with Stream API in Node.js\nDESCRIPTION: Demonstrates how to use the Stream API to consume messages from a Kafka topic. It creates a read stream and logs incoming messages.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\n// Read from the librdtesting-01 topic... note that this creates a new stream on each call!\nconst stream = KafkaConsumer.createReadStream(globalConfig, topicConfig, {\n  topics: ['librdtesting-01']\n});\n\nstream.on('data', (message) => {\n  console.log('Got message');\n  console.log(message.value.toString());\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing and Configuring a Kafka Producer with Node-RdKafka\nDESCRIPTION: Creates a new Kafka producer instance with broker configuration and enables delivery report callbacks. The producer is configured to connect to a local Kafka broker on port 9092.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/examples/producer.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nvar Kafka = require('../');\n\nvar producer = new Kafka.Producer({\n  //'debug' : 'all',\n  'metadata.broker.list': 'localhost:9092',\n  'dr_cb': true  //delivery report callback\n});\n\nvar topicName = 'test';\n```\n\n----------------------------------------\n\nTITLE: Creating a Kafka Producer with advanced configuration\nDESCRIPTION: Shows how to create a Kafka Producer with multiple librdkafka options set.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = new Kafka.Producer({\n  'client.id': 'kafka',\n  'metadata.broker.list': 'localhost:9092',\n  'compression.codec': 'gzip',\n  'retry.backoff.ms': 200,\n  'message.send.max.retries': 10,\n  'socket.keepalive.enable': true,\n  'queue.buffering.max.messages': 100000,\n  'queue.buffering.max.ms': 1000,\n  'batch.num.messages': 1000000,\n  'dr_cb': true\n});\n```\n\n----------------------------------------\n\nTITLE: Using Kafka Producer as a writable stream in JavaScript\nDESCRIPTION: Demonstrates how to use the Kafka Producer as a writable stream for sending messages to Kafka.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst stream = Kafka.Producer.createWriteStream({\n  'metadata.broker.list': 'kafka-host1:9092,kafka-host2:9092'\n}, {}, {\n  topic: 'topic-name'\n});\n\nconst queuedSuccess = stream.write(Buffer.from('Awesome message'));\n\nif (queuedSuccess) {\n  console.log('We queued our message!');\n} else {\n  console.log('Too many messages in our queue already');\n}\n\nstream.on('error', (err) => {\n  console.error('Error in our kafka stream');\n  console.error(err);\n})\n```\n\n----------------------------------------\n\nTITLE: Creating Kafka AdminClient with OAuth Bearer Token in Node.js\nDESCRIPTION: This code snippet shows how to create a Kafka AdminClient using node-rdkafka with OAuth Bearer Token authentication. It initializes the AdminClient with the necessary configuration and demonstrates token refreshing.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/examples/oauthbearer-default-flow.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nvar Kafka = require('../');\n\nvar token = \"your_token\";\n\nvar admin = Kafka.AdminClient.create({\n\t\t'metadata.broker.list': 'localhost:9093',\n\t\t'security.protocol': 'SASL_SSL',\n\t\t'sasl.mechanisms': 'OAUTHBEARER',\n}, token);\n\n//refresh the token\nadmin.refreshOauthBearerToken(token);\n```\n\n----------------------------------------\n\nTITLE: Retrieving Metadata from Kafka in Node.js\nDESCRIPTION: Demonstrates how to use the getMetadata method to retrieve metadata about brokers and topics from Kafka.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nconst opts = {\n  topic: 'librdtesting-01',\n  timeout: 10000\n};\n\nproducer.getMetadata(opts, (err, metadata) => {\n  if (err) {\n    console.error('Error getting metadata');\n    console.error(err);\n  } else {\n    console.log('Got metadata');\n    console.log(metadata);\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Kafka Topic with Admin Client in Node.js\nDESCRIPTION: Demonstrates how to use the Admin Client to create a new Kafka topic with specified configurations.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_20\n\nLANGUAGE: javascript\nCODE:\n```\nclient.createTopic({\n  topic: topicName,\n  num_partitions: 1,\n  replication_factor: 1\n}, (err) => {\n  // Done!\n});\n```\n\n----------------------------------------\n\nTITLE: Creating an Admin Client for Kafka in Node.js\nDESCRIPTION: Shows how to create an Admin Client for performing administrative operations on Kafka topics.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_19\n\nLANGUAGE: javascript\nCODE:\n```\nconst Kafka = require('node-rdkafka');\n\nconst client = Kafka.AdminClient.create({\n  'client.id': 'kafka-admin',\n  'metadata.broker.list': 'broker01'\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Kafka Producer with node-rdkafka\nDESCRIPTION: Creates a new Kafka producer instance with configuration for broker list, client ID, and Snappy compression. Sets up connection event handlers for ready state and errors.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/examples/metadata.md#2025-04-14_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/*\n * node-rdkafka - Node.js wrapper for RdKafka C/C++ library\n *\n * Copyright (c) 2016 Blizzard Entertainment\n *\n * This software may be modified and distributed under the terms\n * of the MIT license.  See the LICENSE.txt file for details.\n */\n\nvar Kafka = require('../');\n\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': 'localhost:9092',\n  'client.id': 'hey',\n  'compression.codec': 'snappy'\n});\n\nproducer.connect()\n  .on('ready', function(i, metadata) {\n    console.log(i);\n    console.log(metadata);\n  })\n  .on('event.error', function(err) {\n    console.log(err);\n  });\n```\n\n----------------------------------------\n\nTITLE: Creating a Kafka Producer with basic configuration\nDESCRIPTION: Demonstrates how to create a Kafka Producer instance with minimal configuration.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst producer = new Kafka.Producer({\n  'metadata.broker.list': 'kafka-host1:9092,kafka-host2:9092'\n});\n```\n\n----------------------------------------\n\nTITLE: Kafka Message Structure Example\nDESCRIPTION: Shows the structure of messages returned by the KafkaConsumer including value, size, topic, offset, partition, key, and timestamp.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  value: Buffer.from('hi'),\n  size: 2,\n  topic: 'librdtesting-01',\n  offset: 1337,\n  partition: 1,\n  key: 'someKey',\n  timestamp: 1510325354780\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Watermark Offsets in Kafka with Node.js\nDESCRIPTION: Shows how to query the latest and earliest offsets for a topic using both consumer and producer instances.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nconst timeout = 5000, partition = 0;\nconsumer.queryWatermarkOffsets('my-topic', partition, timeout, (err, offsets) => {\n  const high = offsets.highOffset;\n  const low = offsets.lowOffset;\n});\n\nproducer.queryWatermarkOffsets('my-topic', partition, timeout, (err, offsets) => {\n  const high = offsets.highOffset;\n  const low = offsets.lowOffset;\n});\n\nAn error will be returned if the client was not connected or the request timed out within the specified interval.\n```\n\n----------------------------------------\n\nTITLE: Committing Offsets with Stream API in Node.js\nDESCRIPTION: Shows how to access the underlying consumer object from a stream to commit offsets manually.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nstream.consumer.commit(); // Commits all locally stored offsets\n```\n\n----------------------------------------\n\nTITLE: Connecting and Starting the Kafka Producer\nDESCRIPTION: Initiates the connection to the Kafka broker. This triggers the connection process which will eventually fire the 'ready' event when the producer is fully connected and ready to send messages.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/examples/producer.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n//starting the producer\nproducer.connect();\n```\n\n----------------------------------------\n\nTITLE: Installing node-rdkafka on Alpine Linux Docker Container\nDESCRIPTION: This Dockerfile snippet demonstrates how to set up an Alpine Linux environment with the necessary dependencies to install node-rdkafka. It includes installing required development tools, SSL libraries, and other dependencies needed for compiling native components of node-rdkafka.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/examples/docker-alpine.md#2025-04-14_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM node:14-alpine\n\nRUN apk --no-cache add \\\n      bash \\\n      g++ \\\n      ca-certificates \\\n      lz4-dev \\\n      musl-dev \\\n      cyrus-sasl-dev \\\n      openssl-dev \\\n      make \\\n      python3\n\nRUN apk add --no-cache --virtual .build-deps gcc zlib-dev libc-dev bsd-compat-headers py-setuptools bash\n\n# Create app directory\nRUN mkdir -p /usr/local/app\n\n# Move to the app directory\nWORKDIR /usr/local/app\n\n# Install node-rdkafka\nRUN npm install node-rdkafka\n# Copy package.json first to check if an npm install is needed\n```\n\n----------------------------------------\n\nTITLE: Accessing librdkafka features in JavaScript\nDESCRIPTION: Demonstrates how to access the supported features of the compiled librdkafka library.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst Kafka = require('node-rdkafka');\nconsole.log(Kafka.features);\n\n// #=> [ 'gzip', 'snappy', 'ssl', 'sasl', 'regex', 'lz4' ]\n```\n\n----------------------------------------\n\nTITLE: Getting librdkafka version in JavaScript\nDESCRIPTION: Shows how to retrieve the version of librdkafka being used.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst Kafka = require('node-rdkafka');\nconsole.log(Kafka.librdkafkaVersion);\n\n// #=> 2.8.0\n```\n\n----------------------------------------\n\nTITLE: Updating librdkafka Version in node-rdkafka\nDESCRIPTION: A bash command sequence showing how to update the librdkafka git submodule to a specific version when upgrading the dependency.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/CONTRIBUTING.md#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd deps/librdkafka\ngit checkout 063a9ae7a65cebdf1cc128da9815c05f91a2a996 # for version 1.8.2\n```\n\n----------------------------------------\n\nTITLE: Importing node-rdkafka module in JavaScript\nDESCRIPTION: Shows how to require the node-rdkafka module in a JavaScript file.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst Kafka = require('node-rdkafka');\n```\n\n----------------------------------------\n\nTITLE: Generating TypeScript Definitions for librdkafka\nDESCRIPTION: A command to run the TypeScript definitions generator script that updates config.d.ts and errors.d.ts based on the current librdkafka version.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/CONTRIBUTING.md#2025-04-14_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnode ci/librdkafka-defs-generator.js\n```\n\n----------------------------------------\n\nTITLE: Debugging C++ in node-rdkafka with GDB\nDESCRIPTION: A command sequence for debugging the C++ components of node-rdkafka using GDB. This shows how to rebuild the module with debug symbols and start a debugging session.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/CONTRIBUTING.md#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnode-gyp rebuild --debug\n\ngdb node\n(gdb) set args \"path/to/file.js\"\n(gdb) run\n[output here]\n```\n\n----------------------------------------\n\nTITLE: Exporting OpenSSL paths for Mac OS High Sierra / Mojave\nDESCRIPTION: Sets environment variables for OpenSSL include and library paths to resolve build issues on Mac OS High Sierra and Mojave.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/README.md#2025-04-14_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport CPPFLAGS=-I/usr/local/opt/openssl/include\nexport LDFLAGS=-L/usr/local/opt/openssl/lib\n```\n\n----------------------------------------\n\nTITLE: Visual Studio Code C++ Configuration for node-rdkafka\nDESCRIPTION: JSON configuration for Visual Studio Code's C++ plugin that sets up proper include paths for intellisense when developing node-rdkafka on macOS.\nSOURCE: https://github.com/Blizzard/node-rdkafka/blob/master/CONTRIBUTING.md#2025-04-14_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"configurations\": [\n        {\n            \"name\": \"Mac\",\n            \"includePath\": [\n                \"${workspaceFolder}/**\",\n                \"${workspaceFolder}\",\n                \"${workspaceFolder}/src\",\n                \"${workspaceFolder}/node_modules/nan\",\n                \"${workspaceFolder}/deps/librdkafka/src\",\n                \"${workspaceFolder}/deps/librdkafka/src-cpp\",\n                \"/usr/local/include/node\",\n                \"/usr/local/include/node/uv\"\n            ],\n            \"defines\": [],\n            \"macFrameworkPath\": [\n                \"/Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/System/Library/Frameworks\"\n            ],\n            \"compilerPath\": \"/usr/bin/clang\",\n            \"cStandard\": \"c11\",\n            \"cppStandard\": \"c++17\",\n            \"intelliSenseMode\": \"clang-x64\"\n        }\n    ],\n    \"version\": 4\n}\n```"
  }
]
[
  {
    "owner": "gpustack",
    "repo": "gpustack",
    "content": "TITLE: Configuring Llama3.2 Model Set in Model Catalog YAML\nDESCRIPTION: Example YAML configuration for a Llama3.2 model set in the GPUStack Model Catalog. This configuration defines model metadata, capabilities, sizes, and deployment templates supporting multiple quantization options across different backends.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/model-catalog.md#2025-04-11_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Llama3.2\n  description: The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n  home: https://www.llama.com/\n  icon: /static/catalog_icons/meta.png\n  categories:\n    - llm\n  capabilities:\n    - context/128k\n    - tools\n  sizes:\n    - 1\n    - 3\n  licenses:\n    - llama3.2\n  release_date: \"2024-09-25\"\n  order: 2\n  templates:\n    - quantizations:\n        - Q3_K_L\n        - Q4_K_M\n        - Q5_K_M\n        - Q6_K_L\n        - Q8_0\n        - f16\n      source: huggingface\n      huggingface_repo_id: bartowski/Llama-3.2-{size}B-Instruct-GGUF\n      huggingface_filename: \"*-{quantization}*.gguf\"\n      replicas: 1\n      backend: llama-box\n      cpu_offloading: true\n      distributed_inference_across_workers: true\n    - quantizations: [\"BF16\"]\n      source: huggingface\n      huggingface_repo_id: unsloth/Llama-3.2-{size}B-Instruct\n      replicas: 1\n      backend: vllm\n      backend_parameters:\n        - --enable-auto-tool-choice\n        - --tool-call-parser=llama3_json\n        - --chat-template={data_dir}/chat_templates/tool_chat_template_llama3.2_json.jinja\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack on Linux or macOS\nDESCRIPTION: This command downloads and runs an installation script for GPUStack on Linux or macOS systems using curl.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/quickstart.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | sh -s -\n```\n\n----------------------------------------\n\nTITLE: Deploying Self-hosted Ollama Models with Docker Registry in Bash\nDESCRIPTION: This bash script demonstrates how to set up a self-hosted OCI registry for Ollama models, push a model to it, and configure GPUStack to use this custom registry. It includes running a Docker registry container, pulling and copying an Ollama model, and starting GPUStack with the custom registry URL.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/model-management.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Run a self-hosted OCI registry\ndocker run -d -p 5001:5000 --name registry registry:2\n\n# Push a model to the registry using Ollama\nollama pull llama3\nollama cp llama3 localhost:5001/library/llama3\nollama push localhost:5001/library/llama3 --insecure\n\n# Start GPUStack server with the custom Ollama library URL\ncurl -sfL https://get.gpustack.ai | sh -s - --ollama-library-base-url http://localhost:5001\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack on Linux/macOS Server Node\nDESCRIPTION: This command installs GPUStack on a Linux or macOS server node using a curl command to download and execute the installation script.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/setting-up-a-multi-node-gpustack-cluster.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | sh -s -\n```\n\n----------------------------------------\n\nTITLE: Using GPUStack API with curl\nDESCRIPTION: This bash script demonstrates how to use the GPUStack API with curl, including setting the API key and making a request to the chat completions endpoint.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/quickstart.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport GPUSTACK_API_KEY=myapikey\ncurl http://myserver/v1-openai/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"llama3.2\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"stream\": true\n  }'\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack on Linux/macOS with Various Configurations\nDESCRIPTION: Examples of installing GPUStack using curl with different configuration options including custom ports, data directories, TLS settings, and database connections. Also shows how to install specific versions and dependencies.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/installation-script.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Run server.\ncurl -sfL https://get.gpustack.ai | sh -s -\n\n# Run server with non-default port.\ncurl -sfL https://get.gpustack.ai | sh -s - --port 8080\n\n# Run server with a custom data path.\ncurl -sfL https://get.gpustack.ai | sh -s - --data-dir /data/gpustack-data\n\n# Run server without the embedded worker.\ncurl -sfL https://get.gpustack.ai | sh -s - --disable-worker\n\n# Run server with TLS.\ncurl -sfL https://get.gpustack.ai | sh -s - --ssl-keyfile /path/to/keyfile --ssl-certfile /path/to/certfile\n\n# Run server with external postgresql database.\ncurl -sfL https://get.gpustack.ai | sh -s - --database-url \"postgresql://username:password@host:port/database_name\"\n\n# Run worker with specified IP.\ncurl -sfL https://get.gpustack.ai | sh -s - --server-url http://myserver --token mytoken --worker-ip 192.168.1.100\n\n# Install with a custom index URL.\ncurl -sfL https://get.gpustack.ai | INSTALL_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple sh -s -\n\n# Install a custom wheel package other than releases form pypi.org.\ncurl -sfL https://get.gpustack.ai | INSTALL_PACKAGE_SPEC=https://repo.mycompany.com/my-gpustack.whl sh -s -\n\n# Install a specific version with extra audio dependencies.\ncurl -sfL https://get.gpustack.ai | INSTALL_PACKAGE_SPEC=gpustack[audio]==0.4.0 sh -s -\n```\n\n----------------------------------------\n\nTITLE: Using GPUStack with OpenAI Python Client\nDESCRIPTION: Demonstrates how to use the official OpenAI Python API library to interact with GPUStack's OpenAI-compatible endpoints. The example shows how to initialize the client with a custom base URL and API key, then create a chat completion using the llama3.2 model.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/README.md#2025-04-11_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nclient = OpenAI(base_url=\"http://myserver/v1-openai\", api_key=\"myapikey\")\n\ncompletion = client.chat.completions.create(\n  model=\"llama3.2\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack on Linux or macOS\nDESCRIPTION: This command installs GPUStack as a service on systemd or launchd based systems with default port 80.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | sh -s -\n```\n\n----------------------------------------\n\nTITLE: Using API Key with cURL to Access GPUStack OpenAI-compatible API\nDESCRIPTION: This example demonstrates how to use a GPUStack API key as a bearer token in a cURL request to access the OpenAI-compatible chat completions endpoint. The request includes setting the API key as an environment variable, specifying content type, and sending a JSON payload with model and message parameters.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/api-key-management.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport GPUSTACK_API_KEY=myapikey\ncurl http://myserver/v1-openai/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"llama3\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"stream\": true\n  }'\n```\n\n----------------------------------------\n\nTITLE: Adding AMD ROCm GPU Worker to GPUStack Server\nDESCRIPTION: Docker command to run a GPUStack worker on AMD GPUs with ROCm and register it with an existing GPUStack server, including all necessary AMD-specific configurations.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack-worker \\\n    --restart=unless-stopped \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    --group-add=video \\\n    --security-opt seccomp=unconfined \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    -v gpustack-worker-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-rocm \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings via GPUStack API using curl\nDESCRIPTION: This script demonstrates how to use the curl command to call the GPUStack API for generating text embeddings. It requires setting the server URL and API key environment variables, then makes a POST request to the embeddings endpoint with the input text and model configuration.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/creating-text-embeddings.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport SERVER_URL=<your-server-url>\nexport GPUSTACK_API_KEY=<your-api-key>\ncurl $SERVER_URL/v1-openai/embeddings \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": \"The food was delicious and the waiter...\",\n    \"model\": \"bge-small-en-v1.5\",\n    \"encoding_format\": \"float\"\n  }'\n```\n\n----------------------------------------\n\nTITLE: Deploying GPUStack Worker Node with MUSA Support\nDESCRIPTION: Docker command to deploy a GPUStack worker node with MUSA GPU support and register it with a GPUStack server. This configuration exposes necessary ports and mounts volumes required for the worker operation.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack-worker \\\n    --restart=unless-stopped \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-worker-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-musa \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n```\n\n----------------------------------------\n\nTITLE: Running Chat with GPUStack LLM Model\nDESCRIPTION: This command demonstrates how to use GPUStack to chat with the llama3.2 language model.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/quickstart.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngpustack chat llama3.2 \"tell me a joke.\"\n```\n\n----------------------------------------\n\nTITLE: Deploying GPUStack Server with Hygon DCU Support\nDESCRIPTION: Docker command to deploy a GPUStack server with built-in worker supporting Hygon DCUs. This configuration mounts the necessary drivers and devices for DCU access and persists GPUStack data in a Docker volume.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    -p 80:80 \\\n    --ipc=host \\\n    --group-add=video \\\n    --security-opt seccomp=unconfined \\\n    --device=/dev/kfd \\\n    --device=/dev/dri \\\n    -v /opt/hyhal:/opt/hyhal:ro \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-dcu\n```\n\n----------------------------------------\n\nTITLE: Using GPUStack Chat Command with Model and Prompt\nDESCRIPTION: Demonstrates the basic usage of the GPUStack chat command with a specified model and optional prompt. This command allows for one-time inference or initiates an interactive chat session.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/cli-reference/chat.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngpustack chat model [prompt]\n```\n\n----------------------------------------\n\nTITLE: Adding Ascend NPU Worker to GPUStack Server\nDESCRIPTION: Docker command to run a GPUStack worker on Huawei Ascend NPUs and register it with an existing GPUStack server, with NPU device selection and registration parameters.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack-worker \\\n    --restart=unless-stopped \\\n    -e ASCEND_VISIBLE_DEVICES=0 \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-worker-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-npu \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n```\n\n----------------------------------------\n\nTITLE: Running GPUStack with NVIDIA GPUs (host network mode)\nDESCRIPTION: Docker command to run GPUStack server with built-in worker on NVIDIA GPUs using host network mode for simplicity, with GPU access and shared memory configuration.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack\n```\n\n----------------------------------------\n\nTITLE: Adding Linux/macOS Worker Node to GPUStack Cluster\nDESCRIPTION: This command adds a Linux or macOS worker node to the GPUStack cluster by installing GPUStack and connecting it to the server using the provided URL and token.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/setting-up-a-multi-node-gpustack-cluster.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | sh -s - --server-url http://myserver --token mytoken\n```\n\n----------------------------------------\n\nTITLE: Starting GPUStack Server with Default Configuration\nDESCRIPTION: Launches the GPUStack server with default settings, using '/var/lib/gpustack' as the data directory, which requires sudo or proper permissions.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/manual-installation.md#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngpustack start\n```\n\n----------------------------------------\n\nTITLE: Adding Windows Worker Node to GPUStack Cluster\nDESCRIPTION: This PowerShell command adds a Windows worker node to the GPUStack cluster by installing GPUStack and connecting it to the server using the provided URL and token.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/setting-up-a-multi-node-gpustack-cluster.md#2025-04-11_snippet_5\n\nLANGUAGE: powershell\nCODE:\n```\nInvoke-Expression \"& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } --server-url http://myserver --token mytoken\"\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack on Windows with Various Configurations\nDESCRIPTION: Examples of installing GPUStack using PowerShell with different configuration options including custom ports, data directories, TLS settings, and database connections. Also demonstrates environment variable configuration.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/installation-script.md#2025-04-11_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\n# Run server.\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n\n# Run server with non-default port.\nInvoke-Expression \"& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --port 8080\"\n\n# Run server with a custom data path.\nInvoke-Expression \"& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --data-dir 'D:\\gpustack-data'\"\n\n# Run server without the embedded worker.\nInvoke-Expression \"& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --disable-worker\"\n\n# Run server with TLS.\nInvoke-Expression \"& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --ssl-keyfile 'C:\\path\\to\\keyfile' --ssl-certfile 'C:\\path\\to\\certfile'\"\n\n# Run server with external postgresql database.\nInvoke-Expression \"& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --database-url 'postgresql://username:password@host:port/database_name'\"\n\n# Run worker with specified IP.\nInvoke-Expression \"& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --server-url 'http://myserver' --token 'mytoken' --worker-ip '192.168.1.100'\"\n\n# Run worker with customize reserved resource.\nInvoke-Expression \"& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --server-url 'http://myserver' --token 'mytoken' --system-reserved '{\"ram\":5, \"vram\":5}'\"\n\n# Install with a custom index URL.\n$env:INSTALL_INDEX_URL = \"https://pypi.tuna.tsinghua.edu.cn/simple\"\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n\n# Install a custom wheel package other than releases form pypi.org.\n$env:INSTALL_PACKAGE_SPEC = \"https://repo.mycompany.com/my-gpustack.whl\"\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n\n# Install a specific version with extra audio dependencies.\n$env:INSTALL_PACKAGE_SPEC = \"gpustack[audio]==0.4.0\"\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n```\n\n----------------------------------------\n\nTITLE: Registering a Worker with GPUStack Server\nDESCRIPTION: Starts a GPUStack worker and registers it with the GPUStack server by specifying the server URL, authentication token, and worker IP address.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/manual-installation.md#2025-04-11_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ngpustack start --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n```\n\n----------------------------------------\n\nTITLE: Configuring Stable-Diffusion-v3-5-Large Model with Example Prompt\nDESCRIPTION: Example prompt and parameter configuration for the Stable-Diffusion-v3-5-Large image generation model. This configuration uses the dpm++2m sampler with discrete scheduler and a CFG value of 5.0.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/recommended-parameters-for-image-generation-models.md#2025-04-11_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nPrompt: Lucky flower pop art style with pink color scheme,happy cute girl character wearing oversized headphones and smiling while listening to music in the air with her eyes closed,vibrant colorful Japanese anime cartoon illustration with bold outlines and bright colors,colorful text \"GPUStack\" on top of background,high resolution,detailed,\nSize: 1024x1024\nSampler: dpm++2m\nScheduler: discrete\nSteps: 25\nCFG: 5\nSeed: 3520225659\n```\n\n----------------------------------------\n\nTITLE: Running GPUStack Docker Container\nDESCRIPTION: Docker command to launch GPUStack container with necessary GPU access permissions and configurations.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-amd-gpus.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -itd \\\n   --network=host \\\n   --ipc=host \\\n   --group-add=video \\\n   --cap-add=SYS_PTRACE \\\n   --security-opt seccomp=unconfined \\\n   --device /dev/kfd \\\n   --device /dev/dri \\\n   gpustack/gpustack:v0.5.0-rocm\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Catalog for Air-Gapped Environments\nDESCRIPTION: Example YAML configuration for using the Model Catalog in air-gapped environments without internet access. This demonstrates how to configure model sets to use local file paths instead of remote repositories.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/model-catalog.md#2025-04-11_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Llama3.2\n  description: The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n  home: https://www.llama.com/\n  icon: /static/catalog_icons/meta.png\n  categories:\n    - llm\n  capabilities:\n    - context/128k\n    - tools\n  sizes:\n    - 1\n    - 3\n  licenses:\n    - llama3.2\n  release_date: \"2024-09-25\"\n  order: 2\n  templates:\n    - quantizations:\n        - Q3_K_L\n        - Q4_K_M\n        - Q5_K_M\n        - Q6_K_L\n        - Q8_0\n        - f16\n      source: local_path\n      # assuming you have all the GGUF model files in /path/to/the/model/directory\n      local_path: /path/to/the/model/directory/Llama-3.2-{size}B-Instruct-{quantization}.gguf\n      replicas: 1\n      backend: llama-box\n      cpu_offloading: true\n      distributed_inference_across_workers: true\n    - quantizations: [\"BF16\"]\n      source: local_path\n      # assuming you have both /path/to/Llama-3.2-1B-Instruct and /path/to/Llama-3.2-3B-Instruct directories\n      local_path: /path/to/Llama-3.2-{size}B-Instruct\n      replicas: 1\n      backend: vllm\n      backend_parameters:\n        - --enable-auto-tool-choice\n        - --tool-call-parser=llama3_json\n        - --chat-template={data_dir}/chat_templates/tool_chat_template_llama3.2_json.jinja\n```\n\n----------------------------------------\n\nTITLE: Launching GPUStack Docker Container\nDESCRIPTION: Docker run command to set up an isolated environment for GPUStack with necessary permissions and device access for Hygon DCUs.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-hygon-dcus.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -itd --shm-size 500g \\\n   --network=host --privileged \\\n   --group-add video \\\n   --cap-add=SYS_PTRACE \\\n   --security-opt seccomp=unconfined \\\n   --device=/dev/kfd --device=/dev/dri \\\n   -v /opt/hyhal:/opt/hyhal:ro \\\n   gpustack/gpustack:v0.5.1-dcu\n```\n\n----------------------------------------\n\nTITLE: Running GPUStack with AMD ROCm GPUs\nDESCRIPTION: Docker command to run GPUStack server with built-in worker on AMD GPUs using ROCm, with necessary device and security configurations for AMD GPU access.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    -p 80:80 \\\n    --ipc=host \\\n    --group-add=video \\\n    --security-opt seccomp=unconfined \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-rocm\n```\n\n----------------------------------------\n\nTITLE: Configuring GPUStack Server and Worker with YAML\nDESCRIPTION: Complete example of a YAML configuration file for GPUStack server and worker settings. Includes common options, server-specific settings, and worker-specific configurations for networking, resource management, authentication, and integrations.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/cli-reference/start.md#2025-04-11_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# Common Options\ndebug: false\ndata_dir: /path/to/data_dir\ncache_dir: /path/to/cache_dir\ntoken: mytoken\nollama_library_base_url: https://registry.mycompany.com\nenable_ray: false\nray_args: [\"--port=6379\", \"--verbose\"]\n\n# Server Options\nhost: 0.0.0.0\nport: 80\ndisable_worker: false\ndatabase_url: postgresql://user:password@hostname:port/db_name\nssl_keyfile: /path/to/keyfile\nssl_certfile: /path/to/certfile\nforce_auth_localhost: false\nbootstrap_password: myadminpassword\ndisable_update_check: false\nmodel_catalog_file: /path_or_url/to/model_catalog_file\nray_port: 40096\nray_client_server_port: 40097\nenable_cors: false\nallow_origins: [\"*\"]\nallow_credentials: false\nallow_methods: [\"GET\", \"POST\"]\nallow_headers: [\"Authorization\", \"Content-Type\"]\n\n# Worker Options\nserver_url: http://myserver\nworker_name: myworker\nworker_ip: 192.168.1.101\ndisable_metrics: false\ndisable_rpc_servers: false\nmetrics_port: 10151\nworker_port: 10150\nservice_port_range: 40000-40063\nrpc_server_port_range: 40064-40095\nray_node_manager_port: 40098\nray_object_manager_port: 40099\nray_worker_port_range: 40100-40131\nlog_dir: /path/to/log_dir\nrpc_server_args: [\"--verbose\"]\nsystem_reserved:\n  ram: 2\n  vram: 1\ntools_download_base_url: https://mirror.mycompany.com\nenable_hf_transfer: false\n```\n\n----------------------------------------\n\nTITLE: Running GPUStack with Ascend NPUs\nDESCRIPTION: Docker command to run GPUStack server with built-in worker on Huawei Ascend NPUs, using device selection through environment variables and the NPU-specific image.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    -e ASCEND_VISIBLE_DEVICES=0 \\\n    -p 80:80 \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-npu\n```\n\n----------------------------------------\n\nTITLE: Custom Dockerfile for GPUStack with Configurable CUDA Version\nDESCRIPTION: Dockerfile to build a custom GPUStack image with a configurable CUDA version. This example extends the NVIDIA CUDA base image, installs dependencies, builds GPUStack from source, and configures the appropriate entrypoint.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_18\n\nLANGUAGE: dockerfile\nCODE:\n```\n# Example Dockerfile\nARG CUDA_VERSION=12.4.1\n\nFROM nvidia/cuda:$CUDA_VERSION-cudnn-runtime-ubuntu22.04\n\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN apt-get update && apt-get install -y \\\n    git \\\n    curl \\\n    wget \\\n    tzdata \\\n    iproute2 \\\n    python3 \\\n    python3-pip \\\n    python3-venv \\\n    && rm -rf /var/lib/apt/lists/*\n\nCOPY . /workspace/gpustack\nRUN cd /workspace/gpustack && \\\n    make build\n\nRUN if [ \"$TARGETPLATFORM\" = \"linux/amd64\" ]; then \\\n    # Install vllm dependencies for x86_64\n    WHEEL_PACKAGE=\"$(ls /workspace/gpustack/dist/*.whl)[all]\"; \\\n    else  \\\n    WHEEL_PACKAGE=\"$(ls /workspace/gpustack/dist/*.whl)[audio]\"; \\\n    fi && \\\n    pip install pipx && \\\n    pip install $WHEEL_PACKAGE && \\\n    pip cache purge && \\\n    rm -rf /workspace/gpustack\n\nRUN gpustack download-tools\n\nENTRYPOINT [ \"gpustack\", \"start\" ]\n```\n\n----------------------------------------\n\nTITLE: Running GPUStack with Moore Threads MUSA GPUs\nDESCRIPTION: Docker command to run GPUStack server with built-in worker on Moore Threads GPUs using the MUSA-specific image and necessary configurations for device access.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    -p 80:80 \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-musa\n```\n\n----------------------------------------\n\nTITLE: Building Custom GPUStack Docker Image\nDESCRIPTION: Command to build a custom GPUStack Docker image with a specific CUDA version. This example shows how to override the default CUDA version (12.4) with version 12.0.0.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\ndocker build -t my/gpustack --build-arg CUDA_VERSION=12.0.0 .\n```\n\n----------------------------------------\n\nTITLE: Creating a Systemd Service File for GPUStack\nDESCRIPTION: Creates a systemd service configuration file to run GPUStack as a system service that starts automatically on boot.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/manual-installation.md#2025-04-11_snippet_7\n\nLANGUAGE: systemd\nCODE:\n```\n[Unit]\nDescription=GPUStack Service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nEnvironmentFile=-/etc/default/%N\nExecStart=gpustack start\nRestart=always\nRestartSec=3\nStandardOutput=append:/var/log/gpustack.log\nStandardError=append:/var/log/gpustack.log\n\n[Install]\nWantedBy=multi-user.target\n```\n\n----------------------------------------\n\nTITLE: Running GPUStack with NVIDIA GPUs (port mapping)\nDESCRIPTION: Docker command to run GPUStack server with built-in worker on NVIDIA GPUs using explicit port mapping, requiring worker IP specification for proper networking.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    -p 80:80 \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack --worker-ip your_host_ip\n```\n\n----------------------------------------\n\nTITLE: Upgrading GPUStack on Linux/macOS using Installation Script\nDESCRIPTION: This command upgrades GPUStack to the latest version on Linux and macOS systems using the installation script. It requires existing installation environment variables and GPUStack arguments.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/upgrade.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | <EXISTING_INSTALL_ENV> sh -s - <EXISTING_GPUSTACK_ARGS>\n```\n\n----------------------------------------\n\nTITLE: Running GPUStack Server without Worker (NVIDIA)\nDESCRIPTION: Docker command to run GPUStack server without a built-in worker, using CPU-only image, suitable for a dedicated server node in a distributed setup.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack-server \\\n    --restart=unless-stopped \\\n    -p 80:80 \\\n    -v gpustack-server-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-cpu \\\n    --disable-worker\n```\n\n----------------------------------------\n\nTITLE: Manual Uninstallation of GPUStack on Linux\nDESCRIPTION: Series of commands for manually removing GPUStack components including service, CLI, and data directory. Includes stopping the service, removing service files, and cleaning up installation directories.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/uninstallation.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Stop and remove the service.\nsystemctl stop gpustack.service\nrm /etc/systemd/system/gpustack.service\nsystemctl daemon-reload\n# Uninstall the CLI.\npip uninstall gpustack\n# Remove the data directory.\nrm -rf /var/lib/gpustack\n```\n\n----------------------------------------\n\nTITLE: Deploying GPUStack Server without Worker\nDESCRIPTION: Docker command to deploy a GPUStack server without a built-in worker. This configuration uses the CPU-based image and explicitly disables the worker functionality, making it suitable for dedicated server nodes.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack-server \\\n    --restart=unless-stopped \\\n    -p 80:80 \\\n    -v gpustack-server-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-cpu \\\n    --disable-worker\n```\n\n----------------------------------------\n\nTITLE: Retrieving GPUStack Admin Password\nDESCRIPTION: Command to retrieve the default admin password from the GPUStack container, necessary for initial login to the web interface.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndocker exec -it gpustack cat /var/lib/gpustack/initial_admin_password\n```\n\n----------------------------------------\n\nTITLE: Verifying ROCm Installation\nDESCRIPTION: Commands to verify the ROCm installation, GPU driver status, and hardware detection.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-amd-gpus.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngroups\ndkms status\nrocminfo\nrocm-smi -i --showmeminfo vram --showpower --showserial --showuse --showtemp --showproductname\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack Docker Container for Moore Threads GPUs\nDESCRIPTION: This command runs a Docker container with the GPUStack image, exposing the web interface on port 9009 and mounting a volume for data storage.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-moorethreads-gpus.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -d --name gpustack-musa -p 9009:80 --ipc=host -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:main-musa\n```\n\n----------------------------------------\n\nTITLE: Verifying Hygon DCU Installation\nDESCRIPTION: Commands to verify the successful installation of Hygon DCU drivers and tools, including checking device type, GPU agent status, and various system metrics.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-hygon-dcus.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Verify the rocminfo.\n# Expected result: Device Type: DCU\nrocminfo | grep DCU\n\n# Check if the GPU is listed as an agent.\nrocminfo\n\n# Check rocm-smi.\nrocm-smi -i --showmeminfo vram --showpower --showserial --showuse --showtemp --showproductname\n\n# Check hy-smi\nhy-smi\n```\n\n----------------------------------------\n\nTITLE: Starting GPUStack Server with Custom Data Directory\nDESCRIPTION: Launches GPUStack with a user-specified data directory to avoid permission issues with the default location.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/manual-installation.md#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngpustack start --data-dir mypath\n```\n\n----------------------------------------\n\nTITLE: HSA System and Agent Configuration Output\nDESCRIPTION: System output showing detailed configuration of HSA runtime environment, CPU and GPU agents, including hardware specifications, memory pools, and computational capabilities. The output includes information about ROCk module version, runtime versions, and detailed agent specifications.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/tests/detectors/rocm_smi/data/amd-gpu-6.2.4-rocminfo.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nROCk module version 6.2.4 is loaded\n=====================\nHSA System Attributes\n=====================\nRuntime Version:         1.13\nRuntime Ext Version:     1.4\nSystem Timestamp Freq.:  1000.000000MHz\nSig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count)\nMachine Model:           LARGE\nSystem Endianness:       LITTLE\nMwaitx:                  DISABLED\nDMAbuf Support:          YES\n\n==========\nHSA Agents\n==========\n*******\nAgent 1\n*******\nName:                    Intel(R) Core(TM) i5-14600KF\nUuid:                    CPU-XX\nMarketing Name:          Intel(R) Core(TM) i5-14600KF\nVendor Name:             CPU\nFeature:                 None specified\nProfile:                 FULL_PROFILE\nFloat Round Mode:        NEAR\nMax Queue Number:        0(0x0)\nQueue Min Size:          0(0x0)\nQueue Max Size:          0(0x0)\nQueue Type:              MULTI\nNode:                    0\nDevice Type:             CPU\nCache Info:\n    L1:                      49152(0xc000) KB\nChip ID:                 0(0x0)\nASIC Revision:           0(0x0)\nCacheline Size:          64(0x40)\nMax Clock Freq. (MHz):   5300\nBDFID:                   0\nInternal Node ID:        0\nCompute Unit:            20\nSIMDs per CU:            0\nShader Engines:          0\nShader Arrs. per Eng.:   0\nWatchPts on Addr. Ranges:1\nFeatures:                None\nPool Info:\n    Pool 1\n    Segment:                 GLOBAL; FLAGS: FINE GRAINED\n    Size:                    65613932(0x3e9306c) KB\n    Allocatable:             TRUE\n    Alloc Granule:           4KB\n    Alloc Recommended Granule:4KB\n    Alloc Alignment:         4KB\n    Accessible by all:       TRUE\n    Pool 2\n    Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED\n    Size:                    65613932(0x3e9306c) KB\n    Allocatable:             TRUE\n    Alloc Granule:           4KB\n    Alloc Recommended Granule:4KB\n    Alloc Alignment:         4KB\n    Accessible by all:       TRUE\n    Pool 3\n    Segment:                 GLOBAL; FLAGS: COARSE GRAINED\n    Size:                    65613932(0x3e9306c) KB\n    Allocatable:             TRUE\n    Alloc Granule:           4KB\n    Alloc Recommended Granule:4KB\n    Alloc Alignment:         4KB\n    Accessible by all:       TRUE\nISA Info:\n*******\nAgent 2\n*******\nName:                    gfx1101\nUuid:                    GPU-5c88007d760374f3\nMarketing Name:          AMD Radeon RX 7800 XT\nVendor Name:             AMD\nFeature:                 KERNEL_DISPATCH\nProfile:                 BASE_PROFILE\nFloat Round Mode:        NEAR\nMax Queue Number:        128(0x80)\nQueue Min Size:          64(0x40)\nQueue Max Size:          131072(0x20000)\nQueue Type:              MULTI\nNode:                    1\nDevice Type:             GPU\nCache Info:\n    L1:                      32(0x20) KB\n    L2:                      4096(0x1000) KB\n    L3:                      65536(0x10000) KB\nChip ID:                 29822(0x747e)\nASIC Revision:           0(0x0)\nCacheline Size:          64(0x40)\nMax Clock Freq. (MHz):   2254\nBDFID:                   768\nInternal Node ID:        1\nCompute Unit:            60\nSIMDs per CU:            2\nShader Engines:          3\nShader Arrs. per Eng.:   2\nWatchPts on Addr. Ranges:4\nCoherent Host Access:    FALSE\nFeatures:                KERNEL_DISPATCH\nFast F16 Operation:      TRUE\nWavefront Size:          32(0x20)\nWorkgroup Max Size:      1024(0x400)\nWorkgroup Max Size per Dimension:\n    x                        1024(0x400)\n    y                        1024(0x400)\n    z                        1024(0x400)\nMax Waves Per CU:        32(0x20)\nMax Work-item Per CU:    1024(0x400)\nGrid Max Size:           4294967295(0xffffffff)\nGrid Max Size per Dimension:\n    x                        4294967295(0xffffffff)\n    y                        4294967295(0xffffffff)\n    z                        4294967295(0xffffffff)\nMax fbarriers/Workgrp:   32\nPacket Processor uCode:: 546\nSDMA engine uCode::      20\nIOMMU Support::          None\nPool Info:\n    Pool 1\n    Segment:                 GLOBAL; FLAGS: COARSE GRAINED\n    Size:                    16760832(0xffc000) KB\n    Allocatable:             TRUE\n    Alloc Granule:           4KB\n    Alloc Recommended Granule:2048KB\n    Alloc Alignment:         4KB\n    Accessible by all:       FALSE\n    Pool 2\n    Segment:                 GLOBAL; FLAGS: EXTENDED FINE GRAINED\n    Size:                    16760832(0xffc000) KB\n    Allocatable:             TRUE\n    Alloc Granule:           4KB\n    Alloc Recommended Granule:2048KB\n    Alloc Alignment:         4KB\n    Accessible by all:       FALSE\n    Pool 3\n    Segment:                 GROUP\n    Size:                    64(0x40) KB\n    Allocatable:             FALSE\n    Alloc Granule:           0KB\n    Alloc Recommended Granule:0KB\n    Alloc Alignment:         0KB\n    Accessible by all:       FALSE\nISA Info:\n    ISA 1\n    Name:                    amdgcn-amd-amdhsa--gfx1101\n    Machine Models:          HSA_MACHINE_MODEL_LARGE\n    Profiles:                HSA_PROFILE_BASE\n    Default Rounding Mode:   NEAR\n    Default Rounding Mode:   NEAR\n    Fast f16:                TRUE\n    Workgroup Max Size:      1024(0x400)\n    Workgroup Max Size per Dimension:\n        x                        1024(0x400)\n        y                        1024(0x400)\n        z                        1024(0x400)\n    Grid Max Size:           4294967295(0xffffffff)\n    Grid Max Size per Dimension:\n        x                        4294967295(0xffffffff)\n        y                        4294967295(0xffffffff)\n        z                        4294967295(0xffffffff)\n    FBarrier Max Size:       32\n*** Done ***\n```\n\n----------------------------------------\n\nTITLE: Installing ROCm Packages on Ubuntu\nDESCRIPTION: Commands to install ROCm packages and dependencies for AMD GPU support on Ubuntu systems.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-amd-gpus.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nwget https://repo.radeon.com/amdgpu-install/6.2.4/ubuntu/jammy/amdgpu-install_6.2.60204-1_all.deb\nsudo apt install ./amdgpu-install_6.2.60204-1_all.deb\n\namdgpu-install -y --usecase=graphics,rocm\nsudo reboot\n```\n\n----------------------------------------\n\nTITLE: NPU Device Status Table Output\nDESCRIPTION: Command output table showing status information for 4 NPU devices including device IDs, health status, power consumption, temperature, memory usage and hugepage allocation. Output generated by npu-smi version 21.0.4.B030.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/tests/detectors/npu_smi/data/ai-server_atlas-800-inference-server_model-3000_version-1.0.11-1.0.15.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n+----------------------------------------------------------------------------------------------------+\n| npu-smi 21.0.4                       Version: 21.0.4.B030                                          |\n+-------------------+-----------------+--------------------------------------------------------------+\n| NPU     Name      | Health          | Power(W)          Temp(C)              Hugepages-Usage(page) |\n| Chip    Device    | Bus-Id          | AICore(%)         Memory-Usage(MB)                           |\n+===================+=================+==============================================================+\n| 6       310       | OK              | 12.8              73             0       / 970               |\n| 0       3         | 0000:89:00.0    | 0                 2703 / 8192                                |\n+-------------------+-----------------+--------------------------------------------------------------+\n| 6       310       | OK              | 12.8              74             0       / 970               |\n| 1       4         | 0000:8A:00.0    | 0                 2867 / 8192                                |\n+-------------------+-----------------+--------------------------------------------------------------+\n| 6       310       | OK              | 12.8              70            0        / 970               |\n| 2       5         | 0000:8B:00.0    | 0                 2867 / 8192                                |\n+-------------------+-----------------+--------------------------------------------------------------+\n| 6       310       | OK              | 12.8              65            0        / 970               |\n| 3       6         | 0000:8C:00.0    | 0                 2867 / 8192                                |\n+===================+=================+==============================================================+\n```\n\n----------------------------------------\n\nTITLE: Checking Dependencies for Ascend NPU Toolkit\nDESCRIPTION: Commands to check for required dependencies before installing the Ascend NPU toolkit and kernels. This includes verifying versions of various system packages and libraries.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-ascend-npus.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngcc --version\ng++ --version\nmake --version\ncmake --version\ndpkg -l zlib1g| grep zlib1g| grep ii\ndpkg -l zlib1g-dev| grep zlib1g-dev| grep ii\ndpkg -l libsqlite3-dev| grep libsqlite3-dev| grep ii\ndpkg -l openssl| grep openssl| grep ii\ndpkg -l libssl-dev| grep libssl-dev| grep ii\ndpkg -l libffi-dev| grep libffi-dev| grep ii\ndpkg -l libbz2-dev| grep libbz2-dev| grep ii\ndpkg -l libxslt1-dev| grep libxslt1-dev| grep ii\ndpkg -l unzip| grep unzip| grep ii\ndpkg -l pciutils| grep pciutils| grep ii\ndpkg -l net-tools| grep net-tools| grep ii\ndpkg -l libblas-dev| grep libblas-dev| grep ii\ndpkg -l gfortran| grep gfortran| grep ii\ndpkg -l libblas3| grep libblas3| grep ii\n```\n\n----------------------------------------\n\nTITLE: Checking GLIBC Version on Linux for GPUStack Compatibility\nDESCRIPTION: Command to check the installed GLIBC version on a Linux system. GPUStack worker requires GLIBC version 2.29 or higher for direct installation, with Docker installation as an alternative for systems with older GLIBC versions.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/installation-requirements.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nldd --version\n```\n\n----------------------------------------\n\nTITLE: Setting Worker Log Level - GPUStack\nDESCRIPTION: Command to configure the log level of GPUStack workers at runtime. Uses the worker-specific port 10150.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/troubleshooting.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X PUT http://localhost:10150/debug/log_level -d \"debug\"\n```\n\n----------------------------------------\n\nTITLE: Verifying GPU Utilization with mthreads-gmi\nDESCRIPTION: This snippet demonstrates how to use the 'mthreads-gmi' command to check GPU utilization and verify if models are offloaded to the GPU during inference.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-moorethreads-gpus.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nroot@a414c45864ee:/# mthreads-gmi\nSat Nov 16 12:00:16 2024\n---------------------------------------------------------------\n    mthreads-gmi:1.14.0          Driver Version:2.7.0\n---------------------------------------------------------------\nID   Name           |PCIe                |%GPU  Mem\n     Device Type    |Pcie Lane Width     |Temp  MPC Capable\n                                         |      ECC Mode\n+-------------------------------------------------------------+\n0    MTT S80        |00000000:01:00.0    |98%   1339MiB(16384MiB)\n     Physical       |16x(16x)            |56C   YES\n                                         |      N/A\n---------------------------------------------------------------\n\n---------------------------------------------------------------\nProcesses:\nID   PID       Process name                         GPU Memory\n                                                         Usage\n+-------------------------------------------------------------+\n0    120       ...ird_party/bin/llama-box/llama-box       2MiB\n0    2022      ...ird_party/bin/llama-box/llama-box    1333MiB\n---------------------------------------------------------------\n```\n\n----------------------------------------\n\nTITLE: Verifying Ascend NPU Driver Installation\nDESCRIPTION: Command to verify the successful installation of the Ascend NPU driver by checking device information using the npu-smi tool.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-ascend-npus.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$npu-smi info\n```\n\n----------------------------------------\n\nTITLE: Installing Ascend NPU Driver\nDESCRIPTION: Commands to install the Ascend NPU driver. This includes setting execute permissions on the driver package and running the installation script with full installation for all users.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-ascend-npus.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo chmod +x Ascend-hdk-xxx-npu-driver_x.x.x_linux-{arch}.run\nsudo sh Ascend-hdk-xxx-npu-driver_x.x.x_linux-{arch}.run --full --install-for-all\n```\n\n----------------------------------------\n\nTITLE: Installing Ascend NPU Firmware\nDESCRIPTION: Commands to install the Ascend NPU firmware. This includes setting execute permissions on the firmware package and running the installation script with full installation.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-ascend-npus.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo chmod +x Ascend-hdk-xxx-npu-firmware_x.x.x.x.X.run\nsudo sh Ascend-hdk-xxx-npu-firmware_x.x.x.x.X.run --full\n```\n\n----------------------------------------\n\nTITLE: Creating User and Group for Ascend NPU\nDESCRIPTION: Commands to create the necessary user and group for running Ascend NPU processes. This includes creating a HwHiAiUser group, adding a user to this group, and modifying the current user's group membership.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-ascend-npus.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo groupadd -g HwHiAiUser\nsudo useradd -g HwHiAiUser -d /home/HwHiAiUser -m HwHiAiUser -s /bin/bash\nsudo usermod -aG HwHiAiUser $USER\n```\n\n----------------------------------------\n\nTITLE: Verifying GPUStack Installation\nDESCRIPTION: Checks the installed GPUStack version to confirm successful installation.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/manual-installation.md#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngpustack version\n```\n\n----------------------------------------\n\nTITLE: Installing Ascend NPU Toolkit and Kernels\nDESCRIPTION: Commands to install the Ascend NPU toolkit and kernels. This includes setting execute permissions on the installation packages and running the installation scripts.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-ascend-npus.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nchmod +x Ascend-cann-toolkit_{vesion}_linux-{arch}.run\nchmod +x Ascend-cann-kernels-{chip_type}_{version}_linux-{arch}.run\n\nsh Ascend-cann-toolkit_{vesion}_linux-{arch}.run --install\nsh Ascend-cann-kernels-{chip_type}_{version}_linux-{arch}.run --install\n```\n\n----------------------------------------\n\nTITLE: Adding NVIDIA GPU Worker to GPUStack Server\nDESCRIPTION: Docker command to run a GPUStack worker on NVIDIA GPUs and register it with an existing GPUStack server using the server URL, token, and worker IP.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack-worker \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-worker-data:/var/lib/gpustack \\\n    gpustack/gpustack \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n```\n\n----------------------------------------\n\nTITLE: Installing Missing Dependencies for Ascend NPU Toolkit\nDESCRIPTION: Command to install missing dependencies required for the Ascend NPU toolkit and kernels using apt-get package manager.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-ascend-npus.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install -y gcc g++ make cmake zlib1g zlib1g-dev openssl libsqlite3-dev libssl-dev libffi-dev libbz2-dev libxslt1-dev unzip pciutils net-tools libblas-dev gfortran libblas3\n```\n\n----------------------------------------\n\nTITLE: Retrieving Worker Token from GPUStack Server\nDESCRIPTION: Command to extract the authentication token from a running GPUStack server container. This token is required when registering worker nodes with the server.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ndocker exec -it gpustack-server cat /var/lib/gpustack/token\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Ascend NPU Toolkit\nDESCRIPTION: Commands to install Python dependencies required for the Ascend NPU toolkit using pip package manager.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-ascend-npus.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip3 install --upgrade pip\npip3 install attrs numpy decorator sympy cffi pyyaml pathlib2 psutil protobuf scipy requests absl-py wheel typing_extensions\n```\n\n----------------------------------------\n\nTITLE: Enabling GPUStack as a System Service\nDESCRIPTION: Reloads systemd configuration and enables the GPUStack service to start automatically on system boot.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/manual-installation.md#2025-04-11_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsystemctl daemon-reload\nsystemctl enable gpustack\n```\n\n----------------------------------------\n\nTITLE: Downloading GPUStack Packages and Tools\nDESCRIPTION: Commands to download required GPUStack packages and tools in an online environment. This includes creating wheel files for offline installation and downloading dependency tools as an archive.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/air-gapped-installation.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nPACKAGE_SPEC=\"gpustack\"\n\n# Download all required packages\npip wheel $PACKAGE_SPEC -w gpustack_offline_packages\n\n# Install GPUStack to access its CLI\npip install gpustack\n\n# Download dependency tools and save them as an archive\ngpustack download-tools --save-archive gpustack_offline_tools.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Uninstalling GPUStack on Windows using PowerShell\nDESCRIPTION: PowerShell command to bypass execution policy and run the GPUStack uninstallation script on Windows. Removes all GPUStack components from the system.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/uninstallation.md#2025-04-11_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\nSet-ExecutionPolicy Bypass -Scope Process -Force; & \"$env:APPDATA\\gpustack\\uninstall.ps1\"\n```\n\n----------------------------------------\n\nTITLE: Installing macOS Additional Dependencies\nDESCRIPTION: Commands for handling additional dependencies required for speech-to-text CosyVoice model on macOS, including environment setup and package installation.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/air-gapped-installation.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AUDIO_DEPENDENCY_PACKAGE_SPEC=\"wetextprocessing==1.0.4.1\"\nexport CPLUS_INCLUDE_PATH=$(brew --prefix openfst@1.8.3)/include\nexport LIBRARY_PATH=$(brew --prefix openfst@1.8.3)/lib\n\npip wheel $AUDIO_DEPENDENCY_PACKAGE_SPEC -w gpustack_audio_dependency_offline_packages\nmv gpustack_audio_dependency_offline_packages/* gpustack_offline_packages/ && rm -rf gpustack_audio_dependency_offline_packages\n```\n\n----------------------------------------\n\nTITLE: Setting User Group Permissions\nDESCRIPTION: Commands to add current user to required groups for AMD GPU access.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-amd-gpus.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo usermod -a -G render,video $LOGNAME\nsudo reboot\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack in Air-Gapped Environment\nDESCRIPTION: Commands for installing GPUStack in the air-gapped environment using pre-downloaded packages and tools.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/air-gapped-installation.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install GPUStack from the downloaded packages\npip install --no-index --find-links=gpustack_offline_packages gpustack\n\n# Load and apply the pre-downloaded tools archive\ngpustack download-tools --load-archive gpustack_offline_tools.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Verifying Container Runtime Configuration for Moore Threads GPUs\nDESCRIPTION: This snippet demonstrates how to set up the Docker runtime for Moore Threads GPUs and verify that 'mthreads' is set as the default runtime.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-moorethreads-gpus.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ (cd /usr/bin/musa && sudo ./docker setup $PWD)\n$ docker info | grep mthreads\n Runtimes: mthreads mthreads-experimental runc\n Default Runtime: mthreads\n```\n\n----------------------------------------\n\nTITLE: Installing macOS Dependencies in Air-Gapped Environment\nDESCRIPTION: Final installation steps for macOS-specific dependencies in the air-gapped environment using pre-downloaded packages.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/air-gapped-installation.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Install the additional dependencies for speech-to-text CosyVoice model on macOS.\n# load-dir is the directory include local-openfst-1.8.3.tar.gz and c9d6de4616377b76b6c6c71920a0b24f1b19aeed734fe12dbd2a169d0893b541--openfst-1.8.3.tar.gz\n./load_macos_dependencies.sh --load-dir ./\n\npip install --no-index --find-links=gpustack_offline_packages wetextprocessing\n```\n\n----------------------------------------\n\nTITLE: Deploying GPUStack Worker with Hygon DCU Support\nDESCRIPTION: Docker command to deploy a GPUStack worker with Hygon DCU support and register it with a server. This configuration mounts the necessary drivers, exposes required ports, and includes device access for DCUs.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\ndocker run -d --name gpustack-worker \\\n    --restart=unless-stopped \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    --group-add=video \\\n    --security-opt seccomp=unconfined \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    -v /opt/hyhal:/opt/hyhal:ro \\\n    -v gpustack-worker-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-dcu \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker for NVIDIA GPUs with cgroupfs driver\nDESCRIPTION: Configuration for Docker daemon to use cgroupfs as the cgroup driver, preventing issues with NVIDIA GPU access in containers when systemd triggers reloads of Unit files with NVIDIA GPU references.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo vim /etc/docker/daemon.json\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"runtimes\": {\n    \"nvidia\": {\n      \"args\": [],\n      \"path\": \"nvidia-container-runtime\"\n    }\n  },\n  \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"]\n}\n```\n\nLANGUAGE: shell\nCODE:\n```\nsudo systemctl daemon-reload && sudo systemctl restart docker\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Ascend NPU\nDESCRIPTION: Commands to configure environment variables for the Ascend NPU toolkit by adding the set_env.sh script to the user's .bashrc file and sourcing it.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-ascend-npus.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\necho \"source ~/Ascend/ascend-toolkit/set_env.sh\" >> ~/.bashrc\nsource ~/.bashrc\n```\n\n----------------------------------------\n\nTITLE: Analyzing Model Resource Requirements using gguf-parser\nDESCRIPTION: Command to estimate RAM and VRAM requirements for the Qwen2.5-72B-Instruct model using the gguf-parser tool. Shows memory requirements for different layer configurations and UMA/NONUMA scenarios.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/performing-distributed-inference-across-workers-llama-box.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ gguf-parser --hf-repo Qwen/Qwen2.5-72B-Instruct-GGUF --hf-file qwen2.5-72b-instruct-q2_k-00001-of-00007.gguf --ctx-size=8192 --in-short --skip-architecture --skip-metadata --skip-tokenizer\n```\n\n----------------------------------------\n\nTITLE: Checking GPUStack Container Logs\nDESCRIPTION: This command shows how to view the logs of the running GPUStack container to verify successful startup.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-moorethreads-gpus.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker logs -f gpustack-musa\n```\n\n----------------------------------------\n\nTITLE: GPUStack Service Environment Variables Configuration\nDESCRIPTION: Example of setting environment variables for the GPUStack service through an environment file, showing how to configure Hugging Face tokens and endpoints.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/installation-script.md#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nHF_TOKEN=\"mytoken\"\nHF_ENDPOINT=\"https://my-hf-endpoint\"\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack Worker on Windows\nDESCRIPTION: PowerShell command to install a GPUStack worker node on Windows systems, connecting it to an existing server. Requires specifying the server URL and authentication token.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/installation-script.md#2025-04-11_snippet_5\n\nLANGUAGE: powershell\nCODE:\n```\nInvoke-Expression \"& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --server-url http://myserver --token mytoken\"\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack Worker on Linux or macOS\nDESCRIPTION: Command to install a GPUStack worker node on Linux or macOS systems, connecting it to an existing server. Requires specifying the server URL and authentication token.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/installation-script.md#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | sh -s - --server-url http://myserver --token mytoken\n```\n\n----------------------------------------\n\nTITLE: Retrieving GPUStack Admin Password on Windows\nDESCRIPTION: This PowerShell command retrieves the initial admin password for GPUStack on Windows systems.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/quickstart.md#2025-04-11_snippet_5\n\nLANGUAGE: powershell\nCODE:\n```\nGet-Content -Path \"$env:APPDATA\\gpustack\\initial_admin_password\" -Raw\n```\n\n----------------------------------------\n\nTITLE: Retrieving Authentication Token on Linux/macOS Server\nDESCRIPTION: Command to retrieve the authentication token from a GPUStack server running on Linux or macOS. This token is required when adding worker nodes to the cluster.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/installation-script.md#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncat /var/lib/gpustack/token\n```\n\n----------------------------------------\n\nTITLE: One-time Chat with GPUStack Using Llama3 Model\nDESCRIPTION: Shows how to use GPUStack chat for a one-time inference with the Llama3 model. This example sends a prompt to tell a joke, demonstrating the immediate response functionality.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/cli-reference/chat.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngpustack chat llama3 \"tell me a joke.\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving Authentication Token on Windows Server\nDESCRIPTION: PowerShell command to retrieve the authentication token from a GPUStack server running on Windows. This token is required when adding worker nodes to the cluster.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/installation-script.md#2025-04-11_snippet_6\n\nLANGUAGE: powershell\nCODE:\n```\nGet-Content -Path \"$env:APPDATA\\gpustack\\token\" -Raw\n```\n\n----------------------------------------\n\nTITLE: Running GPUStack Server or Worker with Basic Command\nDESCRIPTION: The basic command to start GPUStack server or worker. Additional options can be provided to customize the behavior.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/cli-reference/start.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngpustack start [OPTIONS]\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack CLI with pip\nDESCRIPTION: Installs the basic GPUStack package using pip. Python version 3.10 to 3.12 is required as a prerequisite.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/manual-installation.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install gpustack\n```\n\n----------------------------------------\n\nTITLE: Specifying Text Prompt for Image Editing in GPUStack\nDESCRIPTION: This snippet shows the text prompt used to describe the desired changes for editing an image in GPUStack. It specifies the hair style and color to be applied to the selected area of the image.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/editing-images.md#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nPink short hair bang, natural\n```\n\n----------------------------------------\n\nTITLE: Retrieving GPUStack Authentication Token\nDESCRIPTION: Gets the authentication token needed to register workers with the GPUStack server.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/manual-installation.md#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncat /var/lib/gpustack/token\n```\n\n----------------------------------------\n\nTITLE: GPUStack Tools Download Command Usage\nDESCRIPTION: Basic command syntax for downloading dependency tools using gpustack. The command supports various configuration flags for customizing the download location, system architecture, and target devices.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/cli-reference/download-tools.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngpustack download-tools [OPTIONS]\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack on Windows Server Node\nDESCRIPTION: This PowerShell command installs GPUStack on a Windows server node by downloading and executing the installation script using Invoke-Expression and Invoke-WebRequest.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/setting-up-a-multi-node-gpustack-cluster.md#2025-04-11_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n```\n\n----------------------------------------\n\nTITLE: Upgrading GPUStack to Specific Version on Windows\nDESCRIPTION: This PowerShell command upgrades GPUStack to a specific version on Windows systems. It sets the INSTALL_PACKAGE_SPEC environment variable and existing installation environment variables before running the script.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/upgrade.md#2025-04-11_snippet_3\n\nLANGUAGE: powershell\nCODE:\n```\n$env:INSTALL_PACKAGE_SPEC = gpustack==x.y.z\n$env:<EXISTING_INSTALL_ENV> = <EXISTING_INSTALL_ENV_VALUE>\nInvoke-Expression \"& { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } <EXISTING_GPUSTACK_ARGS>\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving GPUStack Token on Linux/macOS\nDESCRIPTION: This command retrieves the GPUStack authentication token on a Linux or macOS server node by reading the contents of the token file.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/setting-up-a-multi-node-gpustack-cluster.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncat /var/lib/gpustack/token\n```\n\n----------------------------------------\n\nTITLE: Upgrading GPUStack Docker Image\nDESCRIPTION: This command pulls the latest GPUStack Docker image with a specific version tag. After pulling the new image, the GPUStack service needs to be restarted with the updated image.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/upgrade.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull gpustack/gpustack:vX.Y.Z\n```\n\n----------------------------------------\n\nTITLE: Retrieving GPUStack Token on Windows\nDESCRIPTION: This PowerShell command retrieves the GPUStack authentication token on a Windows server node by reading the contents of the token file.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/setting-up-a-multi-node-gpustack-cluster.md#2025-04-11_snippet_3\n\nLANGUAGE: powershell\nCODE:\n```\nGet-Content -Path \"$env:APPDATA\\gpustack\\token\" -Raw\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack CLI with Additional Dependencies\nDESCRIPTION: Installs GPUStack with all available dependencies including 'vllm' (Linux AMD64 only) and 'audio' components.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/manual-installation.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# vllm is currently only available for Linux on AMD64\npip install gpustack[all]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Initial Admin Password on Windows\nDESCRIPTION: This PowerShell command retrieves the initial admin password for the GPUStack UI on a Windows server node by reading the contents of the password file.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/setting-up-a-multi-node-gpustack-cluster.md#2025-04-11_snippet_7\n\nLANGUAGE: powershell\nCODE:\n```\nGet-Content -Path \"$env:APPDATA\\gpustack\\initial_admin_password\" -Raw\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response from Rerank API\nDESCRIPTION: Sample JSON output from the GPUStack Rerank API showing reranked documents with their relevance scores. The response includes the model used, results sorted by relevance, and token usage statistics.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/rerank-api.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"bge-reranker-v2-m3\",\n  \"object\": \"list\",\n  \"results\": [\n    {\n      \"document\": {\n        \"text\": \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n      },\n      \"index\": 2,\n      \"relevance_score\": 1.951932668685913\n    },\n    {\n      \"document\": {\n        \"text\": \"it is a bear\"\n      },\n      \"index\": 1,\n      \"relevance_score\": -3.7347371578216553\n    },\n    {\n      \"document\": {\n        \"text\": \"hi\"\n      },\n      \"index\": 0,\n      \"relevance_score\": -6.157620906829834\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 69,\n    \"total_tokens\": 69\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack on Windows\nDESCRIPTION: This PowerShell command installs GPUStack on Windows systems when run as administrator.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/README.md#2025-04-11_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n```\n\n----------------------------------------\n\nTITLE: Curl Command for Create Image Edit API\nDESCRIPTION: Example of using curl to call the Create Image Edit API using multipart/form-data. The request includes an image file, mask file, prompt, and other basic parameters for image editing.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/image-generation-apis.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport GPUSTACK_API_KEY=myapikey\ncurl http://myserver/v1-openai/image/edit \\\n    -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n    -F image=\"@otter.png\" \\\n    -F mask=\"@mask.png\" \\\n    -F prompt=\"A lovely cat\" \\\n    -F n=1 \\\n    -F size=\"512x512\"\n```\n\n----------------------------------------\n\nTITLE: Running Chat with llama3.2 Model\nDESCRIPTION: This command demonstrates how to use GPUStack to chat with the llama3.2 model.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngpustack chat llama3.2 \"tell me a joke.\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Stable-Diffusion-v3-5-Large-Turbo Model with Example Prompt\nDESCRIPTION: Example prompt and parameter configuration for the Stable-Diffusion-v3-5-Large-Turbo image generation model. For this turbo model, CFG is disabled (set to 1.0) and uses the heun sampler with karras scheduler.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/recommended-parameters-for-image-generation-models.md#2025-04-11_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nPrompt: This dreamlike digital art captures a vibrant, kaleidoscopic bird in a lush rainforest\nSize: 768x1024\nSampler: heun\nScheduler: karras\nSteps: 15\nCFG: 1.0\nSeed: 2536656539\n```\n\n----------------------------------------\n\nTITLE: Retrieving Admin Password on Linux or macOS\nDESCRIPTION: This command retrieves the initial admin password for GPUStack on Linux or macOS systems.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncat /var/lib/gpustack/initial_admin_password\n```\n\n----------------------------------------\n\nTITLE: Configuring Stable-Diffusion-v3-5-Medium Model with Example Prompt\nDESCRIPTION: Example prompt and parameter configuration for the Stable-Diffusion-v3-5-Medium image generation model. This configuration includes both a positive and negative prompt with the euler sampler and discrete scheduler.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/recommended-parameters-for-image-generation-models.md#2025-04-11_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nPrompt: Plush toy, a box of French fries, pink bag, long French fries, smiling expression, round eyes, smiling mouth, bright colors, simple composition, clean background, jellycat style,\nNegative Prompt: ng_deepnegative_v1_75t,(badhandv4:1.2),EasyNegative,(worst quality:2)\nSize: 768x1024\nSampler: euler\nScheduler: discrete\nSteps: 28\nCFG: 4.5\nSeed: 3353126565\n```\n\n----------------------------------------\n\nTITLE: Using GPUStack API with curl\nDESCRIPTION: This bash script demonstrates how to use the GPUStack API with curl, including setting the API key and making a request to the chat completions endpoint.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/README.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport GPUSTACK_API_KEY=myapikey\ncurl http://myserver/v1-openai/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"llama3.2\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"stream\": true\n  }'\n```\n\n----------------------------------------\n\nTITLE: Configuring Stable-Diffusion-v3-Medium Model with Example Prompt\nDESCRIPTION: Example prompt and parameter configuration for the Stable-Diffusion-v3-Medium image generation model. This configuration uses the euler sampler with discrete scheduler at a custom resolution of 768x1280.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/recommended-parameters-for-image-generation-models.md#2025-04-11_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nPrompt: A guitar crafted from a watermelon, realistic, close-up, ultra-HD, digital art, with smoke and ice cubes, soft lighting, dramatic stage effects of light and shadow, pastel aesthetic filter, time-lapse photography, macro photography, ultra-high resolution, perfect design composition, surrealism, hyper-imaginative, ultra-realistic, ultra-HD quality\nSize: 768x1280\nSampler: euler\nScheduler: discrete\nSteps: 30\nCFG: 5.0\nSeed: 1937760054\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack on Windows\nDESCRIPTION: This PowerShell command downloads and executes an installation script for GPUStack on Windows systems.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/quickstart.md#2025-04-11_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n```\n\n----------------------------------------\n\nTITLE: Generating Image with Stable Diffusion Model\nDESCRIPTION: This command uses GPUStack to generate an image using the stable-diffusion-v3-5-large-turbo model with a specific prompt.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngpustack draw hf.co/gpustack/stable-diffusion-v3-5-large-turbo-GGUF:stable-diffusion-v3-5-large-turbo-Q4_0.gguf \\\n\"A minion holding a sign that says 'GPUStack'. The background is filled with futuristic elements like neon lights, circuit boards, and holographic displays. The minion is wearing a tech-themed outfit, possibly with LED lights or digital patterns. The sign itself has a sleek, modern design with glowing edges. The overall atmosphere is high-tech and vibrant, with a mix of dark and neon colors.\" \\\n--sample-steps 5 --show\n```\n\n----------------------------------------\n\nTITLE: Generating Image with GPUStack Stable Diffusion Model\nDESCRIPTION: This command uses GPUStack to generate an image using a stable diffusion model with a detailed prompt.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/quickstart.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngpustack draw hf.co/gpustack/stable-diffusion-v3-5-large-turbo-GGUF:stable-diffusion-v3-5-large-turbo-Q4_0.gguf \\\n\"A minion holding a sign that says 'GPUStack'. The background is filled with futuristic elements like neon lights, circuit boards, and holographic displays. The minion is wearing a tech-themed outfit, possibly with LED lights or digital patterns. The sign itself has a sleek, modern design with glowing edges. The overall atmosphere is high-tech and vibrant, with a mix of dark and neon colors.\" \\\n--sample-steps 5 --show\n```\n\n----------------------------------------\n\nTITLE: Retrieving Admin Password on Windows\nDESCRIPTION: This PowerShell command retrieves the initial admin password for GPUStack on Windows systems.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/README.md#2025-04-11_snippet_5\n\nLANGUAGE: powershell\nCODE:\n```\nGet-Content -Path \"$env:APPDATA\\gpustack\\initial_admin_password\" -Raw\n```\n\n----------------------------------------\n\nTITLE: Retrieving GPUStack Admin Password on Linux or macOS\nDESCRIPTION: This command retrieves the initial admin password for GPUStack on Linux or macOS systems.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/quickstart.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncat /var/lib/gpustack/initial_admin_password\n```\n\n----------------------------------------\n\nTITLE: Resetting Admin Password - GPUStack\nDESCRIPTION: Command to reset the admin password on the GPUStack server node when the current password is forgotten or needs to be changed.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/troubleshooting.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngpustack reset-admin-password\n```\n\n----------------------------------------\n\nTITLE: Upgrading GPUStack to Specific Version on Linux/macOS\nDESCRIPTION: This command upgrades GPUStack to a specific version on Linux and macOS systems. It uses the INSTALL_PACKAGE_SPEC environment variable to specify the desired version.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/upgrade.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | INSTALL_PACKAGE_SPEC=gpustack==x.y.z <EXISTING_INSTALL_ENV> sh -s - <EXISTING_GPUSTACK_ARGS>\n```\n\n----------------------------------------\n\nTITLE: Upgrading GPUStack on Windows using Installation Script\nDESCRIPTION: This PowerShell command upgrades GPUStack to the latest version on Windows systems. It requires setting existing installation environment variables before running the script.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/upgrade.md#2025-04-11_snippet_2\n\nLANGUAGE: powershell\nCODE:\n```\n$env:<EXISTING_INSTALL_ENV> = <EXISTING_INSTALL_ENV_VALUE>\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n```\n\n----------------------------------------\n\nTITLE: Manually Upgrading GPUStack using pip\nDESCRIPTION: This command upgrades GPUStack to the latest version using pip. After upgrading, the GPUStack service needs to be restarted according to the specific setup.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/upgrade.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade gpustack\n```\n\n----------------------------------------\n\nTITLE: Uninstalling GPUStack on Linux/macOS using Script\nDESCRIPTION: Command to execute the automated uninstallation script on Linux or macOS systems. Requires sudo privileges and removes all GPUStack components including local datastore, configuration, and cache.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/uninstallation.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo /var/lib/gpustack/uninstall.sh\n```\n\n----------------------------------------\n\nTITLE: Setting Up GPUStack Development Environment\nDESCRIPTION: Command to set up the development environment for the GPUStack project. This installs all necessary dependencies required for development.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/development.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake install\n```\n\n----------------------------------------\n\nTITLE: Running GPUStack Application\nDESCRIPTION: Command to run the GPUStack application using Poetry. This executes the main application entry point.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/development.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry run gpustack\n```\n\n----------------------------------------\n\nTITLE: Building GPUStack Package\nDESCRIPTION: Command to build the GPUStack package. This creates distribution artifacts that can be found in the 'dist' directory.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/development.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Running GPUStack Tests\nDESCRIPTION: Command to run the test suite for the GPUStack project. This executes all tests to verify the functionality of the codebase.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/development.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Adding Regular Dependencies to GPUStack\nDESCRIPTION: Command to add a new dependency to the GPUStack project using Poetry. This updates the project's dependencies for runtime use.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/development.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npoetry add <something>\n```\n\n----------------------------------------\n\nTITLE: Adding Development Dependencies to GPUStack\nDESCRIPTION: Command to add a new development or testing dependency to the GPUStack project using Poetry. This updates the project's dev dependencies without affecting runtime dependencies.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/development.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npoetry add --group dev <something>\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack with Hugging Face Authentication\nDESCRIPTION: Command to install GPUStack using a curl installer script with a Hugging Face API key for model access authorization.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/using-vision-language-models.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sfL https://get.gpustack.ai | sh -s - --huggingface-token <Hugging Face API Key>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Default Admin Password for GPUStack UI\nDESCRIPTION: Command to read the initial admin password from the GPUStack installation directory, required for first-time login to the web interface.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/using-vision-language-models.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncat /var/lib/gpustack/initial_admin_password\n```\n\n----------------------------------------\n\nTITLE: Image Generation Prompt Example\nDESCRIPTION: Example prompt text for generating an image of a fantasy female character with ethereal elements using Stable Diffusion model. The prompt describes specific visual elements including hair style, background colors, and overall artistic style.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/using-image-generation-models.md#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\na female character with long, flowing hair that appears to be made of ethereal, swirling patterns resembling the Northern Lights or Aurora Borealis. The background is dominated by deep blues and purples, creating a mysterious and dramatic atmosphere. The character's face is serene, with pale skin and striking features. She wears a dark-colored outfit with subtle patterns. The overall style of the artwork is reminiscent of fantasy or supernatural genres.\n```\n\n----------------------------------------\n\nTITLE: Configuring Image Edit Parameters in GPUStack\nDESCRIPTION: This code block provides the parameters used to reproduce a specific image editing result in GPUStack. It includes settings for image size, sampling method, guidance, CFG scale, strength, and seed.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/editing-images.md#2025-04-11_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nSize: 768x1024(3:4)\nSample Method: euler\nSchedule Method: discrete\nSampling Steps: 50\nGuidance: 30.0\nCFG Scale: 1.0\nStrength: 1.0\nSeed: 656821733471329\nText Prompt: Pink short hair bang, natural\n```\n\n----------------------------------------\n\nTITLE: Configuring FLUX.1-dev Model with Example Prompt\nDESCRIPTION: Example prompt and parameter configuration for the FLUX.1-dev image generation model. This configuration disables CFG (set to 1.0) as recommended for FLUX models and uses the euler sampler with discrete scheduler.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/recommended-parameters-for-image-generation-models.md#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nPrompt: A kangaroo holding a beer,wearing ski goggles and passionately singing silly songs.\nSize: 1024x1024\nSampler: euler\nScheduler: discrete\nSteps: 20\nCFG: 1.0\nSeed: 838887451\n```\n\n----------------------------------------\n\nTITLE: Configuring FLUX.1-schnell Model with Example Prompt\nDESCRIPTION: Example prompt and parameter configuration for the FLUX.1-schnell image generation model. This fast model works best with CFG disabled (set to 1.0) and requires only 2-4 steps with the euler sampler and discrete scheduler.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/recommended-parameters-for-image-generation-models.md#2025-04-11_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nPrompt: A mischievous ferret with a playful grin squeezes itself into a large glass jar, surrounded by colorful candy. The jar sits on a wooden table in a cozy kitchen, and warm sunlight filters through a nearby window\nSize: 1024x1024\nSampler: euler\nScheduler: discrete\nSteps: 3\nCFG: 1.0\nSeed: 1565801500\n```\n\n----------------------------------------\n\nTITLE: Configuring SDXL-base-v1.0 Model with Example Prompt\nDESCRIPTION: Example prompt and parameter configuration for the SDXL-base-v1.0 image generation model. This configuration uses the dpm++2m sampler with exponential scheduler and includes both positive and negative prompts.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/recommended-parameters-for-image-generation-models.md#2025-04-11_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nPrompt: Weeds blowing in the wind,By the seaside,Ultra-realistic,Majestic epic scenery,excessively splendid ancient rituals,vibrant,beautiful Eastern fantasy,bright sunshine,pink peach blossoms,daytime perspective.\nNegative Prompt: ng_deepnegative_v1_75t,(badhandv4:1.2),EasyNegative,(worst quality:2),\nSize: 768x1280\nSampler: dpm++2m\nScheduler: exponential\nSteps: 30\nCFG: 5.0\nSeed: 3754742591\n```\n\n----------------------------------------\n\nTITLE: Configuring Stable-Diffusion-v2-1-Turbo Model with Example Prompt\nDESCRIPTION: Example prompt and parameter configuration for the Stable-Diffusion-v2-1-Turbo image generation model. This fast turbo model works with CFG disabled (set to 1.0) and requires only 6 steps with the euler_a sampler.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/recommended-parameters-for-image-generation-models.md#2025-04-11_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nPrompt: A burger patty, with the bottom bun and lettuce and tomatoes.\nSize: 512x512\nSampler: euler_a\nScheduler: discrete\nSteps: 6\nCFG: 1.0\nSeed: 1375548153\n```\n\n----------------------------------------\n\nTITLE: Streaming Image Generation API Request and Response Format\nDESCRIPTION: Example of JSON request and text/event-stream response for the image generation API with streaming enabled. The response shows progress updates and final image data with usage statistics.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/image-generation-apis.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\nREQUEST : (application/json)\n{\n  \"n\": 1,\n  \"response_format\": \"b64_json\",\n  \"size\": \"512x512\",\n  \"prompt\": \"A lovely cat\",\n  \"quality\": \"standard\",\n  \"stream\": true,\n  \"stream_options\": {\n    \"include_usage\": true, // return usage information\n  }\n}\n\nRESPONSE : (text/event-stream)\ndata: {\"created\":1731916353,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":10.0}], ...}\n...\ndata: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":50.0}], ...}\n...\ndata: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":100.0,\"b64_json\":\"...\"}], \"usage\":{\"generation_per_second\":...,\"time_per_generation_ms\":...,\"time_to_process_ms\":...}, ...}\ndata: [DONE]\n```\n\n----------------------------------------\n\nTITLE: Advanced Options for Image Generation API\nDESCRIPTION: Example of JSON request with advanced options for the image generation API. Options include sampler, schedule, seed, cfg_scale, sample_steps, and negative_prompt to fine-tune the generation process.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/image-generation-apis.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\nREQUEST : (application/json)\n{\n  \"n\": 1,\n  \"response_format\": \"b64_json\",\n  \"size\": \"512x512\",\n  \"prompt\": \"A lovely cat\",\n  \"sampler\": \"euler\",      // required, select from euler_a;euler;heun;dpm2;dpm++2s_a;dpm++2m;dpm++2mv2;ipndm;ipndm_v;lcm\n  \"schedule\": \"default\",   // optional, select from default;discrete;karras;exponential;ays;gits\n  \"seed\": null,            // optional, random seed\n  \"cfg_scale\": 4.5,        // optional, for sampler, the scale of classifier-free guidance in the output phase\n  \"sample_steps\": 20,      // optional, number of sample steps\n  \"negative_prompt\": \"\",   // optional, negative prompt\n  \"stream\": true,\n  \"stream_options\": {\n    \"include_usage\": true, // return usage information\n  }\n}\n\nRESPONSE : (text/event-stream)\ndata: {\"created\":1731916353,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":10.0}], ...}\n...\ndata: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":50.0}], ...}\n...\ndata: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":100.0,\"b64_json\":\"...\"}], \"usage\":{\"generation_per_second\":...,\"time_per_generation_ms\":...,\"time_to_process_ms\":...}, ...}\ndata: [DONE]\n```\n\n----------------------------------------\n\nTITLE: Streaming Image Edit API Request and Response Format\nDESCRIPTION: Example of multipart/form-data request and text/event-stream response for the image edit API with streaming enabled. The response shows two cases: successful image editing with progress updates and error for invalid input image.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/image-generation-apis.md#2025-04-11_snippet_2\n\nLANGUAGE: json\nCODE:\n```\nREQUEST: (multipart/form-data)\nn=1\nresponse_format=b64_json\nsize=512x512\nprompt=\"A lovely cat\"\nquality=standard\nimage=...                         // required\nmask=...                          // optional\nstream=true\nstream_options_include_usage=true // return usage information\n\nRESPONSE : (text/event-stream)\nCASE 1: correct input image\n  data: {\"created\":1731916353,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":10.0}], ...}\n  ...\n  data: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":50.0}], ...}\n  ...\n  data: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":100.0,\"b64_json\":\"...\"}], \"usage\":{\"generation_per_second\":...,\"time_per_generation_ms\":...,\"time_to_process_ms\":...}, ...}\n  data: [DONE]\nCASE 2: illegal input image\n  error: {\"code\": 400, \"message\": \"Invalid image\", \"type\": \"invalid_request_error\"}\n```\n\n----------------------------------------\n\nTITLE: Advanced Options for Image Edit API\nDESCRIPTION: Example of multipart/form-data request with advanced options for the image edit API. Options include sampler, schedule, seed, cfg_scale, sample_steps, and negative_prompt to fine-tune the editing process.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/image-generation-apis.md#2025-04-11_snippet_3\n\nLANGUAGE: json\nCODE:\n```\nREQUEST: (multipart/form-data)\nn=1\nresponse_format=b64_json\nsize=512x512\nprompt=\"A lovely cat\"\nimage=...                         // required\nmask=...                          // optional\nsampler=euler                     // required, select from euler_a;euler;heun;dpm2;dpm++2s_a;dpm++2m;dpm++2mv2;ipndm;ipndm_v;lcm\nschedule=default                  // optional, select from default;discrete;karras;exponential;ays;gits\nseed=null                         // optional, random seed\ncfg_scale=4.5                     // optional, for sampler, the scale of classifier-free guidance in the output phase\nsample_steps=20                   // optional, number of sample steps\nnegative_prompt=\"\"                // optional, negative prompt\nstream=true\nstream_options_include_usage=true // return usage information\n\nRESPONSE : (text/event-stream)\nCASE 1: correct input image\n  data: {\"created\":1731916353,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":10.0}], ...}\n  ...\n  data: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":50.0}], ...}\n  ...\n  data: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":100.0,\"b64_json\":\"...\"}], \"usage\":{\"generation_per_second\":...,\"time_per_generation_ms\":...,\"time_to_process_ms\":...}, ...}\n  data: [DONE]\nCASE 2: illegal input image\n  error: {\"code\": 400, \"message\": \"Invalid image\", \"type\": \"invalid_request_error\"}\n```\n\n----------------------------------------\n\nTITLE: Curl Command for Create Image API\nDESCRIPTION: Example of using curl to call the Create Image API with streaming enabled. The request includes basic parameters like prompt, size, and response format along with streaming options.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/image-generation-apis.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport GPUSTACK_API_KEY=myapikey\ncurl http://myserver/v1-openai/image/generate \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n    -d '{\n        \"n\": 1,\n        \"response_format\": \"b64_json\",\n        \"size\": \"512x512\",\n        \"prompt\": \"A lovely cat\",\n        \"quality\": \"standard\",\n        \"stream\": true,\n        \"stream_options\": {\n        \"include_usage\": true\n        }\n    }'\n```\n\n----------------------------------------\n\nTITLE: Image Generation Parameters Table\nDESCRIPTION: Markdown table defining the available parameters for image generation, including their default values and descriptions. Parameters control aspects like image count, size, sampling method, guidance, and preview options.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/playground/image.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Parameter         | Default    | Description                                                                                                                                                         |\n| ----------------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `Counts`          | `1`        | Number of images to generate.                                                                                                                                       |\n| `Size`            | `512x512`  | The size of the generated image in 'widthxheight' format.                                                                                                           |\n| `Sample Method`   | `euler_a`  | The sampler algorithm for image generation. Options include 'euler_a', 'euler', 'heun', 'dpm2', 'dpm++2s_a', 'dpm++2m', 'dpm++2mv2', 'ipndm', 'ipndm_v', and 'lcm'. |\n| `Schedule Method` | `discrete` | The noise scheduling method.                                                                                                                                        |\n| `Sampling Steps`  | `10`       | The number of sampling steps to perform. Higher values may improve image quality at the cost of longer processing time.                                             |\n| `Guidance`       | `3.5`      | The scale for classifier-free guidance. A higher value increases adherence to the prompt.                                                                           |\n| `CFG Scale`      | `4.5`      | The scale for classifier-free guidance. A higher value increases adherence to the prompt.                                                                           |\n| `Negative Prompt` | (empty)    | A negative prompt to specify what the image should avoid.                                                                                                           |\n| `Preview`         | `Faster`   | Controls how the image generation process is displayed. Options include 'Faster', 'Normal', 'None'.                                                                                                    |\n| `Seed`            | (empty)    | Random seed.                                                                                                                                                        |\n```\n\n----------------------------------------\n\nTITLE: Executing GPUStack Draw Command in Bash\nDESCRIPTION: This snippet shows the basic syntax for using the 'gpustack draw' command to generate an image with a diffusion model. It requires specifying a model and a text prompt.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/cli-reference/draw.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngpustack draw [model] [prompt]\n```\n\n----------------------------------------\n\nTITLE: Example Response from GPUStack Embeddings API\nDESCRIPTION: This JSON snippet shows the expected response format when calling the embeddings API. It returns the generated embedding vector, metadata about the request, and token usage information.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/creating-text-embeddings.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.012189436703920364, 0.016934078186750412, 0.003965042531490326,\n        -0.03453584015369415, -0.07623119652271271, -0.007116147316992283,\n        0.11278388649225235, 0.019714849069714546, 0.010370955802500248,\n        -0.04219457507133484, -0.029902394860982895, 0.01122555136680603,\n        0.022912170737981796, 0.031186765059828758, 0.006303929258137941,\n        # ... additional values\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"bge-small-en-v1.5\",\n  \"object\": \"list\",\n  \"usage\": { \"prompt_tokens\": 12, \"total_tokens\": 12 }\n}\n```\n\n----------------------------------------\n\nTITLE: Reranking Documents using GPUStack API with curl\nDESCRIPTION: This bash script demonstrates how to use curl to send a request to the GPUStack API for reranking documents. It sets environment variables for the server URL and API key, then sends a POST request with a JSON payload containing the model, query, and documents to be reranked.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/using-reranker-models.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport SERVER_URL=<your-server-url>\nexport GPUSTACK_API_KEY=<your-api-key>\ncurl $SERVER_URL/v1/rerank \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n    -d '{\n        \"model\": \"bge-reranker-v2-m3\",\n        \"query\": \"What is a panda?\",\n        \"top_n\": 3,\n        \"documents\": [\n            \"hi\",\n            \"it is a bear\",\n            \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n        ]\n    }' | jq\n```\n\n----------------------------------------\n\nTITLE: Retrieving Initial Admin Password on Linux/macOS\nDESCRIPTION: This command retrieves the initial admin password for the GPUStack UI on a Linux or macOS server node by reading the contents of the password file.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/setting-up-a-multi-node-gpustack-cluster.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncat /var/lib/gpustack/initial_admin_password\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response from GPUStack Reranker API\nDESCRIPTION: This JSON snippet shows an example response from the GPUStack Reranker API. It includes the model used, a list of reranked results with relevance scores, and usage information for token counts.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/using-reranker-models.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"bge-reranker-v2-m3\",\n  \"object\": \"list\",\n  \"results\": [\n    {\n      \"document\": {\n        \"text\": \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n      },\n      \"index\": 2,\n      \"relevance_score\": 1.951932668685913\n    },\n    {\n      \"document\": {\n        \"text\": \"it is a bear\"\n      },\n      \"index\": 1,\n      \"relevance_score\": -3.7347371578216553\n    },\n    {\n      \"document\": {\n        \"text\": \"hi\"\n      },\n      \"index\": 0,\n      \"relevance_score\": -6.157620906829834\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 69,\n    \"total_tokens\": 69\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Making a Rerank API Request with cURL in Bash\nDESCRIPTION: Example showing how to make a request to the GPUStack Rerank API using cURL. The request includes an API key, query text, and a list of documents to be reranked based on relevance to the query.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/rerank-api.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport GPUSTACK_API_KEY=myapikey\ncurl http://myserver/v1/rerank \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n    -d '{\n        \"model\": \"bge-reranker-v2-m3\",\n        \"query\": \"What is a panda?\",\n        \"top_n\": 3,\n        \"documents\": [\n            \"hi\",\n            \"it is a bear\",\n            \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n        ]\n    }' | jq\n```\n\n----------------------------------------\n\nTITLE: Initiating Interactive Chat Session with GPUStack\nDESCRIPTION: Illustrates how to start an interactive chat session with GPUStack using the Llama3 model. This mode allows for continuous conversation with the AI model, including special commands for managing the chat session.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/cli-reference/chat.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngpustack chat llama3\n```\n\n----------------------------------------\n\nTITLE: Example Prompt for DeepSeek R1 Text Generation in GPUStack\nDESCRIPTION: This snippet shows an example prompt to be used with the DeepSeek R1 model in GPUStack's Playground for text generation. It demonstrates a simple sequence completion task.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/using-deepseek-r1.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```\n2, 4, 6, 8, > What is the next number?\n```\n```\n\n----------------------------------------\n\nTITLE: Making Tool-Enabled API Request with curl\nDESCRIPTION: Example of making an API request to GPUStack for weather information using tool calling capabilities. The script demonstrates how to structure the request with proper headers, tools configuration, and authentication.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/inference-with-tool-calling.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport GPUSTACK_SERVER_URL=<your-server-url>\nexport GPUSTACK_API_KEY=<your-api-key>\ncurl $GPUSTACK_SERVER_URL/v1-openai/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n-d '{\n  \"model\": \"qwen2.5-7b-instruct\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What\\'s the weather like in Boston today?\"\n    }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n              \"type\": \"string\",\n              \"enum\": [\"celsius\", \"fahrenheit\"]\n            }\n          },\n          \"required\": [\"location\"]\n        }\n      }\n    }\n  ],\n  \"tool_choice\": \"auto\"}'\n```\n\n----------------------------------------\n\nTITLE: API Response Structure for Tool Calling\nDESCRIPTION: Example response from the GPUStack API showing the structure of a tool calling response, including tool execution details and token usage information.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/inference-with-tool-calling.md#2025-04-11_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"qwen2.5-7b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": null,\n        \"tool_calls\": [\n          {\n            \"id\": \"chatcmpl-tool-b99d32848b324eaea4bac5a5830d00b8\",\n            \"type\": \"function\",\n            \"function\": {\n              \"name\": \"get_current_weather\",\n              \"arguments\": \"{\\\"location\\\": \\\"Boston, MA\\\", \\\"unit\\\": \\\"fahrenheit\\\"}\"\n            }\n          }\n        ]\n      },\n      \"finish_reason\": \"tool_calls\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 212,\n    \"total_tokens\": 242,\n    \"completion_tokens\": 30\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Making API Requests with curl\nDESCRIPTION: Example of making a chat completion request to GPUStack's OpenAI-compatible API using curl. Demonstrates setting API key, headers, and sending a JSON payload for streaming chat completion.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/openai-compatible-apis.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport GPUSTACK_API_KEY=myapikey\ncurl http://myserver/v1-openai/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"llama3\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"stream\": true\n  }'\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Python Client\nDESCRIPTION: Implementation using the official OpenAI Python client library to make chat completion requests. Shows how to configure the client with custom base URL and API key.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/openai-compatible-apis.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://myserver/v1-openai\", api_key=\"myapikey\")\n\ncompletion = client.chat.completions.create(\n  model=\"llama3\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Node.js Client\nDESCRIPTION: Implementation using the official OpenAI Node.js client library for chat completions. Demonstrates async/await pattern and client configuration with custom endpoints.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/openai-compatible-apis.md#2025-04-11_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst OpenAI = require(\"openai\");\n\nconst openai = new OpenAI({\n  apiKey: \"myapikey\",\n  baseURL: \"http://myserver/v1-openai\",\n});\n\nasync function main() {\n  const params = {\n    model: \"llama3\",\n    messages: [\n      {\n        role: \"system\",\n        content: \"You are a helpful assistant.\",\n      },\n      {\n        role: \"user\",\n        content: \"Hello!\",\n      },\n    ],\n  };\n  const chatCompletion = await openai.chat.completions.create(params);\n  console.log(chatCompletion.choices[0].message);\n}\nmain();\n```\n\n----------------------------------------\n\nTITLE: Adding GPUStack URL to Chrome's Trusted List\nDESCRIPTION: Example of how to add the GPUStack URL to Chrome's trusted list for enabling microphone access on non-HTTPS connections. This involves navigating to chrome://flags/ and adding the URL to 'Insecure origins treated as secure'.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/user-guide/playground/audio.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n**Example:**\nIn Chrome, navigate to `chrome://flags/`, add the GPUStack URL to \"Insecure origins treated as secure,\" and enable this option.\n```\n\n----------------------------------------\n\nTITLE: Installing GPUStack using PowerShell on Snapdragon X devices\nDESCRIPTION: This command downloads and executes the GPUStack installation script from the gpustack.ai website. It requires running PowerShell as administrator and uses basic parsing for the web request.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-on-copilot-plus-pcs-with-snapdragon-x.md#2025-04-11_snippet_0\n\nLANGUAGE: powershell\nCODE:\n```\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n```\n\n----------------------------------------\n\nTITLE: Accessing GPUStack Logs - Linux/macOS\nDESCRIPTION: Path to access GPUStack log files on Linux or macOS systems. Logs are stored in the system's standard logging directory.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/troubleshooting.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n/var/log/gpustack.log\n```\n\n----------------------------------------\n\nTITLE: Accessing GPUStack Logs - Windows\nDESCRIPTION: Path to access GPUStack log files on Windows systems. Logs are stored in the user's AppData directory.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/troubleshooting.md#2025-04-11_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\n\"$env:APPDATA\\gpustack\\log\\gpustack.log\"\n```\n\n----------------------------------------\n\nTITLE: Setting Server Log Level - GPUStack\nDESCRIPTION: Command to configure the log level of the GPUStack server at runtime. Supports multiple log levels including trace, debug, info, warning, error, and critical.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/troubleshooting.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X PUT http://localhost/debug/log_level -d \"debug\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving GPUStack Server Token\nDESCRIPTION: Command to retrieve the token from GPUStack server, required for registering additional worker nodes to the server.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndocker exec -it gpustack-server cat /var/lib/gpustack/token\n```\n\n----------------------------------------\n\nTITLE: Retrieving GPUStack Admin Password\nDESCRIPTION: Command to extract the initial admin password from a running GPUStack container. This password is required for the first login to the GPUStack web interface.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/installation/docker-installation.md#2025-04-11_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\ndocker exec -it gpustack cat /var/lib/gpustack/initial_admin_password\n```\n\n----------------------------------------\n\nTITLE: GPUStack Container Success Message\nDESCRIPTION: Example output showing successful container startup messages.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-amd-gpus.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n2024-11-15T23:37:46+00:00 - gpustack.server.server - INFO - Serving on 0.0.0.0:80.\n2024-11-15T23:37:46+00:00 - gpustack.worker.worker - INFO - Starting GPUStack worker.\n```\n\n----------------------------------------\n\nTITLE: GPUStack Container Success Message\nDESCRIPTION: Example output indicating successful startup of the GPUStack container and worker processes.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/tutorials/running-inference-with-hygon-dcus.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n2024-11-15T23:37:46+00:00 - gpustack.server.server - INFO - Serving on 0.0.0.0:80.\n2024-11-15T23:37:46+00:00 - gpustack.worker.worker - INFO - Starting GPUStack worker.\n```\n\n----------------------------------------\n\nTITLE: Implementing Automatic Redirect in HTML for GPUStack Overview\nDESCRIPTION: This HTML meta tag automatically redirects the user to the 'overview/' page after 0 seconds. It's typically used in index files to send visitors to a main content page.\nSOURCE: https://github.com/gpustack/gpustack/blob/main/docs/index.md#2025-04-11_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<meta http-equiv=\"refresh\" content=\"0; url=overview/\" />\n```"
  }
]
[
  {
    "owner": "openvinotoolkit",
    "repo": "openvino",
    "content": "TITLE: TensorFlow GraphDef to OpenVINO\nDESCRIPTION: Converts a TensorFlow GraphDef to an OpenVINO model. Similar to the graph conversion, it sets up a TensorFlow session, defines input placeholders, performs a ReLU operation, and then converts the GraphDef to an OpenVINO model using `ov.convert_model`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nwith tf.compat.v1.Session() as sess:\n    inp1 = tf.compat.v1.placeholder(tf.float32, [100], 'Input1')\n    inp2 = tf.compat.v1.placeholder(tf.float32, [100], 'Input2')\n    output = tf.nn.relu(inp1 + inp2, name='Relu')\n    tf.compat.v1.global_variables_initializer()\n    model = sess.graph_def\n```\n\nLANGUAGE: python\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model(model)\n```\n\n----------------------------------------\n\nTITLE: Switching PyTorch Inference to OpenVINO with ResNet50\nDESCRIPTION: This snippet demonstrates how to switch inference from PyTorch to OpenVINO in an existing PyTorch application using ResNet50. It prepares the model, preprocesses the image, performs inference in both PyTorch and OpenVINO, and prints the classification results. It requires the `torchvision`, `torch`, `PIL`, `requests`, and `openvino` libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-pytorch.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision.io import read_image\nfrom torchvision.models import resnet50, ResNet50_Weights\nimport requests, PIL, io, torch\n\n# Get a picture of a cat from the web:\nimg = PIL.Image.open(io.BytesIO(requests.get(\"https://placekitten.com/200/300\").content))\n\n# Torchvision model and input data preparation from https://pytorch.org/vision/stable/models.html\n\nweights = ResNet50_Weights.DEFAULT\nmodel = resnet50(weights=weights)\nmodel.eval()\npreprocess = weights.transforms()\nbatch = preprocess(img).unsqueeze(0)\n\n# PyTorch model inference and post-processing\n\nprediction = model(batch).squeeze(0).softmax(0)\nclass_id = prediction.argmax().item()\nscore = prediction[class_id].item()\ncategory_name = weights.meta[\"categories\"][class_id]\nprint(f\"{category_name}: {100 * score:.1f}% (with PyTorch)\")\n\n# OpenVINO model preparation and inference with the same post-processing\n\nimport openvino as ov\ncompiled_model = ov.compile_model(ov.convert_model(model, example_input=batch))\n\nprediction = torch.tensor(compiled_model(batch)[0]).squeeze(0).softmax(0)\nclass_id = prediction.argmax().item()\nscore = prediction[class_id].item()\ncategory_name = weights.meta[\"categories\"][class_id]\nprint(f\"{category_name}: {100 * score:.1f}% (with OpenVINO)\")\n```\n\n----------------------------------------\n\nTITLE: Converting TensorFlow Model to OpenVINO IR via CLI\nDESCRIPTION: This command-line instruction shows how to convert a TensorFlow model (.pb file) to OpenVINO IR using the `ovc` command-line tool. The resulting IR can be read by OpenVINO for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\novc <INPUT_MODEL>.pb\n```\n\n----------------------------------------\n\nTITLE: Convert Llama 2 to OpenVINO IR using optimum-cli\nDESCRIPTION: Example usage of the optimum-cli tool to convert the Llama 2 model from Hugging Face to OpenVINO IR format and name it ov_llama_2.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noptimum-cli export openvino --model meta-llama/Llama-2-7b-chat-hf ov_llama_2\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenVINO Core in C++\nDESCRIPTION: This code includes the necessary OpenVINO header files and creates an `ov::Core` object. The `ov::Core` object is the entry point for using OpenVINO Runtime and is required to compile models and perform inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nov::Core core;\n```\n\n----------------------------------------\n\nTITLE: Convert SavedModel Format (TF1) using CLI\nDESCRIPTION: This snippet shows how to convert a TensorFlow 1 SavedModel format (directory containing ``.pb``, ``variables``, etc.) to OpenVINO IR using the `ovc` command-line tool. The input is the path to the SavedModel directory. The output is the OpenVINO IR model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\novc path_to_saved_model_dir\n```\n\n----------------------------------------\n\nTITLE: Process Inference Results in Python\nDESCRIPTION: This Python snippet illustrates the processing of inference results using the OpenVINO Python API. It demonstrates how to retrieve output tensors from the inference request and access the resulting data. This snippet requires the `openvino` package to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n//! [part6]\nresults = infer_request.get_output_tensor(0).data\n//! [part6]\n```\n\n----------------------------------------\n\nTITLE: Stable Diffusion 2 with OpenVINO and torch.compile (Python)\nDESCRIPTION: This code snippet demonstrates how to use torch.compile with OpenVINO to optimize Stable Diffusion 2 for image generation. It utilizes the diffusers library and the DPMSolverMultistepScheduler. Key steps include loading the pipeline, setting the scheduler, compiling the text encoder, UNet, and VAE decoder with the 'openvino' backend, and generating an image from a text prompt.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n\nmodel_id = \"stabilityai/stable-diffusion-2-1\"\n\n# Use the DPMSolverMultistepScheduler (DPM-Solver++) scheduler here instead\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n\n+ pipe.text_encoder = torch.compile(pipe.text_encoder, backend=\"openvino\") #Optional\n+ pipe.unet = torch.compile(pipe.unet, backend=“openvino”)\n+ pipe.vae.decode = torch.compile(pipe.vae.decode, backend=“openvino”) #Optional\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"astronaut_rides_horse.png\")\n```\n\n----------------------------------------\n\nTITLE: ROI Tensor Usage in Infer Request (C++)\nDESCRIPTION: Demonstrates pre-allocating input and cropping a frame to the size of generated bounding boxes using `ov::Tensor` with `ov::Tensor` and `ov::Coordinate` as parameters in C++. This technique optimizes inference by focusing on relevant regions of the input data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_25\n\nLANGUAGE: cpp\nCODE:\n```\n// Create random input data\nstd::vector<float> random_data(input_tensor.get_size());\nstd::generate(random_data.begin(), random_data.end(), []() { return static_cast<float>(std::rand()) / RAND_MAX; });\n\n// Fill input tensor with random data\nov::Tensor roi_tensor(input_tensor.get_element_type(), input_tensor.get_shape());\nstd::memcpy(roi_tensor.data(), random_data.data(), input_tensor.get_size() * sizeof(float));\n\n// Create ROI tensor from existing input tensor\nov::Tensor roi_tensor(input_tensor, {0, 0}, {1, 3, 224, 224});\ninfer_request.infer({input_tensor : roi_tensor});\n```\n\n----------------------------------------\n\nTITLE: Chat Scenario using LLM in C++\nDESCRIPTION: This C++ code snippet demonstrates a chat scenario using the OpenVINO GenAI LLMPipeline. It initializes the pipeline, starts a chat session, takes user input from stdin as a prompt, generates text using the LLM, and prints the output to stdout. It also uses a streamer function to print subwords as they are generated. The chat continues until the user enters an EOF character.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n\n            int main(int argc, char* argv[]) try {\n                if (2 != argc) {\n                    throw std::runtime_error(std::string{\"Usage: \"} + argv[0] + \" <MODEL_DIR>\");\n                }\n                std::string prompt;\n                std::string models_path = argv[1];\n\n                std::string device = \"CPU\";  // GPU, NPU can be used as well\n                ov::genai::LLMPipeline pipe(models_path, device);\n\n                ov::genai::GenerationConfig config;\n                config.max_new_tokens = 100;\n                std::function<bool(std::string)> streamer = [](std::string word) {\n                    std::cout << word << std::flush;\n                    return false;\n                };\n\n                pipe.start_chat();\n                std::cout << \"question:\\n\";\n                while (std::getline(std::cin, prompt)) {\n                    pipe.generate(prompt, config, streamer);\n                    std::cout << \"\\n----------\\n\"\n                        \"question:\\n\";\n                }\n                pipe.finish_chat();\n            } catch (const std::exception& error) {\n```\n\n----------------------------------------\n\nTITLE: Grouped Beam Search in C++\nDESCRIPTION: This C++ code configures grouped beam search for text generation. It retrieves the generation config from the LLMPipeline, modifies parameters such as `max_new_tokens`, `num_beam_groups`, `num_beams`, and `diversity_penalty`, and then generates text using the modified config. The result is printed to the console.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\nint main(int argc, char* argv[]) {\n   std::string model_path = argv[1];\n   ov::genai::LLMPipeline pipe(model_path, \"CPU\");\n   ov::genai::GenerationConfig config = pipe.get_generation_config();\n   config.max_new_tokens = 256;\n   config.num_beam_groups = 3;\n   config.num_beams = 15;\n   config.diversity_penalty = 1.0f;\n\n   cout << pipe.generate(\"The Sun is yellow because\", config);\n}\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch ResNet50 Model to OpenVINO\nDESCRIPTION: This snippet demonstrates the simplest way to convert a PyTorch ResNet50 model from `torchvision` to an OpenVINO model using the `ov.convert_model` function. It requires the `torchvision`, `torch`, and `openvino` libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-pytorch.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision\nimport torch\nimport openvino as ov\n\nmodel = torchvision.models.resnet50(weights='DEFAULT')\nov_model = ov.convert_model(model)\n```\n\n----------------------------------------\n\nTITLE: Load LLM with KV-Cache Quantization - Python\nDESCRIPTION: Loads an LLM with KV-cache quantization enabled to reduce memory consumption during inference. It demonstrates setting the precision for the KV-cache and optionally specifying a dynamic quantization group size. `PERFORMANCE_HINT` is set to `LATENCY` to optimize for inference speed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmodel = OVModelForCausalLM.from_pretrained(\n    model_path,\n    ov_config={\"KV_CACHE_PRECISION\": \"u8\", \"DYNAMIC_QUANTIZATION_GROUP_SIZE\": \"32\", \"PERFORMANCE_HINT\": \"LATENCY\"}\n)\n```\n\n----------------------------------------\n\nTITLE: Load Hugging Face Model using Optimum Intel\nDESCRIPTION: Demonstrates how to load a Hugging Face model using Optimum Intel's OVModelForCausalLM.  It highlights the key differences in the import statement and the from_pretrained call, including the export=True parameter for on-the-fly OpenVINO IR conversion.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n-from transformers import AutoModelForCausalLM\n+from optimum.intel import OVModelForCausalLM\nmodel_id = \"meta-llama/Llama-2-7b-chat-hf\"\n-model = AutoModelForCausalLM.from_pretrained(model_id)\n+model = OVModelForCausalLM.from_pretrained(model_id, export=True)\n```\n\n----------------------------------------\n\nTITLE: Compile Model with Specific GPUs and CUMULATIVE_THROUGHPUT in C++\nDESCRIPTION: This snippet demonstrates compiling a model with AUTO, specifying particular GPU devices (GPU.1, GPU.0) and using the CUMULATIVE_THROUGHPUT performance hint in C++. This provides control over the exact GPUs employed for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\nov::CompiledModel compiled_model = core.compile_model(model, \"AUTO:GPU.1,GPU.0\", ov::hint::performance_mode(ov::hint::PerformanceMode::CUMULATIVE_THROUGHPUT));\n```\n\n----------------------------------------\n\nTITLE: Compile Model for Heterogeneous Execution with OpenVINO (C++)\nDESCRIPTION: This C++ snippet compiles an OpenVINO model for heterogeneous execution on 'GPU,CPU'.  OpenVINO will automatically assign operations to the specified devices based on device capabilities and priorities. The OpenVINO runtime library is required.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/hetero-execution.rst#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"path_to_your_model.xml\");\n\n    // [compile_model]\n    ov::CompiledModel compiled_model = core.compile_model(model, \"HETERO:GPU,CPU\");\n    // [compile_model]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Exporting LLM with Group Quantization\nDESCRIPTION: This snippet exports a TinyLlama-1.1B-Chat-v1.0 model using Optimum-Intel with group quantization. The model is exported in INT4 format with symmetrical quantization and a group size of 128.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_7\n\nLANGUAGE: Console\nCODE:\n```\noptimum-cli export openvino -m TinyLlama/TinyLlama-1.1B-Chat-v1.0 --weight-format int4 --sym --ratio 1.0 --group-size 128 TinyLlama-1.1B-Chat-v1.0\n```\n\n----------------------------------------\n\nTITLE: Export OpenVINO Model with Low Precision (int4)\nDESCRIPTION: This snippet exports a model to OpenVINO IR format while converting it to a lower precision (int4). It uses the `optimum-cli export openvino` command, specifying the model ID and the weight format as `int4`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/genai-model-preparation.rst#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\noptimum-cli export openvino --model <model_id> --weight-format int4 <exported_model_name>\n```\n\n----------------------------------------\n\nTITLE: TensorFlow Graph to OpenVINO\nDESCRIPTION: Converts a TensorFlow graph to an OpenVINO model. It initializes a TensorFlow session, defines input placeholders and a ReLU operation, and then converts the graph to an OpenVINO model using `ov.convert_model`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nwith tf.compat.v1.Session() as sess:\n    inp1 = tf.compat.v1.placeholder(tf.float32, [100], 'Input1')\n    inp2 = tf.compat.v1.placeholder(tf.float32, [100], 'Input2')\n    output = tf.nn.relu(inp1 + inp2, name='Relu')\n    tf.compat.v1.global_variables_initializer()\n    model = sess.graph\n```\n\nLANGUAGE: python\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model(model)\n```\n\n----------------------------------------\n\nTITLE: Export LLM to OpenVINO with Low Precision (int4)\nDESCRIPTION: This example exports the Llama-2-7b-chat-hf LLM to OpenVINO IR format with int4 precision, using the `optimum-cli export openvino` command.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/genai-model-preparation.rst#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\noptimum-cli export openvino --model meta-llama/Llama-2-7b-chat-hf --weight-format int4 ov_llama_2\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with LoRA Adapters (Python)\nDESCRIPTION: This Python snippet demonstrates how to use LoRA adapters with the `Text2ImagePipeline` from the `openvino_genai` library to generate an image from a text prompt. It allows specifying multiple LoRA adapters and their corresponding alphas via command line arguments.  It generates an image with LoRA adapters and without LoRA adapters for comparison, and saves the images as BMP files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino as ov\nimport openvino_genai\n\n\ndef image_write(path: str, image_tensor: ov.Tensor):\n    from PIL import Image\n    image = Image.fromarray(image_tensor.data[0])\n    image.save(path)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('models_path')\n    parser.add_argument('prompt')\n    args, adapters = parser.parse_known_args()\n\n    prompt = args.prompt\n\n    device = \"CPU\"  # GPU, NPU can be used as well\n    adapter_config = openvino_genai.AdapterConfig()\n\n    # Multiple LoRA adapters applied simultaneously are supported, parse them all and corresponding alphas from cmd parameters:\n    for i in range(int(len(adapters) / 2)):\n        adapter = openvino_genai.Adapter(adapters[2 * i])\n        alpha = float(adapters[2 * i + 1])\n        adapter_config.add(adapter, alpha)\n\n    # LoRA adapters passed to the constructor will be activated by default in next generates\n    pipe = openvino_genai.Text2ImagePipeline(args.models_path, device, adapters=adapter_config)\n\n    print(\"Generating image with LoRA adapters applied, resulting image will be in lora.bmp\")\n    image = pipe.generate(prompt,\n                          width=512,\n                          height=896,\n                          num_inference_steps=20,\n                          rng_seed=42)\n\n    image_write(\"lora.bmp\", image)\n    print(\"Generating image without LoRA adapters applied, resulting image will be in baseline.bmp\")\n    image = pipe.generate(prompt,\n                          # passing adapters in generate overrides adapters set in the constructor; openvino_genai.AdapterConfig() means no adapters\n                          adapters=openvino_genai.AdapterConfig(),\n                          width=512,\n                          height=896,\n                          num_inference_steps=20,\n                          rng_seed=42)\n    image_write(\"baseline.bmp\", image)\n```\n\n----------------------------------------\n\nTITLE: Install NNCF via PyPI\nDESCRIPTION: This command installs the NNCF package using pip, the Python package installer. It allows users to utilize NNCF's model compression capabilities within their Python environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install nncf\n```\n\n----------------------------------------\n\nTITLE: Convert ONNX Model Hub Model to OpenVINO IR (Python)\nDESCRIPTION: This code snippet demonstrates the conversion of a ResNet50 model from the ONNX Model Hub to OpenVINO IR. It loads the ONNX model, saves it to a temporary file, converts it to OpenVINO IR using `openvino.convert_model`, and provides options to save or compile and infer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation.rst#_snippet_4\n\nLANGUAGE: py\nCODE:\n```\nimport onnx\n\nmodel = onnx.hub.load(\"resnet50\")\nonnx.save(model, 'resnet50.onnx')  # use a temporary file for model\n\nimport openvino as ov\nov_model = ov.convert_model('resnet50.onnx')\n\n###### Option 1: Save to OpenVINO IR:\n\n# save model to OpenVINO IR for later use\nov.save_model(ov_model, 'model.xml')\n\n###### Option 2: Compile and infer with OpenVINO:\n\n# compile model\ncompiled_model = ov.compile_model(ov_model)\n\n# prepare input_data\nimport numpy as np\ninput_data = np.random.rand(1, 3, 224, 224)\n\n# run inference\nresult = compiled_model(input_data)\n```\n\n----------------------------------------\n\nTITLE: Run Hello Classification Sample - Python\nDESCRIPTION: This command line instruction demonstrates how to run the Hello Classification sample in Python. It takes the path to the model, the path to the image, and the device name as command-line arguments. The model should be in OpenVINO's IR format (XML file).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-classification.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npython hello_classification.py ./models/alexnet/alexnet.xml ./images/banana.jpg GPU\n```\n\n----------------------------------------\n\nTITLE: Convert and compress weights to INT4 via CLI\nDESCRIPTION: Uses the optimum-cli to convert a model to OpenVINO IR format and compress its weights to INT4.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\noptimum-cli export openvino --model meta-llama/Llama-2-7b-chat-hf --weight-format int4 ov_llama_2\n```\n\n----------------------------------------\n\nTITLE: Convert Frozen Model Format (TF1) using CLI\nDESCRIPTION: This snippet demonstrates how to convert a TensorFlow 1 Frozen Model (``.pb``) to OpenVINO IR using the `ovc` command-line tool. The input is the path to the ``.pb`` file. The output is the OpenVINO IR model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\novc your_model_file.pb\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with LoRA Adapters (C++)\nDESCRIPTION: This C++ snippet demonstrates how to use LoRA adapters with the `Text2ImagePipeline` from the `openvino_genai` library to generate images from text prompts.  It takes the model directory, prompt, LoRA adapter paths, and alpha values as command-line arguments. It initializes the pipeline with LoRA adapters and generates an image with and without them for comparison, saving the results as BMP files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n#include \"openvino/genai/image_generation/text2image_pipeline.hpp\"\n\n#include \"imwrite.hpp\"\n\nint32_t main(int32_t argc, char* argv[]) try {\n    OPENVINO_ASSERT(argc >= 3 && (argc - 3) % 2 == 0, \"Usage: \", argv[0], \" <MODEL_DIR> '<PROMPT>' [<LORA_SAFETENSORS> <ALPHA> ...]]\");\n\n    const std::string models_path = argv[1], prompt = argv[2];\n    const std::string device = \"CPU\";  // GPU, NPU can be used as well\n\n    ov::genai::AdapterConfig adapter_config;\n    // Multiple LoRA adapters applied simultaneously are supported, parse them all and corresponding alphas from cmd parameters:\n    for(size_t i = 0; i < (argc - 3)/2; ++i) {\n        ov::genai::Adapter adapter(argv[3 + 2*i]);\n        float alpha = std::atof(argv[3 + 2*i + 1]);\n        adapter_config.add(adapter, alpha);\n    }\n\n    // LoRA adapters passed to the constructor will be activated by default in next generates\n    ov::genai::Text2ImagePipeline pipe(models_path, device, ov::genai::adapters(adapter_config));\n\n    std::cout << \"Generating image with LoRA adapters applied, resulting image will be in lora.bmp\\n\";\n    ov::Tensor image = pipe.generate(prompt,\n        ov::genai::width(512),\n        ov::genai::height(896),\n        ov::genai::num_inference_steps(20),\n        ov::genai::rng_seed(42));\n    imwrite(\"lora.bmp\", image, true);\n\n    std::cout << \"Generating image without LoRA adapters applied, resulting image will be in baseline.bmp\\n\";\n    image = pipe.generate(prompt,\n        ov::genai::adapters(),  // passing adapters in generate overrides adapters set in the constructor; adapters() means no adapters\n        ov::genai::width(512),\n        ov::genai::height(896),\n        ov::genai::num_inference_steps(20),\n        ov::genai::rng_seed(42));\n    imwrite(\"baseline.bmp\", image, true);\n\n    return EXIT_SUCCESS;\n} catch (const std::exception& error) {\n    try {\n        std::cerr << error.what() << '\\n';\n    } catch (const std::ios_base::failure&) {}\n    return EXIT_FAILURE;\n} catch (...) {\n    try {\n        std::cerr << \"Non-exception object thrown\\n\";\n```\n\n----------------------------------------\n\nTITLE: Convert Keras Application Model to OpenVINO IR (Python)\nDESCRIPTION: This code snippet demonstrates the conversion of a ResNet50 model from Keras Applications to OpenVINO IR using `openvino.convert_model`. It loads the model, converts it, and then either saves it as OpenVINO IR or compiles and performs inference. It uses TensorFlow as the backend for Keras.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation.rst#_snippet_2\n\nLANGUAGE: py\nCODE:\n```\nimport tensorflow as tf\nimport openvino as ov\n\ntf_model = tf.keras.applications.ResNet50(weights=\"imagenet\")\nov_model = ov.convert_model(tf_model)\n\n###### Option 1: Save to OpenVINO IR:\n\n# save model to OpenVINO IR for later use\nov.save_model(ov_model, 'model.xml')\n\n###### Option 2: Compile and infer with OpenVINO:\n\n# compile model\ncompiled_model = ov.compile_model(ov_model)\n\n# prepare input_data\nimport numpy as np\ninput_data = np.random.rand(1, 224, 224, 3)\n\n# run inference\nresult = compiled_model(input_data)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking asl-recognition model on CPU in latency mode (C++)\nDESCRIPTION: This snippet demonstrates how to run the OpenVINO Benchmark Tool in C++ to measure the latency of the 'asl-recognition' model on a CPU. It uses the `./benchmark_app` command with the `-m` option to specify the model path, `-d` to select the CPU device, and `-hint` to set the performance hint to latency. The model is expected to be in the OpenVINO Intermediate Representation (IR) format ('.xml' file).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\n./benchmark_app -m omz_models/intel/asl-recognition-0004/FP16/asl-recognition-0004.xml -d CPU -hint latency\n```\n\n----------------------------------------\n\nTITLE: Cascade of Models in OpenVINO with C++\nDESCRIPTION: This C++ code illustrates how to implement a cascade of models using `ov::InferRequest`. The output tensor from the first model is used as input to the second. Requires OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model_1 = core.read_model(\"model_1.xml\");\nov::CompiledModel compiled_model_1 = core.compile_model(model_1, \"CPU\");\nov::InferRequest infer_request_1 = compiled_model_1.create_infer_request();\n\nstd::shared_ptr<ov::Model> model_2 = core.read_model(\"model_2.xml\");\nov::CompiledModel compiled_model_2 = core.compile_model(model_2, \"CPU\");\nov::InferRequest infer_request_2 = compiled_model_2.create_infer_request();\n\nov::Tensor input_tensor_1 = infer_request_1.get_input_tensor();\nstd::vector<float> input_data(input_tensor_1.get_size());\nstd::generate(input_data.begin(), input_data.end(), []() { return static_cast<float>(rand()) / RAND_MAX; });\nstd::memcpy(input_tensor_1.data(), input_data.data(), input_data.size() * sizeof(float));\n\ninfer_request_1.infer();\n\nov::Tensor output_tensor_1 = infer_request_1.get_output_tensor();\ninfer_request_2.set_input_tensor(output_tensor_1);\n```\n\n----------------------------------------\n\nTITLE: INT4 Symmetric Weight Compression with NNCF\nDESCRIPTION: This code snippet demonstrates how to compress model weights using the INT4 Symmetric mode (INT4_SYM) in NNCF. This mode prioritizes speed and size reduction over accuracy by quantizing weights symmetrically without a zero point.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression/4-bit-weight-quantization.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nncf import compress_weights\nfrom nncf import CompressWeightsMode\n\ncompressed_model = compress_weights(model, mode=CompressWeightsMode.INT4_SYM)\n```\n\n----------------------------------------\n\nTITLE: Stable Diffusion XL with OpenVINO and torch.compile (Python)\nDESCRIPTION: This code snippet demonstrates the use of torch.compile and OpenVINO with Stable Diffusion XL.  It loads the UNet model and compiles the text encoder, UNet, and VAE decoder with OpenVINO using the 'openvino' backend to optimize image generation from a given text prompt.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_2\n\nLANGUAGE: py\nCODE:\n```\nimport torch\nfrom diffusers import UNet2DConditionModel, DiffusionPipeline, LCMScheduler\n\nunet = UNet2DConditionModel.from_pretrained(\"latent-consistency/lcm-sdxl\", torch_dtype=torch.float16, variant=\"fp16\")\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", unet=unet, torch_dtype=torch.float16, variant=\"fp16\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n+ pipe.text_encoder = torch.compile(pipe.text_encoder, backend=\"openvino\") #Optional\n+ pipe.unet = torch.compile(pipe.unet, backend=\"openvino\")\n+ pipe.vae.decode = torch.compile(pipe.vae.decode, backend=\"openvino\") #Optional\n\nprompt = \"a close-up picture of an old man standing in the rain\"\nimage = pipe(prompt, num_inference_steps=5, guidance_scale=8.0).images[0]\nimage.save(\"result.png\")\n```\n\n----------------------------------------\n\nTITLE: Fuse LoRA Adaptors into LLM - Python\nDESCRIPTION: Fuses trained LoRA adaptors into a base LLM to improve deployment efficiency by avoiding extra computation during inference. It loads the base model and the LoRA adaptor, merges them, and saves the fused model. The fused model can then be converted to OpenVINO format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmodel_id = \"meta-llama/Llama-2-7b-chat-hf\"\nlora_adaptor = \"./lora_adaptor\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, use_cache=True)\nmodel = PeftModelForCausalLM.from_pretrained(model, lora_adaptor)\nmodel.merge_and_unload()\nmodel.get_base_model().save_pretrained(\"fused_lora_model\")\n```\n\n----------------------------------------\n\nTITLE: Export Whisper Model to OpenVINO with Full Precision (fp16)\nDESCRIPTION: This example exports the openai/whisper-base model to OpenVINO IR format with fp16 precision, enabling trust for remote code execution with `--trust-remote-code`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/genai-model-preparation.rst#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\noptimum-cli export openvino --trust-remote-code --model openai/whisper-base ov_whisper\n```\n\n----------------------------------------\n\nTITLE: Quantize with Transformer Type\nDESCRIPTION: Quantizes the model using the Transformer quantization scheme, which is designed to preserve accuracy when quantizing Transformer models like BERT or DistilBERT. `nncf.ModelType.Transformer` indicates the type of model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nnncf.quantize(model, dataset, model_type=nncf.ModelType.Transformer)\n\n```\n\n----------------------------------------\n\nTITLE: Load and Compress Model to INT4 with Optimum-Intel API\nDESCRIPTION: Loads a pre-trained Hugging Face model, compresses it to INT4 using the Optimum Intel API with OVWeightQuantizationConfig, and then performs inference with a text phrase. Configuration parameters for the quantizer include bits, quantization method (AWQ), scale estimation, dataset, group size, and ratio.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel.openvino import OVModelForCausalLM, OVWeightQuantizationConfig\nfrom transformers import AutoTokenizer, pipeline\n\n# Load and compress a model from Hugging Face.\nmodel_id = \"microsoft/Phi-3.5-mini-instruct\"\nmodel = OVModelForCausalLM.from_pretrained(\n    model_id,\n    export=True,\n    quantization_config=OVWeightQuantizationConfig(\n        bits=4,\n        quant_method=\"awq\",\n        scale_estimation=True,\n        dataset=\"wikitext2\",\n        group_size=64,\n        ratio=1.0\n    )\n)\n\n# Inference\ntokenizer = AutoTokenizer.from_pretrained(model_id)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nphrase = \"The weather is\"\nresults = pipe(phrase)\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Download Models from Hugging Face Hub using huggingface_hub\nDESCRIPTION: This snippet demonstrates how to download pre-converted models from the Hugging Face Hub using the `huggingface_hub` package. It first installs the package and then downloads a specific model to a local directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/genai-model-preparation.rst#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install huggingface_hub\nhuggingface-cli download \"OpenVINO/phi-2-fp16-ov\" --local-dir model_path\n```\n\n----------------------------------------\n\nTITLE: Speech Recognition with Whisper in C++\nDESCRIPTION: This C++ code snippet demonstrates speech recognition using the OpenVINO GenAI WhisperPipeline. It reads a WAV file, initializes the pipeline with a model directory and device, configures generation parameters, generates text from the audio, and prints the results. It includes the header files `audio_utils.hpp` and `openvino/genai/whisper_pipeline.hpp`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n#include \"audio_utils.hpp\"\n#include \"openvino/genai/whisper_pipeline.hpp\"\n\n            int main(int argc, char* argv[]) try {\n                if (3 > argc) {\n                    throw std::runtime_error(std::string{\"Usage: \"} + argv[0] + \" <MODEL_DIR> \\\"<WAV_FILE_PATH>\\\"\");\n                }\n\n                std::filesystem::path models_path = argv[1];\n                std::string wav_file_path = argv[2];\n                std::string device = \"CPU\";  // GPU or NPU can be used as well.\n\n                ov::genai::WhisperPipeline pipeline(models_path, device);\n\n                ov::genai::WhisperGenerationConfig config(models_path / \"generation_config.json\");\n                config.max_new_tokens = 100;\n                config.language = \"<|en|>\";\n                config.task = \"transcribe\";\n                config.return_timestamps = true;\n\n                // The pipeline expects normalized audio with a sampling rate of 16kHz.\n                ov::genai::RawSpeechInput raw_speech = utils::audio::read_wav(wav_file_path);\n                auto result = pipeline.generate(raw_speech, config);\n\n                std::cout << result << \"\\n\";\n\n                for (auto& chunk : *result.chunks) {\n                    std::cout << \"timestamps: [\" << chunk.start_ts << \", \" << chunk.end_ts << \"] text: \" << chunk.text << \"\\n\";\n                }\n\n            } catch (const std::exception& error) {\n```\n\n----------------------------------------\n\nTITLE: Inference with OpenVINO - PyTorch\nDESCRIPTION: Performs inference on a PyTorch model converted to OpenVINO Intermediate Representation (IR). This shows how to deploy the quantized model using OpenVINO for accelerated inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/quantization-aware-training.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nie = Core()\n        net = ie.read_model(model=path_to_xml)\n        compiled_model = ie.compile_model(model=net, device_name=\"CPU\")\n        output_layer = next(iter(compiled_model.outputs))\n        req = compiled_model.create_infer_request()\n        results = req.infer({input_tensor_name: image})\n        return results[output_layer]\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Requests Hint in C++\nDESCRIPTION: This C++ snippet shows how to set the `ov::hint::num_requests` configuration key to limit the batch size for the GPU and the number of inference streams for the CPU. It optimizes performance for applications processing a limited number of video streams (e.g., 4). It assumes the `core` object and `model` object are already defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/high-level-performance-hints.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nov::CompiledModel compiled_model = core.compile_model(\n    model,\n    \"CPU\",\n    ov::hint::performance_mode(ov::hint::PerformanceMode::THROUGHPUT),\n    ov::hint::num_requests(4));\n```\n\n----------------------------------------\n\nTITLE: Creating Infer Request in OpenVINO with Python\nDESCRIPTION: This code snippet demonstrates how to create an `InferRequest` object from a `CompiledModel` in Python using the OpenVINO library.  It initializes the OpenVINO runtime and compiles a model before creating the infer request. No specific dependencies beyond OpenVINO are required.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nie = Core()\nmodel = ie.read_model(\"model.xml\")\ncompiled_model = ie.compile_model(model, \"CPU\")\ninfer_request = compiled_model.create_infer_request()\n```\n\n----------------------------------------\n\nTITLE: Set Latency Performance Hint (C++)\nDESCRIPTION: These commands demonstrate how to set the latency performance hint for the OpenVINO benchmark application. Setting the latency hint prioritizes low latency inference. The application automatically adjusts parameters such as processing streams and batch size to achieve the best latency on the target device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n./benchmark_app -m model.xml -hint latency\n./benchmark_app -m model.xml -hint throughput\n```\n\n----------------------------------------\n\nTITLE: Exporting PyTorch Model to ONNX and Converting to OpenVINO IR\nDESCRIPTION: This code snippet demonstrates an alternative method of converting a PyTorch model to OpenVINO IR by first exporting the model to ONNX format using `torch.onnx.export` and then converting the resulting `.onnx` file to OpenVINO IR using `openvino.convert_model`. It uses `torchvision` to load a ResNet50 model. The example requires `torch`, `torchvision`, and `openvino` libraries. It creates an 'model.onnx' file in the working directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-pytorch.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport torchvision\nimport torch\nimport openvino as ov\n\nmodel = torchvision.models.resnet50(weights='DEFAULT')\n# 1. Export to ONNX\ntorch.onnx.export(model, (torch.rand(1, 3, 224, 224), ), 'model.onnx')\n# 2. Convert to OpenVINO\nov_model = ov.convert_model('model.onnx')\n```\n\n----------------------------------------\n\nTITLE: Export OpenVINO Model with Full Precision (fp16)\nDESCRIPTION: This snippet exports a model to OpenVINO IR format while keeping the full model precision (fp16). It uses the `optimum-cli export openvino` command, specifying the model ID and the weight format as `fp16`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/genai-model-preparation.rst#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\noptimum-cli export openvino --model <model_id> --weight-format fp16 <exported_model_name>\n```\n\n----------------------------------------\n\nTITLE: Chat with Vision Language Model in Python\nDESCRIPTION: This Python code snippet showcases a chat application using the OpenVINO GenAI VLMPipeline, which integrates image input for multimodal text generation.  It initializes the pipeline with optional compile cache, starts a chat session, loads images, accepts a text prompt, generates text using the VLM, and streams the output.  The libraries required are `numpy`, `openvino_genai`, `PIL`, and `openvino`. Input is a model directory, image directory and prompt, the output is generated text.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport openvino_genai\nfrom PIL import Image\nfrom openvino import Tensor\nfrom pathlib import Path\n\n\n            def streamer(subword: str) -> bool:\n                print(subword, end='', flush=True)\n\n\n            def read_image(path: str) -> Tensor:\n                pic = Image.open(path).convert(\"RGB\")\n                image_data = np.array(pic.getdata()).reshape(1, pic.size[1], pic.size[0], 3).astype(np.uint8)\n                return Tensor(image_data)\n\n\n            def read_images(path: str) -> list[Tensor]:\n                entry = Path(path)\n                if entry.is_dir():\n                    return [read_image(str(file)) for file in sorted(entry.iterdir())]\n                return [read_image(path)]\n\n\n            def infer(model_dir: str, image_dir: str):\n                rgbs = read_images(image_dir)\n                device = 'CPU'  # GPU can be used as well.\n                enable_compile_cache = dict()\n                if \"GPU\" == device:\n                    enable_compile_cache[\"CACHE_DIR\"] = \"vlm_cache\"\n                pipe = openvino_genai.VLMPipeline(model_dir, device, **enable_compile_cache)\n\n                config = openvino_genai.GenerationConfig()\n                config.max_new_tokens = 100\n\n                pipe.start_chat()\n                prompt = input('question:\\n')\n                pipe.generate(prompt, images=rgbs, generation_config=config, streamer=streamer)\n\n                while True:\n                    try:\n                        prompt = input(\"\\n----------\\n\"\n                            \"question:\\n\")\n                    except EOFError:\n                        break\n                    pipe.generate(prompt, generation_config=config, streamer=streamer)\n                pipe.finish_chat()\n```\n\n----------------------------------------\n\nTITLE: Getting Inference Results in OpenVINO Python\nDESCRIPTION: This snippet demonstrates various ways to obtain inference results from the OpenVINO Python API.  Results can be accessed directly from the output tensor or through other means, depending on the inference method used. This allows for flexible handling of the inference results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model)\n\ninput_data = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n\n# Infer and get the results\nresults = compiled_model([input_data])\n\n# Access the results\noutput_key = compiled_model.outputs[0]\noutput_tensor = results[output_key]\nprint(output_tensor)\n```\n\n----------------------------------------\n\nTITLE: Llama-3.2-1B with OpenVINO and torch.compile (Python)\nDESCRIPTION: This code showcases how to integrate torch.compile and OpenVINO with the Llama-3.2-1B model for text generation. It loads the model and tokenizer, compiles the model's forward function with OpenVINO using the 'openvino' backend with the 'aot_autograd' option enabled, and then generates text from a prompt using a pipeline.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_3\n\nLANGUAGE: py\nCODE:\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path = \"meta-llama/Llama-3.2-1B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.float32)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n    trust_remote_code=True,\n    device_map='cpu',\n    torch_dtype=torch.float32\n)\n\nprompt = \"Tell me about AI\"\n\n+ model.forward = torch.compile(model.forward, backend=\"openvino\", options={'aot_autograd': True})\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=64\n)\nresult = pipe(prompt)\nprint(result[0]['generated_text'])\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO LLMs with LlamaIndex\nDESCRIPTION: This code shows how to configure and use OpenVINO with LlamaIndex for building context-augmented generative AI applications. It includes setting performance hints, number of streams, and other parameters for the OpenVINO LLM.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-integrations.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nov_config = {\n    \"PERFORMANCE_HINT\": \"LATENCY\",\n    \"NUM_STREAMS\": \"1\",\n    \"CACHE_DIR\": \"\",\n}\n\nov_llm = OpenVINOLLM(\n    model_id_or_path=\"HuggingFaceH4/zephyr-7b-beta\",\n    context_window=3900,\n    max_new_tokens=256,\n    model_kwargs={\"ov_config\": ov_config},\n    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n    messages_to_prompt=messages_to_prompt,\n    completion_to_prompt=completion_to_prompt,\n    device_map=\"cpu\",\n)\n```\n\n----------------------------------------\n\nTITLE: Model weights compression using NNCF\nDESCRIPTION: This code snippet demonstrates how to compress a model's weights using the Neural Network Compression Framework (NNCF). It involves importing necessary libraries and using the `nncf.compress_model` function to create a compressed model. The compressed model is then compiled with the OpenVINO backend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport nncf\nimport openvino.torch\nimport torch\n\n# Weights compression\ncompressed_model = nncf.compress_model(model)\n\ncompressed_model = torch.compile(compressed_model, backend=\"openvino\")\n```\n\n----------------------------------------\n\nTITLE: Specifying Device Priorities - C++\nDESCRIPTION: This C++ snippet demonstrates how to specify device priorities when compiling a model with the AUTO device. It creates a `Core` object, reads the model, and compiles it, setting the priorities to GPU then CPU.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n#include <openvino/runtime.hpp>\n\nint main() {\n    ov::Core core;\n    auto model = core.read_model(\"path_to_model.xml\");\n    ov::CompiledModel compiled_model = core.compile_model(model, \"AUTO:GPU,CPU\");\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX Model using Python\nDESCRIPTION: This snippet shows how to convert an ONNX model to OpenVINO IR format using the `openvino.convert_model` function. It requires the `openvino` package to be installed. The input is the path to the ONNX model file (e.g., `your_model_file.onnx`). The output is the converted OpenVINO IR model files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-onnx.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nov.convert_model('your_model_file.onnx')\n```\n\n----------------------------------------\n\nTITLE: TensorFlow Inference with OpenVINO\nDESCRIPTION: This Python code demonstrates how to load a TensorFlow model (MobileNetV2), convert it to OpenVINO format, compile it for the CPU, and perform inference. It requires the openvino, numpy, and tensorflow libraries. It showcases a basic inference pipeline for a TensorFlow model using OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport openvino as ov\nimport tensorflow as tf\n\n# load TensorFlow model into memory\nmodel = tf.keras.applications.MobileNetV2(weights='imagenet')\n\n# convert the model into OpenVINO model\nov_model = ov.convert_model(model)\n\n# compile the model for CPU device\ncore = ov.Core()\ncompiled_model = core.compile_model(ov_model, 'CPU')\n\n# infer the model on random data\ndata = np.random.rand(1, 224, 224, 3)\noutput = compiled_model({0: data})\n```\n\n----------------------------------------\n\nTITLE: Grouped Beam Search in Python\nDESCRIPTION: This Python code configures grouped beam search for text generation. It retrieves the generation config from the LLMPipeline, modifies parameters such as `max_new_tokens`, `num_beam_groups`, `num_beams`, and `diversity_penalty`, and then generates text using the modified config.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(model_path, \"CPU\")\nconfig = pipe.get_generation_config()\nconfig.max_new_tokens = 256\nconfig.num_beam_groups = 3\nconfig.num_beams = 15\nconfig.diversity_penalty = 1.0\npipe.generate(\"The Sun is yellow because\", config)\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (ONNX) with OpenVINO in C++\nDESCRIPTION: This code compiles a model in ONNX format using the `core.compile_model()` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nov::CompiledModel compiled_model_onnx = core.compile_model(\"path_to_model.onnx\", \"AUTO\");\n```\n\n----------------------------------------\n\nTITLE: Llama-2-7B-GPTQ with OpenVINO and torch.compile (Python)\nDESCRIPTION: This code demonstrates using torch.compile and OpenVINO with the Llama-2-7B-GPTQ model for text generation. It loads the model and tokenizer, compiles the model's forward function with OpenVINO ('openvino' backend, 'aot_autograd' option), and generates text from a prompt using a transformers pipeline.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_4\n\nLANGUAGE: py\nCODE:\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path = \"TheBloke/Llama-2-7B-GPTQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.float32)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n    trust_remote_code=True,\n    device_map='cpu',\n    torch_dtype=torch.float32\n)\n\nprompt = \"Tell me about AI\"\n\n+ model.forward = torch.compile(model.forward, backend=\"openvino\", options={'aot_autograd': True})\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=64\n)\nresult = pipe(prompt)\nprint(result[0]['generated_text'])\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (ov::Model) with OpenVINO in C++\nDESCRIPTION: This code compiles a model of type `ov::Model` using the `core.compile_model()` method. It specifies the model itself and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\nauto model = core.read_model(\"path_to_model.xml\");\nov::CompiledModel compiled_model_ov = core.compile_model(model, \"AUTO\");\n```\n\n----------------------------------------\n\nTITLE: Run Benchmark on GPU (C++)\nDESCRIPTION: This command runs the OpenVINO benchmark application on the GPU device. The application will load the specified model and perform inference using the GPU. Note that the system must have the appropriate drivers installed for the GPU to be used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\n./benchmark_app -m model.xml -d GPU\n```\n\n----------------------------------------\n\nTITLE: Get/Set Tensor (Single Input/Output) in Python\nDESCRIPTION: This Python code gets and sets the input tensor and retrieves the output tensor when the model has a single input and a single output. It uses `get_input_tensor()` and `get_output_tensor()` without arguments. Requires OpenVINO and a compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\ncompiled_model = ie.compile_model(model, \"CPU\")\ninfer_request = compiled_model.create_infer_request()\n\ninput_tensor = infer_request.get_input_tensor()\ninput_tensor.data[:] = np.random.normal(0, 1, input_tensor.shape)\n\ninfer_request.infer()\n\noutput_tensor = infer_request.get_output_tensor()\nprint(output_tensor.data)\n```\n\n----------------------------------------\n\nTITLE: Modify String Tensor Elements (C++)\nDESCRIPTION: Modifies the contents of a string tensor by directly accessing the underlying data using the `.data` template method and assigning new values. Requires the `openvino/openvino.hpp` header.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/string-tensors.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nstd::string new_content[] = {\"one\", \"two\", \"three\"};\nstd::string* data = tensor.data<std::string>();\nfor(size_t i = 0; i < tensor.get_size(); ++i)\n   data[i] = new_content[i];\n```\n\n----------------------------------------\n\nTITLE: Limiting Batch Size with Hint in Python\nDESCRIPTION: This Python snippet shows how to limit the batch size using the `ov::hint::num_requests` configuration key within the performance hint. This limits the batch size for the GPU and the number of inference streams for the CPU, improving performance when the parallel slack is bounded.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model, device_name=\"GPU\", config={\n    ov.hint.performance_mode: ov.hint.PerformanceMode.THROUGHPUT,\n    ov.hint.num_requests: 4\n})\n```\n\n----------------------------------------\n\nTITLE: Compress Model to INT8 with NNCF\nDESCRIPTION: Loads a pre-trained Hugging Face model using the Optimum Intel API, compresses it to INT8_ASYM using NNCF, and then performs inference with a text phrase. It leverages `nncf.compress_weights()` to apply weight quantization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom nncf import compress_weights, CompressWeightsMode\nfrom optimum.intel.openvino import OVModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline\n\n# Load a model and compress it with NNCF.\nmodel_id = \"microsoft/Phi-3.5-mini-instruct\"\nmodel = OVModelForCausalLM.from_pretrained(model_id, export=True, load_in_8bit=False, compile=False)\nmodel.model = compress_weights(model.model, mode=CompressWeightsMode.INT8_ASYM)\n\n# Inference\nmodel.compile()\ntokenizer = AutoTokenizer.from_pretrained(model_id)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nphrase = \"The weather is\"\nresults = pipe(phrase)\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: INT4 Asymmetric Weight Compression with Group Size and Ratio\nDESCRIPTION: This code snippet demonstrates how to compress model weights using the INT4 Asymmetric mode (INT4_ASYM) in NNCF, while also specifying a group size and ratio. It quantizes 90% of the model's layers to INT4 asymmetrically with a group size of 64.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression/4-bit-weight-quantization.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom nncf import compress_weights, CompressWeightsMode\n\n# Example: Compressing weights with INT4_ASYM mode, group size of 64, and 90% INT4 ratio\ncompressed_model = compress_weights(\n  model,\n  mode=CompressWeightsMode.INT4_ASYM,\n  group_size=64,\n  ratio=0.9,\n)\n```\n\n----------------------------------------\n\nTITLE: Include Headers for Model Creation (C++)\nDESCRIPTION: This snippet shows the necessary include statements in C++ to create a model using OpenVINO operations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n// [ov:include]\n```\n\n----------------------------------------\n\nTITLE: C Project Structure\nDESCRIPTION: This C snippet provides a basic project structure for OpenVINO applications using the C API. It includes initialization of the core, model loading, compilation, and inference request creation, as well as input/output handling. The provided code requires memory deallocation to prevent leaks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_38\n\nLANGUAGE: cpp\nCODE:\n```\n//! [part7]\n#include <stdio.h>\n#include <stdlib.h>\n#include <openvino/c/openvino.h>\n\nint main() {\n    // -------- Step 1. Initialize OpenVINO runtime core\n    ov_core_t* core = NULL;\n    ov_core_create(&core);\n\n    // -------- Step 2. Read a model\n    ov_model_t* model = NULL;\n    const char* model_path = \"path_to_your_model/your_model.xml\";\n    ov_core_read_model(core, model_path, &model);\n\n    // -------- Step 3. Set up input\n    // ... (Input setup code here)\n\n    // -------- Step 4. Configure preprocessing (Not shown in this minimal example)\n\n    // -------- Step 5. Compile the model\n    ov_compiled_model_t* compiled_model = NULL;\n    const char* device_name = \"CPU\";\n    ov_core_compile_model(core, model, device_name, NULL, &compiled_model);\n\n    // -------- Step 6. Create an inference request\n    ov_infer_request_t* infer_request = NULL;\n    ov_compiled_model_create_infer_request(compiled_model, &infer_request);\n\n    // -------- Step 7. Prepare input data\n    // ... (Input data preparation code here)\n\n    // -------- Step 8. Measure inference latency\n    // ... (Inference and latency measurement code here)\n\n    // -------- Step 9. Process inference results\n    // ... (Result processing code here)\n\n    printf(\"Inference completed successfully.\\n\");\n\n    // -------- Step 10. Release allocated objects\n    ov_infer_request_free(infer_request);\n    ov_compiled_model_free(compiled_model);\n    ov_model_free(model);\n    ov_core_free(core);\n\n    return 0;\n}\n//! [part7]\n```\n\n----------------------------------------\n\nTITLE: Compress Model to INT4 with NNCF\nDESCRIPTION: Loads a pre-trained Hugging Face model using the Optimum Intel API, compresses it to INT4_SYM using NNCF, and then performs inference with a text phrase. It uses `nncf.compress_weights()` to compress the model's weights.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom nncf import compress_weights, CompressWeightsMode\nfrom optimum.intel.openvino import OVModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline\n\n# Load a model and compress it with NNCF.\nmodel_id = \"microsoft/Phi-3.5-mini-instruct\"\nmodel = OVModelForCausalLM.from_pretrained(model_id, export=True, load_in_8bit=False, compile=False)\nmodel.model = compress_weights(model.model, mode=CompressWeightsMode.INT4_SYM)\n\n# Inference\nmodel.compile()\ntokenizer = AutoTokenizer.from_pretrained(model_id)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nphrase = \"The weather is\"\nresults = pipe(phrase)\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Convert and compress weights to INT8 via CLI\nDESCRIPTION: Uses the optimum-cli to convert a model to OpenVINO IR format and compress its weights to INT8. This reduces model size and improves inference latency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\noptimum-cli export openvino --model meta-llama/Llama-2-7b-chat-hf --weight-format int8 ov_llama_2\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Converting Layout Python\nDESCRIPTION: Demonstrates how to implicitly convert the layout (transposing) of a user's tensor to match the layout expected by the original model using the OpenVINO preprocessing API in Python.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n# 1) Set input tensor information. We apply preprocessing\n#    demanding input tensor to have 'u8' type and 'NHWC' layout.\nppp.input().tensor().set_element_type(ov.Type.u8).set_layout(ov.Layout('NHWC'))\n# 2) Set input model information. We assume that the original model has 'NCHW' layout\nppp.input().model().set_layout(ov.Layout('NCHW'))\n# 3) Apply preprocessing modifying the original model\nmodel = ppp.build()\n```\n\n----------------------------------------\n\nTITLE: Importing Model Blob with OpenVINO GenAI (C++)\nDESCRIPTION: This C++ snippet demonstrates how to configure the OpenVINO GenAI pipeline to import a pre-compiled model from a blob file. It specifies the path to the blob file using the `BLOB_PATH` option. This allows you to load a previously compiled model, avoiding recompilation. Requires the OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\nov::AnyMap pipeline_config = { { \"BLOB_PATH\",  \".npucache\\\\compiled_model.blob\" } };\nov::genai::LLMPipeline pipe(model_path, \"NPU\", pipeline_config);\n```\n\n----------------------------------------\n\nTITLE: Handling failure in C++\nDESCRIPTION: This code snippet demonstrates handling exceptions in C++ within the OpenVINO GenAI context. It catches both standard exceptions and unknown objects, printing error messages to cerr. It uses a nested try-catch block to handle potential failures during error reporting. The function returns EXIT_FAILURE upon catching any exception.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n} catch (const std::exception& error) {\n                try {\n                    std::cerr << error.what() << '\\n';\n                } catch (const std::ios_base::failure&) {}\n                return EXIT_FAILURE;\n            } catch (...) {\n                try {\n                    std::cerr << \"Non-exception object thrown\\n\";\n                } catch (const std::ios_base::failure&) {}\n                return EXIT_FAILURE;\n            }\n```\n\n----------------------------------------\n\nTITLE: Declaring Model Layout (C++)\nDESCRIPTION: This snippet demonstrates how to declare the layout of the model's input data using the `ov::preprocess::PrePostProcessor::input().model().set_layout` method in C++. It specifies that the model expects the input data in the `NCHW` layout. This step ensures the preprocessing steps correctly transform the input data to match the model's expected format.  It assumes that `ov::Layout` is defined within the `ov` namespace.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nppp.input().model().set_layout(\"NCHW\");\n```\n\n----------------------------------------\n\nTITLE: Compile Converted Tokenizer Model\nDESCRIPTION: Reads the converted tokenizer and detokenizer models from the specified directory using OpenVINO Core, then compiles them for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nfrom pathlib import Path\nimport openvino_tokenizers\nfrom openvino import Core\n\n\ntokenizer_dir = Path(\"tokenizer/\")\ncore = Core()\nov_tokenizer = core.read_model(tokenizer_dir / \"openvino_tokenizer.xml\")\nov_detokenizer = core.read_model(tokenizer_dir / \"openvino_detokenizer.xml\")\n\ntokenizer, detokenizer = core.compile_model(ov_tokenizer), core.compile_model(ov_detokenizer)\n```\n\n----------------------------------------\n\nTITLE: Convert TensorFlow Hub Trackable Model to OpenVINO\nDESCRIPTION: This snippet demonstrates how to convert a TensorFlow Hub Trackable model loaded using `hub.load()` to an OpenVINO model using `openvino.convert_model`.  Requires the `tensorflow_hub` package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_13\n\nLANGUAGE: py\nCODE:\n```\nimport tensorflow_hub as hub\nimport openvino as ov\n\nmodel = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\nov_model = ov.convert_model(model)\n```\n\n----------------------------------------\n\nTITLE: Check for dynamic dimensions in input layers - Python\nDESCRIPTION: This Python snippet shows how to check if a model already has dynamic dimensions.  It iterates through each input layer, retrieves the partial shape, and prints the name and dimensions of each input.  Dynamic dimensions are reported as '?'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# ! [check_inputs]\nie = Core()\nmodel = ie.read_model(\"model.xml\")\nfor input_layer in model.inputs:\n    print(f\"Input: {input_layer.get_any_name()} {input_layer.partial_shape}\")\n# ! [check_inputs]\n```\n\n----------------------------------------\n\nTITLE: Create a Simple Model (C++)\nDESCRIPTION: This snippet shows how to create a simple model with a single output in OpenVINO using C++.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n// [ov:create_simple_model]\n```\n\n----------------------------------------\n\nTITLE: Excluding Devices - C++\nDESCRIPTION: This C++ snippet demonstrates how to exclude a device (CPU in this case) from the device candidate list when compiling a model using the AUTO device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nov::CompiledModel compiled_model = core.compile_model(model, \"AUTO:-CPU\");\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (IR) with OpenVINO in C++\nDESCRIPTION: This code compiles a model in IR format using the `core.compile_model()` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\nov::CompiledModel compiled_model_ir = core.compile_model(\"path_to_model.xml\", \"AUTO\");\n```\n\n----------------------------------------\n\nTITLE: Infer with OpenVINO Backend in Keras 3 (Python)\nDESCRIPTION: This code snippet shows how to perform inference using the OpenVINO backend in Keras 3. It sets the `KERAS_BACKEND` environment variable to `openvino` and then loads and uses a Keras model. Requires numpy, keras, and keras_hub libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-keras.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nos.environ[\"KERAS_BACKEND\"] = \"openvino\"\nimport numpy as np\nimport keras\nimport keras_hub\n\nfeatures = {\n    \"token_ids\": np.ones(shape=(2, 12), dtype=\"int32\"),\n    \"segment_ids\": np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]] * 2),\n    \"padding_mask\": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2),\n}\n\n# take a model from KerasHub\nbert = keras_hub.models.BertTextClassifier.from_preset(\n    \"bert_base_en_uncased\",\n    num_classes=4,\n    preprocessor=None,\n)\n\npredictions = bert.predict(features)\n```\n\n----------------------------------------\n\nTITLE: Run Generation using OpenVINO GenAI in Python\nDESCRIPTION: This Python snippet uses the OpenVINO GenAI API to perform text generation on an NPU. It initializes an LLMPipeline with a specified model path and device ('NPU') and then generates text based on the given prompt.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai as ov_genai\nmodel_path = \"TinyLlama\"\npipe = ov_genai.LLMPipeline(model_path, \"NPU\")\nprint(pipe.generate(\"The Sun is yellow because\", max_new_tokens=100))\n```\n\n----------------------------------------\n\nTITLE: Get Optimal Number of Infer Requests (Python)\nDESCRIPTION: This code demonstrates how to get the optimal number of inference requests for a compiled model in Python using `ov::CompiledModel::get_property`. It retrieves the optimal number and prints the value. Requires a compiled OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ncore = ov.Core()\nmodel_path = \"path_to_model.xml\"\ndevice_name = \"CPU\"\nmodel = core.read_model(model_path)\ncompiled_model = core.compile_model(model, device_name)\nnum_requests = compiled_model.get_property(\"optimal_number_of_infer_requests\")\nprint(f\"Optimal number of infer requests: {num_requests}\")\n```\n\n----------------------------------------\n\nTITLE: Dynamic Batch Inference with GPU Plugin in OpenVINO (Python)\nDESCRIPTION: This code snippet demonstrates how to handle dynamic batch sizes within the OpenVINO GPU plugin using Python. It shows the configuration required to leverage the plugin's ability to create multiple execution graphs based on powers of 2 to efficiently process varying batch sizes up to a defined upper bound. This is useful for improving performance with variable input sizes but requires additional memory and compilation time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n# The code snippet is not available in the provided text.\n```\n\n----------------------------------------\n\nTITLE: Replace Constant in OpenVINO Model (C++)\nDESCRIPTION: This C++ snippet demonstrates how to replace a constant in an OpenVINO model. This is helpful for modifying problematic constants before running inference.  Requires the OpenVINO runtime library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nov::Model replace_const(ov::Model model, const std::string& const_name, const std::shared_ptr<ov::Node>& new_const) {\n    for (auto& node : model.get_ops()) {\n        if (node->get_type_name() == \"Constant\" && node->get_friendly_name() == const_name) {\n            auto old_const_output = node->output(0);\n            auto new_const_output = new_const->output(0);\n            old_const_output.replace(new_const_output);\n        }\n    }\n    return model;\n}\n```\n\n----------------------------------------\n\nTITLE: Compiling Model on NPU with ov::Core in C++\nDESCRIPTION: This snippet demonstrates compiling a model for inference on the NPU device in C++ using `ov::Core::compile_model()`. It explicitly defines `ov::Core` and uses it to compile the model by specifying the target device as 'NPU'. The result is a compiled model ready for execution on the NPU.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/npu-device.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"path_to_model\");\n// or\nstd::shared_ptr<ov::Model> model = core.read_model(model_stream);\nauto compiled_model = core.compile_model(model, \"NPU\");\n```\n\n----------------------------------------\n\nTITLE: Benchmarking asl-recognition model on GPU in throughput mode (C++)\nDESCRIPTION: This snippet demonstrates how to run the OpenVINO Benchmark Tool in C++ to measure the throughput of the 'asl-recognition' model on a GPU. It uses the `./benchmark_app` command with the `-m` option to specify the model path, `-d` to select the GPU device, and `-hint` to set the performance hint to throughput. The model is expected to be in the OpenVINO Intermediate Representation (IR) format ('.xml' file).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\n./benchmark_app -m omz_models/intel/asl-recognition-0004/FP16/asl-recognition-0004.xml -d GPU -hint throughput\n```\n\n----------------------------------------\n\nTITLE: Enabling Auto Batching with compile_model in Python\nDESCRIPTION: This snippet shows how to enable Automatic Batching in Python by specifying `ov::hint::PerformanceMode::THROUGHPUT` for the `ov::hint::performance_mode` property when calling `compile_model` or `set_property`. It compiles a model for the GPU device with the throughput performance hint, implicitly enabling auto-batching.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model, device_name=\"GPU\", config={ov.hint.performance_mode: ov.hint.PerformanceMode.THROUGHPUT})\n```\n\n----------------------------------------\n\nTITLE: Custom Streamer in C++\nDESCRIPTION: This C++ code demonstrates how to create a custom streamer class in C++ inheriting from `StreamerBase`.  The `put` method allows custom processing of each token ID. It returns a boolean indicating if generation should be stopped. The `end` method is called after generation finishes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\n#include <streamer_base.hpp>\n\nclass CustomStreamer: publict StreamerBase {\npublic:\n   bool put(int64_t token) {\n      bool stop_flag = false;\n      /*\n      custom decoding/tokens processing code\n      tokens_cache.push_back(token);\n      std::string text = m_tokenizer.decode(tokens_cache);\n      ...\n      */\n      return stop_flag;  // Flag indicating whether generation should be stopped. If True, generation stops.\n   };\n\n   void end() {\n      /* custom finalization */\n   };\n};\n\nint main(int argc, char* argv[]) {\n   auto custom_streamer = std::make_shared<CustomStreamer>();\n\n   std::string model_path = argv[1];\n   ov::genai::LLMPipeline pipe(model_path, \"CPU\");\n   pipe.generate(\"The Sun is yellow because\", ov::genai::streamer(custom_streamer), ov::genai::max_new_tokens(100));\n}\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (PaddlePaddle) with OpenVINO in C++\nDESCRIPTION: This code compiles a model in PaddlePaddle format using the `core.compile_model()` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\nov::CompiledModel compiled_model_paddle = core.compile_model(\"path_to_model.pdmodel\", \"AUTO\");\n```\n\n----------------------------------------\n\nTITLE: Checking Available Devices - Python\nDESCRIPTION: This Python snippet shows how to retrieve a list of available devices using the `available_devices` property of the `Core` class in OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nopenvino.runtime.Core.available_devices\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Environment Variables\nDESCRIPTION: This command sources the `setupvars.sh` script, which sets the necessary environment variables for OpenVINO. Sourcing the script executes the commands within it in the current shell. The provided path should point to the setupvars.sh script within the OpenVINO installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\nsource /opt/intel/openvino_2025/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: Retrieve Specific Input/Output Port by Name (C++)\nDESCRIPTION: This C++ snippet shows how to retrieve a specific input or output port of an OpenVINO model using its tensor name. It relies on the availability and correctness of the tensor names within the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nauto ov_model_input = ov_model->input(original_fw_in_tensor_name);\nauto ov_model_output = ov_model->output(original_fw_out_tensor_name);\n```\n\n----------------------------------------\n\nTITLE: Configuring prompt and response length in Python\nDESCRIPTION: This Python snippet configures the maximum prompt length and minimum response length for the LLMPipeline. It creates a dictionary with the desired values for 'MAX_PROMPT_LEN' and 'MIN_RESPONSE_LEN' and passes it to the LLMPipeline constructor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\npipeline_config = { \"MAX_PROMPT_LEN\": 1024, \"MIN_RESPONSE_LEN\": 512 }\npipe = ov_genai.LLMPipeline(model_path, \"NPU\", pipeline_config)\n```\n\n----------------------------------------\n\nTITLE: Reshape by Input Index - Python\nDESCRIPTION: This Python snippet demonstrates how to reshape an OpenVINO model by specifying the input using its index.  The new shape is represented as a tuple.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/changing-input-shape.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\n\n# ! [idx_to_shape]\ninput_index = 0  # Index of the input\nnew_shape = (1, 3, 256, 256)\nmodel.reshape({input_index: new_shape})\n# ! [idx_to_shape]\n```\n\n----------------------------------------\n\nTITLE: Freeze Custom Model (TF1) and Convert to OpenVINO\nDESCRIPTION: This snippet shows how to freeze a TensorFlow 1 custom model defined in Python and then convert the frozen graph to OpenVINO IR using `openvino.convert_model`. It utilizes `tf.compat.v1.graph_util.convert_variables_to_constants` to freeze the graph.  The `sess` variable is assumed to exist and hold the TensorFlow session.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_12\n\nLANGUAGE: py\nCODE:\n```\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_io\nfrozen = tf.compat.v1.graph_util.convert_variables_to_constants(sess, sess.graph_def, [\"name_of_the_output_node\"])\n\nimport openvino as ov\nov_model = ov.convert_model(frozen)\n```\n\n----------------------------------------\n\nTITLE: LLM Pipeline Initialization and Generation in Python\nDESCRIPTION: This Python code initializes an LLMPipeline with a specified model path and device (CPU) and then generates text with a maximum number of new tokens. The generated text is printed to the console.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(model_path, \"CPU\")\nprint(pipe.generate(\"The Sun is yellow because\", max_new_tokens=100))\n```\n\n----------------------------------------\n\nTITLE: Export Diffusion Model to OpenVINO with Low Precision (int4)\nDESCRIPTION: This example exports the stabilityai/stable-diffusion-xl-base-1.0 diffusion model to OpenVINO IR format with int4 precision, using the `optimum-cli export openvino` command.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/genai-model-preparation.rst#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\noptimum-cli export openvino --model stabilityai/stable-diffusion-xl-base-1.0 --weight-format int4 ov_SDXL\n```\n\n----------------------------------------\n\nTITLE: Set Core Property Then Compile (Python)\nDESCRIPTION: This code shows how to set a global property using `ov::Core::set_property` and then compile a model using Python.  It sets the inference precision hint globally for the `CPU` device before compiling the model. Requires an initialized OpenVINO `Core` object and a model path.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ncore = ov.Core()\nmodel_path = \"path_to_model.xml\"\ndevice_name = \"CPU\"\ncore.set_property(device_name, {\"inference_precision_hint\": \"f32\"})\nmodel = core.read_model(model_path)\ncompiled_model = core.compile_model(model, device_name)\n```\n\n----------------------------------------\n\nTITLE: Load LLM with Dynamic Quantization - Python\nDESCRIPTION: Loads an LLM using the Optimum Intel API with dynamic quantization enabled. This allows adjusting the group size for quantization, potentially impacting performance and accuracy. The `ov_config` dictionary is used to configure the dynamic quantization group size.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel = OVModelForCausalLM.from_pretrained(\n    model_path,\n    ov_config={\"DYNAMIC_QUANTIZATION_GROUP_SIZE\": \"64\"}\n)\n```\n\n----------------------------------------\n\nTITLE: ROI Tensor Usage in Infer Request (Python)\nDESCRIPTION: Shows how to pre-allocate an input tensor and crop a frame to the size of the generated bounding boxes using `ov::Tensor` with `ov::Tensor` and `ov::Coordinate` as parameters in Python. This approach allows focusing inference on specific regions of interest within the input frame.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nroi_tensor = ov.Tensor(element_type=np.float32, shape=(1, 3, 224, 224))\nroi_tensor.data[:] = np.random.normal(size=(1, 3, 224, 224))\n\n# Create ROI tensor\nroi_tensor = ov.Tensor(input_tensor, coordinate=(0, 0), shape=(1, 3, 224, 224))\ninfer_request.infer({input_tensor : roi_tensor})\n```\n\n----------------------------------------\n\nTITLE: Convert TensorFlow Hub Model to OpenVINO IR (Python)\nDESCRIPTION: This code snippet shows how to convert a MobileNetV1 model from TensorFlow Hub to OpenVINO IR. The model is loaded from TensorFlow Hub, built with the correct input shape, converted to OpenVINO IR using `openvino.convert_model`, and then either saved as IR or compiled and used for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation.rst#_snippet_3\n\nLANGUAGE: py\nCODE:\n```\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport openvino as ov\n\nmodel = tf.keras.Sequential([\n      hub.KerasLayer(\"https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/classification/5\")\n])\n\n# Check model page for information about input shape: https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/classification/5\nmodel.build([None, 224, 224, 3])\n\nov_model = ov.convert_model(model)\n\n###### Option 1: Save to OpenVINO IR:\n\nov.save_model(ov_model, 'model.xml')\n\n###### Option 2: Compile and infer with OpenVINO:\n\ncompiled_model = ov.compile_model(ov_model)\n\n# prepare input_data\nimport numpy as np\ninput_data = np.random.rand(1, 224, 224, 3)\n\n# run inference\nresult = compiled_model(input_data)\n```\n\n----------------------------------------\n\nTITLE: MatcherPass Transformation Implementation (C++)\nDESCRIPTION: This code snippet demonstrates the implementation of a MatcherPass transformation in C++.  It includes the definition of the callback function that is executed when the pattern is matched.  The callback accesses the matched nodes using the Matcher object and performs the desired transformation. The return value indicates if the root node was replaced.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/matcher-pass.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// [graph_rewrite:template_transformation_cpp]\n#include \"template_pattern_transformation.hpp\"\n\n#include <memory>\n\n#include <openvino/core/rt_info.hpp>\n#include <openvino/opsets/opset10.hpp>\n#include <openvino/pass/pattern/matcher.hpp>\n#include <openvino/pass/pattern/op/wrap_type.hpp>\n\nusing namespace std;\nusing namespace ov::opset10;\n\nnamespace {\nbool constant_folding(const shared_ptr<Add>& add_node) {\n    Output<Node> left_arg = add_node->get_input_node_shared_ptr(0)->output(0);\n    Output<Node> right_arg = add_node->get_input_node_shared_ptr(1)->output(0);\n\n    auto left_constant = as_type_ptr<Constant>(left_arg.get_node_shared_ptr());\n    auto right_constant = as_type_ptr<Constant>(right_arg.get_node_shared_ptr());\n\n    if (!left_constant || !right_constant) {\n        return false;\n    }\n\n    auto left_value = left_constant->get_element_type();\n    auto right_value = right_constant->get_element_type();\n\n    if (left_value != right_value) {\n        return false;\n    }\n\n    auto left_shape = left_constant->get_shape();\n    auto right_shape = right_constant->get_shape();\n\n    if (left_shape != right_shape) {\n        return false;\n    }\n\n    vector<float> left_data = left_constant->get_vector<float>();\n    vector<float> right_data = right_constant->get_vector<float>();\n    vector<float> add_data(left_data.size());\n\n    for (size_t i = 0; i < left_data.size(); ++i) {\n        add_data[i] = left_data[i] + right_data[i];\n    }\n\n    auto new_constant = Constant::create(left_value, left_shape, add_data);\n\n    add_node->output(0).replace_destination(new_constant->output(0));\n    return true;\n}\n}  // namespace\n// [graph_rewrite:template_transformation_cpp]\n```\n\n----------------------------------------\n\nTITLE: Replace Constant in OpenVINO Model (Python)\nDESCRIPTION: This Python snippet demonstrates how to replace a constant in an OpenVINO model. This can be useful when the model contains problematic constants that need modification before inference.  Requires the openvino package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\n\ndef replace_const(model: ov.Model, const_name: str, new_value: np.ndarray) -> ov.Model:\n    \"\"\"Replace constant with provided value in model\"\"\"\n    for node in model.get_ops():\n        if node.get_type_name() == \"Constant\" and node.get_friendly_name() == const_name:\n            old_const_output = node.output(0)\n            new_const = ov.opset8.constant(new_value, dtype=node.get_element_type(), name=const_name)\n            new_const_output = new_const.output(0)\n            old_const_output.replace(new_const_output)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Creating Infer Request in OpenVINO with C++\nDESCRIPTION: This C++ code snippet illustrates how to create an `InferRequest` object from a `CompiledModel` using the OpenVINO library. It initializes the `Core` object, reads a model, compiles it, and then creates an infer request from the compiled model. Requires the OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\nov::InferRequest infer_request = compiled_model.create_infer_request();\n```\n\n----------------------------------------\n\nTITLE: TensorFlow Checkpoint to OpenVINO\nDESCRIPTION: Converts a TensorFlow checkpoint to an OpenVINO model. This example creates a Keras model, saves it as a checkpoint, restores the checkpoint, and then converts it to an OpenVINO model using `ov.convert_model`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nmodel = tf.keras.Model(...)\ncheckpoint = tf.train.Checkpoint(model)\nsave_path = checkpoint.save(save_directory)\n# ...\ncheckpoint.restore(save_path)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model(checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Create Empty String Tensor (Python)\nDESCRIPTION: Creates an OpenVINO tensor of a specified shape with empty strings as elements. Requires the `openvino` package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/string-tensors.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntensor = ov.Tensor(dtype=str, shape=[3])\n```\n\n----------------------------------------\n\nTITLE: Fine-tune Quantized Model - PyTorch\nDESCRIPTION: Fine-tunes a quantized PyTorch model with a small learning rate to restore accuracy after quantization. This step involves training the model for a few epochs, modeling quantization errors in both forward and backward passes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/quantization-aware-training.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrainer.train_epoch(epoch)\n    trainer.validate(epoch)\n    scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: Setting Input Tensor in C\nDESCRIPTION: This code creates an `ov::Tensor` and sets it as the input tensor for the inference request. It retrieves the input port by name and uses it to set the tensor. `input_data` would be your input data array. Memory management must be handled manually in C.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_29\n\nLANGUAGE: cpp\nCODE:\n```\nov_tensor_t* input_tensor = NULL;\nov_tensor_create(ov_element_type_e.FP32, input_shape, input_data, &input_tensor);\nov_infer_request_set_tensor(infer_request, \"input_port_name\", input_tensor);\n```\n\n----------------------------------------\n\nTITLE: State API Usage in C++\nDESCRIPTION: This code demonstrates how to use the OpenVINO State API in C++ to manage states between inference runs. It initializes the OpenVINO Core, reads a model, creates an inference request, retrieves and resets states, sets input data, performs inference, and retrieves state data. The code assumes that the model has stateful components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nauto core = ov::Core{};\nauto model = core.read_model(\"model.xml\");\n\n// Create infer request and get all states\nauto request = core.compile_model(model, \"CPU\").create_infer_request();\nauto states = request.query_state();\n\n// Reset states if needed\nfor (auto& state : states) {\n    state.reset();\n}\n\n// Set input data for the current request\nauto input_tensor = ov::Tensor{ov::element::f32, ov::Shape{1, 10}}; // Example shape\n// Fill input_tensor with data...\nrequest.set_tensor(\"input\", input_tensor);\n\n// Perform inference\nrequest.infer();\n\n// Get state data after inference\nfor (auto& state : states) {\n    auto state_tensor = state.get_state();\n    // Process state_tensor...\n}\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with OpenVINO GenAI (Python)\nDESCRIPTION: This Python snippet demonstrates how to use the `Text2ImagePipeline` from the `openvino_genai` library to generate an image from a text prompt. It takes a model directory and a prompt as input arguments. It initializes the pipeline, generates the image, and saves it as a BMP file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport argparse\n\nimport openvino_genai\nfrom PIL import Image\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('model_dir')\n    parser.add_argument('prompt')\n    args = parser.parse_args()\n\n    device = 'CPU'  # GPU can be used as well\n    pipe = openvino_genai.Text2ImagePipeline(args.model_dir, device)\n\n    image_tensor = pipe.generate(\n        args.prompt,\n        width=512,\n        height=512,\n        num_inference_steps=20,\n        num_images_per_prompt=1)\n\n    image = Image.fromarray(image_tensor.data[0])\n    image.save(\"image.bmp\")\n```\n\n----------------------------------------\n\nTITLE: Copy OpenVINO Tokenizers Library (MacOS arm64)\nDESCRIPTION: Specifies the directory to copy the OpenVINO Tokenizers prebuilt library for MacOS arm64 systems.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n<openvino_dir>/runtime/lib/arm64/Release/\n```\n\n----------------------------------------\n\nTITLE: Setting Callback for Inference in C++\nDESCRIPTION: This C++ code shows how to set a callback function that is executed when inference completes, using `ov::InferRequest::set_callback`.  It uses a lambda function as the callback. Requires OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\nov::InferRequest infer_request = compiled_model.create_infer_request();\n\nov::Tensor input_tensor = infer_request.get_input_tensor();\nstd::vector<float> input_data(input_tensor.get_size());\nstd::generate(input_data.begin(), input_data.end(), []() { return static_cast<float>(rand()) / RAND_MAX; });\nstd::memcpy(input_tensor.data(), input_data.data(), input_data.size() * sizeof(float));\n\ninfer_request.set_callback([](ov::InferRequest req) {\n    std::cout << \"Inference complete!\" << std::endl;\n});\ninfer_request.start_async();\n```\n\n----------------------------------------\n\nTITLE: Get/Set Tensor (Single Input/Output) in C++\nDESCRIPTION: This C++ code demonstrates getting and setting input and output tensors for a model with a single input and output. It uses the `get_input_tensor()` and `get_output_tensor()` methods. Requires OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\nov::InferRequest infer_request = compiled_model.create_infer_request();\n\nov::Tensor input_tensor = infer_request.get_input_tensor();\nstd::vector<float> input_data(input_tensor.get_size());\nstd::generate(input_data.begin(), input_data.end(), []() { return static_cast<float>(rand()) / RAND_MAX; });\nstd::memcpy(input_tensor.data(), input_data.data(), input_data.size() * sizeof(float));\n\ninfer_request.infer();\n\nov::Tensor output_tensor = infer_request.get_output_tensor();\nconst float* output_data = output_tensor.data<const float>();\n```\n\n----------------------------------------\n\nTITLE: Converting TensorFlow Model to OpenVINO IR in Python\nDESCRIPTION: This code snippet shows how to convert a TensorFlow SavedModel to OpenVINO IR using the `convert_model()` method.  It imports the openvino library, converts 'saved_model.pb' to an OpenVINO model, and compiles it for execution on 'AUTO' device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_1\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\nov_model = ov.convert_model(\"saved_model.pb\")\ncompiled_model = ov.compile_model(ov_model, \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Passing NumPy Arrays for Inference in OpenVINO Python\nDESCRIPTION: This snippet demonstrates how to pass data as NumPy arrays directly to the inference methods in the OpenVINO Python API. The data can be organized in Python dictionaries or lists, providing flexibility for different input structures. This simplifies the process of feeding data to the model for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model)\n\ninput_data = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n\n# Pass the data as a numpy array\nresults = compiled_model([input_data])\n\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Compiling Model with Performance Hints in Python\nDESCRIPTION: This code snippet demonstrates how to compile a model using OpenVINO's performance hints in Python.  The hint is used to optimize for throughput.  The code requires the openvino package to be installed. It compiles the model using `ov.Core().compile_model()` with the specified device and configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-throughput.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncore = ov.Core()\ncompiled_model = core.compile_model(\n    model_path, device_name=\"CPU\", config={\"hint\": ov.hint.performance_mode(\"THROUGHPUT\")}\n)\n```\n\n----------------------------------------\n\nTITLE: Create Python Virtual Environment (Linux/macOS)\nDESCRIPTION: This command creates a Python 3 virtual environment named `openvino_env` in the current directory. It is recommended to use python3 for newer projects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-pip.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython3 -m venv openvino_env\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch ExportedProgram from Disk using CLI\nDESCRIPTION: This command demonstrates how to convert a PyTorch model in `ExportedProgram` format from disk to OpenVINO using the OpenVINO Model Converter (ovc) command-line tool. It assumes the exported program is saved as `exported_program.pt2`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-pytorch.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\novc exported_program.pt2\n```\n\n----------------------------------------\n\nTITLE: Defining InputTensorInfo Interface in TypeScript\nDESCRIPTION: This code snippet defines the InputTensorInfo interface, which specifies methods for setting the element type, layout, and shape of an input tensor in OpenVINO. These methods are crucial for configuring the input data to match the model's requirements. Each method returns the InputTensorInfo object, enabling method chaining.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InputTensorInfo.rst#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\ninterface InputTensorInfo {\n    setElementType(elementType): InputTensorInfo;\n    setLayout(layout): InputTensorInfo;\n    setShape(shape): InputTensorInfo;\n}\n```\n\n----------------------------------------\n\nTITLE: Compiling Model with Performance Hints in C++\nDESCRIPTION: This code snippet illustrates how to compile a model using OpenVINO's performance hints in C++. The performance hint is set to optimize for throughput. It utilizes the `ov::Core::compile_model` method with a configuration specifying the performance hint for THROUGHPUT. It requires the OpenVINO C++ API headers to be included.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-throughput.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nauto compiled_model = core.compile_model(\n    model_path, \"CPU\", ov::hint::performance_mode(ov::hint::PerformanceMode::THROUGHPUT));\n```\n\n----------------------------------------\n\nTITLE: Custom Post-processing Step in OpenVINO (Python)\nDESCRIPTION: This snippet demonstrates how to add a custom post-processing step to the execution graph using Python. These steps are integrated into the graph and executed on the selected device, operating in the reverse direction: Model output -> Steps -> User tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nie = Core()\nmodel = ie.read_model(model=MODEL_PATH)\n\nout = model.output()\n\n# Define a custom postprocessing operation: insert a node after the output\ndef scale_node(node: ov.Node) -> ov.Node:\n    # node: ov.Node - OpenVINO node to insert after\n    # Constant 2.0\n    const = ov.Constant(np.array(2.0, dtype=np.float32), ov.Type.f32)\n    # node * 2.0\n    mul = ov.opset8.Multiply(node, const)\n    # Result\n    return mul.output(0).get_node()\n\n\n# 1. Create Preprocessor object\npp = Preprocessor(model)\n# 2. Get a Tensor object, that describes output of the model\noutput_tensor = pp.output(out_tensor_name)\n# 3. Set custom postprocessing operation.\noutput_tensor.postprocess().custom(scale_node)\n# 4. Apply preprocessing modification to the model\nmodel = pp.build()\n\ncompiled_model = ie.compile_model(model, device_name=DEVICE_NAME)\n```\n\n----------------------------------------\n\nTITLE: Inference with Quantized Model - OpenVINO\nDESCRIPTION: This code snippet shows how to perform inference with the quantized OpenVINO model. It loads the model into the OpenVINO runtime and compiles it for a specific device. Then, it performs inference on a dummy input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/quantizing-with-accuracy-control.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# [inference]\nimport numpy as np\nfrom openvino.runtime import Core\n\nie = Core()\ncompiled_model = ie.compile_model(\"model.xml\", \"CPU\")\noutput_layer = next(iter(compiled_model.outputs))\ninput_layer = next(iter(compiled_model.inputs))\n\ninput_data = np.random.normal(0, 1, input_layer.shape).astype(np.float32)\nres = compiled_model([input_data])[output_layer]\n# [inference]\n```\n\n----------------------------------------\n\nTITLE: Querying Optimal Number of Requests in Python\nDESCRIPTION: This Python snippet shows how to query the `ov::optimal_number_of_infer_requests` property to determine the optimal number of inference requests for automatic batch size selection. The application should create this number of requests and run them simultaneously for best performance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\nnum_requests = model.get_property(ov.properties.optimal_number_of_infer_requests)\nprint(f\"The optimal number of requests is: {num_requests}\")\n```\n\n----------------------------------------\n\nTITLE: Compile Model with Property (Python)\nDESCRIPTION: This code snippet demonstrates how to compile a model with specific properties in Python using `ov::Core::compile_model`. It loads a model from a file and compiles it with a specified performance hint. It requires an initialized `Core` object, a model path, and a device name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ncore = ov.Core()\nmodel_path = \"path_to_model.xml\"\ndevice_name = \"CPU\"\nmodel = core.read_model(model_path)\ncompiled_model = core.compile_model(model, device_name, {\"performance_hint\": \"LATENCY\"})\n```\n\n----------------------------------------\n\nTITLE: Defining Preprocessing Steps (C++)\nDESCRIPTION: This snippet demonstrates how to define a sequence of preprocessing steps using the `ov::preprocess::PrePostProcessor` in C++. It includes conversion to FP32 precision, color conversion from BGR to RGB, resizing to match model dimensions, and normalization by subtracting mean values and dividing by scale factors.  The snippet assumes that `ov::ResizeAlgorithm` and `ov::Layout` are defined within the `ov` namespace.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nppp.input().preprocess()\n    .convert_element_type(ov::element::f32)\n    .convert_color(ov::ColorFormat::BGR)\n    .resize(ov::ResizeAlgorithm::RESIZE_LINEAR)\n    .mean({100.5, 101.5, 102.5})\n    .scale({50.0, 51.0, 52.0});\n// .convert_layout(\"NHWC\");\n```\n\n----------------------------------------\n\nTITLE: Image Classification Async Sample Usage in Node.js\nDESCRIPTION: This command demonstrates the general structure for running the image classification script. It requires the path to the model file (*path_to_model_file*), the path to the image(s) (*path_to_img1*, *path_to_img2*), and the target device (*device*). Replace the placeholders with actual file paths and device names.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/js/node/classification_sample_async/README.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nnode classification_sample_async.js -m *path_to_model_file* -i *path_to_img1* -i *path_to_img2* -d *device*\n```\n\n----------------------------------------\n\nTITLE: Configuring Individual Devices with Auto-Device plugin in Python\nDESCRIPTION: This snippet shows how to configure individual devices and create the Auto-Device plugin on top in Python. The implementation is located in `docs/articles_en/assets/snippets/ov_auto.py` section `[part5]`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndocs/articles_en/assets/snippets/ov_auto.py\n```\n\n----------------------------------------\n\nTITLE: Checking OpenVINO™ Runtime version with Python\nDESCRIPTION: This Python code snippet retrieves and displays the version of the installed OpenVINO™ Runtime.  It relies on the `openvino.runtime` module and its `get_version` function.  No specific input is needed; the output is the version string.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/troubleshooting-install-config.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.runtime import get_version\nget_version()\n```\n\n----------------------------------------\n\nTITLE: Adjust Spatial Dimensions with Reshape - C++\nDESCRIPTION: This C++ snippet adjusts the spatial dimensions of an input image using the `reshape` method. It requires the OpenVINO Inference Engine library and assumes a model is loaded and ready for reshaping. This example demonstrates reshaping with a single input by passing the new shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/changing-input-shape.rst#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\n\n    // ! [spatial_reshape]\n    // Example: adjust input spatial dimensions to 512x512\n    model->reshape({1, 3, 512, 512});\n    // ! [spatial_reshape]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Operation Constructor in Python\nDESCRIPTION: This snippet demonstrates how to define a constructor for a custom operation in Python using the OpenVINO API.  The constructor initializes the operation with inputs and attributes. It inherits from `openvino.Op` and allows setting a name for the node.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-openvino-operations.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass ExampleOp(op.Op):\n    \"\"\"Example custom operation.\"\"\"\n    def __init__(self, input_node, val, name=None):\n        \"\"\"Example constructor.\n\n        :param input_node: Input node. It can be a list of nodes.\n        :param val: Example attribute.\n        :param name: Optional output node name.\n        \"\"\"\n        # A node name can be specified in two ways:\n        #   - At creation time (as an argument to the constructor here).\n        #   - Afterwards, by calling node.set_friendly_name method.\n        # The first way is preferable because certain OpenVINO transformations\n        # can be applied only to named nodes.\n        super().__init__(inputs=[input_node], attrs=dict(val=val), name=name)\n        # After the super call, the node is created and available in self.node.\n        # The inputs are connected, node type and shape are propagated.\n        # The attributes are also set, use self.get_attribute('val') to read them.\n```\n\n----------------------------------------\n\nTITLE: Usage Example in C++\nDESCRIPTION: This C++ code provides an example of using OpenVINO's genai capabilities for visual language models. It loads images, sets up the VLMPipeline with specified model directory and device, configures generation settings like max_new_tokens, starts a chat session, and generates text based on user input. It includes error handling for exceptions and non-exception objects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\nthrow std::runtime_error(std::string{\"Usage \"} + argv[0] + \" <MODEL_DIR> <IMAGE_FILE OR DIR_WITH_IMAGES>\");\n                }\n\n                std::vector<ov::Tensor> rgbs = utils::load_images(argv[2]);\n\n                std::string device = \"CPU\";  // GPU can be used as well.\n                ov::AnyMap enable_compile_cache;\n                if (\"GPU\" == device) {\n                    enable_compile_cache.insert({ov::cache_dir(\"vlm_cache\")});\n                }\n                ov::genai::VLMPipeline pipe(argv[1], device, enable_compile_cache);\n\n                ov::genai::GenerationConfig generation_config;\n                generation_config.max_new_tokens = 100;\n\n                std::string prompt;\n\n                pipe.start_chat();\n                std::cout << \"question:\\n\";\n\n                std::getline(std::cin, prompt);\n                pipe.generate(prompt,\n                              ov::genai::images(rgbs),\n                              ov::genai::generation_config(generation_config),\n                              ov::genai::streamer(print_subword));\n                std::cout << \"\\n----------\\n\"\n                    \"question:\\n\";\n                while (std::getline(std::cin, prompt)) {\n                    pipe.generate(prompt,\n                                  ov::genai::generation_config(generation_config),\n                                  ov::genai::streamer(print_subword));\n                    std::cout << \"\\n----------\\n\"\n                        \"question:\\n\";\n                }\n                pipe.finish_chat();\n            } catch (const std::exception& error) {\n                try {\n                    std::cerr << error.what() << '\\n';\n                } catch (const std::ios_base::failure&) {}\n                return EXIT_FAILURE;\n            } catch (...) {\n                try {\n                    std::cerr << \"Non-exception object thrown\\n\";\n                } catch (const std::ios_base::failure&) {}\n                return EXIT_FAILURE;\n            }\n```\n\n----------------------------------------\n\nTITLE: Setting Execution Mode in OpenVINO (C++)\nDESCRIPTION: This C++ snippet shows how to configure the `execution_mode` for inference.  It demonstrates setting the execution mode to either `ACCURACY` or `PERFORMANCE` using the `ov::hint::execution_mode` property. The execution mode influences the device's optimization strategy balancing accuracy and speed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/precision-control.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n\n    // Get supported properties\n    for (auto& property : core.get_property(\"CPU\", ov::supported_properties)) {\n        std::cout << property.name() << std::endl;\n    }\n\n    // Set execution mode to ACCURACY\n    core.set_property(\"CPU\", ov::hint::execution_mode(ov::hint::ExecutionMode::ACCURACY));\n\n    // Set execution mode to PERFORMANCE\n    core.set_property(\"CPU\", ov::hint::execution_mode(ov::hint::ExecutionMode::PERFORMANCE));\n}\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (TensorFlow Lite) with OpenVINO in C++\nDESCRIPTION: This code compiles a model in TensorFlow Lite format using the `core.compile_model()` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\nov::CompiledModel compiled_model_tflite = core.compile_model(\"path_to_model.tflite\", \"AUTO\");\n```\n\n----------------------------------------\n\nTITLE: Get Inference Number of Threads (C++)\nDESCRIPTION: This snippet demonstrates how to query the number of threads used for inference on CPU device in C++ using `ov::CompiledModel::get_property`. It gets the number of threads and prints it to console. Requires a compiled OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\nov::Core core;\nstd::string model_path = \"path_to_model.xml\";\nstd::string device_name = \"CPU\";\nauto model = core.read_model(model_path);\nauto compiled_model = core.compile_model(model, device_name);\nauto inference_num_threads = compiled_model.get_property(ov::inference_num_threads);\nstd::cout << \"Inference num threads: \" << inference_num_threads.as<int>() << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Set Manual Affinities in Heterogeneous Execution with OpenVINO (Python)\nDESCRIPTION: This snippet demonstrates how to manually set the affinity of operations in an OpenVINO model to specific devices using Python.  It retrieves runtime information for each node and sets the 'affinity' attribute to the desired device. This requires the openvino package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/hetero-execution.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom openvino.runtime import Core\n\ncore = Core()\nmodel = core.read_model(\"path_to_your_model.xml\")\n\n# [set_manual_affinities]\nfor node in model.get_ops():\n    layer_name = node.get_friendly_name()\n    if 'fully_connected' in layer_name:\n        node.get_rt_info()['affinity'] = 'GPU'\n    else:\n        node.get_rt_info()['affinity'] = 'CPU'\n# [set_manual_affinities]\n```\n\n----------------------------------------\n\nTITLE: Convert OCR Model with Named Input Shapes via CLI\nDESCRIPTION: This command-line snippet converts an ONNX OCR model using the `ovc` tool, specifying shapes for two inputs, `data` and `seq_len`, by using the `--input` argument with comma-separated input names and shapes. Input shapes should be specified in the format `input_name[shape]`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/setting-input-shapes.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\novc ocr.onnx --input data[3,150,200,1],seq_len[3]\n```\n\n----------------------------------------\n\nTITLE: Install NNCF via Conda\nDESCRIPTION: This command installs the NNCF package using conda, a package, dependency and environment management for any language—Python, R, Ruby, Lua, Java, C/ C++, Fortran, Scala, etc.  It ensures NNCF and its dependencies are correctly installed within a conda environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconda install -c conda-forge nncf\n```\n\n----------------------------------------\n\nTITLE: Exporting LLM with Channel-wise Data-aware Quantization\nDESCRIPTION: This snippet exports a Llama-2-7b-chat-hf model using Optimum-Intel with channel-wise data-aware quantization. The command uses --awq and --scale_estimation to improve accuracy and also specifies the dataset used for quantization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_6\n\nLANGUAGE: Console\nCODE:\n```\noptimum-cli export openvino -m meta-llama/Llama-2-7b-chat-hf --weight-format int4 --sym --group-size -1 --ratio 1.0 --awq --scale-estimation --dataset wikitext2  Llama-2-7b-chat-hf\n```\n\n----------------------------------------\n\nTITLE: Synchronous Inference with OpenCV and OpenVINO (C++)\nDESCRIPTION: This code snippet demonstrates a basic synchronous inference pipeline where a frame is captured using OpenCV and then immediately processed by the OpenVINO inference engine using `ov::InferRequest::infer()`. This is a standard, straightforward approach where the execution flow is serialized within the current application thread.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/general-optimizations.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ncv::Mat frame = cv::imread(\"image.bmp\");\n// Resize and convert image to required network input format\ncv::Mat resized_image;\ncv::resize(frame, resized_image, cv::Size(width, height), cv::INTER_LINEAR);\n\n// Create input tensor from image\nov::Tensor input_tensor = ov::Tensor(input_type, input_shape, resized_image.data);\n\n// Set input tensor for inference request\ninfer_request.set_input_tensor(input_tensor);\n\n// Synchronously run inference\ninfer_request.infer();\n\n// Process output tensor\nconst ov::Tensor& output_tensor = infer_request.get_output_tensor();\n```\n\n----------------------------------------\n\nTITLE: Using AsyncInferQueue for Asynchronous Inference in OpenVINO Python\nDESCRIPTION: This snippet demonstrates how to use `AsyncInferQueue` for asynchronous inference pipelines in the OpenVINO Python API. This class automatically manages a pool of `InferRequest` objects and provides synchronization mechanisms to control the flow of the pipeline. It includes waiting for all requests to complete using `wait_all`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model)\n\n# Create an AsyncInferQueue with 4 infer requests\ninfer_queue = ov.AsyncInferQueue(compiled_model, 4)\n\ninput_data = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n\n# Start asynchronous inference for all the requests\nfor i in range(len(infer_queue)):\n    infer_queue.start_async({compiled_model.inputs[0]: input_data})\n\n# Wait for all the requests to finish\ninfer_queue.wait_all()\n\nprint(\"Inference complete\")\n```\n\n----------------------------------------\n\nTITLE: Querying Optimal Number of Requests in C++\nDESCRIPTION: This C++ snippet demonstrates how to query the optimal number of inference requests using `ov::optimal_number_of_infer_requests`. It returns the recommended number of requests that should be run simultaneously for optimal device utilization. It assumes the `compiled_model` object is already defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/high-level-performance-hints.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nint nireq = compiled_model.get_property(ov::optimal_number_of_infer_requests);\n```\n\n----------------------------------------\n\nTITLE: Save a model converted to OpenVINO IR\nDESCRIPTION: Saves a Hugging Face model that has been converted to OpenVINO IR format to a specified directory. This allows for efficient loading and deployment of the model at a later time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel.save_pretrained(\"ov_model\")\n```\n\n----------------------------------------\n\nTITLE: Controlling CPU Thread Pinning in C++ OpenVINO\nDESCRIPTION: This snippet demonstrates how to enable or disable CPU thread pinning using the `ov::hint::enable_cpu_pinning` property in C++. Setting this property allows users to control whether OpenVINO threads are pinned to specific CPU cores. Disabling thread pinning may improve performance in applications running multiple workloads concurrently, by allowing more flexible scheduling by the OS.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device/performance-hint-and-thread-scheduling.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// ! [ov:intel_cpu:multi_threading:part1]\n#include <openvino/runtime.hpp>\n\nint main() {\n    ov::Core core;\n    core.set_property(\"CPU\", ov::hint::enable_cpu_pinning(false));\n    return 0;\n}\n// ! [ov:intel_cpu:multi_threading:part1]\n```\n\n----------------------------------------\n\nTITLE: Retrieve All Inputs and Outputs with OpenVINO (C++)\nDESCRIPTION: This C++ snippet uses the `ov::Model::inputs()` and `ov::Model::outputs()` methods to retrieve vectors containing all input and output ports of an OpenVINO model. This demonstrates a fundamental step in accessing model structure and manipulating inputs and outputs for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nauto inputs = model->inputs();\nauto outputs = model->outputs();\n```\n\n----------------------------------------\n\nTITLE: Waiting for Inference with Timeout in C++\nDESCRIPTION: This C++ code shows how to wait for inference completion with a timeout using `ov::InferRequest::wait_for`. It checks the returned status to see if the inference completed within the specified time. Requires OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\nov::InferRequest infer_request = compiled_model.create_infer_request();\n\nov::Tensor input_tensor = infer_request.get_input_tensor();\nstd::vector<float> input_data(input_tensor.get_size());\nstd::generate(input_data.begin(), input_data.end(), []() { return static_cast<float>(rand()) / RAND_MAX; });\nstd::memcpy(input_tensor.data(), input_data.data(), input_data.size() * sizeof(float));\n\ninfer_request.start_async();\nov::WaitMode status = infer_request.wait_for(std::chrono::milliseconds(1000));\n```\n\n----------------------------------------\n\nTITLE: Loading Preprocessed Model - Python\nDESCRIPTION: Illustrates how to load a preprocessed OpenVINO IR model from a file in Python. This allows skipping the preprocessing steps during runtime, reducing startup time.  The code also showcases setting device preferences and enabling model caching.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details/integrate-save-preprocessing-use-case.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef save_load():\n    core = Core()\n    # the preprocessed model is loaded, preprocessing is not required\n    model = core.read_model(\"ov_model.xml\")\n    # Set device preferences and enable model caching\n    compiled_model = core.compile_model(model, device_name=\"CPU\", config={\"CACHE_DIR\": \"cache\"})\n```\n\n----------------------------------------\n\nTITLE: Chat Scenario using LLM in Python\nDESCRIPTION: This Python code snippet demonstrates a chat scenario using the OpenVINO GenAI LLMPipeline.  It initializes the pipeline, starts a chat session, takes user input as a prompt, generates text using the LLM, and prints the output.  It also defines a streamer function to print subwords as they are generated. The chat continues until the user enters an EOF character. The required library is `openvino_genai`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai\n\n\n            def streamer(subword):\n                print(subword, end='', flush=True)\n                return False\n\n\n            def infer(model_dir: str):\n                device = 'CPU'  # GPU can be used as well.\n                pipe = openvino_genai.LLMPipeline(model_dir, device)\n\n                config = openvino_genai.GenerationConfig()\n                config.max_new_tokens = 100\n\n                pipe.start_chat()\n                while True:\n                    try:\n                        prompt = input('question:\\n')\n                    except EOFError:\n                        break\n                    pipe.generate(prompt, config, streamer)\n                    print('\\n----------')\n                pipe.finish_chat()\n```\n\n----------------------------------------\n\nTITLE: Run Generation using OpenVINO GenAI in C++\nDESCRIPTION: This C++ snippet demonstrates text generation using the OpenVINO GenAI API on an NPU. It includes necessary headers, initializes the LLMPipeline with the model path and device ('NPU'), sets generation configurations, and then generates text based on a prompt.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n   std::string model_path = \"TinyLlama\";\n   ov::genai::LLMPipeline pipe(models_path, \"NPU\");\n   ov::genai::GenerationConfig config;\n   config.max_new_tokens=100;\n   std::cout << pipe.generate(\"The Sun is yellow because\", config);\n}\n```\n\n----------------------------------------\n\nTITLE: Waiting for Inference with Timeout in Python\nDESCRIPTION: This Python code demonstrates waiting for inference completion with a timeout using `ov::InferRequest::wait_for`. If inference doesn't complete within the timeout, it returns `False`. Requires OpenVINO and a compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\ncompiled_model = ie.compile_model(model, \"CPU\")\ninfer_request = compiled_model.create_infer_request()\n\ninput_tensor = infer_request.get_input_tensor()\ninput_tensor.data[:] = np.random.normal(0, 1, input_tensor.shape)\n\ninfer_request.start_async()\nstatus = infer_request.wait_for(1000)\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Model to OpenVINO IR in Python\nDESCRIPTION: This code snippet demonstrates how to convert a PyTorch model to OpenVINO IR using the `convert_model()` method. It imports the necessary libraries, loads a ResNet50 model from torchvision, converts it to an OpenVINO model, and then compiles the model for execution on the 'AUTO' device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nimport torchvision\nimport openvino as ov\n\nmodel = torchvision.models.resnet50(weights='DEFAULT')\nov_model = ov.convert_model(model)\ncompiled_model = ov.compile_model(ov_model, \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Set Performance Mode to Best Performance (C++)\nDESCRIPTION: This C++ snippet demonstrates how to configure the OpenVINO GenAI pipeline for best performance by setting the `GENERATE_HINT` option to `BEST_PERF`. This optimizes the pipeline for maximum throughput at the expense of increased compilation time. Requires the OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_22\n\nLANGUAGE: cpp\nCODE:\n```\nov::AnyMap pipeline_config = { { \"GENERATE_HINT\",  \"BEST_PERF\" } };\nov::genai::LLMPipeline pipe(model_path, \"NPU\", pipeline_config);\n```\n\n----------------------------------------\n\nTITLE: Export Compiled Model with OpenVINO in Python\nDESCRIPTION: This Python snippet demonstrates how to export a compiled model for inference on a specific device using OpenVINO. This is the recommended approach to replace the discontinued `Compile tool`. It requires the `openvino` package to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/legacy-features.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef export_compiled_model(model_path: str, device: str, output_path: str):\n    from openvino.runtime import Core\n\n    core = Core()\n    # read the model\n    model = core.read_model(model_path)\n    # compile the model for specified device\n    compiled_model = core.compile_model(model=model, device_name=device)\n    # export the compiled model\n    compiled_model.export(output_path)\n```\n\n----------------------------------------\n\nTITLE: Converting PaddlePaddle Model File with OpenVINO (Python)\nDESCRIPTION: This snippet demonstrates how to convert a PaddlePaddle model file (``.pdmodel``) to OpenVINO IR using the ``openvino.convert_model`` function in Python. It requires the `openvino` package to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-paddle.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nov.convert_model('your_model_file.pdmodel')\n```\n\n----------------------------------------\n\nTITLE: Replacing a Node and Preserving Friendly Name (C++)\nDESCRIPTION: This code snippet demonstrates how to replace a node with another node and ensure that the friendly name of the original node is preserved. The friendly name represents the name from the model's perspective and should be maintained during transformations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov:replace_friendly_name]\nauto new_op = std::make_shared<ov::op::v0::Relu>(input);\nnew_op->set_friendly_name(op->get_friendly_name());\nov::replace_node(op, new_op);\n//! [ov:replace_friendly_name]\n```\n\n----------------------------------------\n\nTITLE: Install latest OpenVINO Runtime\nDESCRIPTION: Installs the latest version of the OpenVINO Runtime using the APT package manager. It assumes that the OpenVINO repository has been properly set up.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt install openvino\n```\n\n----------------------------------------\n\nTITLE: Process Inference Results in C\nDESCRIPTION: This C code snippet focuses on processing the output from an OpenVINO model after inference. It extracts the output tensors and processes the data.  It assumes the OpenVINO C API is already initialized and the model is loaded.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_32\n\nLANGUAGE: cpp\nCODE:\n```\n//! [part5]\n// Prepare input data\nsize_t num_of_inputs = 0;\nOPENVINO_ASSERT(ov_get_inputs_size(model, &num_of_inputs));\nOPENVINO_ASSERT(num_of_inputs == 1);\n\nov_input_tensor_info_t input_tensor = {0};\nOPENVINO_ASSERT(ov_input_tensor_info_create(model, 0, &input_tensor));\nov_tensor input_tensor_data = {0};\nOPENVINO_ASSERT(ov_input_tensor_info_get_tensor(input_tensor, &input_tensor_data));\n\nsize_t input_size = 0;\nOPENVINO_ASSERT(ov_tensor_get_size(input_tensor_data, &input_size));\n\nfloat* input_data = (float*)malloc(input_size * sizeof(float));\nOPENVINO_ASSERT(input_data != NULL);\nOPENVINO_ASSERT(ov_tensor_set_data(input_tensor_data, OPENVINO_TYPE_FP32, input_data, input_size * sizeof(float)) == 0);\n\n// Prepare output data\nsize_t num_of_outputs = 0;\nOPENVINO_ASSERT(ov_get_outputs_size(model, &num_of_outputs));\nOPENVINO_ASSERT(num_of_outputs == 1);\n\nov_output_tensor_info_t output_tensor = {0};\nOPENVINO_ASSERT(ov_output_tensor_info_create(model, 0, &output_tensor));\nov_tensor output_tensor_data = {0};\nOPENVINO_ASSERT(ov_output_tensor_info_get_tensor(output_tensor, &output_tensor_data));\n\nsize_t output_size = 0;\nOPENVINO_ASSERT(ov_tensor_get_size(output_tensor_data, &output_size));\n\nfloat* output_data = (float*)malloc(output_size * sizeof(float));\nOPENVINO_ASSERT(output_data != NULL);\nOPENVINO_ASSERT(ov_tensor_set_data(output_tensor_data, OPENVINO_TYPE_FP32, output_data, output_size * sizeof(float)) == 0);\n\n// Create executable network\nov_compiled_model compiled_model = {0};\nOPENVINO_ASSERT(ov_compile_model(model, core, NULL, &compiled_model));\n\n// Create inference request\nov_infer_request infer_request = {0};\nOPENVINO_ASSERT(ov_create_infer_request(compiled_model, &infer_request));\nOPENVINO_ASSERT(ov_infer_request_infer(infer_request));\n\n//! [part5]\n```\n\n----------------------------------------\n\nTITLE: Combining Hint with Low-Level Setting in C++\nDESCRIPTION: This C++ snippet illustrates how to combine a high-level performance hint (`THROUGHPUT`) with a device-specific low-level setting (`ov::streams::num`). This allows for fine-grained control over performance, overriding the hint's default behavior if needed. It assumes the `core` object and `model` object are already defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/high-level-performance-hints.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nov::CompiledModel compiled_model = core.compile_model(\n    model,\n    \"CPU\",\n    ov::hint::performance_mode(ov::hint::PerformanceMode::THROUGHPUT),\n    ov::streams::num(1));\n```\n\n----------------------------------------\n\nTITLE: Infer Request Operations: Set and Get Tensors in OpenVINO (C++)\nDESCRIPTION: The `ov::InferRequest::set_tensor` and `ov::InferRequest::get_tensor` APIs are used to operate with tensors within an inference request. `set_tensor` allows you to set input data for the model, while `get_tensor` lets you retrieve output data after inference. These are crucial for interacting with the model's input and output layers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_nv12_input_classification/README.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n``ov::InferRequest::set_tensor``\n```\n\nLANGUAGE: C++\nCODE:\n```\n``ov::InferRequest::get_tensor``\n```\n\n----------------------------------------\n\nTITLE: Compile Model for Heterogeneous Execution with OpenVINO (Python)\nDESCRIPTION: This snippet compiles an OpenVINO model for heterogeneous execution using Python.  It specifies the device priorities as 'GPU,CPU', allowing OpenVINO to automatically assign operations to available devices based on their capabilities. The openvino package is required.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/hetero-execution.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom openvino.runtime import Core\n\ncore = Core()\nmodel = core.read_model(\"path_to_your_model.xml\")\n\n# [compile_model]\ncompiled_model = core.compile_model(model, \"HETERO:GPU,CPU\")\n# [compile_model]\n```\n\n----------------------------------------\n\nTITLE: Save Converted Tokenizer Models\nDESCRIPTION: Saves the converted OpenVINO tokenizer and detokenizer models to the specified directory. This allows for reusing the converted models without reconversion.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nfrom openvino import save_model\n\ntokenizer_dir = Path(\"tokenizer/\")\nsave_model(ov_tokenizer, tokenizer_dir / \"openvino_tokenizer.xml\")\nsave_model(ov_detokenizer, tokenizer_dir / \"openvino_detokenizer.xml\")\n```\n\n----------------------------------------\n\nTITLE: Running Image Classification Sample (C++, Linux)\nDESCRIPTION: This command runs the `classification_sample_async` executable with a specified model (`googlenet-v1.xml`), input image (`dog.bmp`), and target device (`CPU`) on a Linux system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_32\n\nLANGUAGE: sh\nCODE:\n```\n./classification_sample_async -i ~/Downloads/dog.bmp -m ~/ir/googlenet-v1.xml -d CPU\n```\n\n----------------------------------------\n\nTITLE: Convert Hugging Face Transformer Model to OpenVINO IR (Python)\nDESCRIPTION: This code snippet illustrates how to convert a BERT model from Hugging Face Transformers to OpenVINO IR. It loads the tokenizer and model, prepares the input data using the tokenizer, converts the model using `openvino.convert_model`, and provides options to save the model to IR or compile and run inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation.rst#_snippet_1\n\nLANGUAGE: py\nCODE:\n```\nfrom transformers import BertTokenizer, BertModel\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\n\nimport openvino as ov\nov_model = ov.convert_model(model, example_input={**encoded_input})\n\n###### Option 1: Save to OpenVINO IR:\n\n# save model to OpenVINO IR for later use\nov.save_model(ov_model, 'model.xml')\n\n###### Option 2: Compile and infer with OpenVINO:\n\n# compile model\ncompiled_model = ov.compile_model(ov_model)\n\n# prepare input_data using HF tokenizer or your own tokenizer\n# encoded_input is reused here for simplicity\n\n# run inference\nresult = compiled_model({**encoded_input})\n```\n\n----------------------------------------\n\nTITLE: Using Shared Memory with Tensors in OpenVINO Python\nDESCRIPTION: This snippet shows how to create `Tensor` objects that share memory with NumPy arrays in the OpenVINO Python API. By specifying `shared_memory=True`, the `Tensor` object does not copy data but instead accesses the memory of the NumPy array. This can improve performance by avoiding data copies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\n# Create a numpy array\ndata = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n\n# Create a Tensor object that shares memory with the numpy array\ntensor = ov.Tensor(data, shared_memory=True)\n\n# Modify the numpy array\ndata[0] = 10.0\n\n# The tensor will also be updated\nprint(tensor.data[0])\n```\n\n----------------------------------------\n\nTITLE: Install OpenCL packages on Ubuntu 22.04/24.04\nDESCRIPTION: Installs OpenCL ICD loader, Intel OpenCL ICD, Intel Level Zero GPU, and Level Zero packages on Ubuntu 22.04 LTS or 24.04 LTS. It also adds the user to the render group. Requires apt package manager.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/configurations-intel-gpu.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\napt-get install -y ocl-icd-libopencl1 intel-opencl-icd intel-level-zero-gpu level-zero\nsudo usermod -a -G render $LOGNAME\n```\n\n----------------------------------------\n\nTITLE: Inference Execution and Output Token Retrieval\nDESCRIPTION: This code snippet performs inference using an OpenVINO inference request and retrieves the output token. It also handles token concatenation and checks for the end-of-sequence (EOS) token. The model input is updated for subsequent inferences by modifying input IDs, attention mask, and position IDs based on the current output token.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ninfer_request.wait()\n\n# get a prediction for the last token on the first inference\noutput_token = infer_request.get_output_tensor().data[:, -1:]\ntokens_result = np.hstack((tokens_result, output_token))\nif output_token[0, 0] == eos_token:\n   break\n\n# prepare input for new inference\nmodel_input[\"input_ids\"] = output_token\nmodel_input[\"attention_mask\"] = np.hstack((model_input[\"attention_mask\"].data, [[1]]))\nmodel_input[\"position_ids\"] = np.hstack(\n   (model_input[\"position_ids\"].data, [[model_input[\"position_ids\"].data.shape[-1]]])\n)\n```\n\n----------------------------------------\n\nTITLE: Convert Checkpoint Model (TF1) using Python\nDESCRIPTION: This snippet demonstrates how to convert a TensorFlow 1 Checkpoint model to OpenVINO IR using `openvino.convert_model` in Python. The input is a list containing the path to the inference graph (``.pb``) and the path to the checkpoint file (``.ckpt``). The output is the OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_8\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model(['path_to_inference_graph.pb', 'path_to_checkpoint_file.ckpt'])\n```\n\n----------------------------------------\n\nTITLE: Tensor Creation C API\nDESCRIPTION: Demonstrates how to create a tensor from host memory using `ov_tensor_create_from_host_ptr`. This is necessary to pass input data to the inference engine.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/c/hello_classification/README.md#_snippet_4\n\nLANGUAGE: C\nCODE:\n```\n``ov_tensor_create_from_host_ptr``\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (IR) with OpenVINO in C\nDESCRIPTION: This code compiles a model in IR format using the `ov_core_compile_model` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests. Memory management must be handled manually in C.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\nov_compiled_model_t* compiled_model_ir = NULL;\nov_core_compile_model(core, \"path_to_model.xml\", \"AUTO\", &compiled_model_ir);\n```\n\n----------------------------------------\n\nTITLE: Copy OpenVINO Tokenizers Library (Linux arm64)\nDESCRIPTION: Specifies the directory to copy the OpenVINO Tokenizers prebuilt library for Linux arm64 systems.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n<openvino_dir>/runtime/lib/aarch64/\n```\n\n----------------------------------------\n\nTITLE: Convert Checkpoint Model (TF1) using CLI\nDESCRIPTION: This snippet shows how to convert a TensorFlow 1 Checkpoint model to OpenVINO IR using the `ovc` command-line tool. The input consists of the path to the inference graph (``.pb``) and the path to the checkpoint file (``.ckpt``). The output is the OpenVINO IR model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\novc path_to_inference_graph.pb path_to_checkpoint_file.ckpt\n```\n\n----------------------------------------\n\nTITLE: Convert TensorFlow Hub KerasLayer to OpenVINO\nDESCRIPTION: This snippet demonstrates how to convert a KerasLayer loaded from TensorFlow Hub to an OpenVINO model using `openvino.convert_model`. Requires the `tensorflow` and `tensorflow_hub` packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_16\n\nLANGUAGE: py\nCODE:\n```\nimport tensorflow_hub as hub\nimport openvino as ov\n\nmodel = hub.KerasLayer(\"https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/classification/5\")\nov_model = ov.convert_model(model)\n```\n\n----------------------------------------\n\nTITLE: Setting Input Tensor with Dynamic Shapes - Python\nDESCRIPTION: This Python snippet demonstrates how to set an input tensor for inference with dynamic shapes. It loads a model, reshapes the input, creates input tensors with different shapes, and performs inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\n\n# ! [set_input_tensor]\ninput_shape = ov.PartialShape([1, -1])\nmodel.reshape({model.input().get_any_name(): input_shape})\ncompiled_model = core.compile_model(model, \"CPU\")\nrequest = compiled_model.create_infer_request()\n\ninput_tensor_1 = ov.Tensor(np.zeros((1, 128), dtype=np.float32))\nrequest.infer({model.input().get_any_name(): input_tensor_1})\n\ninput_tensor_2 = ov.Tensor(np.zeros((1, 200), dtype=np.float32))\nrequest.infer({model.input().get_any_name(): input_tensor_2})\n# ! [set_input_tensor]\n```\n\n----------------------------------------\n\nTITLE: Compiling TensorFlow Lite Model in OpenVINO using C++\nDESCRIPTION: This C++ code snippet compiles a TensorFlow Lite model for OpenVINO using the `compile_model` method of the `ov::Core` class. It compiles '<INPUT_MODEL>.tflite' for execution on the 'AUTO' device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nov::CompiledModel compiled_model = core.compile_model(\"<INPUT_MODEL>.tflite\", \"AUTO\");\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX Model using CLI\nDESCRIPTION: This snippet demonstrates how to convert an ONNX model to OpenVINO IR format using the `ovc` command-line tool. The input is the path to the ONNX model file (e.g., `your_model_file.onnx`). The output is the converted OpenVINO IR model files. Ensure that the OpenVINO environment is properly configured for the `ovc` command to be available.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-onnx.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\novc your_model_file.onnx\n```\n\n----------------------------------------\n\nTITLE: Run Quantization with Accuracy Control - ONNX\nDESCRIPTION: This example shows how to quantize an ONNX model with accuracy control using NNCF's `quantize_with_accuracy_control()` function.  It utilizes a model, calibration and validation datasets, and a validation function to quantize the model while minimizing accuracy loss, specified by `max_drop`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/quantizing-with-accuracy-control.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# [quantization]\nimport nncf\nimport onnx\nfrom onnx.checker import check_model\n\nmodel = onnx.load_model(\"model.onnx\")\ncheck_model(model)\n\n\ndef dataset_fn(model):\n    # actual dataset\n    return None\n\n\ndef validate(model, validation_dataset):\n    # actual validation\n    return 0.5\n\n\ncalibration_dataset = dataset_fn(model)\nvalidation_dataset = dataset_fn(model)\n\nquantized_model = nncf.quantize_with_accuracy_control(model=model,\n                                                       calibration_dataset=calibration_dataset,\n                                                       validation_dataset=validation_dataset,\n                                                       validation_fn=validate,\n                                                       max_drop=0.01)\n# [quantization]\n```\n\n----------------------------------------\n\nTITLE: Automatic Model Compilation with AUTO mode in OpenVINO Python\nDESCRIPTION: This snippet demonstrates how to easily compile a model using the `CompiledModel` helper method, which automatically creates a `Core` object and applies the `AUTO` inference mode by default. This simplifies model compilation in OpenVINO using Python.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\n# creates compiled model with AUTO inference mode by default\ncompiled_model = core.compile_model(model)\n```\n\n----------------------------------------\n\nTITLE: Build Project with CMake\nDESCRIPTION: This snippet shows the commands to build a project using CMake.  It first navigates to the build directory, then runs `cmake` to generate the build files, and finally uses `cmake --build` to compile the project. This assumes that CMake is installed and configured correctly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_42\n\nLANGUAGE: sh\nCODE:\n```\ncd build/\ncmake ../project\ncmake --build .\n```\n\n----------------------------------------\n\nTITLE: Quantize with Subset Size\nDESCRIPTION: Quantizes the model using a specified subset size from the calibration dataset. This example uses a subset size of 1000 to estimate quantization parameters of activations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_20\n\nLANGUAGE: sh\nCODE:\n```\nnncf.quantize(model, dataset, subset_size=1000)\n\n```\n\n----------------------------------------\n\nTITLE: LLM Pipeline Initialization and Generation in C++\nDESCRIPTION: This C++ code initializes an LLMPipeline with a specified model path and device (CPU) using command-line arguments. It then generates text with a maximum number of new tokens using the `ov::genai::max_new_tokens` helper and prints the result to the console. It includes necessary header files for LLMPipeline and standard output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n   std::string model_path = argv[1];\n   ov::genai::LLMPipeline pipe(model_path, \"CPU\");\n   std::cout << pipe.generate(\"The Sun is yellow because\", ov::genai::max_new_tokens(100));\n}\n```\n\n----------------------------------------\n\nTITLE: Synchronous Benchmark Implementation (Python)\nDESCRIPTION: This Python snippet is the main part of the synchronous benchmark sample. It measures the performance of a given OpenVINO model using synchronous inference. The code generates random input data, performs inference multiple times, and then calculates and reports performance metrics like latency, throughput, and iteration count.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/sync-benchmark.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsamples/python/benchmark/sync_benchmark/sync_benchmark.py\n```\n\n----------------------------------------\n\nTITLE: Quantize Model for QAT - PyTorch\nDESCRIPTION: Quantizes a PyTorch model using NNCF for Quantization-Aware Training. This step prepares the model for fine-tuning with quantization awareness.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/quantization-aware-training.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nnncf.quantize(model, quantization_config=quantization_config)\n```\n\n----------------------------------------\n\nTITLE: Setting Input Tensor with Dynamic Shapes - C++\nDESCRIPTION: This C++ snippet shows how to set an input tensor for a model with dynamic shapes and perform inference. The model is loaded, reshaped, and inference is executed with different input shapes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n#include <iostream>\n#include <vector>\n\nint main() {\n    // ! [ov_dynamic_shapes:set_input_tensor]\n    ov::Core core;\n    auto model = core.read_model(\"model.xml\");\n\n    ov::PartialShape input_shape = ov::PartialShape({1, -1});\n    model->reshape({model->input().get_any_name(), input_shape});\n    ov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\n    ov::InferRequest request = compiled_model.create_infer_request();\n\n    ov::Tensor input_tensor_1 = ov::Tensor(ov::element::f32, ov::Shape{1, 128});\n    request.infer({model->input().get_any_name(), input_tensor_1});\n\n    ov::Tensor input_tensor_2 = ov::Tensor(ov::element::f32, ov::Shape{1, 200});\n    request.infer({model->input().get_any_name(), input_tensor_2});\n    // ! [ov_dynamic_shapes:set_input_tensor]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (ov::Model) with OpenVINO in C\nDESCRIPTION: This code compiles a model of type `ov::Model` using the `ov_core_compile_model` method.  It specifies the model itself and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests. Memory management must be handled manually in C.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\nov_model_t* model = NULL;\nov_core_read_model(core, \"path_to_model.xml\", &model);\nov_compiled_model_t* compiled_model_ov = NULL;\nov_core_compile_model(core, model, \"AUTO\", &compiled_model_ov);\n```\n\n----------------------------------------\n\nTITLE: Setting Input Tensor in C++\nDESCRIPTION: This code creates an `ov::Tensor` and sets it as the input tensor for the inference request. It retrieves the input port by name and uses it to set the tensor.  `input_data` would be your input data array.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_28\n\nLANGUAGE: cpp\nCODE:\n```\nov::Tensor input_tensor(ov::element::f32, input_shape, input_data);\ninfer_request.set_tensor(\"input_port_name\", input_tensor);\n```\n\n----------------------------------------\n\nTITLE: Get Optimal Number of Infer Requests (C++)\nDESCRIPTION: This snippet shows how to get the optimal number of infer requests for a compiled model in C++ using `ov::CompiledModel::get_property`. It retrieves the optimal number and prints it to the console. Requires a compiled OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\nov::Core core;\nstd::string model_path = \"path_to_model.xml\";\nstd::string device_name = \"CPU\";\nauto model = core.read_model(model_path);\nauto compiled_model = core.compile_model(model, device_name);\nauto num_requests = compiled_model.get_property(ov::optimal_number_of_infer_requests);\nstd::cout << \"Optimal number of infer requests: \" << num_requests.as<int>() << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Using Torch.compile with OpenVINO\nDESCRIPTION: This snippet demonstrates how to utilize OpenVINO with PyTorch's torch.compile feature to compile PyTorch models into optimized kernels for inference. This can enhance inference performance on Intel hardware.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-integrations.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino.torch\n\n...\nmodel = torch.compile(model, backend='openvino')\n...\n```\n\n----------------------------------------\n\nTITLE: Detecting Dynamic Output Shapes - Python\nDESCRIPTION: This Python code snippet checks whether an output layer has dynamic dimensions using the `partial_shape.is_dynamic()` property and prints the result.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model, \"CPU\")\n\n# ! [detect_dynamic]\noutput_shape = compiled_model.output(0).get_partial_shape()\n\nif output_shape.is_dynamic:\n    print(\"Output shape is dynamic\")\nelse:\n    print(\"Output shape is static\")\n# ! [detect_dynamic]\n```\n\n----------------------------------------\n\nTITLE: Reshape Model in OpenVINO (C)\nDESCRIPTION: This function allows reshaping an OpenVINO model with a list of tensor names and partial shapes. It takes a pointer to the `ov_model_t`, a list of tensor names (`const char**`), a list of partial shapes (`const ov_partial_shape_t*`), and the size of the list. A status code indicates success or failure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_33\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_model_reshape(const ov_model_t* model,\n                 const char** tensor_names,\n                 const ov_partial_shape_t* partial_shapes,\n                 size_t size)\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (TensorFlow Lite) with OpenVINO in Python\nDESCRIPTION: This code compiles a model in TensorFlow Lite format using the `core.compile_model()` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel_tflite = core.compile_model(\"path_to_model.tflite\", \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Adding OpenVINO Sample with CMake\nDESCRIPTION: This CMake code snippet uses the `ov_add_sample` macro to define a sample named `throughput_benchmark`. It specifies the source file as `main.cpp` located in the current source directory and declares a dependency on the `ie_samples_utils` library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/benchmark/throughput_benchmark/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_sample(NAME throughput_benchmark\n              SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/main.cpp\"\n              DEPENDENCIES ie_samples_utils)\n```\n\n----------------------------------------\n\nTITLE: Convert SavedModel Format (TF1) using Python\nDESCRIPTION: This snippet demonstrates how to convert a TensorFlow 1 SavedModel format (directory containing ``.pb``, ``variables``, etc.) to OpenVINO IR using `openvino.convert_model` in Python. The input is the path to the SavedModel directory. The output is the OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_6\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model('path_to_saved_model_dir')\n```\n\n----------------------------------------\n\nTITLE: Exporting OpenVINO model using optimum-cli\nDESCRIPTION: This command exports an LLM model using Hugging Face Optimum-Intel to the OpenVINO format, using int4 precision and trusting remote code. It includes the necessary information for execution, like tokenizer/detokenizer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\noptimum-cli export openvino --model \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" --weight-format int4 --trust-remote-code \"TinyLlama-1.1B-Chat-v1.0\"\n```\n\n----------------------------------------\n\nTITLE: Access String Tensor Elements (Python)\nDESCRIPTION: Accesses and prints elements of a 1D string tensor using `str_data`.  Alternatively shows how to use `byte_data` to access encoded strings as bytes. Requires the `openvino` package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/string-tensors.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndata = tensor.str_data  # use tensor.byte_data instead to access encoded strings as `bytes`\nfor i in range(tensor.get_size()):\n   print(data[i])\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Inference and Accuracy Check (Python)\nDESCRIPTION: This snippet demonstrates how to convert a PyTorch model to OpenVINO, compile it, and compare its output with the original PyTorch model's output using `numpy.testing.assert_allclose`.  It uses absolute and relative tolerances to account for potential differences, particularly when random operations are involved.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/pytorch/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport openvino as ov\nimport numpy as np\n\nexample = (torch.randn(1, 3, 100, 100),)\nmodel = example_model()\nov_model = ov.convert_model(model, example_input=example)\ncore = ov.Core()\ncompiled = core.compile_model(ov_model, \"CPU\")\n\npt_res = model(*example)\nov_res = compiled(example)\nnp.testing.assert_allclose(pt_res.detach().numpy(), ov_res[0], atol=1e-4, rtol=1e-4)\n```\n\n----------------------------------------\n\nTITLE: Custom Pre-processing Step in OpenVINO (C++)\nDESCRIPTION: This C++ snippet illustrates how to add a custom pre-processing step to the execution graph. It accepts the current input node, applies user-defined preprocessing operations, and returns a new node. Custom pre-processing functions are inserted during model compilation and are not invoked during the execution phase.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_25\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nauto model = core.read_model(MODEL_PATH);\n\nauto in = model->input();\n\n// Define a custom preprocessing operation: insert a node after the input\nauto scale_node = [](const ov::Output<ov::Node>& node) {\n    // node: ov::Output<ov::Node> - OpenVINO node to insert after\n    // Constant 2.0\n    auto const_node = ov::op::v0::Constant::create(ov::element::f32, ov::Shape{}, {2.0f});\n    // node * 2.0\n    auto mul = std::make_shared<ov::op::v1::Multiply>(node, const_node);\n    // Result\n    return mul->output(0);\n};\n\n// 1. Create Preprocessor object\nov::preprocess::Preprocessor pp(model);\n// 2. Get a Tensor object, that describes input of the model\nauto input_tensor = pp.input(in.get_any_name());\n// 3. Set custom preprocessing operation.\ninput_tensor.preprocess().custom(scale_node);\n// 4. Apply preprocessing modification to the model\nmodel = pp.build();\n\nauto compiled_model = core.compile_model(model, DEVICE_NAME);\n```\n\n----------------------------------------\n\nTITLE: Compile Model with Property (C++)\nDESCRIPTION: This snippet demonstrates how to compile a model with a performance hint property in C++ using the `ov::Core::compile_model` method. It reads a model from file and compiles it with the specified property set. Requires a valid model path, device name, and an initialized `ov::Core` object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nov::Core core;\nstd::string model_path = \"path_to_model.xml\";\nstd::string device_name = \"CPU\";\nauto model = core.read_model(model_path);\nauto compiled_model = core.compile_model(model, device_name, ov::hint::performance_mode(ov::hint::PerformanceMode::LATENCY));\n```\n\n----------------------------------------\n\nTITLE: Synchronous Inference in C++\nDESCRIPTION: This snippet demonstrates how to execute inference synchronously using the `infer` method of the `InferRequest` object. This method blocks until the inference is complete.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/README.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\ninferRequest.infer();\n```\n\n----------------------------------------\n\nTITLE: Copying Runtime Info During Transformations (C++)\nDESCRIPTION: This code snippet demonstrates how to copy runtime info from one or more source nodes to destination nodes during a transformation. Runtime info represents additional attributes of a node that need to be preserved when modifying the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov:copy_runtime_info]\nov::copy_runtime_info({op, const_node}, new_op);\n//! [ov:copy_runtime_info]\n```\n\n----------------------------------------\n\nTITLE: Set Latency Performance Hint (Python)\nDESCRIPTION: These commands demonstrate how to set the latency performance hint for the OpenVINO benchmark application. Setting the latency hint prioritizes low latency inference. The application automatically adjusts parameters such as processing streams and batch size to achieve the best latency on the target device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nbenchmark_app -m model.xml -hint latency\nbenchmark_app -m model.xml -hint throughput\n```\n\n----------------------------------------\n\nTITLE: Set Batch Size - Python\nDESCRIPTION: This Python snippet demonstrates how to use the `set_batch` method in OpenVINO to change the batch dimension of a model. It requires an OpenVINO model and sets the batch size to a specified value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/changing-input-shape.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\n\n# ! [set_batch]\nnew_batch_size = 32\nmodel.set_batch(new_batch_size)\n# ! [set_batch]\n```\n\n----------------------------------------\n\nTITLE: Basic Inference Flow in Python\nDESCRIPTION: This snippet illustrates the core OpenVINO API calls involved in performing inference: creating a `Core` object, compiling a model, and configuring input tensors using `compile_model` and `get_tensor`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/python/benchmark/sync_benchmark/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nopenvino.runtime.Core\n```\n\nLANGUAGE: Python\nCODE:\n```\nopenvino.runtime.Core.compile_model\n```\n\nLANGUAGE: Python\nCODE:\n```\nopenvino.runtime.InferRequest.get_tensor\n```\n\n----------------------------------------\n\nTITLE: Convert MobileNet Model with Static Input Shape via CLI\nDESCRIPTION: This command-line snippet shows how to convert a TensorFlow MobileNet model using the `ovc` tool and specify a static input shape of [2,300,300,3] for the input. The `--input` argument is used to define the shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/setting-input-shapes.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\novc MobileNet.pb --input [2,300,300,3]\n```\n\n----------------------------------------\n\nTITLE: Automatic Batching Python\nDESCRIPTION: This Python snippet demonstrates how to enable automatic batching by setting the `ov::hint::performance_mode` to `ov::hint::PerformanceMode::THROUGHPUT` with `ov::Core::compile_model()`. It utilizes the `compile_model_auto_batch` fragment from the specified Python file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n# [compile_model_auto_batch]\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model=model, device_name=\"GPU\", config={ov.hint.performance_mode: ov.hint.PerformanceMode.THROUGHPUT})\n# [compile_model_auto_batch]\n```\n\n----------------------------------------\n\nTITLE: Disable FP16 Compression when Saving OpenVINO Model (Python)\nDESCRIPTION: This code demonstrates how to disable FP16 compression when saving an OpenVINO model to the Intermediate Representation (IR) format using `openvino.save_model`. Disabling compression can be useful if the default FP16 compression affects model accuracy. The `compress_to_fp16` parameter is set to `False`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/conversion-parameters.rst#_snippet_2\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\nov_model = ov.convert_model(original_model)\nov.save_model(ov_model, 'model.xml', compress_to_fp16=False)\n```\n\n----------------------------------------\n\nTITLE: Get Element Type in OpenVINO\nDESCRIPTION: This snippet shows how to get the element type of an input in OpenVINO using both Python and C++. The element type represents the data type of the input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov_input->get_element_type();\n```\n\n----------------------------------------\n\nTITLE: Retrieve Specific Input/Output Port by Index (Python)\nDESCRIPTION: This Python snippet demonstrates retrieving a specific input or output port of an OpenVINO model using its index. This method is recommended when tensor names are not available or reliable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nov_model_input = model.input(index)\nov_model_output = model.output(index)\n```\n\n----------------------------------------\n\nTITLE: Get Inference Precision - Python\nDESCRIPTION: This Python snippet demonstrates how to retrieve the inference precision of a compiled model using `ov::CompiledModel::get_property` and `ov::hint::inference_precision`. It retrieves the compiled model's inference precision to check whether it is set to f16, bf16 or f32. Dependency: OpenVINO Python API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_4\n\nLANGUAGE: py\nCODE:\n```\n# [part1]\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model=model, device_name=\"CPU\")\n\ninference_precision = compiled_model.get_property(ov.hint.inference_precision)\nprint(f\"Inference precision: {inference_precision}\\n\")\n# [part1]\n```\n\n----------------------------------------\n\nTITLE: Example LowLatency2 Transformation in Python\nDESCRIPTION: This snippet illustrates how to apply the LowLatency2 transformation in Python, potentially revealing state names after the transformation.  It shows applying the transformation without specific configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nov.pass.LowLatency2().run_on_model(model)\nprint([variable.name for variable in model.get_variables()])\n```\n\n----------------------------------------\n\nTITLE: Controlling CPU Thread Pinning in Python OpenVINO\nDESCRIPTION: This snippet shows how to enable or disable CPU thread pinning using the `ov::hint::enable_cpu_pinning` property in Python. Disabling thread pinning can be useful in complex applications with parallel workloads where the overhead of pinning outweighs the benefits. The code configures OpenVINO to either pin inference threads to specific CPU cores or allow the operating system to schedule them more freely.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device/performance-hint-and-thread-scheduling.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# ! [ov:intel_cpu:multi_threading:part1]\nimport openvino.runtime as ov\n\ncore = ov.Core()\ncore.set_property(\"CPU\", ov.hint.enable_cpu_pinning(False))\n# ! [ov:intel_cpu:multi_threading:part1]\n```\n\n----------------------------------------\n\nTITLE: Access String Tensor Elements (C++)\nDESCRIPTION: Accesses and prints elements of a 1D string tensor using the `.data` template method. Requires the `openvino/openvino.hpp` and `iostream` headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/string-tensors.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n#include <iostream>\n\nstd::string* data = tensor.data<std::string>();\nfor(size_t i = 0; i < tensor.get_size(); ++i)\n   std::cout << data[i] << '\\n';\n```\n\n----------------------------------------\n\nTITLE: Dumping the execution graph in C++\nDESCRIPTION: This C++ snippet shows how to retrieve the execution graph from a compiled model using `get_runtime_model()` and serialize it to an XML file. This is a key debugging technique for analyzing the device-specific graph and identifying performance issues.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_debug_utils.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n    ov::CompiledModel compiled_model;\n    // Load some model into the plugin\n    std::shared_ptr<ov::Model> runtime_model = compiled_model.get_runtime_model();\n    ov::serialize(runtime_model, \"/path/to/serialized/exec/graph.xml\");\n```\n\n----------------------------------------\n\nTITLE: Install optimum-intel for Model Optimization\nDESCRIPTION: This snippet installs the `optimum-intel` package with the `openvino` extra. This package is required for converting and optimizing generative models for OpenVINO using the `optimum-cli` command.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/genai-model-preparation.rst#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npip install optimum-intel[openvino]\n```\n\n----------------------------------------\n\nTITLE: Allocate USM host memory (C)\nDESCRIPTION: This C snippet demonstrates how to allocate Unified Shared Memory (USM) on the host using the OpenVINO GPU plugin. It allocates memory accessible by both the host and the GPU. The snippet requires OpenCL and OpenVINO libraries. The allocated USM memory can then be used as an input or output tensor in an OpenVINO inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_25\n\nLANGUAGE: c\nCODE:\n```\n// example usage\n{\n    size_t buffer_size;\n    auto remote_blob = context.create_tensor(ov::element::f32, ov::Shape{buffer_size}, ov::intel_gpu::memory_type::usm_host);\n    float* mapped_ptr = remote_blob.data<float>();\n    // use mapped_ptr\n}\n```\n\n----------------------------------------\n\nTITLE: Quantize with Target Device\nDESCRIPTION: Quantizes the model, taking into account the specificity of the target device (e.g., CPU, GPU). The optimization is tailored to the target device for better performance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_25\n\nLANGUAGE: sh\nCODE:\n```\nnncf.quantize(model, dataset, target_device=nncf.TargetDevice.CPU)\n\n```\n\n----------------------------------------\n\nTITLE: Defining Preprocessing Steps (Python)\nDESCRIPTION: This snippet showcases how to define a sequence of preprocessing steps using the `ov::preprocess::PrePostProcessor` in Python. It includes conversion to FP32 precision, color conversion from BGR to RGB, resizing to match model dimensions, and normalization by subtracting mean values and dividing by scale factors. This illustrates the flexibility of the API in chaining various preprocessing operations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nppp.input().preprocess()\n    .convert_element_type(ov.Type.f32)\n    .convert_color(ov.ColorFormat.BGR)\n    .resize(resize_algorithm=ov.ResizeAlgorithm.RESIZE_LINEAR)\n    .mean([100.5, 101.5, 102.5])\n    .scale([50.0, 51.0, 52.0])\n# .convert_layout(ov.Layout('NHWC'))\n```\n\n----------------------------------------\n\nTITLE: Direct Inference with CompiledModel in OpenVINO (Python)\nDESCRIPTION: Demonstrates direct inference using the `CompiledModel` class in OpenVINO's Python API. The `__call__` method runs synchronous inference, reusing the `InferRequest` object for subsequent calls, which reduces overhead.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-advanced-inference.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nresults = compiled_model([input_tensor_1, input_tensor_2])\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: JAX/Flax Simple Dense Module Conversion to OpenVINO\nDESCRIPTION: This code snippet demonstrates the conversion of a simple Dense module written in JAX/Flax to an OpenVINO model. It initializes the module with random parameters, binds the parameters, and then converts the module to an OpenVINO model using `ov.convert_model`. A sample input is created to determine the input shape of the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nreturn nn.Dense(features=self.features)(x)\n```\n\nLANGUAGE: python\nCODE:\n```\nmodule = SimpleDenseModule(features=4)\n\n# create example_input used in training\nexample_input = jnp.ones((2, 3))\n\n# prepare parameters to initialize the module\n# they can be also loaded from a disk\n# using pickle, flax.serialization for deserialization\nkey = jax.random.PRNGKey(0)\nparams = module.init(key, example_input)\nmodule = module.bind(params)\n\nov_model = ov.convert_model(module, example_input=example_input)\ncompiled_model = ov.compile_model(ov_model, \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO specific configuration options\nDESCRIPTION: This code snippet demonstrates how to set OpenVINO specific configuration options, such as the performance hint, by adding them as a dictionary under the 'config' key in the options when compiling the model with the OpenVINO backend using torch.compile. This optimizes the model for latency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nopts = {\"device\" : \"CPU\", \"config\" : {\"PERFORMANCE_HINT\" : \"LATENCY\"}}\nmodel = torch.compile(model, backend=\"openvino\", options=opts)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenVINO Core in Python\nDESCRIPTION: This code imports the necessary OpenVINO modules and creates an `Core` object. The `Core` object is the entry point for using OpenVINO Runtime and is required to compile models and perform inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.runtime import Core\n\ncore = Core()\n```\n\n----------------------------------------\n\nTITLE: Reshape multiple inputs with dynamic dimension - C++\nDESCRIPTION: This C++ snippet shows how to reshape multiple input layers of a model, setting the second dimension of each input to be dynamic. The code iterates through the inputs, updates the shape, and applies the reshape method.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov_dynamic_shapes:reshape_multiple_inputs]\nfor (const auto& input : model->inputs() ) {\n    auto input_shape = input.get_shape();\n    input_shape[1] = ov::Dimension(-1);\n    model->reshape({{input.get_any_name(), input_shape}});\n}\n//! [ov_dynamic_shapes:reshape_multiple_inputs]\n```\n\n----------------------------------------\n\nTITLE: Compiling Model on NPU with ov::Core in Python\nDESCRIPTION: This snippet demonstrates how to compile a model for inference on the NPU device using the `ov::Core::compile_model()` method in Python.  It shows how to specify the device name ('NPU') to target the NPU during model compilation. The compiled model can then be used for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/npu-device.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\ncore = ov.Core()\nmodel = core.read_model(\"path_to_model\")\n# or\nmodel = core.read_model(model_stream)\ncompiled_model = core.compile_model(model=model, device_name=\"NPU\")\n```\n\n----------------------------------------\n\nTITLE: Setting Input Tensor with Dynamic Shapes - C\nDESCRIPTION: This C snippet demonstrates how to set input tensors with varying shapes for a model configured with dynamic shapes, and then execute inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_12\n\nLANGUAGE: C\nCODE:\n```\n#include <openvino/c/openvino.h>\n#include <stdio.h>\n#include <stdlib.h>\n\n#define OV_CHECK(status, message)                                  \\\n    if (status != 0) {                                            \\\n        printf(\"%s\\n\", message);                               \\\n        return 1;                                                  \\\n    }\n\nint main() {\n    // ! [ov_dynamic_shapes:set_input_tensor]\n    ov_core_t* core = NULL;\n    OV_CHECK(ov_core_create(&core), \"Failed to create Core\");\n\n    ov_model_t* model = NULL;\n    OV_CHECK(ov_core_read_model(core, \"model.xml\", &model), \"Failed to read model\");\n\n    ov_partial_shape_t input_shape = {-1, 1, -1};\n    ov_reshape(model, &input_shape); // Assuming ov_reshape is implemented for C API\n\n    ov_compiled_model_t* compiled_model = NULL;\n    OV_CHECK(ov_core_compile_model(core, model, \"CPU\", &compiled_model), \"Failed to compile model\");\n\n    ov_infer_request_t* infer_request = NULL;\n    OV_CHECK(ov_compiled_model_create_infer_request(compiled_model, &infer_request), \"Failed to create infer request\");\n\n    // Create dummy data\n    float input_data_1[1 * 128] = {0.0f}; // Example data\n    ov_tensor_t* input_tensor_1 = NULL;\n    ov_shape_t shape_1 = {2, {1, 128}};\n    OV_CHECK(ov_tensor_create(OV_ELEMENT_TYPE_F32, &shape_1, input_data_1, &input_tensor_1), \"Failed to create input tensor 1\");\n\n    OV_CHECK(ov_infer_request_infer(infer_request, &input_tensor_1, NULL), \"Failed to run inference\");\n\n    float input_data_2[1 * 200] = {0.0f}; // Example data\n    ov_tensor_t* input_tensor_2 = NULL;\n    ov_shape_t shape_2 = {2, {1, 200}};\n    OV_CHECK(ov_tensor_create(OV_ELEMENT_TYPE_F32, &shape_2, input_data_2, &input_tensor_2), \"Failed to create input tensor 2\");\n\n    OV_CHECK(ov_infer_request_infer(infer_request, &input_tensor_2, NULL), \"Failed to run inference\");\n\n    ov_tensor_free(input_tensor_1);\n    ov_tensor_free(input_tensor_2);\n    ov_infer_request_free(infer_request);\n    ov_compiled_model_free(compiled_model);\n    ov_model_free(model);\n    ov_core_free(core);\n\n    // ! [ov_dynamic_shapes:set_input_tensor]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Accessing Input by Index/Name\nDESCRIPTION: Demonstrates how to access a model's input using the `ov::preprocess::PrePostProcessor` class when the model has only one input, or when there are multiple inputs, by name or index. This is useful for configuring preprocessing steps for specific inputs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n# 1) Set input tensor information. We apply preprocessing\n#    demanding input tensor to have 'u8' type and 'NCHW' layout.\nppp.input().tensor().set_element_type(ov.Type.u8).set_layout(ov.Layout('NCHW'))\n# 2) Adding explicit preprocessing steps\nppp.input().preprocess().convert_element_type(ov.Type.f32).convert_layout(ov.Layout('NHWC'))\n# 3) Set input model information.\nppp.input().model().set_layout(ov.Layout('NHWC'))\n# 4) Apply preprocessing modifying the original model\nmodel = ppp.build()\n```\n\n----------------------------------------\n\nTITLE: Compiling Model with Read Model and Caching\nDESCRIPTION: This snippet compiles a model using `read_model` and enables caching.  It initializes an OpenVINO Core object, reads a model from the specified path, then compiles the model with the specified device and configuration, including the cache directory, in Python and C++.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n/// [ov:caching:part2]\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n\n    std::string model_path = \"path_to_model.xml\";\n    std::string cache_dir = \"/path/to/cache/dir\";\n\n    std::shared_ptr<ov::Model> model = core.read_model(model_path);\n    ov::CompiledModel compiled_model = core.compile_model(model, \"CPU\", {\n        {ov::cache_dir(), cache_dir}\n    });\n\n    return 0;\n}\n/// [ov:caching:part2]\n```\n\n----------------------------------------\n\nTITLE: Load and Compress to INT8 programmatically\nDESCRIPTION: Loads a model with INT8 weight compression applied, using the load_in_8bit parameter.  Demonstrates loading the model directly from Hugging Face or from a pre-converted OpenVINO IR format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel = OVModelForCausalLM.from_pretrained(model_id, export=True, load_in_8bit=True)\n\n# or if the model has been already converted\nmodel = OVModelForCausalLM.from_pretrained(model_path, load_in_8bit=True)\n\n# save the model after optimization\nmodel.save_pretrained(optimized_model_path)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Automatic Batching with benchmark_app using performance hints (sh)\nDESCRIPTION: This code snippet demonstrates how to use the benchmark_app tool with performance hints (tput) to evaluate the performance of automatic batching with a specified model and device (GPU).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app -hint tput -d GPU -m 'path to your favorite model'\n```\n\n----------------------------------------\n\nTITLE: Get Available Devices with OpenVINO Core (C++)\nDESCRIPTION: This snippet demonstrates how to retrieve a list of available devices using the `ov::Core::get_available_devices` method in C++. It creates an `ov::Core` object, queries the available devices, and prints them to the console. No additional dependencies beyond the OpenVINO runtime headers are needed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nov::Core core;\nauto available_devices = core.get_available_devices();\nstd::cout << \"Available devices: \";\nfor (const auto& device : available_devices) {\n    std::cout << device << \" \";\n}\nstd::cout << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Exporting Model Blob with OpenVINO GenAI (Python)\nDESCRIPTION: This snippet demonstrates how to configure the OpenVINO GenAI pipeline to export the compiled model as a blob. It sets the `EXPORT_BLOB` option to \"YES\" and specifies the path to save the blob using the `BLOB_PATH` option. This allows you to save the compiled model for later use.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\npipeline_config = { \"EXPORT_BLOB\": \"YES\", \"BLOB_PATH\": \".npucache\\\\compiled_model.blob\" }\npipe = ov_genai.LLMPipeline(model_path, \"NPU\", pipeline_config)\n```\n\n----------------------------------------\n\nTITLE: Wrap USM pointer (C)\nDESCRIPTION: This C snippet shows how to wrap a USM pointer into an OpenVINO RemoteTensor using the GPU plugin.  This enables using existing USM memory allocations within an OpenVINO inference pipeline.  Requires OpenCL and OpenVINO libraries.  The wrapped USM pointer becomes a RemoteTensor for processing by the OpenVINO engine.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_20\n\nLANGUAGE: c\nCODE:\n```\n// example usage\n{\n    void* usm_ptr;\n    size_t buffer_size;\n    // fill usm_ptr with data\n\n    auto remote_blob = context.create_tensor(buffer_size, usm_ptr);\n}\n```\n\n----------------------------------------\n\nTITLE: Apply LowLatency2 Transformation with Parameters in C++\nDESCRIPTION: This snippet demonstrates how to apply the LowLatency2 transformation with parameters in C++, specifically disabling the constant initializer. It utilizes `ov::pass::LowLatency2(false)` to avoid inserting a constant subgraph.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nmodel->transform(std::make_shared<ov::pass::LowLatency2>(false));\n```\n\n----------------------------------------\n\nTITLE: Convert Model using OpenVINO - Python\nDESCRIPTION: This code snippet demonstrates how to convert a model to OpenVINO's intermediate representation (IR) format using the `openvino.convert_model` function in Python. It takes a path to the original model as input and returns an OpenVINO model object. The example covers both passing the path to the model and passing a python model object directly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-classification.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino as ov\n\nov_model = ov.convert_model('./models/alexnet')\n# or, when model is a Python model object\nov_model = ov.convert_model(alexnet)\n```\n\n----------------------------------------\n\nTITLE: Convert SavedModel Format (TF2) using CLI\nDESCRIPTION: This snippet shows how to convert a TensorFlow 2 SavedModel format to OpenVINO IR using the `ovc` command-line tool. The input is the path to the SavedModel directory. The output is an OpenVINO IR model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\novc path_to_saved_model_dir\n```\n\n----------------------------------------\n\nTITLE: Converting paddle.fluid.executor.Executor to OpenVINO IR\nDESCRIPTION: This code converts a PaddlePaddle model of type `paddle.fluid.executor.Executor` to OpenVINO IR format. It requires defining both `example_input` and `output` parameters, which must be lists containing `paddle.static.data` variables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-paddle.rst#_snippet_6\n\nLANGUAGE: py\nCODE:\n```\nimport paddle\nimport openvino as ov\n\npaddle.enable_static()\n\n# create a paddle.fluid.executor.Executor format model\nx = paddle.static.data(name=\"x\", shape=[1,3,224])\ny = paddle.static.data(name=\"y\", shape=[1,3,224])\nrelu = paddle.nn.ReLU()\nsigmoid = paddle.nn.Sigmoid()\ny = sigmoid(relu(x))\n\nexe = paddle.static.Executor(paddle.CPUPlace())\nexe.run(paddle.static.default_startup_program())\n\n# convert to OpenVINO IR format\nov_model = ov.convert_model(exe, example_input=[x], output=[y])\n```\n\n----------------------------------------\n\nTITLE: Get/Set Tensor by Port in C++\nDESCRIPTION: This C++ code gets and sets tensors by port using `get_tensor(port)` and `set_tensor(port, tensor)`. Requires OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\nov::InferRequest infer_request = compiled_model.create_infer_request();\n\nov::Output<ov::Node> input_port = model->input(0);\nov::Tensor input_tensor = infer_request.get_tensor(input_port);\nstd::vector<float> input_data(input_tensor.get_size());\nstd::generate(input_data.begin(), input_data.end(), []() { return static_cast<float>(rand()) / RAND_MAX; });\nstd::memcpy(input_tensor.data(), input_data.data(), input_data.size() * sizeof(float));\n\ninfer_request.infer();\n\nov::Output<ov::Node> output_port = model->output(0);\nov::Tensor output_tensor = infer_request.get_tensor(output_port);\nconst float* output_data = output_tensor.data<const float>();\n```\n\n----------------------------------------\n\nTITLE: Combining Hint with Low-Level Setting in Python\nDESCRIPTION: This Python snippet illustrates how to combine a high-level performance hint (`THROUGHPUT`) with a device-specific low-level setting (`ov.streams.num`). This allows for fine-grained control over performance, overriding the hint's default behavior if needed. It assumes the `core` object and `model` object are already defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/high-level-performance-hints.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncompiled_model = core.compile_model(model=model, device_name=\"CPU\", config={\n    ov.hint.performance_mode: ov.hint.PerformanceMode.THROUGHPUT,\n    \"CPU_THROUGHPUT_STREAMS\": \"1\"\n})\n```\n\n----------------------------------------\n\nTITLE: Automatic Batching C++\nDESCRIPTION: This C++ snippet demonstrates how to enable automatic batching by setting the `ov::hint::performance_mode` to `ov::hint::PerformanceMode::THROUGHPUT` with `ov::Core::compile_model()`. It utilizes the `compile_model_auto_batch` fragment from the specified C++ file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\n// [compile_model_auto_batch]\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"GPU\", {{ov::hint::performance_mode.name(), ov::hint::PerformanceMode::THROUGHPUT}});\n// [compile_model_auto_batch]\n```\n\n----------------------------------------\n\nTITLE: Model Input/Output C API\nDESCRIPTION: This section shows how to get inputs and outputs of a model via `ov_model_const_input` and `ov_model_const_output`. These are used to identify and interact with model interfaces.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/c/hello_classification/README.md#_snippet_3\n\nLANGUAGE: C\nCODE:\n```\n``ov_model_const_input``,\n``ov_model_const_output``\n```\n\n----------------------------------------\n\nTITLE: Convert Frozen Model Format (TF1) using Python\nDESCRIPTION: This snippet shows how to convert a TensorFlow 1 Frozen Model (``.pb``) to OpenVINO IR using `openvino.convert_model` in Python. The input is the path to the ``.pb`` file. The output is the OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_4\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model('your_model_file.pb')\n```\n\n----------------------------------------\n\nTITLE: Convert Model using OpenVINO CLI\nDESCRIPTION: This command demonstrates how to convert a model to the OpenVINO Intermediate Representation (IR) format using the `ovc` command-line tool.  It requires the OpenVINO command line tools to be installed and accessible in the system's PATH. The command takes the path to the model as input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/conversion-parameters.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\novc path_to_your_model\n```\n\n----------------------------------------\n\nTITLE: Load Checkpoint - PyTorch\nDESCRIPTION: This code snippet demonstrates how to restore the model from a checkpoint in PyTorch using NNCF's API. This is useful for resuming training from a saved state.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncompression_ctrl.load_state(\"checkpoint.pth\")\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Inference in OpenVINO with Python\nDESCRIPTION: This Python code demonstrates asynchronous inference using `ov::InferRequest::start_async` and `ov::InferRequest::wait`. It starts inference asynchronously, then waits for its completion. Requires OpenVINO and a compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\ncompiled_model = ie.compile_model(model, \"CPU\")\ninfer_request = compiled_model.create_infer_request()\n\ninput_tensor = infer_request.get_input_tensor()\ninput_tensor.data[:] = np.random.normal(0, 1, input_tensor.shape)\n\ninfer_request.start_async()\ninfer_request.wait()\n```\n\n----------------------------------------\n\nTITLE: Converting TensorFlow Lite Model to OpenVINO IR in Python\nDESCRIPTION: This code snippet shows how to convert a TensorFlow Lite model to OpenVINO IR using the `convert_model()` method in Python. It imports the openvino library, converts a TensorFlow Lite model named '<INPUT_MODEL>.tflite' to an OpenVINO model, and then compiles the OpenVINO model for execution on the 'AUTO' device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_6\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\nov_model = ov.convert_model(\"<INPUT_MODEL>.tflite\")\ncompiled_model = ov.compile_model(ov_model, \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Synchronous Inference in OpenVINO with Python\nDESCRIPTION: This Python code demonstrates synchronous inference using `ov::InferRequest::infer`. It populates an input tensor with random data, performs inference, and retrieves the output tensor.  Requires OpenVINO and a compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\ncompiled_model = ie.compile_model(model, \"CPU\")\ninfer_request = compiled_model.create_infer_request()\n\ninput_tensor = infer_request.get_input_tensor()\ninput_tensor.data[:] = np.random.normal(0, 1, input_tensor.shape)\n\ninfer_request.infer()\n\noutput_tensor = infer_request.get_output_tensor()\nprint(output_tensor.data)\n```\n\n----------------------------------------\n\nTITLE: Allocate USM device memory (C++)\nDESCRIPTION: This C++ snippet demonstrates how to allocate Unified Shared Memory (USM) on the device using the OpenVINO GPU plugin.  This allocates memory that resides on the GPU and is directly accessible by the GPU kernels. Requires OpenCL and OpenVINO libraries.  This memory can be used for efficient GPU-based computations within the OpenVINO framework.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\n// example usage\n{\n    size_t buffer_size;\n    auto remote_blob = context.create_tensor(ov::element::f32, ov::Shape{buffer_size}, ov::intel_gpu::memory_type::usm_device);\n}\n```\n\n----------------------------------------\n\nTITLE: Bert Benchmark Script\nDESCRIPTION: This snippet represents the main script for benchmarking a BERT model using OpenVINO. It encompasses model loading, reshaping, compilation, dataset preparation, and asynchronous inference benchmarking to measure performance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/bert-benchmark.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsamples/python/benchmark/bert_benchmark/bert_benchmark.py\n```\n\n----------------------------------------\n\nTITLE: Setting Cache Directory\nDESCRIPTION: This snippet demonstrates how to set the cache directory in OpenVINO using Python or C++.  It specifies the device name and the path to the cache directory using configuration options. The OpenVINO Core object is initialized and used to compile a model with these configuration options.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n### [ov:caching:part0]\nimport openvino.runtime as ov\n\ncore = ov.Core()\n\nmodel_path = \"path_to_model.xml\"\ncache_dir = \"/path/to/cache/dir\"\n\ncompiled_model = core.compile_model(model_path, \"CPU\", config={\n    \"CACHE_DIR\": cache_dir\n})\n### [ov:caching:part0]\n```\n\n----------------------------------------\n\nTITLE: Quantize with Accurate Bias Correction\nDESCRIPTION: Quantizes the model while disabling fast bias correction.  This enables a more accurate bias correction algorithm, which can improve the accuracy of the quantized model, especially for OpenVINO and ONNX models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\nnncf.quantize(model, dataset, fast_bias_correction=False)\n\n```\n\n----------------------------------------\n\nTITLE: Check BFloat16 Support - Python\nDESCRIPTION: This Python code snippet demonstrates how to query device capabilities to check for BF16 support on the CPU using `ov::device::capabilities`. It retrieves the device properties and checks if 'BF16' is present in the capabilities list, indicating that the CPU supports BFloat16. Dependency: OpenVINO Python API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_2\n\nLANGUAGE: py\nCODE:\n```\n# [part0]\ncpu_device_name = \"CPU\"\ncore = ov.Core()\nif cpu_device_name in core.available_devices:\n    device_capabilities = core.get_property(cpu_device_name, \"device::capabilities\")\n    if \"BF16\" in device_capabilities:\n        print(\"CPU device supports BF16\\n\")\n# [part0]\n```\n\n----------------------------------------\n\nTITLE: Load and Compress Model to INT8 with Optimum-Intel API\nDESCRIPTION: Loads a pre-trained Hugging Face model, compresses it to INT8_ASYM using the Optimum Intel API with OVWeightQuantizationConfig, and then performs inference with a text phrase. The `OVModelForCausalLM` class is used for loading and quantizing the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel.openvino import OVModelForCausalLM, OVWeightQuantizationConfig\nfrom transformers import AutoTokenizer, pipeline\n\n# Load and compress a model from Hugging Face.\nmodel_id = \"microsoft/Phi-3.5-mini-instruct\"\nmodel = OVModelForCausalLM.from_pretrained(\n    model_id,\n    export=True,\n    quantization_config=OVWeightQuantizationConfig(bits=8)\n)\n\n# Inference\ntokenizer = AutoTokenizer.from_pretrained(model_id)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nphrase = \"The weather is\"\nresults = pipe(phrase)\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Enabling Weightless Caching\nDESCRIPTION: This snippet enables weightless caching in OpenVINO using Python or C++.  It initializes an OpenVINO Core object, reads a model, and compiles it, setting the `CacheMode` property to `OPTIMIZE_SIZE` to enable weightless caching, reducing the size of the cache file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n/// [ov:caching:part4]\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n\n    std::string model_path = \"path_to_model.xml\";\n    std::string cache_dir = \"/path/to/cache/dir\";\n\n    std::shared_ptr<ov::Model> model = core.read_model(model_path);\n    ov::CompiledModel compiled_model = core.compile_model(model, \"GPU\", {\n        {ov::cache_dir(), cache_dir},\n        {ov::cache_mode.name(), ov::cache_mode(ov::CacheMode::OPTIMIZE_SIZE)}\n    });\n\n    return 0;\n}\n/// [ov:caching:part4]\n```\n\n----------------------------------------\n\nTITLE: Export OV Model with INT8 Weight Format using CLI\nDESCRIPTION: Exports an OpenVINO model from a Hugging Face model using the `optimum-cli` command, compressing the weights to INT8 format. This command line tool streamlines the process of converting and quantizing models for OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\noptimum-cli export openvino --model microsoft/Phi-3.5-mini-instruct --weight-format int8 ov_phi-3.5-mini-instruct\n```\n\n----------------------------------------\n\nTITLE: Export VLM to OpenVINO with Low Precision (int4)\nDESCRIPTION: This example demonstrates exporting a VLM (openbmb/MiniCPM-V-2_6) to OpenVINO IR format with int4 quantization using `optimum-cli`. It specifies the model path, task as `text-generation-with-past`, and the weight format as `int4`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/genai-model-preparation.rst#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\noptimum-cli export openvino -m model_path --task text-generation-with-past --weight-format int4 ov_MiniCPM-V-2_6\n```\n\n----------------------------------------\n\nTITLE: Convert PaddlePaddle model to IR using ovc\nDESCRIPTION: This snippet shows how to convert a PaddlePaddle model to OpenVINO Intermediate Representation (IR) format using the `ovc` command-line tool. The resulting IR can then be read by `read_model()` and inferred.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\novc <INPUT_MODEL>.pdmodel\n```\n\n----------------------------------------\n\nTITLE: Query Device Information Python\nDESCRIPTION: This Python snippet queries the OpenVINO Runtime for available devices and prints their supported metrics and plugin configuration parameters. It demonstrates the usage of the Query Device API to retrieve device-specific information.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-query-device.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.runtime import Core\n\nif __name__ == '__main__':\n    print('Hello Query Device Sample')\n\n    core = Core()\n\n    available_devices = core.available_devices\n\n    print(f'[ INFO ] Available devices: {available_devices}')\n\n    for device_name in available_devices:\n        print(f'[ INFO ] {device_name} :')\n        print(f'[ INFO ] \\tSUPPORTED_PROPERTIES:')\n        for property_name in core.get_property(device_name, 'SUPPORTED_PROPERTIES'):\n            try:\n                property_value = core.get_property(device_name, property_name)\n            except Exception as e:\n                property_value = 'Cannot get value' + str(e)\n            print(f'[ INFO ] \\t\\t{property_name}: {property_value}')\n\n```\n\n----------------------------------------\n\nTITLE: Get Model Inputs in Python\nDESCRIPTION: This snippet uses `CompiledModel.inputs` to retrieve the input layers of a compiled model. It's a crucial step in setting up the inference pipeline, as the inputs need to be populated with data before inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/python/benchmark/sync_benchmark/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nopenvino.runtime.CompiledModel.inputs\n```\n\n----------------------------------------\n\nTITLE: Custom Pre-processing Step in OpenVINO (Python)\nDESCRIPTION: This snippet demonstrates how to insert a custom pre-processing step into the execution graph using Python. It takes the input node, applies user-defined operations, and returns a new node. Custom pre-processing functions should insert nodes after the input during model compilation, and will not be called during the execution phase.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nie = Core()\nmodel = ie.read_model(model=MODEL_PATH)\n\nin = model.input()\n\n# Define a custom preprocessing operation: insert a node after the input\ndef scale_node(node: ov.Node) -> ov.Node:\n    # node: ov.Node - OpenVINO node to insert after\n    # Constant 2.0\n    const = ov.Constant(np.array(2.0, dtype=np.float32), ov.Type.f32)\n    # node * 2.0\n    mul = ov.opset8.Multiply(node, const)\n    # Result\n    return mul.output(0).get_node()\n\n\n# 1. Create Preprocessor object\npp = Preprocessor(model)\n# 2. Get a Tensor object, that describes input of the model\ninput_tensor = pp.input(in_tensor_name)\n# 3. Set custom preprocessing operation.\ninput_tensor.preprocess().custom(scale_node)\n# 4. Apply preprocessing modification to the model\nmodel = pp.build()\n\ncompiled_model = ie.compile_model(model, device_name=DEVICE_NAME)\n```\n\n----------------------------------------\n\nTITLE: Setting Cache Directory\nDESCRIPTION: This snippet demonstrates how to set the cache directory in OpenVINO using Python or C++.  It specifies the device name and the path to the cache directory using configuration options. The OpenVINO Core object is initialized and used to compile a model with these configuration options.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n/// [ov:caching:part0]\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n\n    std::string model_path = \"path_to_model.xml\";\n    std::string cache_dir = \"/path/to/cache/dir\";\n\n    ov::CompiledModel compiled_model = core.compile_model(model_path, \"CPU\", {\n        {ov::cache_dir(), cache_dir}\n    });\n    return 0;\n}\n/// [ov:caching:part0]\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (PaddlePaddle) with OpenVINO in Python\nDESCRIPTION: This code compiles a model in PaddlePaddle format using the `core.compile_model()` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel_paddle = core.compile_model(\"path_to_model.pdmodel\", \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (ONNX) with OpenVINO in Python\nDESCRIPTION: This code compiles a model in ONNX format using the `core.compile_model()` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel_onnx = core.compile_model(\"path_to_model.onnx\", \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Generate Text using OpenVINO\nDESCRIPTION: Generates text by iteratively running inference on an OpenVINO model. The loop resets the model's state before each inference and continues for a specified number of iterations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ntokens_result = np.array([[]], dtype=np.int64)\n\n# reset KV cache inside the model before inference\ninfer_request.reset_state()\nmax_infer = 10\n\nfor _ in range(max_infer):\n   infer_request.start_async(model_input)\n```\n\n----------------------------------------\n\nTITLE: Benchmark App Command with AUTO - Shell\nDESCRIPTION: This command line instruction shows how to run the OpenVINO benchmark_app tool with the AUTO device. This allows OpenVINO to automatically select the best available device for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app -m ../public/alexnet/FP32/alexnet.xml -d AUTO -niter 128\n```\n\n----------------------------------------\n\nTITLE: Converting Model with OpenVINO Model Converter (Windows)\nDESCRIPTION: This command uses the `ovc` tool to convert a model to the Intermediate Representation (IR) format and compress it to FP16 precision.  `<path-to-your-model>` should be replaced with the path to the model file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\novc <path-to-your-model> --compress_to_fp16=True\n```\n\n----------------------------------------\n\nTITLE: Convert TensorFlow Lite Model using CLI\nDESCRIPTION: This snippet shows how to convert a TensorFlow Lite model using the OpenVINO Model Converter (OVC) command-line tool. The tool takes the path to the `.tflite` model file as input and converts it to the OpenVINO IR format. The OVC tool must be installed and available in the system's PATH.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow-lite.rst#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\novc your_model_file.tflite\n```\n\n----------------------------------------\n\nTITLE: Checking Tensor Contiguity in TypeScript\nDESCRIPTION: Demonstrates the `isContinuous` method for determining whether the tensor's data is stored in a contiguous memory block. This information can be useful for optimizing certain operations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Tensor.rst#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nisContinuous(): boolean;\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Inference in OpenVINO with C++\nDESCRIPTION: This C++ code performs asynchronous inference using `ov::InferRequest::start_async` and `ov::InferRequest::wait`. It initializes input data, starts the inference, and waits for completion. Requires OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\nov::InferRequest infer_request = compiled_model.create_infer_request();\n\nov::Tensor input_tensor = infer_request.get_input_tensor();\nstd::vector<float> input_data(input_tensor.get_size());\nstd::generate(input_data.begin(), input_data.end(), []() { return static_cast<float>(rand()) / RAND_MAX; });\nstd::memcpy(input_tensor.data(), input_data.data(), input_data.size() * sizeof(float));\n\ninfer_request.start_async();\ninfer_request.wait();\n```\n\n----------------------------------------\n\nTITLE: Printing Dynamic Output Shapes - C++\nDESCRIPTION: This C++ snippet demonstrates how to retrieve and print the partial shape of the output layer in an OpenVINO model, which can be used to identify dynamic output shapes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n#include <iostream>\n\nint main() {\n    // ! [ov_dynamic_shapes:print_dynamic]\n    ov::Core core;\n    auto model = core.read_model(\"model.xml\");\n    ov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\n\n    ov::PartialShape output_shape = compiled_model.output(0).get_partial_shape();\n    std::cout << output_shape << std::endl;\n    // ! [ov_dynamic_shapes:print_dynamic]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Convert OCR Model with Dynamic Input Shapes via CLI\nDESCRIPTION: This command-line snippet converts an ONNX OCR model using the `ovc` tool and specifies dynamic batch dimensions for inputs `data` and `seq_len`, using `?` to indicate dynamic dimension. The `--input` argument is used to define the shapes as comma-separated strings with question marks as dynamic dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/setting-input-shapes.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\novc ocr.onnx --input \"data[?,150,200,1],seq_len[?]\"\n```\n\n----------------------------------------\n\nTITLE: Convert PaddlePaddle model using convert_model in Python\nDESCRIPTION: This snippet demonstrates how to convert a PaddlePaddle model to an OpenVINO model using the `convert_model()` method in Python. The converted model is then compiled using `compile_model()` for execution on the specified device (AUTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_18\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\nov_model = ov.convert_model(\"<INPUT_MODEL>.pdmodel\")\ncompiled_model = ov.compile_model(ov_model, \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Adding nlohmann_json Dependency\nDESCRIPTION: Adds the nlohmann_json library as a dependency. It checks for its existence in predefined locations (thirdparty directory). If not found, it raises a fatal error, otherwise it links the library to the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/benchmark_app/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT TARGET nlohmann_json::nlohmann_json)\n    if(EXISTS \"${Samples_SOURCE_DIR}/thirdparty/nlohmann_json\")\n        # OpenVINO package puts thirdparty to samples dir\n        # suppress shadowing names warning\n        set(JSON_SystemInclude ON CACHE BOOL \"\" FORCE)\n        add_subdirectory(\"${Samples_SOURCE_DIR}/thirdparty/nlohmann_json\"\n                            \"${Samples_BINARY_DIR}/thirdparty/nlohmann_json\" EXCLUDE_FROM_ALL)\n    elseif(EXISTS \"${Samples_SOURCE_DIR}/../../thirdparty/json/nlohmann_json\")\n        # Allow running samples CMakeLists.txt as stand alone from openvino sources\n        # suppress shadowing names warning\n        set(JSON_SystemInclude ON CACHE BOOL \"\" FORCE)\n        add_subdirectory(\"${Samples_SOURCE_DIR}/../../thirdparty/json/nlohmann_json\"\n                            \"${Samples_BINARY_DIR}/thirdparty/nlohmann_json\" EXCLUDE_FROM_ALL)\n    else()\n        message(FATAL_ERROR \"Failed to find / build nlohmann_json library\")\n    endif()\nendif()\n\ntarget_link_libraries(${TARGET_NAME} PRIVATE nlohmann_json::nlohmann_json)\n```\n\n----------------------------------------\n\nTITLE: Using torch.compile with OpenVINO (Conda/Older Versions) (Python)\nDESCRIPTION: This code snippet demonstrates how to use `torch.compile` with the `openvino` backend for OpenVINO installations via conda or for versions older than 2024.1. It requires an additional import statement for `openvino.torch` before compiling the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.torch\n\n   ...\n   model = torch.compile(model, backend='openvino')\n   ...\n```\n\n----------------------------------------\n\nTITLE: Initializing Preprocessing with GPU Plugin (Python)\nDESCRIPTION: This Python code snippet shows how to initialize preprocessing with the OpenVINO GPU plugin. It demonstrates setting the memory type for input tensors using `ov::preprocess::InputTensorInfo::set_memory_type()`, specifically for scenarios like NV12 image processing, providing a hint to the plugin for optimal kernel generation. The input tensor uses `ov::intel_gpu::memory_type::surface` or `ov::intel_gpu::memory_type::buffer`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n# The code snippet is not available in the provided text.\n```\n\n----------------------------------------\n\nTITLE: Example LowLatency2 Transformation in C++\nDESCRIPTION: This snippet demonstrates how to apply the LowLatency2 transformation in C++, allowing you to print the names of the states (variables) after transformation. It uses `model->get_variables()` to obtain the variables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\nmodel->transform(std::make_shared<ov::pass::LowLatency2>());\nfor (auto& variable : model->get_variables()) {\n  std::cout << variable.get_name() << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Reading Model C\nDESCRIPTION: This code snippet reads a model from IR/ONNX/PDPD/TF/TFLite formats and creates an `ov_model_t` object. It requires the core instance, model path, and bin path.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_19\n\nLANGUAGE: C\nCODE:\n```\nov_core_t* core = NULL;\nov_core_create(&core);\nov_model_t* model = NULL;\nov_core_read_model(core, \"model.xml\", \"model.bin\", &model);\n```\n\n----------------------------------------\n\nTITLE: Apply LowLatency2 Transformation in C++\nDESCRIPTION: This snippet shows how to apply the LowLatency2 transformation in C++. It makes use of `ov::pass::LowLatency2()` to automatically detect and replace Parameter and Result operation pairs with Assign and ReadValue operations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\nmodel->transform(std::make_shared<ov::pass::LowLatency2>());\n```\n\n----------------------------------------\n\nTITLE: Enabling Auto Batching with compile_model in C++\nDESCRIPTION: This C++ snippet demonstrates enabling Automatic Batching by setting the `ov::hint::performance_mode` property to `ov::hint::PerformanceMode::THROUGHPUT` during model compilation. It compiles a model for the GPU device, enabling automatic batching through the performance hint.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"GPU\", {{ov::hint::performance_mode(ov::hint::PerformanceMode::THROUGHPUT)}});\n```\n\n----------------------------------------\n\nTITLE: Run Benchmark on GPU (Python)\nDESCRIPTION: This command runs the OpenVINO benchmark application on the GPU device. The application will load the specified model and perform inference using the GPU. Note that the system must have the appropriate drivers installed for the GPU to be used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nbenchmark_app -m model.xml -d GPU\n```\n\n----------------------------------------\n\nTITLE: Compile Model on CPU - C++\nDESCRIPTION: This snippet shows how to compile a model for CPU inference using the OpenVINO C++ API.  The `ov::Core::compile_model()` method is used with the device name \"CPU\".  This ensures that the model is compiled and executed on the CPU device. Required dependency: OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// [compile_model_default]\ncore = ov::Core();\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nauto compiled_model = core.compile_model(model, \"CPU\");\n// [compile_model_default]\n```\n\n----------------------------------------\n\nTITLE: Wrap Model with NNCF - TensorFlow 2\nDESCRIPTION: This code snippet demonstrates how to wrap the original model with the NNCF object using the `create_compressed_model()` API in TensorFlow 2. This API will transform the model and add required operations for compression. The model can then be used normally.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncompression_ctrl, model = create_compressed_model(model, nncf_config)\n```\n\n----------------------------------------\n\nTITLE: Wrap NV12 surface with RemoteTensor (C++)\nDESCRIPTION: This C++ snippet shows how to wrap an NV12 surface into an OpenVINO RemoteTensor using the GPU plugin. This is useful for processing video data directly from a hardware decoder output. The NV12 surface is assumed to be already populated with data before being wrapped. No specific dependencies are listed beyond the OpenVINO and OpenCL libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\n// example usage\n{\n    size_t width;\n    size_t height;\n\n    cl_mem y_plane;\n    cl_mem uv_plane;\n    // fill surfaces with data\n\n    auto remote_blob = context.create_tensor_nv12(height, width, y_plane, uv_plane);\n}\n```\n\n----------------------------------------\n\nTITLE: Shared Memory Inference in OpenVINO (Python)\nDESCRIPTION: Illustrates how to enable shared memory for inputs and outputs using the `share_inputs` and `share_outputs` flags. This creates shared `Tensor` instances with a \"zero-copy\" approach, reducing the overhead of setting inputs and accessing outputs, especially for large data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-advanced-inference.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninfer_request = compiled_model.create_infer_request()\ninfer_request.infer({\"input_tensor_name_1\": input_tensor_1, \"input_tensor_name_2\": input_tensor_2}, share_inputs=True, share_outputs=True)\nresults = infer_request.get_tensors()\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Inference Example with Optimum Intel and OpenVINO\nDESCRIPTION: Demonstrates how to use Optimum Intel and OpenVINO for inference with a Hugging Face model.  It includes loading the model, creating a tokenizer, generating input tensors, and decoding the output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVModelForCausalLM\n# new imports for inference\nfrom transformers import AutoTokenizer\n# load the model\nmodel_id = \"meta-llama/Llama-2-7b-chat-hf\"\nmodel = OVModelForCausalLM.from_pretrained(model_id, export=True)\n# inference\nprompt = \"The weather is:\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n----------------------------------------\n\nTITLE: Running Image Classification Sample (Linux/macOS)\nDESCRIPTION: This command runs the `classification_sample_async` application with a specified image (`dog.bmp`), model (`model.xml`), and device (`CPU`). It assumes the environment variables are set and the application is built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ncd ~/openvino_cpp_samples_build/<architecture>/Release\n./classification_sample_async -i <path-to-input-image>/dog.bmp -m <path-to-your-model>/model.xml -d CPU\n```\n\n----------------------------------------\n\nTITLE: TorchFX: Quantize Model\nDESCRIPTION: Quantizes a TorchFX model using NNCF. This involves applying 8-bit quantization to the model with a calibration dataset, preparing it for efficient inference on target hardware.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nquantized_model = nncf.quantize(model=model, calibration_dataset=dataset)\n\n```\n\n----------------------------------------\n\nTITLE: Converting YOLOv3 PaddlePaddle Model (CLI)\nDESCRIPTION: This example converts a YOLOv3 PaddlePaddle model file to OpenVINO IR using the `ovc` command-line interface. It demonstrates a practical application of the conversion tool.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-paddle.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\novc yolov3.pdmodel\n```\n\n----------------------------------------\n\nTITLE: Streaming Output in Python\nDESCRIPTION: This Python code demonstrates streaming output from an LLMPipeline.  It defines a lambda function `streamer` that prints each generated word immediately to the console.  The `pipe.generate` function is called with the streamer and a maximum number of new tokens.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\npipe = ov_genai.LLMPipeline(model_path, \"CPU\")\n\nstreamer = lambda x: print(x, end='', flush=True)\npipe.generate(\"The Sun is yellow because\", streamer=streamer, max_new_tokens=100)\n```\n\n----------------------------------------\n\nTITLE: Remote Tensor Usage (C++)\nDESCRIPTION: Demonstrates how to create and use a remote tensor with `ov::RemoteContext` in C++ to work with remote device memory. This enables efficient processing of data that resides on a remote device, minimizing data transfer overhead.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_27\n\nLANGUAGE: cpp\nCODE:\n```\n// Get remote context\nov::RemoteContext remote_context = core.get_context(device_name, remote_device_id);\n\n// Create remote tensor\nov::Tensor remote_tensor = ov::Tensor(remote_context, input_tensor.get_shape(), input_tensor.get_element_type());\n\n// Fill input tensor with random data copies to remote_tensor\nstd::vector<float> input_data(input_tensor.get_size());\nstd::generate(input_data.begin(), input_data.end(), []() { return static_cast<float>(std::rand()) / RAND_MAX; });\nov::Tensor input_tensor = ov::Tensor(remote_context, input_tensor.get_shape(), input_tensor.get_element_type(), input_data.data());\ninfer_request.infer({input_tensor : remote_tensor});\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (PaddlePaddle) with OpenVINO in C\nDESCRIPTION: This code compiles a model in PaddlePaddle format using the `ov_core_compile_model` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests. Memory management must be handled manually in C.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\nov_compiled_model_t* compiled_model_paddle = NULL;\nov_core_compile_model(core, \"path_to_model.pdmodel\", \"AUTO\", &compiled_model_paddle);\n```\n\n----------------------------------------\n\nTITLE: Declaring Model Layout (Python)\nDESCRIPTION: This snippet demonstrates how to declare the layout of the model's input data using the `ov::preprocess::PrePostProcessor::input().model().set_layout` method in Python. It specifies that the model expects the input data in the `NCHW` layout. This step is crucial for ensuring that the preprocessing steps correctly transform the input data to match the model's expected format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nppp.input().model().set_layout(ov.Layout('NCHW'))\n```\n\n----------------------------------------\n\nTITLE: Benchmark App Command - Unlimited Device Choice\nDESCRIPTION: This command runs the OpenVINO Benchmark Application with the AUTO plugin for device selection, allowing it to choose from all available devices. It uses a specified model and input, running 1000 iterations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_22\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app –d AUTO –m <model> -i <input> -niter 1000\n```\n\n----------------------------------------\n\nTITLE: Basic Infer Flow C++\nDESCRIPTION: Demonstrates the basic inference flow using OpenVINO, including reading and compiling the model, creating an infer request, and configuring input and output tensors. It involves multiple API calls for model loading and preparation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_classification/README.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n``ov::Core::read_model``\n```\n\nLANGUAGE: C++\nCODE:\n```\n``ov::Core::compile_model``\n```\n\nLANGUAGE: C++\nCODE:\n```\n``ov::CompiledModel::create_infer_request``\n```\n\nLANGUAGE: C++\nCODE:\n```\n``ov::InferRequest::set_input_tensor``\n```\n\nLANGUAGE: C++\nCODE:\n```\n``ov::InferRequest::get_output_tensor``\n```\n\n----------------------------------------\n\nTITLE: Function Overloading Usage Python\nDESCRIPTION: Shows how to use the overloaded `say_hello` function in Python, including calling it with and without a message. Also demonstrates the `TypeError` that is raised when calling the function with an incorrect argument type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport openvino._pyopenvino.mymodule as mymodule\na = mymodule.MyTensor([1,2,3])\na.say_hello()\n>>> Hello there!\n>>> 1.0\n>>> 2.0\n>>> 3.0\na.say_hello(\"New message!\")\n>>> New message!\n>>> 1.0\n>>> 2.0\n>>> 3.0\n# Let's try different message\na.say_hello(777)\n>>> Traceback (most recent call last):\n>>>   File \"<stdin>\", line 1, in <module>\n>>> TypeError: say_hello(): incompatible function arguments. The following argument types are supported:\n>>>     1. (self: openvino._pyopenvino.mymodule.MyTensor) -> None\n>>>     2. (self: openvino._pyopenvino.mymodule.MyTensor, arg0: str) -> None\n>>> \n>>> Invoked with: <openvino._pyopenvino.mymodule.MyTensor object at >>> 0x7fdfef5bb4f0>, 777\n```\n\n----------------------------------------\n\nTITLE: Check if shape is static C++\nDESCRIPTION: This code snippet checks if a `ov::PartialShape` is static before converting it to a `ov::Shape` to avoid exceptions when running the code for dynamic models.\nIt uses the `is_static()` method to determine if the shape is static.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/docs/shape_propagation.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nov::PartialShape shape = ...;\nif (shape.is_static())\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (ONNX) with OpenVINO in C\nDESCRIPTION: This code compiles a model in ONNX format using the `ov_core_compile_model` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests. Memory management must be handled manually in C.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\nov_compiled_model_t* compiled_model_onnx = NULL;\nov_core_compile_model(core, \"path_to_model.onnx\", \"AUTO\", &compiled_model_onnx);\n```\n\n----------------------------------------\n\nTITLE: Mapping to Standard Relu Operation (Python)\nDESCRIPTION: Illustrates mapping a custom operation ('MyRelu') to a standard OpenVINO 'Relu' operation in Python. This allows leveraging existing optimized implementations for custom operations with equivalent functionality.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/frontend-extensions.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\ncore.add_extension(ov.frontend.OpExtension(\"MyRelu\", \"Relu\"))\nmodel = core.read_model(\"model.xml\")\n```\n\n----------------------------------------\n\nTITLE: Compile Model with CUMULATIVE_THROUGHPUT Hint in C++\nDESCRIPTION: This snippet shows how to compile a model using the AUTO plugin with a specific device priority (GPU, then CPU) and the CUMULATIVE_THROUGHPUT performance hint in C++. This configuration aims to leverage both GPU and CPU for maximized throughput.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nov::CompiledModel compiled_model = core.compile_model(model, \"AUTO:GPU,CPU\", ov::hint::performance_mode(ov::hint::PerformanceMode::CUMULATIVE_THROUGHPUT));\n```\n\n----------------------------------------\n\nTITLE: Running C++ NV12 Classification Sample\nDESCRIPTION: This command executes the C++ version of the `hello_nv12_input_classification` sample. It takes the model file path, the image file path, image size, and the target device (e.g., CPU) as arguments.  The sample loads the model, preprocesses the NV12 image, performs inference, and outputs classification results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-nv12-input-classification.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhello_nv12_input_classification ./models/alexnet.xml ./images/cat.yuv 300x300 CPU\n```\n\n----------------------------------------\n\nTITLE: Converting YOLOv3 PaddlePaddle Model (Python)\nDESCRIPTION: This example demonstrates converting a YOLOv3 PaddlePaddle model file to OpenVINO IR using the `openvino.convert_model` function in Python.  It highlights a specific use case and requires the `openvino` package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-paddle.rst#_snippet_2\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nov.convert_model('yolov3.pdmodel')\n```\n\n----------------------------------------\n\nTITLE: Install Benchmark Tool with pip\nDESCRIPTION: This command installs the OpenVINO Benchmark Tool using pip, specifying the path to the 'benchmark_tool' directory. This makes the benchmark_tool executable and its Python modules available in the current Python environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/README.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\npython3 -m pip install benchmark_tool/\n```\n\n----------------------------------------\n\nTITLE: Convert TensorFlow SavedModel to OpenVINO IR with ovc (Bash)\nDESCRIPTION: This command demonstrates how to convert a TensorFlow SavedModel to OpenVINO IR using the `ovc` tool. The input is a directory containing the SavedModel artifact.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-keras.rst#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\novc bert_base\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (TensorFlow Lite) with OpenVINO in C\nDESCRIPTION: This code compiles a model in TensorFlow Lite format using the `ov_core_compile_model` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests. Memory management must be handled manually in C.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_22\n\nLANGUAGE: cpp\nCODE:\n```\nov_compiled_model_t* compiled_model_tflite = NULL;\nov_core_compile_model(core, \"path_to_model.tflite\", \"AUTO\", &compiled_model_tflite);\n```\n\n----------------------------------------\n\nTITLE: Reshape OpenVINO Model in Python\nDESCRIPTION: This snippet demonstrates how to reshape an OpenVINO Model in Python, typically for models with sequence length inputs. It modifies the input shape to set the sequence dimension to exactly 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nmodel.reshape({input_layer: [1, 100]})\n```\n\n----------------------------------------\n\nTITLE: Checking Device Caching Support\nDESCRIPTION: This snippet checks if a device supports model caching in OpenVINO using Python or C++. It queries the device's capabilities to determine if model export/import is supported. The snippet prints whether caching is supported or not for the specified device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n### [ov:caching:part3]\nimport openvino.runtime as ov\n\ncore = ov.Core()\n\ndevice_name = \"CPU\"\nif core.get_property(device_name, \"MODEL_IMPORT_EXPORT\"):\n    print(f\"{device_name} supports model caching\\n\")\nelse:\n    print(f\"{device_name} doesn't support model caching\\n\")\n### [ov:caching:part3]\n```\n\n----------------------------------------\n\nTITLE: Reshape model input with dynamic batch size - C++\nDESCRIPTION: This C++ snippet shows how to reshape a model's input layer to allow for a dynamic batch size. The code uses `ov::Dimension()` to define the dynamic dimension. The initial shape is assumed to be [1, 3, 224, 224].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov_dynamic_shapes:reshape_undefined]\nauto model = core.read_model(\"model.xml\");\nmodel->reshape({{model->input().get_any_name(), {ov::Dimension(-1), 3, 224, 224}}});\n//! [ov_dynamic_shapes:reshape_undefined]\n```\n\n----------------------------------------\n\nTITLE: Create work directory and set environment variable\nDESCRIPTION: Creates a work directory for the OpenVINO Android build process and sets the OPV_HOME_DIR environment variable to the absolute path of the created directory. This variable is used to reference the base directory throughout the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_android.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmkdir openvino-android\nexport OPV_HOME_DIR=${PWD}/openvino-android\n```\n\n----------------------------------------\n\nTITLE: Queue Sharing with OpenVINO GPU Plugin (C++)\nDESCRIPTION: Demonstrates how to share an OpenCL command queue with the OpenVINO GPU plugin. Sharing the queue changes the behavior of the ov::InferRequest::start_async() method to ensure inference primitives submission is finished before returning. Requires OpenVINO runtime and OpenCL.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_41\n\nLANGUAGE: cpp\nCODE:\n```\n//! [queue_sharing]\nauto device_id = \"GPU\";\nov::AnyMap properties;\nproperties.insert({ov::intel_gpu::context_property(CL_CONTEXT_PLATFORM), platform});\nproperties.insert({ov::intel_gpu::context_property(CL_CONTEXT_INTEROP_USER_SYNC), true});\nproperties.insert({ov::intel_gpu::queue_property(queue), queue});\nauto remote_context = core.create_context(device_id, properties);\ncompiled_model = core.compile_model(model, remote_context);\nauto infer_request = compiled_model.create_infer_request();\n//! [queue_sharing]\n```\n\n----------------------------------------\n\nTITLE: Using Predefined Layout Names in Python\nDESCRIPTION: This snippet demonstrates how to use predefined layout names (N, C, H, W) in Python with `ov::Layout` to easily specify common image tensor layouts. This leverages established conventions for better code readability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlayout = ov.Layout(\"NCHW\")\nchannel_idx = layout.get_index(\"channel\")\nheight_idx = layout.get_index(\"height\")\n```\n\n----------------------------------------\n\nTITLE: Get/Set Tensor by Name in C++\nDESCRIPTION: This C++ code demonstrates how to get and set tensors using their names, using `get_tensor(tensor_name)` and `set_tensor(tensor_name, tensor)`. Requires OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\nov::InferRequest infer_request = compiled_model.create_infer_request();\n\nov::Tensor input_tensor = infer_request.get_tensor(\"input_tensor_name\");\nstd::vector<float> input_data(input_tensor.get_size());\nstd::generate(input_data.begin(), input_data.end(), []() { return static_cast<float>(rand()) / RAND_MAX; });\nstd::memcpy(input_tensor.data(), input_data.data(), input_data.size() * sizeof(float));\n\ninfer_request.infer();\n\nov::Tensor output_tensor = infer_request.get_tensor(\"output_tensor_name\");\nconst float* output_data = output_tensor.data<const float>();\n```\n\n----------------------------------------\n\nTITLE: Get Device Properties with OpenVINO C++ API\nDESCRIPTION: This snippet utilizes the `ov::Core::get_property` API from the OpenVINO library to retrieve specific properties of a given device. This function can be used to get various device metrics and configurations. The input is the device name and the property key, and the output is the value of the requested property.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_query_device/README.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n``ov::Core::get_property``\n```\n\n----------------------------------------\n\nTITLE: Reshape model input with dynamic batch size - Python\nDESCRIPTION: This snippet demonstrates how to reshape a model input in Python to make the batch size dynamic using `ov.Dimension()` to specify an undefined dimension. It assumes the model has a single input layer with an initial shape of [1, 3, 224, 224].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nie = Core()\nmodel = ie.read_model(\"model.xml\")\n# ! [reshape_undefined]\nmodel.reshape({model.inputs[0].get_name(): [ov.Dimension(-1), 3, 224, 224]})\n# ! [reshape_undefined]\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Accessing Input by Name C++\nDESCRIPTION: Illustrates how to access a specific input of a model by its name using the `ov::preprocess::PrePostProcessor` in C++. This is essential when dealing with models that have multiple inputs with distinct names.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n// Get input Tensor by name and set input tensor information.\n// We apply preprocessing demanding input tensor to have 'u8' type and 'NCHW' layout.\nppp.input(\"tensor_input_name\").tensor().set_element_type(ov::element::u8).set_layout(\"NCHW\");\n// 2) Adding explicit preprocessing steps\nppp.input(\"tensor_input_name\").preprocess().convert_element_type(ov::element::f32).convert_layout(\"NHWC\");\n// 3) Set input model information.\nppp.input(\"tensor_input_name\").model().set_layout(\"NHWC\");\n// 4) Apply preprocessing modifying the original model\nmodel = ppp.build();\n```\n\n----------------------------------------\n\nTITLE: PyTorch: Run Inference\nDESCRIPTION: Runs inference on the quantized PyTorch model. The code sets the model to evaluation mode, performs a forward pass with an input tensor, and prints the results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel.eval()\nresults = model(input_tensor)\nprint(results)\n\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (ov::Model) with OpenVINO in Python\nDESCRIPTION: This code compiles a model of type `ov::Model` using the `core.compile_model()` method. It specifies the model itself and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel = core.read_model(\"path_to_model.xml\")\nmodel_ov = core.compile_model(model, \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Allocate Tensor with Level Zero Host Memory - OpenVINO™ C++\nDESCRIPTION: This code snippet shows how to allocate a Tensor with Level Zero host memory without involving the remote context directly. The NPU plugin allocates the memory using Level Zero and wraps it in a regular Tensor object. The code uses ov::Tensor to create the tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/npu-device/remote-tensor-api-npu-plugin.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nov::Tensor t{remote_context, ov::element::f32, {1, 3, 224, 224}};\n```\n\n----------------------------------------\n\nTITLE: Torchvision to OpenVINO Preprocessing Example\nDESCRIPTION: This example demonstrates how to use the `PreprocessConverter.from_torchvision` to integrate a torchvision preprocessing pipeline into an OpenVINO model. It defines a torchvision Compose pipeline, exports a PyTorch model to ONNX, reads the model using OpenVINO, applies the preprocessing pipeline using `PreprocessConverter`, compiles the model, and then runs inference.  It requires `torchvision`, `torch`, `onnx`, `openvino`, `numpy`, and `PIL` to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/openvino/preprocess/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npreprocess_pipeline = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize(256, interpolation=transforms.InterpolationMode.NEAREST),\n        torchvision.transforms.CenterCrop((216, 218)),\n        torchvision.transforms.Pad((2, 3, 4, 5), fill=3),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.ConvertImageDtype(torch.float32),\n        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\n\ntorch_model = SimpleConvnet(input_channels=3)\n\ntorch.onnx.export(torch_model, torch.randn(1, 3, 224, 224), \"test_convnet.onnx\", verbose=False, input_names=[\"input\"], output_names=[\"output\"])\ncore = Core()\nov_model = core.read_model(model=\"test_convnet.onnx\")\n\ntest_input = np.random.randint(255, size=(260, 260, 3), dtype=np.uint16)\nov_model = PreprocessConverter.from_torchvision(\n    model=ov_model, transform=preprocess_pipeline, input_example=Image.fromarray(test_input.astype(\"uint8\"), \"RGB\")\n)\nov_model = core.compile_model(ov_model, \"CPU\")\nov_input = np.expand_dims(test_input, axis=0)\noutput = ov_model.output(0)\nov_result = ov_model(ov_input)[output]\n```\n\n----------------------------------------\n\nTITLE: Create NNCF Configuration - PyTorch\nDESCRIPTION: This code snippet demonstrates how to create an NNCF configuration object for filter pruning in PyTorch. It involves defining model-related parameters (input_info) and optimization method parameters (compression). The config defines parameters of the compression algorithm.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnncf_config_dict = {\n    \"input_info\": {\"sample_size\": [1, 3, 224, 224]},\n    \"compression\": {\n        \"algorithm\": \"filter_pruning\",\n        \"params\": {\n            \"pruning_init\": 0.05,\n            \"pruning_target\": 0.5,\n            \"pruning_steps\": 1000\n        }\n    }\n}\nnncf_config = NNCFConfig.from_dict(nncf_config_dict)\n```\n\n----------------------------------------\n\nTITLE: C++ Asynchronous Inference Example\nDESCRIPTION: This is a console command example that shows how to run the classification_sample_async executable with a specified model and input image, utilizing the GPU device for inference. It illustrates the basic syntax for executing the C++ sample.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/image-classification-async.rst#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nclassification_sample_async -m ./models/googlenet-v1.xml -i ./images/dog.bmp -d GPU\n```\n\n----------------------------------------\n\nTITLE: Read Model with Extension (C++)\nDESCRIPTION: Demonstrates how to read a model with a custom operation and register the extension.  This involves creating an `OpExtension` instance for the custom operation and adding it to the `Core` object before reading the model. This ensures that OpenVINO recognizes and can process the custom operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/frontend-extensions.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nCore core;\ncore.add_extension(std::make_shared<ov::frontend::OpExtension<TemplateExtension::Identity>>());\nauto model = core.read_model(\"model.onnx\");\n```\n\n----------------------------------------\n\nTITLE: Import OpenVINO Node.js addon\nDESCRIPTION: This code imports the OpenVINO Node.js addon into a JavaScript file. It uses the `require` function to load the `openvino-node` package and assigns the `addon` object to the `ov` variable, making the OpenVINO API available for use.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/README.md#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst { addon: ov } = require('openvino-node');\n```\n\n----------------------------------------\n\nTITLE: Compiling Model on Default GPU C++\nDESCRIPTION: This C++ snippet demonstrates how to compile a model on the default GPU device using `ov::Core::compile_model()`. It utilizes the `compile_model_default_gpu` fragment from the specified C++ file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n// [compile_model_default_gpu]\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"GPU\");\n// [compile_model_default_gpu]\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Runtime Version Retrieval C++\nDESCRIPTION: Demonstrates how to retrieve the OpenVINO API version using the `ov::get_openvino_version` function. This provides insight into the OpenVINO runtime being used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/benchmark/throughput_benchmark/README.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n``ov::get_openvino_version``\n```\n\n----------------------------------------\n\nTITLE: Load Checkpoint - PyTorch\nDESCRIPTION: Restores the model from a checkpoint during QAT using NNCF API. This is used for resuming training from a saved state or loading a pre-trained quantized model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/quantization-aware-training.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnncf.load_checkpoint(model, f=\"{checkpoint_path}/model_quantized.pth\")\n```\n\n----------------------------------------\n\nTITLE: Converting paddle.fluid.dygraph.layers.Layer to OpenVINO IR\nDESCRIPTION: This code demonstrates converting a PaddlePaddle model of type `paddle.fluid.dygraph.layers.Layer` to OpenVINO IR format. It shows the usage of `example_input` parameter which is required for conversion and it accepts a list with a tensor as input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-paddle.rst#_snippet_5\n\nLANGUAGE: py\nCODE:\n```\nimport paddle\nimport openvino as ov\n\n# create a paddle.fluid.dygraph.layers.Layer format model\nmodel = paddle.vision.models.resnet50()\nx = paddle.rand([1,3,224,224])\n\n# convert to OpenVINO IR format\nov_model = ov.convert_model(model, example_input=[x])\n```\n\n----------------------------------------\n\nTITLE: Caching compiled models in Python\nDESCRIPTION: This Python snippet demonstrates how to configure the LLMPipeline to cache compiled models using the 'CACHE_DIR' option. This can shorten the initialization time for future pipeline runs. The specified directory '.npucache' will be used to store the cached models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\npipeline_config = { \"CACHE_DIR\": \".npucache\" }\npipe = ov_genai.LLMPipeline(model_path, \"NPU\", pipeline_config)\n```\n\n----------------------------------------\n\nTITLE: Tensor Operations C++\nDESCRIPTION: Demonstrates how to get the shape and data of a tensor using `ov::Tensor::get_shape` and `ov::Tensor::data` respectively.  These operations are fundamental for working with model inputs and outputs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/benchmark/throughput_benchmark/README.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n``ov::Tensor::get_shape``,\n``ov::Tensor::data``\n```\n\n----------------------------------------\n\nTITLE: Creating a Virtual Environment with pyenv\nDESCRIPTION: This shell command creates a new virtual environment named 'ov-py310' based on the installed Python version 3.10.7 using pyenv. This environment will be isolated from the system-wide Python installation, providing a dedicated space for OpenVINO™ development.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/build.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npyenv virtualenv 3.10.7 ov-py310\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch ExportedProgram from Disk\nDESCRIPTION: This snippet shows how to convert a PyTorch model saved as an `ExportedProgram` to OpenVINO. The model is loaded from a file named `exported_program.pt2` and then converted using `openvino.convert_model`. It requires the `openvino` library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-pytorch.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model('exported_program.pt2')\n```\n\n----------------------------------------\n\nTITLE: Make Stateful Model Using Parameter/Result Operations in C++\nDESCRIPTION: This snippet demonstrates how to apply the MakeStateful transformation to an OpenVINO model using Parameter and Result operations in C++. It uses Parameter and Result operation nodes instead of tensor names.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nov::pass::MakeStateful::ParamResMap param_res_map = {{\n    parameter_node,\n    result_node,\n}};\n\nmodel->transform(std::make_shared<ov::pass::MakeStateful>(param_res_map));\n```\n\n----------------------------------------\n\nTITLE: Convert ONNX model using convert_model in Python\nDESCRIPTION: This snippet demonstrates how to convert an ONNX model to an OpenVINO model using the `convert_model()` method in Python. The converted model is then compiled using `compile_model()` for execution on the specified device (AUTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_12\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\nov_model = ov.convert_model(\"<INPUT_MODEL>.onnx\")\ncompiled_model = ov.compile_model(ov_model, \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This CMake code includes subdirectories based on whether specific targets for different frontends are defined. It adds subdirectories for ONNX, TensorFlow, Paddle, PyTorch, and JAX if their respective targets (e.g., `openvino::frontend::onnx`) exist. This allows for conditional compilation of frontend support.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(TARGET openvino::frontend::onnx)\n    add_subdirectory(frontend/onnx)\nendif()\n\nif(TARGET openvino::frontend::tensorflow)\n    add_subdirectory(frontend/tensorflow)\nendif()\n\nif(TARGET openvino::frontend::paddle)\n    add_subdirectory(frontend/paddle)\nendif()\n\nif(TARGET openvino::frontend::pytorch)\n    add_subdirectory(frontend/pytorch)\nendif()\n\nif(TARGET openvino::frontend::jax)\n    add_subdirectory(frontend/jax)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Printing Dynamic Output Shapes - Python\nDESCRIPTION: This Python code shows how to check and print the dynamic output shapes of a model in OpenVINO. It reads a model, compiles it, and then prints the partial shape of the output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model, \"CPU\")\n\n# ! [print_dynamic]\noutput_shape = compiled_model.output(0).get_partial_shape()\nprint(output_shape)\n# ! [print_dynamic]\n```\n\n----------------------------------------\n\nTITLE: Identity Operation Implementation (C++)\nDESCRIPTION: Implements the Identity operation within the TemplateExtension. This is a minimal example of a custom operation that simply passes its input to the output. It demonstrates the core functionality required for an OpenVINO extension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/frontend-extensions.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nTemplateExtension::Identity::Identity(const Output<Node>& arg) : Op({arg}) {\n    constructor_validate_and_infer_types();\n}\n\nstd::shared_ptr<Node> TemplateExtension::Identity::clone_with_new_inputs(const OutputVector& new_args) const {\n    OPENVINO_ASSERT(new_args.size() == 1, \"Incorrect number of new arguments\");\n    return std::make_shared<Identity>(new_args.at(0));\n}\n\nvoid TemplateExtension::Identity::validate_and_infer_types() {\n    const auto& input_type = get_input_element_type(0);\n    set_output_type(0, input_type, get_input_partial_shape(0));\n}\n\nbool TemplateExtension::Identity::visit_attributes(AttributeVisitor& visitor) {\n    return true;\n}\n```\n\n----------------------------------------\n\nTITLE: Get Available Devices with OpenVINO Core (Python)\nDESCRIPTION: This snippet demonstrates how to retrieve a list of available devices using the `ov::Core::get_available_devices` method in Python. It utilizes the `ov.Core` object to query the available devices and prints the result.  No external dependencies beyond the OpenVINO runtime are required.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ncore = ov.Core()\navailable_devices = core.get_available_devices()\nprint(f\"Available devices: {available_devices}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Callbacks for AsyncInferQueue in OpenVINO Python\nDESCRIPTION: This snippet demonstrates how to set callbacks for `AsyncInferQueue` in the OpenVINO Python API. A callback function is executed upon the completion of each inference request. The callback function receives the request and a user-defined data object as arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\ndef callback(request, user_data):\n    print(f\"Inference complete for request id: {request.id}\")\n    print(f\"User data: {user_data}\")\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model)\n\n# Create an AsyncInferQueue with 4 infer requests\ninfer_queue = ov.AsyncInferQueue(compiled_model, 4)\n\n# Set callback function\ninfer_queue.set_callback(callback)\n\ninput_data = np.array([1, 2, 3, 4, 5], dtype=np.float32)\nuser_data = \"some runtime values\"\n\n# Start asynchronous inference for all the requests\nfor i in range(len(infer_queue)):\n    infer_queue.start_async({compiled_model.inputs[0]: input_data}, user_data)\n\n# Wait for all the requests to finish\ninfer_queue.wait_all()\n```\n\n----------------------------------------\n\nTITLE: Chatglm-4-GPTQ with OpenVINO and torch.compile (Python)\nDESCRIPTION: This code demonstrates the integration of torch.compile and OpenVINO with the Chatglm-4-GPTQ model for generating text. The code compiles the model's transformer encoder's forward function with the 'openvino' backend and 'aot_autograd' enabled.  It uses the model to generate text based on a user query.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_5\n\nLANGUAGE: py\nCODE:\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nquery = \"tell me about AI“\n\ntokenizer = AutoTokenizer.from_pretrained(\"mcavus/glm-4v-9b-gptq-4bit-dynamo\", trust_remote_code=True)\ninputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": query}],\n                                                      add_generation_prompt=True,\n                                                      tokenize=True,\n                                                      return_tensors=\"pt\",\n                                                      return_dict=True\n                                                      )\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mcavus/glm-4v-9b-gptq-4bit-dynamo\",\n    torch_dtype=torch.float32,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n)\n\n+ model.transformer.encoder.forward = torch.compile(model.transformer.encoder.forward, backend=\"openvino\", options={\"aot_autograd\":True})\n\ngen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\nwith torch.no_grad():\n    outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenVINO Core in C\nDESCRIPTION: This code includes the necessary OpenVINO header files and creates an `ov::Core` object. The `ov::Core` object is the entry point for using OpenVINO Runtime and is required to compile models and perform inference. Since C API is based on a set of C-compatible functions, the initialization uses `ov_core_create`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n#include <openvino/c/openvino.h>\n\nov_core_t* core = NULL;\nov_core_create(&core);\n```\n\n----------------------------------------\n\nTITLE: Basic Inference Flow C++\nDESCRIPTION: Shows the basic inference flow using OpenVINO C++ API, which includes compiling the model using `ov::Core::compile_model`, creating an infer request with `ov::CompiledModel::create_infer_request`, and getting tensor using `ov::InferRequest::get_tensor`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/benchmark/throughput_benchmark/README.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n``ov::Core``, ``ov::Core::compile_model``,\n``ov::CompiledModel::create_infer_request``,\n``ov::InferRequest::get_tensor``\n```\n\n----------------------------------------\n\nTITLE: Convert ONNX model to IR using ovc\nDESCRIPTION: This snippet shows how to convert an ONNX model to OpenVINO Intermediate Representation (IR) format using the `ovc` command-line tool. The resulting IR can then be read by `read_model()` and inferred.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\novc <INPUT_MODEL>.onnx\n```\n\n----------------------------------------\n\nTITLE: CMake Integration for C++\nDESCRIPTION: This snippet shows how to configure a CMake build for a C++ application that integrates with OpenVINO. It includes finding the OpenVINO package, setting up include directories, and linking the necessary libraries. This configuration assumes that the OpenVINO environment variables are set up correctly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_39\n\nLANGUAGE: cpp\nCODE:\n```\n#! [cmake:integration_example_cpp]\ncmake_minimum_required(VERSION 3.13)\nproject(openvino_example)\n\nfind_package(OpenVINO REQUIRED)\n\nadd_executable(${PROJECT_NAME} main.cpp)\ntarget_include_directories(${PROJECT_NAME} PUBLIC ${OpenVINO_INCLUDE_DIRS})\ntarget_link_libraries(${PROJECT_NAME} ${OpenVINO_LIBRARIES})\n#! [cmake:integration_example_cpp]\n```\n\n----------------------------------------\n\nTITLE: Accessing Preprocessing Steps in TypeScript\nDESCRIPTION: This code snippet defines the `preprocess()` method within the `InputInfo` interface.  It retrieves the `PreProcessSteps` object, which contains the sequence of preprocessing operations to be applied to the input. The method's return type is explicitly defined as `PreProcessSteps`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InputInfo.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\npreprocess(): PreProcessSteps\n```\n\n----------------------------------------\n\nTITLE: Convert Keras H5 to SavedModel (TF2) and Save\nDESCRIPTION: This code snippet demonstrates how to load a Keras H5 model in TensorFlow 2, save it in SavedModel format using the `tf.saved_model.save` function. This is a necessary preprocessing step before converting to OpenVINO IR.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_2\n\nLANGUAGE: py\nCODE:\n```\nimport tensorflow as tf\nmodel = tf.keras.models.load_model('model.h5')\ntf.saved_model.save(model,'model')\n```\n\n----------------------------------------\n\nTITLE: Compile Model on CPU - Python\nDESCRIPTION: This snippet shows how to compile a model for CPU inference using the OpenVINO Python API.  The `ov::Core::compile_model()` method is used with the device name \"CPU\".  This ensures that the model is compiled and executed on the CPU device. Required dependency: OpenVINO Python package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\n# [compile_model_default]\ncompiled_model = core.compile_model(model=model, device_name=\"CPU\")\n# [compile_model_default]\n```\n\n----------------------------------------\n\nTITLE: ONNX: Run Inference\nDESCRIPTION: Executes inference on the quantized ONNX model using the ONNX Runtime. This involves creating an inference session, preparing input data, and running the model to obtain predictions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nsession = onnxruntime.InferenceSession(quantized_model.SerializeToString(), None)\ninput_name = session.get_inputs()[0].name\nresults = session.run(None, {input_name: input_tensor.numpy()})\nprint(results)\n\n```\n\n----------------------------------------\n\nTITLE: Move Model to GPU\nDESCRIPTION: Shows how to move the loaded OpenVINO model to a different device, such as the GPU, using the .to() method. The device naming convention is the same as in OpenVINO native API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmodel.to(\"GPU\")\n```\n\n----------------------------------------\n\nTITLE: Save the Quantized Model - OpenVINO\nDESCRIPTION: This example showcases how to save the quantized model to OpenVINO Intermediate Representation (IR) format. Setting `compress_to_fp16=False` preserves FP32 precision for operations reverted from INT8 to FP32, maintaining better accuracy.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/quantizing-with-accuracy-control.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# [save]\nfrom openvino.runtime import Core\nfrom openvino.runtime import serialize\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\nserialize(model, \"model.xml\", compress_to_fp16=False)\n# [save]\n```\n\n----------------------------------------\n\nTITLE: Get/Set Tensor by Index in Python\nDESCRIPTION: This Python code retrieves input and output tensors using their index numbers when the model has multiple inputs/outputs. It calls `get_input_tensor(index)` and `get_output_tensor(index)`. Requires OpenVINO and a compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\ncompiled_model = ie.compile_model(model, \"CPU\")\ninfer_request = compiled_model.create_infer_request()\n\ninput_tensor = infer_request.get_input_tensor(0)\ninput_tensor.data[:] = np.random.normal(0, 1, input_tensor.shape)\n\ninfer_request.infer()\n\noutput_tensor = infer_request.get_output_tensor(0)\nprint(output_tensor.data)\n```\n\n----------------------------------------\n\nTITLE: Query Available Devices with OpenVINO C++ API\nDESCRIPTION: This snippet retrieves the available devices using the `ov::Core::get_available_devices` API from the OpenVINO library. It allows determining which devices are accessible for inference execution. No specific input is required, and the output is a list of available device names.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_query_device/README.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n``ov::Core::get_available_devices``\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark App with Input Image (Python)\nDESCRIPTION: This command runs the OpenVINO benchmark application using Python, specifying the path to the model and an input image.  It uses the `benchmark_app.py` script. It requires the OpenVINO Python package to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app -m model.xml -i test1.jpg\n```\n\n----------------------------------------\n\nTITLE: Download Models from Model Scope using modelscope\nDESCRIPTION: This snippet demonstrates how to download models from Model Scope using the `modelscope` package. It installs the package and then downloads a specific model to a local directory. The downloaded models are in PyTorch format and must be converted to OpenVINO IR format before inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/genai-model-preparation.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npip install modelscope\nmodelscope download --model \"Qwen/Qwen2-7b\" --local_dir model_path\n```\n\n----------------------------------------\n\nTITLE: Exporting LLM with Channel-wise Data-free Quantization\nDESCRIPTION: This snippet demonstrates how to export a Llama-2-7b-chat-hf model using Optimum-Intel with channel-wise data-free quantization. The model is exported in INT4 format with symmetrical quantization and a group size of -1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_5\n\nLANGUAGE: Console\nCODE:\n```\noptimum-cli export openvino -m meta-llama/Llama-2-7b-chat-hf --weight-format int4 --sym --ratio 1.0 --group-size -1 Llama-2-7b-chat-hf\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies on Windows using pip\nDESCRIPTION: This snippet installs the required dependencies for using OpenVINO GenAI on a Windows system. It creates a virtual environment, activates it, and then installs the necessary packages including nncf, onnx, optimum-intel, openvino, openvino-tokenizers, and openvino-genai.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_2\n\nLANGUAGE: Console\nCODE:\n```\npython -m venv npu-env\nnpu-env\\Scripts\\activate\npip install  nncf==2.14.1 onnx==1.17.0 optimum-intel==1.22.0\npip install openvino==2025.1 openvino-tokenizers==2025.1 openvino-genai==2025.1\n```\n\n----------------------------------------\n\nTITLE: Configuring BATCH device for Automatic Batching in OpenVINO using benchmark_app (sh)\nDESCRIPTION: This code snippet shows how to configure the BATCH device with performance hints (tput/ctput) to enable automatic batching using the benchmark_app tool. It demonstrates using different device configurations, including AUTO and explicit device specification (e.g., AUTO:GPU).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\n./benchmark_app -m <model> -d GPU -hint tput\n./benchmark_app -m <model> -d AUTO -hint tput\n./benchmark_app -m <model> -d AUTO -hint ctput\n./benchmark_app -m <model> -d AUTO:GPU -hint ctput\n```\n\n----------------------------------------\n\nTITLE: Reshape Model with New Batch Size - C++\nDESCRIPTION: This C++ snippet shows how to use the `reshape` method to set a new batch size for an OpenVINO model.  It requires the OpenVINO Inference Engine library and assumes the model has already been loaded and is ready to be reshaped.  The reshape method updates input shapes and propagates these changes to all subsequent layers in the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/changing-input-shape.rst#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\n\n    // ! [picture_snippet]\n    ov::Shape new_shape = {1, 3, 256, 256}; // New shape with batch size 1 and spatial size 256x256.\n    model->reshape(new_shape);\n    // ! [picture_snippet]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Get Input Port by Name in OpenVINO (C)\nDESCRIPTION: This function retrieves an input port of an OpenVINO model by its tensor name. It takes a pointer to the `ov_model_t`, the tensor name as a `const char*`, and returns a pointer to the `ov_output_port_t` representing the input port. The return value indicates the status of the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_26\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_model_input_by_name(const ov_model_t* model, const char* tensor_name, ov_output_port_t** input_port)\n```\n\n----------------------------------------\n\nTITLE: Compile Model for CPU with Auto Device Selection (Python)\nDESCRIPTION: This snippet demonstrates how to compile a model for the CPU device using automatic device selection in Python. It shows the basic usage of the `compile_model` function for CPU inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncore = ov.Core()\ncompiled_model = core.compile_model(\"model.xml\", \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Converting PaddlePaddle TopK Operation in OpenVINO\nDESCRIPTION: This code snippet demonstrates how to convert the PaddlePaddle `top_k_v2` operation within the OpenVINO framework extension. It showcases the use of `NamedOutputs` to explicitly map the outputs of the PaddlePaddle operation to the corresponding OpenVINO node outputs. The output names are crucial for correct conversion and are usually found in the PaddlePaddle operation's source code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/frontend-extensions.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nOPENVINO_FRONTEND_EXTENSION(frontend_extension_paddle, frontend_extension_paddle_TopK)\n{\n    using namespace ov::frontend;\n    add_operation(\"top_k_v2\",\n        [&](const NodeContext& node)\n        {\n            NamedOutputs outputs;\n            outputs[\"Out\"] = {node.get_output(0)};\n            outputs[\"Index\"] = {node.get_output(1)};\n            return outputs;\n        });\n}\n```\n\n----------------------------------------\n\nTITLE: Compile PaddlePaddle model using compile_model in Python\nDESCRIPTION: This snippet shows how to compile a PaddlePaddle model directly using the `compile_model()` method in Python. The model is compiled for execution on the specified device (AUTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_20\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\ncompiled_model = ov.compile_model(\"<INPUT_MODEL>.pdmodel\", \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Limiting Batch Size with Hint in C++\nDESCRIPTION: This C++ snippet shows how to limit the batch size for automatic batching using the `ov::hint::num_requests` configuration option.  This is particularly useful when the application processes a limited number of streams, preventing the system from using an unnecessarily large batch size.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"GPU\", {{\nov::hint::performance_mode(ov::hint::PerformanceMode::THROUGHPUT),\nov::hint::num_requests(4)\n}});\n```\n\n----------------------------------------\n\nTITLE: Synchronous Inference in OpenVINO Python\nDESCRIPTION: This snippet demonstrates synchronous inference calls in the OpenVINO Python API, which block the application execution until the inference is complete. These calls also return the results of the inference, providing a simple way to execute and obtain results from the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model)\n\ninput_data = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n\n# Perform synchronous inference\nresults = compiled_model(input_data)\n\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark App with Model and Device\nDESCRIPTION: This code snippet demonstrates how to run the OpenVINO benchmark application with a specified model, device, and input. It is a basic example for performance evaluation of a model on a specific device using the OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/performance-benchmarks/getting-performance-numbers.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app -m <model> -d <device> -i <input>\n```\n\n----------------------------------------\n\nTITLE: ReLU Layer Definition in XML - OpenVINO\nDESCRIPTION: This XML snippet demonstrates how to define a ReLU layer within an OpenVINO model. It specifies the input and output ports, including their dimensions. The 'type' attribute is set to 'ReLU', indicating the use of the Rectified Linear Unit activation function. This definition shows a ReLU layer taking a 256x56 tensor as input and producing a 256x56 tensor as output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/relu-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ReLU\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating Tensor from Host Pointer in OpenVINO (C)\nDESCRIPTION: This function constructs an OpenVINO tensor using a specified element type, shape, and a pre-allocated host memory pointer. The function takes the tensor element type, shape, the host memory pointer, and a pointer to a tensor pointer as input. It returns a status code indicating success or failure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_42\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_tensor_create_from_host_ptr(const ov_element_type_e type,\n                               const ov_shape_t shape,\n                               void* host_ptr,\n                               ov_tensor_t** tensor)\n```\n\n----------------------------------------\n\nTITLE: TensorFlow Session to OpenVINO\nDESCRIPTION: Converts a TensorFlow session directly to an OpenVINO model. This involves creating a TensorFlow session, defining input placeholders, applying a ReLU operation, and then converting the active session to an OpenVINO model using `ov.convert_model`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nwith tf.compat.v1.Session() as sess:\n    inp1 = tf.compat.v1.placeholder(tf.float32, [100], 'Input1')\n    inp2 = tf.compat.v1.placeholder(tf.float32, [100], 'Input2')\n    output = tf.nn.relu(inp1 + inp2, name='Relu')\n    tf.compat.v1.global_variables_initializer()\n```\n\nLANGUAGE: python\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model(sess)\n```\n\n----------------------------------------\n\nTITLE: Read PaddlePaddle model using read_model in Python\nDESCRIPTION: This snippet shows how to read a PaddlePaddle model using the `read_model()` method from the OpenVINO Core object in Python. The model is then compiled using `compile_model()` for execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_19\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\ncore = ov.Core()\nov_model = core.read_model(\"<INPUT_MODEL>.pdmodel\")\ncompiled_model = ov.compile_model(ov_model, \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: infer() Method C++\nDESCRIPTION: This code snippet shows the implementation of the `infer()` method, which is the core of the synchronous inference request. It calls the actual pipeline stages synchronously, checks input/output tensors, moves external tensors to the backend, and runs the inference. The plugin should implement the tensor movement and the actual inference execution within this method.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/synch-inference-request.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nvoid InferRequest::infer() {\n    // Check input/output tensors\n\n    // Move external tensors to backend\n\n    // Run the inference\n    infer_preprocess();\n    start_pipeline();\n    wait_pipeline();\n    infer_postprocess();\n}\n```\n\n----------------------------------------\n\nTITLE: Prepare Calibration and Validation Datasets - OpenVINO\nDESCRIPTION: This snippet demonstrates how to prepare calibration and validation datasets for the OpenVINO framework. This step is crucial for quantizing models with accuracy control in NNCF, ensuring the quantized model maintains acceptable performance on a validation dataset.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/quantizing-with-accuracy-control.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [dataset]\nimport numpy as np\nfrom openvino.runtime import Core\nfrom openvino.tools import pot\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\n\n\ndef dataset_fn(model):\n    shapes = pot.get_model_shapes(model)\n    input_name = next(iter(shapes))\n    shape = shapes[input_name]\n    calibration_size = 100\n    for _ in range(calibration_size):\n        yield {input_name: np.random.normal(0, 1, shape).astype(np.float32)}\n\n\ncalibration_dataset = dataset_fn(model)\nvalidation_dataset = dataset_fn(model)\n# [dataset]\n```\n\n----------------------------------------\n\nTITLE: Compiling Model Synchronously (Model Path)\nDESCRIPTION: Synchronously reads a model and creates a compiled model from a file path. This is a synchronous version of the asynchronous compileModel method. The config parameter allows specifying properties relevant only for this load operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\ncompileModelSync(modelPath, deviceName, config?): CompiledModel\n```\n\n----------------------------------------\n\nTITLE: Batching via BATCH Plugin C++\nDESCRIPTION: This C++ snippet demonstrates how to enable batching by using the BATCH plugin with `ov::Core::compile_model()`. It utilizes the `compile_model_batch_plugin` fragment from the specified C++ file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n// [compile_model_batch_plugin]\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"BATCH:GPU\");\n// [compile_model_batch_plugin]\n```\n\n----------------------------------------\n\nTITLE: Stable Diffusion 3 with OpenVINO and torch.compile (Python)\nDESCRIPTION: This code shows how to compile the transformer component of the Stable Diffusion 3 pipeline using torch.compile and the OpenVINO backend. It utilizes the diffusers library.  The optimized transformer is then used for image generation from a text prompt.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_1\n\nLANGUAGE: py\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\n\npipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float32)\n\n+ pipe.transformer = torch.compile(pipe.transformer, backend=\"openvino\")\n\nimage = pipe(\n    \"A cat holding a sign that says hello world\",\n    negative_prompt=\"\",\n    num_inference_steps=28,\n    guidance_scale=7.0,\n).images[0]\n\nimage.save('out.png')\n```\n\n----------------------------------------\n\nTITLE: Reading and Compiling TensorFlow Model in OpenVINO using Python\nDESCRIPTION: This code snippet shows how to read a TensorFlow SavedModel and compile it using OpenVINO in Python. It initializes the OpenVINO Core, reads the model from 'saved_model.pb', and then compiles the model for execution on the 'AUTO' device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_2\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\ncore = ov.Core()\nov_model = core.read_model(\"saved_model.pb\")\ncompiled_model = ov.compile_model(ov_model, \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Convert OCR Model with Named Input Shapes in Python\nDESCRIPTION: This Python code snippet converts an ONNX OCR model using `openvino.convert_model`, specifying shapes for two inputs, `data` and `seq_len`, by using a list of tuples containing the input name and its corresponding shape.  It imports the `openvino` library and then calls `ov.convert_model` with the model file name and the input shapes as a list of tuples.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/setting-input-shapes.rst#_snippet_2\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model(\"ocr.onnx\", input=[(\"data\", [3,150,200,1]), (\"seq_len\", [3])])\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Mean/Scale Normalization C++\nDESCRIPTION: Shows how to perform mean/scale normalization using the OpenVINO preprocessing API in C++. This involves subtracting the mean and dividing by the standard deviation for each data item. The example integrates these normalization steps into the execution graph.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n// 1) Set input tensor information. We apply preprocessing\n//    demanding input tensor to have 'f32' type and 'NCHW' layout.\nppp.input().tensor().set_element_type(ov::element::f32).set_layout(\"NCHW\");\n// 2) Adding explicit preprocessing steps\nppp.input().preprocess().mean(127.5).scale(127.5);\n// 3) Set input model information.\nppp.input().model().set_layout(\"NCHW\");\n// 4) Apply preprocessing modifying the original model\nmodel = ppp.build();\n```\n\n----------------------------------------\n\nTITLE: Running MatcherPass with Manager (C++)\nDESCRIPTION: This code snippet demonstrates how to run a MatcherPass on an `ov::Model` using `ov::pass::Manager`.  This approach allows you to register the MatcherPass for execution on the model along with other transformation types, providing a unified way to manage and apply transformations to your model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/matcher-pass.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n// [matcher_pass:manager]\n#include <openvino/core/rt_info.hpp>\n#include <openvino/opsets/opset10.hpp>\n#include <openvino/pass/matcher_pass.hpp>\n\n#include \"openvino/pass/graph_rewrite.hpp\"\n\nusing namespace std;\nusing namespace ov::opset10;\n\n{\n    auto pass = make_shared<TemplateTransformation>();\n    ov::pass::Manager manager;\n    manager.register_pass<TemplateTransformation>();\n    // Create model\n    auto input0 = make_shared<Parameter>(element::f32, Shape{1, 3, 64, 64});\n    auto input1 = make_shared<Parameter>(element::f32, Shape{1, 3, 64, 64});\n    auto add = make_shared<Add>(input0, input1);\n    auto model = make_shared<ov::Model>(add, ov::ParameterVector{input0, input1});\n\n    manager.run_on_model(model);\n}\n// [matcher_pass:manager]\n```\n\n----------------------------------------\n\nTITLE: Compiling TensorFlow Model in OpenVINO using C++\nDESCRIPTION: This C++ code snippet compiles a TensorFlow model for OpenVINO. It uses the `compile_model` method of the `ov::Core` class to compile 'saved_model.pb' for execution on the 'AUTO' device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nov::CompiledModel compiled_model = core.compile_model(\"saved_model.pb\", \"AUTO\");\n```\n\n----------------------------------------\n\nTITLE: Using OpenVINO with Hugging Face Optimum\nDESCRIPTION: This code snippet demonstrates how to use OpenVINO with Hugging Face Optimum to load and utilize pre-optimized OpenVINO IR models for generative AI tasks. It involves replacing standard transformers classes with their Optimum Intel counterparts.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-integrations.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n-from transformers import AutoModelForCausalLM\n+from optimum.intel.openvino import OVModelForCausalLM\n\nfrom transformers import AutoTokenizer, pipeline\nmodel_id = \"togethercomputer/RedPajama-INCITE-Chat-3B-v1\"\n\n-model = AutoModelForCausalLM.from_pretrained(model_id)\n+model = OVModelForCausalLM.from_pretrained(model_id, export=True)\n```\n\n----------------------------------------\n\nTITLE: Specifying Device Priorities - Python\nDESCRIPTION: This Python snippet demonstrates how to specify device priorities when compiling a model with the AUTO device. It shows how to create a `Core` object, load the model, and compile it using a device list.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"path_to_model.xml\")\ncompiled_model = core.compile_model(model=model, device_name=\"AUTO:GPU,CPU\")\n```\n\n----------------------------------------\n\nTITLE: PyTorch: Create Calibration Dataset\nDESCRIPTION: Creates an instance of the nncf.Dataset class using a PyTorch dataset.  The transformation function extracts the input data tensor from the dataset sample for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset = nncf.Dataset(torch_dataset, transform_fn)\n\n```\n\n----------------------------------------\n\nTITLE: Get Partial Shape in OpenVINO (C++)\nDESCRIPTION: This snippet shows how to get a partial shape of a node in OpenVINO using C++. Partial shapes can represent static or dynamic shapes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// [ov:partial_shape]\n```\n\n----------------------------------------\n\nTITLE: Run a specific GPU unit test using gtest_filter\nDESCRIPTION: This command runs a specific unit test by using the `gtest_filter` option. The filter string specifies the test(s) to be executed, allowing for targeted testing of specific functionalities or modules. The * character acts as a wildcard.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_unit_test.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./ov_gpu_unit_tests --gtest_filter='*filter_name*'\n```\n\n----------------------------------------\n\nTITLE: Get OpenVINO Runtime Version C++\nDESCRIPTION: Retrieves the OpenVINO API version. This is essential for verifying the installed OpenVINO runtime and ensuring compatibility with the application.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_classification/README.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n``ov::get_openvino_version``\n```\n\n----------------------------------------\n\nTITLE: Device-Specific Transformations in OpenVINO C++\nDESCRIPTION: This C++ code snippet demonstrates how to modify the transformation function to a device-specific operation set within OpenVINO for optimized performance on target hardware.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/advanced-guides/low-precision-transformations.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nauto& transformation = *transformation;\n\n```\n\n----------------------------------------\n\nTITLE: Run Hello Classification Sample - C++\nDESCRIPTION: This command line instruction demonstrates how to run the Hello Classification sample in C++. It requires the path to the model XML file, the path to the image, and the device name as arguments. The compiled C++ executable `hello_classification` is assumed to be in the current directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-classification.rst#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nhello_classification ./models/googlenet-v1.xml ./images/car.bmp GPU\n```\n\n----------------------------------------\n\nTITLE: Convert JAX ClosedJaxpr to OpenVINO Model in Python\nDESCRIPTION: This snippet demonstrates how to convert a JAX `ClosedJaxpr` object to an OpenVINO model using the `convert_model()` method in Python. It shows the creation of a simple JAX function and its conversion to a `ClosedJaxpr` object.  The model is then compiled using `compile_model()` for execution on the specified device (AUTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_24\n\nLANGUAGE: py\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport openvino as ov\n\n# let user have some JAX function\ndef jax_func(x, y):\n    return jax.lax.tanh(jax.lax.max(x, y))\n\n# use example inputs for creation of ClosedJaxpr object\nx = jnp.array([1.0, 2.0])\ny = jnp.array([-1.0, 10.0])\njaxpr = jax.make_jaxpr(jax_func)(x, y)\n\nov_model = ov.convert_model(jaxpr)\ncompiled_model = ov.compile_model(ov_model, \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Converting Model with OpenVINO Model Converter (Linux/macOS)\nDESCRIPTION: This command uses the `ovc` tool to convert a model to the Intermediate Representation (IR) format and compress it to FP16 precision.  `<path-to-your-model>` should be replaced with the path to the model file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\novc <path-to-your-model> --compress_to_fp16=True\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Mean/Scale RGB Normalization Python\nDESCRIPTION: Demonstrates how to perform mean/scale normalization separately for R, G, B values using the OpenVINO preprocessing API in Python. This is typically done in Computer Vision and requires defining the layout with the 'C' dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n# 1) Set input tensor information. We apply preprocessing\n#    demanding input tensor to have 'f32' type and 'NCHW' layout.\nppp.input().tensor().set_element_type(ov.Type.f32).set_layout(ov.Layout('NCHW'))\n# 2) Adding explicit preprocessing steps\nppp.input().preprocess().mean([127.5, 127.5, 127.5]).scale([127.5, 127.5, 127.5])\n# 3) Set input model information.\nppp.input().model().set_layout(ov.Layout('NCHW'))\n# 4) Apply preprocessing modifying the original model\nmodel = ppp.build()\n```\n\n----------------------------------------\n\nTITLE: Compile PaddlePaddle model using compile_model in C++\nDESCRIPTION: This snippet demonstrates how to compile a PaddlePaddle model using the `compile_model()` method in C++. The model is compiled for execution on the specified device (AUTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\nov::CompiledModel compiled_model = core.compile_model(\"<INPUT_MODEL>.pdmodel\", \"AUTO\");\n```\n\n----------------------------------------\n\nTITLE: Get Output Size of Model in OpenVINO (C)\nDESCRIPTION: This function retrieves the output size of a given OpenVINO model. It takes a pointer to the `ov_model_t` as input and returns the output size via the `output_size` parameter. A status code is returned to indicate success or failure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_31\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_model_outputs_size(const ov_model_t* model, size_t* output_size);\n```\n\n----------------------------------------\n\nTITLE: Free Function Binding C++\nDESCRIPTION: Defines a free function named `get_smile` at the module level that prints a smiley face. This function is added directly to the pybind11 module.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nmymodule.def(\"get_smile\", []() {\n    py::print(\":)\");\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Python with Shared Library using pyenv\nDESCRIPTION: This shell command installs a specific Python version (3.10.7) using pyenv, configured with a shared library.  The `PYTHON_CONFIGURE_OPTS` environment variable ensures that Python is built with shared library support, which is necessary for dynamically linked OpenVINO™ builds.  The `--verbose` flag provides detailed output during the installation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/build.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nenv PYTHON_CONFIGURE_OPTS=\"--enable-shared\" pyenv install --verbose 3.10.7\n```\n\n----------------------------------------\n\nTITLE: Enabling Cache Encryption (CPU)\nDESCRIPTION: This snippet demonstrates how to enable cache encryption for the CPU plugin in OpenVINO using Python or C++. It shows how to use the `cache_encryption_callbacks` config option with `compile_model` to encrypt the model while caching it and decrypt it when loading it from the cache.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n### [ov:caching:part5]\nimport openvino.runtime as ov\n\ncore = ov.Core()\n\ndef encryption_callback(data):\n    return data\n\nmodel_path = \"path_to_model.xml\"\ncache_dir = \"/path/to/cache/dir\"\n\ncompiled_model = core.compile_model(\n    model_path,\n    \"CPU\",\n    config={\n        \"CACHE_DIR\": cache_dir,\n        \"cache_encryption_callbacks\": {\n            \"encrypt\": encryption_callback,\n            \"decrypt\": encryption_callback\n        }\n    }\n)\n### [ov:caching:part5]\n```\n\n----------------------------------------\n\nTITLE: Create a Python virtual environment\nDESCRIPTION: Creates a dedicated Python virtual environment for quantization tasks using NNCF, ensuring dependencies are isolated and managed effectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython3 -m venv nncf_ptq_env\n```\n\n----------------------------------------\n\nTITLE: Caching compiled models in C++\nDESCRIPTION: This C++ snippet shows how to configure the LLMPipeline to cache compiled models using the 'CACHE_DIR' option. This can shorten the initialization time for future pipeline runs. The specified directory '.npucache' will be used to store the cached models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\nov::AnyMap pipeline_config = { { \"CACHE_DIR\",  \".npucache\" } };\nov::genai::LLMPipeline pipe(model_path, \"NPU\", pipeline_config);\n```\n\n----------------------------------------\n\nTITLE: Installing pyopenvino Target in CMake\nDESCRIPTION: This CMake snippet installs the built `pyopenvino` target into the `${OV_CPACK_PYTHONDIR}/openvino` directory, assigning it to the CPack component `${OV_CPACK_COMP_PYTHON_OPENVINO}_${pyversion}` and potentially excluding it from all other components using `${OV_CPACK_COMP_PYTHON_OPENVINO_EXCLUDE_ALL}`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${PROJECT_NAME}\n        DESTINATION ${OV_CPACK_PYTHONDIR}/openvino\n        COMPONENT ${OV_CPACK_COMP_PYTHON_OPENVINO}_${pyversion}\n        ${OV_CPACK_COMP_PYTHON_OPENVINO_EXCLUDE_ALL})\n```\n\n----------------------------------------\n\nTITLE: Running Inference on CPU\nDESCRIPTION: This snippet demonstrates how to run inference on the CPU using the `single-image-test.exe` application. It specifies the network file, input image, input and output precisions, device, color format, and input/output layouts. The command collects reference results for comparison.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/README.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nsingle-image-test.exe \\\n        --network \\\n        mobilenet-v2.xml \\\n        --input \\\n        validation-set/224x224/watch.bmp \\\n        --ip \\\n        FP16 \\\n        --op \\\n        FP16 \\\n        --device \\\n        CPU \\\n        --color_format \\\n        RGB \\\n        --il \\\n        NCHW \\\n        --ol \\\n        NC \\\n        --iml \\\n        NCHW \\\n        --oml \\\n        NC\n```\n\n----------------------------------------\n\nTITLE: Compile Model C++\nDESCRIPTION: This snippet demonstrates the `compile_model` method, which is the core function for creating a compiled model instance.  This instance holds a backend-dependent compiled model in an internal representation, ready for inference. It uses the configuration to tailor the compilation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<ov::ICompiledModel> Plugin::compile_model(const std::shared_ptr<const ov::Model>& model, const ov::AnyMap& properties) {\n    // 1. Update plugin config with compile config\n    auto cfg = Configuration{properties, get_stream_executor(properties)};\n    auto compiled_model = std::make_shared<CompiledModel>(model, cfg, shared_from_this());\n\n    return compiled_model;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Distributed Training - PyTorch\nDESCRIPTION: This code snippet shows how to configure NNCF for distributed multi-GPU training in PyTorch. Calling `compression_ctrl.distributed()` before fine-tuning is essential to inform optimization methods about the distributed mode and enable necessary adjustments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncompression_ctrl.distributed()\n```\n\n----------------------------------------\n\nTITLE: Creating Inference Request in C++\nDESCRIPTION: This code creates an inference request object from a compiled model using the `create_infer_request()` method. The inference request is used to set input tensors, execute the model, and retrieve the results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_25\n\nLANGUAGE: cpp\nCODE:\n```\nov::InferRequest infer_request = compiled_model_ir.create_infer_request();\n```\n\n----------------------------------------\n\nTITLE: Preprocessing and Saving Model to IR - C++\nDESCRIPTION: Demonstrates how to load an ONNX model, set preprocessing operations, and serialize the modified model into OpenVINO IR format in C++. The C++ code mirrors the Python example, showing how to configure resizing, color conversion, and mean/scale adjustments for image preprocessing before saving the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details/integrate-save-preprocessing-use-case.rst#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nvoid save_model() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"model.onnx\");\n\n    ov::preprocess::PrePostProcessor ppp = ov::preprocess::PrePostProcessor(model);\n    ppp.input().tensor()\n        .set_shape({1, 360, 640, 3})\n        .set_layout(\"NHWC\")\n        .set_color_format(ov::preprocess::ColorFormat::BGR);\n    ppp.input().preprocess()\n        .resize(ov::preprocess::ResizeAlgorithm::RESIZE_LINEAR)\n        .convert_color(ov::preprocess::ColorFormat::RGB)\n        .mean({0.485f, 0.456f, 0.406f})\n        .scale({0.229f, 0.224f, 0.225f});\n    ppp.input().model().set_layout(\"NCHW\");\n\n    model = ppp.build();\n    ov::serialize(model, \"ov_model.xml\");\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Inference Request in C\nDESCRIPTION: This code creates an inference request object from a compiled model using the `ov_compiled_model_create_infer_request` method. The inference request is used to set input tensors, execute the model, and retrieve the results. Memory management must be handled manually in C.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_26\n\nLANGUAGE: cpp\nCODE:\n```\nov_infer_request_t* infer_request = NULL;\nov_compiled_model_create_infer_request(compiled_model_ir, &infer_request);\n```\n\n----------------------------------------\n\nTITLE: Using torch.compile with OpenVINO (Python)\nDESCRIPTION: This code snippet demonstrates the basic usage of `torch.compile` with the `openvino` backend. It shows how to compile a PyTorch model for execution with OpenVINO. This approach works specifically for OpenVINO packages installed via pip.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n   ...\n   model = torch.compile(model, backend='openvino')\n   ...\n```\n\n----------------------------------------\n\nTITLE: 2D Convolution Example in OpenVINO XML\nDESCRIPTION: This XML snippet shows an example configuration for a 2D convolution layer in OpenVINO. It specifies the layer type, data attributes (dilations, pads_begin, pads_end, strides, auto_pad), and input/output port dimensions.  The 'explicit' auto_pad means that the pads_begin and pads_end values are used for padding.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/convolution-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"Convolution\" ...>\n    <data dilations=\"1,1\" pads_begin=\"2,2\" pads_end=\"2,2\" strides=\"1,1\" auto_pad=\"explicit\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>64</dim>\n            <dim>3</dim>\n            <dim>5</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Starting Asynchronous Inference in Python\nDESCRIPTION: This code starts asynchronous inference using `infer_request.start_async()` and waits for the results using `infer_request.wait()`. Asynchronous inference allows the application to perform other tasks while the inference is running.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ninfer_request.start_async()\ninfer_request.wait()\n```\n\n----------------------------------------\n\nTITLE: Dynamic Batch Inference with GPU Plugin in OpenVINO (C++)\nDESCRIPTION: This C++ code snippet illustrates dynamic batch processing within the OpenVINO GPU plugin. It involves configuring the plugin to handle a range of batch sizes by constructing multiple internal networks based on powers of 2, enabling efficient execution for varying batch sizes up to a specified maximum. Using this approach can improve the performance of handling dynamic batch sizes, though it can lead to increased memory usage and longer compilation times.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\n# The code snippet is not available in the provided text.\n```\n\n----------------------------------------\n\nTITLE: Callback Example (C++)\nDESCRIPTION: This code snippet demonstrates the structure of a callback function used in MatcherPass.  It shows how to access the nodes detected by the pattern using the Matcher object. The callback returns a boolean indicating whether the root node was replaced, preventing further pattern matching on the same node.  It's recommended to avoid manipulating nodes under the root node within the callback.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/matcher-pass.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// [pattern:callback_example]\n#include <openvino/core/node.hpp>\n#include <openvino/opsets/opset10.hpp>\n\n#include \"openvino/pass/graph_rewrite.hpp\"\n\nusing namespace std;\nusing namespace ov::opset10;\n\n{\n    ov::matcher_pass_callback callback = [=](ov::pass::Matcher& m) {\n        auto& pattern_to_output = m.get_pattern_value_map();\n        // ...\n        return false;\n    };\n}\n// [pattern:callback_example]\n```\n\n----------------------------------------\n\nTITLE: Setting Input Tensor in Python\nDESCRIPTION: This code creates an `ov::Tensor` and sets it as the input tensor for the inference request. It retrieves the input port by name and uses it to set the tensor.  `input_data` would be your input data array.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ninput_tensor = ov.Tensor(shape=input_shape, data=input_data)\ninfer_request.set_tensor(\"input_port_name\", input_tensor)\n```\n\n----------------------------------------\n\nTITLE: Setting Multi-threading Properties in Python OpenVINO\nDESCRIPTION: This snippet shows how to set the number of threads, scheduling core type, and enable hyper-threading for the CPU device in OpenVINO using Python. It configures inference requests to leverage CPU resources effectively by specifying the number of threads, core type preference and hyper-threading option. These properties control OpenVINO's CPU multi-threading behavior.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device/performance-hint-and-thread-scheduling.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# ! [ov:intel_cpu:multi_threading:part0]\nimport openvino.runtime as ov\n\ncore = ov.Core()\ncore.set_property(\"CPU\", ov.inference_num_threads(20))\ncore.set_property(\"CPU\", ov.hint.scheduling_core_type(ov.CoreType.ANY))\ncore.set_property(\"CPU\", ov.hint.enable_hyper_threading(True))\n# ! [ov:intel_cpu:multi_threading:part0]\n```\n\n----------------------------------------\n\nTITLE: Setting Layout of Input Tensor in TypeScript\nDESCRIPTION: This code snippet shows the `setLayout` method of the InputTensorInfo interface. This method allows setting the data layout of the input tensor. It accepts a string representing the layout as a parameter and returns an `InputTensorInfo` object for chaining.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InputTensorInfo.rst#_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nsetLayout(layout): InputTensorInfo\n```\n\n----------------------------------------\n\nTITLE: Wrap cl_mem (C)\nDESCRIPTION: This C snippet shows how to wrap an existing OpenCL memory object (cl_mem) into an OpenVINO RemoteTensor using the GPU plugin.  This allows the integration of existing OpenCL memory buffers with OpenVINO without requiring data copies. Requires OpenCL and OpenVINO libraries. The wrapped cl_mem object becomes a RemoteTensor for use in OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_21\n\nLANGUAGE: c\nCODE:\n```\n// example usage\n{\n    cl_mem mem;\n    size_t buffer_size;\n    // fill mem with data\n\n    auto remote_blob = context.create_tensor(buffer_size, mem);\n}\n```\n\n----------------------------------------\n\nTITLE: Linking OpenVINO Runtime using CMake's find_package (cmake)\nDESCRIPTION: This snippet demonstrates how to link static OpenVINO Runtime libraries using CMake's `find_package` command. It finds the OpenVINO package and links the `openvino::runtime` target to the application.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/static_libaries.md#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenVINO REQUIRED)\ntarget_link_libraries(<application> PRIVATE openvino::runtime)\n```\n\n----------------------------------------\n\nTITLE: Convert TensorFlow Lite Model using Python API\nDESCRIPTION: This snippet demonstrates how to convert a TensorFlow Lite model to OpenVINO IR format using the `openvino.convert_model` function. It requires the `openvino` package to be installed. The function takes the path to the `.tflite` model file as input and converts it to the OpenVINO IR.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow-lite.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino as ov\nov.convert_model('your_model_file.tflite')\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Distributed Training - TensorFlow 2\nDESCRIPTION: This code snippet shows how to configure NNCF for distributed multi-GPU training in TensorFlow 2. Calling `compression_ctrl.distributed()` before fine-tuning is essential to inform optimization methods about the distributed mode and enable necessary adjustments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncompression_ctrl.distributed()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO backend for compiled model\nDESCRIPTION: This code snippet shows how to set the torch.compile backend to OpenVINO when quantizing a model with PyTorch 2 export quantization. This directs the compilation process to use the OpenVINO backend for optimized execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\noptimized_model = torch.compile(converted_model, backend=\"openvino\")\n```\n\n----------------------------------------\n\nTITLE: Example Buffer Description\nDESCRIPTION: This shows an example of how the buffer description looks in a layer dump file. The description contains the shape of the buffer, the number of elements, and the original format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_debug_utils.md#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nshape: [b:1, f:1280, x:1, y:1, z:1, w:1, g:1] (count: 1280, original format: b_fs_yx_fsv16)\n```\n\n----------------------------------------\n\nTITLE: Converting PaddlePaddle Model File with OpenVINO (CLI)\nDESCRIPTION: This snippet shows how to convert a PaddlePaddle model file (``.pdmodel``) to OpenVINO IR using the `ovc` command-line tool. It assumes the OpenVINO environment is properly configured.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-paddle.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\novc your_model_file.pdmodel\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target CMake\nDESCRIPTION: Conditionally adds a clang-format target if the `ov_add_clang_format_target` command is defined.  This allows for easy code formatting using clang-format. The target is named `${TARGET_NAME}_clang` and is associated with the `${TARGET_NAME}` target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/utils/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(COMMAND ov_add_clang_format_target)\n    ov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Reading Model Asynchronously in OpenVINO (TypeScript)\nDESCRIPTION: This snippet presents the `readModel` function that asynchronously reads models from various formats like IR, ONNX, PDPD, TF, and TFLite. It accepts the model path and an optional weights path as parameters and returns a Promise that resolves to a Model object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nreadModel(modelPath, weightsPath?): Promise<Model>\n```\n\nLANGUAGE: typescript\nCODE:\n```\nweightsPath: string\n```\n\nLANGUAGE: typescript\nCODE:\n```\nreadModel(model, weights): Promise<Model>\n```\n\nLANGUAGE: typescript\nCODE:\n```\nreadModel(modelBuffer, weightsBuffer?): Promise<Model>\n```\n\nLANGUAGE: typescript\nCODE:\n```\nweightsBuffer: Uint8Array\n```\n\n----------------------------------------\n\nTITLE: Install specific OpenVINO components using vcpkg\nDESCRIPTION: This command installs specific components of OpenVINO, such as core, cpu, and ir, using vcpkg. It allows for a more selective installation based on project requirements.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-vcpkg.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nvcpkg install 'openvino[core,cpu,ir]'\n```\n\n----------------------------------------\n\nTITLE: Query HETERO Device Priorities (C++)\nDESCRIPTION: This snippet demonstrates how to query the `HETERO` device priority using `ov::Core::get_property` in C++.  It fetches the device priority and prints it to the console. Requires an initialized `ov::Core` object and a device name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nov::Core core;\nstd::string device_name = \"HETERO\";\nauto priority = core.get_property(device_name, ov::device::priorities.name());\nstd::cout << device_name << \" priority: \" << priority.as<std::string>() << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Torchvision Preprocessing Example in Python\nDESCRIPTION: This code snippet demonstrates the use of torchvision transforms like Compose, Normalize, and ToTensor within the OpenVINO environment. It shows how to integrate torchvision preprocessing into an OpenVINO model for image input preparation. The transforms adjust image dimensions and data types.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/torchvision-preprocessing-converter.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntransforms.Compose\ntransforms.Normalize\ntransforms.ConvertImageDtype\ntransforms.Grayscale\ntransforms.Pad\ntransforms.ToTensor\ntransforms.CenterCrop\ntransforms.Resize\n```\n\n----------------------------------------\n\nTITLE: Create RemoteContext from ID3D11Device (C)\nDESCRIPTION: This snippet demonstrates how to create an `ov::RemoteContext` from an existing Direct3D 11 device (`ID3D11Device`) using the GPU plugin's C API. It initializes the OpenVINO core and retrieves the extension. Requires the OpenVINO runtime and intel_gpu ocl headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_5\n\nLANGUAGE: c\nCODE:\n```\n//! [context_from_d3d_device]\n#include <openvino/runtime.h>\n#include <openvino/runtime/intel_gpu/ocl/dx.h>\n\n#if defined(_WIN32)\n#include <d3d11.h>\n#endif\n\nvoid create_context_from_d3d_device(ID3D11Device* device) {\n    // context from user-defined d3d device\n    ov_core_t* core = ov_core_create();\n    ov_remote_context remote_context = ov_core_get_extension_dx(core, device);\n    ov_remote_context_free(remote_context);\n    ov_core_free(core);\n}\n//! [context_from_d3d_device]\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Setup C API\nDESCRIPTION: This section lists C API calls used for preprocessing input data, including creating a preprocessor, setting input/output information, and building the preprocessor. This allows for automatic resizing and layout conversion.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/c/hello_classification/README.md#_snippet_5\n\nLANGUAGE: C\nCODE:\n```\n``ov_preprocess_prepostprocessor_create``,\n``ov_preprocess_prepostprocessor_get_input_info_by_index``,\n``ov_preprocess_input_info_get_tensor_info``,\n``ov_preprocess_input_tensor_info_set_from``,\n``ov_preprocess_input_tensor_info_set_layout``,\n``ov_preprocess_input_info_get_preprocess_steps``,\n``ov_preprocess_preprocess_steps_resize``,\n``ov_preprocess_input_model_info_set_layout``,\n``ov_preprocess_output_set_element_type``,\n``ov_preprocess_prepostprocessor_build``\n```\n\n----------------------------------------\n\nTITLE: Enable Performance Hints in C++\nDESCRIPTION: This snippet showcases enabling performance hints in C++ using OpenVINO. It references external code in `docs/articles_en/assets/snippets/AUTO3.cpp` focusing on `[part3]` for implementation details.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\ndocs/articles_en/assets/snippets/AUTO3.cpp\n```\n\n----------------------------------------\n\nTITLE: Creating a simple model and pattern with WrapType\nDESCRIPTION: This C++ snippet creates a simple OpenVINO model and a pattern using WrapType for the Relu node. WrapType allows for more flexible pattern matching by wrapping specific node types.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nshared_ptr<ov::Model> create_simple_model_and_pattern_wrap_type() {\n    // Model creation\n    ov::Shape shape = {1, 3, 224, 224};\n    auto parameter_node = opset13::Parameter::create(ov::element::f32, shape);\n    parameter_node->set_friendly_name(\"input\");\n\n    auto add_node = make_shared<opset13::Add>(parameter_node, opset13::Constant::create(ov::element::f32, ov::Shape{}, {1.0f}));\n    auto relu_node = make_shared<opset13::Relu>(add_node);\n    auto result_node = make_shared<opset13::Result>(relu_node);\n    result_node->set_friendly_name(\"result\");\n\n    auto model = make_shared<ov::Model>(ov::ResultVector{result_node}, ov::ParameterVector{parameter_node});\n\n    return model;\n}\n```\n\n----------------------------------------\n\nTITLE: Compile ONNX model using compile_model in C++\nDESCRIPTION: This snippet demonstrates how to compile an ONNX model using the `compile_model()` method in C++. The model is compiled for execution on the specified device (AUTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\nov::CompiledModel compiled_model = core.compile_model(\"<INPUT_MODEL>.onnx\", \"AUTO\");\n```\n\n----------------------------------------\n\nTITLE: Setting Verbose Mode\nDESCRIPTION: This environment variable enables verbose mode, providing more detailed output during execution. This helps in understanding the internal workings of the OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nOV_CPU_VERBOSE=1\n```\n\n----------------------------------------\n\nTITLE: Serialize Execution Graph OpenVINO (sh)\nDESCRIPTION: This snippet shows how to serialize the execution graph using the OV_CPU_EXEC_GRAPH_PATH environment variable. It demonstrates the basic syntax for specifying the serialization option, which can be either 'cout' for console output or a file path for XML and binary files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/graph_serialization.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_EXEC_GRAPH_PATH=<option> binary ...\n```\n\n----------------------------------------\n\nTITLE: Convert MetaGraph Model (TF1) using Python\nDESCRIPTION: This snippet demonstrates how to convert a TensorFlow 1 MetaGraph model to OpenVINO IR using `openvino.convert_model` in Python. The input is the path to the ``.meta`` file. The output is the OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_10\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model('path_to_meta_graph.meta')\n```\n\n----------------------------------------\n\nTITLE: Synchronous Inference C++\nDESCRIPTION: Performs synchronous inference using the OpenVINO API. This method executes the inference request and blocks until the results are available, providing a straightforward way to perform inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_classification/README.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n``ov::InferRequest::infer``\n```\n\n----------------------------------------\n\nTITLE: Configuring and building OpenVINO with CMake\nDESCRIPTION: These commands use CMake to configure and build the OpenVINO project. The first command generates Unix makefiles, and the second command initiates the build process using the generated makefiles. The `-DCMAKE_BUILD_TYPE=Release` flag specifies a release build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_linux.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_BUILD_TYPE=Release ..\ncmake --build . --parallel\n```\n\n----------------------------------------\n\nTITLE: Running a Python Sample (Windows)\nDESCRIPTION: This command executes a Python sample. It takes the path to the Python file, the model, the input media, and the target device as arguments. This is for Windows.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_24\n\nLANGUAGE: bat\nCODE:\n```\npython <sample.py file> -m <path_to_model> -i <path_to_media> -d <target_device>\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Compressed LLM with OpenVINO and Transformers\nDESCRIPTION: This code snippet shows how to save a compressed large language model using `save_pretrained` method and load it back later. It also saves and loads the tokenizer, ensuring that the same tokenization is used before and after compression. The `OVModelForCausalLM` class is used for loading the compressed model into OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Save compressed model for faster loading later\nmodel.save_pretrained(\"Phi-3.5-mini-instruct-int4-sym-ov\")\ntokenizer.save_pretrained(\"Phi-3.5-mini-instruct-int4-sym-ov\")\n\n# Load a saved model\nmodel = OVModelForCausalLM.from_pretrained(\"Phi-3.5-mini-instruct-int4-sym-ov\")\ntokenizer = AutoTokenizer.from_pretrained(\"Phi-3.5-mini-instruct-int4-sym-ov\")\n```\n\n----------------------------------------\n\nTITLE: Check BFloat16 Support - C++\nDESCRIPTION: This C++ code snippet demonstrates how to query device capabilities to check for BF16 support on the CPU using `ov::device::capabilities`. It retrieves the device properties and checks if 'BF16' is present in the capabilities list, indicating that the CPU supports BFloat16. Dependency: OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// [part0]\nstd::string cpu_device_name = \"CPU\";\noc::Core core;\nif (std::find(core.get_available_devices().begin(), core.get_available_devices().end(), cpu_device_name) != core.get_available_devices().end()) {\n    auto device_capabilities = core.get_property(cpu_device_name, ov::device::capabilities.name()).as<std::vector<std::string>>();\n    if (std::find(device_capabilities.begin(), device_capabilities.end(), std::string(\"BF16\")) != device_capabilities.end()) {\n        std::cout << \"CPU device supports BF16\\n\";\n    }\n}\n// [part0]\n```\n\n----------------------------------------\n\nTITLE: Dump Output Tensors from CPU Plugin\nDESCRIPTION: Dumps each output tensor from the CPU plugin. Replace `/path/to/model` with the path to the model file, and `dump1` with the desired name for the dump file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/dump_check/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 dump_check.py -m=/path/to/model dump1\n```\n\n----------------------------------------\n\nTITLE: Creating a simple model and pattern\nDESCRIPTION: This snippet demonstrates how to create a simple OpenVINO model with Add, Relu, and Result operations and a corresponding pattern using the same operations. It initializes nodes and defines the model structure, enabling pattern matching within the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef create_simple_model_and_pattern():\n    # Model creation\n    shape = [1, 3, 224, 224]\n    parameter_node = parameter(shape, dtype=np.float32, name=\"input\")\n    add_node = add(parameter_node, constant(1.0, dtype=np.float32))\n    relu_node = relu(add_node)\n    result_node = result(relu_node, name=\"result\")\n\n    model = Model([result_node], [parameter_node])\n\n    # Pattern creation\n    parameter_node_pattern = parameter(shape, dtype=np.float32, name=\"input\")\n    add_node_pattern = add(parameter_node_pattern, constant(1.0, dtype=np.float32))\n    relu_node_pattern = relu(add_node_pattern)\n\n    pattern = [parameter_node_pattern, add_node_pattern, relu_node_pattern]\n\n    return model, pattern\n```\n\n----------------------------------------\n\nTITLE: Loading Encrypted Model with Empty Weights C++\nDESCRIPTION: This C++ code snippet illustrates how to load an ONNX model when external weights are not available in memory. The `ov::Core::read_model` method is called with the model path and an empty `ov::Tensor` for the weights. This allows the model structure to be loaded without requiring the weights to be immediately available.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-security.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n//! [part1]\n#include <openvino/runtime/core.hpp>\n#include <openvino/runtime/tensor.hpp>\n\n// Create an OpenVINO runtime Core\nov::Core core;\n// Create an empty tensor for weights\nov::Tensor empty_weights;\nstd::shared_ptr<ov::Model> model = core.read_model(\"path_to_your_model.onnx\", empty_weights);\n//! [part1]\n```\n\n----------------------------------------\n\nTITLE: Running Background Removal Sample in Node.js\nDESCRIPTION: This command executes the `vision_background_removal.js` script to remove the background from a foreground image and replace it with a specified background image. It requires paths to the model, foreground image, background image, and the device to run inference on.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/js/node/vision_background_removal/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnode vision_background_removal.js ../../assets/models/unet_ir_model.xml ../../assets/images/coco_hollywood.jpg ../../assets/images/wall.jpg AUTO\n```\n\n----------------------------------------\n\nTITLE: Getting Available Devices\nDESCRIPTION: Retrieves a list of available inference devices. This method queries all registered plugins to determine the available devices and returns a string array of device names.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\ngetAvailableDevices(): string[]\n```\n\n----------------------------------------\n\nTITLE: OpenCL Context Sharing (User-Supplied) with OpenVINO (C++)\nDESCRIPTION: Demonstrates how to run OpenVINO inference within a user-supplied shared OpenCL context. This example creates an OpenVINO context from a user-provided OpenCL context handle. Requires OpenVINO runtime and OpenCL.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_43\n\nLANGUAGE: cpp\nCODE:\n```\n//! [context_sharing_user_handle]\n// Create OpenVINO context from user handle.\nov::AnyMap properties;\nproperties.insert({ov::intel_gpu::context_property(handle), context});\nproperties.insert({ov::intel_gpu::context_property(user_queue), true});\nauto remote_context = core.create_context(device_name, properties);\n// Compile model.\nauto compiled_model = core.compile_model(model, remote_context);\n\n// Create tensor from user handle.\nov::AnyMap tensor_properties;\ntensor_properties.insert({ov::intel_gpu::context_property(handle), buffer});\nauto remote_tensor = remote_context.create_tensor(element_type, shape, tensor_properties);\n// Set tensor to infer request and run inference.\ninfer_request.set_tensor(input_tensor_name, remote_tensor);\ninfer_request.infer();\n//! [context_sharing_user_handle]\n```\n\n----------------------------------------\n\nTITLE: Querying Target Runtime Devices in C++\nDESCRIPTION: This C++ snippet demonstrates how to query the runtime target devices on which inferences are being executed with the AUTO plugin. The code can be found in `docs/articles_en/assets/snippets/AUTO7.cpp`, focusing on section `[part7]`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\ndocs/articles_en/assets/snippets/AUTO7.cpp\n```\n\n----------------------------------------\n\nTITLE: Disabling Auto Batching with compile_model in C++\nDESCRIPTION: This C++ code snippet shows how to disable Auto-Batching by setting the `ov::hint::allow_auto_batching` configuration option to `false`. This prevents automatic batching from being enabled even when the `ov::hint::performance_mode` is set to `ov::hint::PerformanceMode::THROUGHPUT`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"GPU\", {{\nov::hint::performance_mode(ov::hint::PerformanceMode::THROUGHPUT),\nov::hint::allow_auto_batching(false)\n}});\n```\n\n----------------------------------------\n\nTITLE: Loading Extensions in Python\nDESCRIPTION: This snippet demonstrates how to load extensions into the OpenVINO Core object using Python. It adds both the custom operation class and a mapping extension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncore = ov.Core()\n# [add_extension]\ncore.add_extension(Identity().get_opset().name, Identity)\n# [add_extension]\n\n# [add_frontend_extension]\ncore.add_extension(TemplateFrontend()\n# [add_frontend_extension]\n```\n\n----------------------------------------\n\nTITLE: Compile Model for CPU with Auto Device Selection (C++)\nDESCRIPTION: This C++ snippet demonstrates compiling a model for CPU using automatic device selection. The `compile_model` function is used with the \"AUTO\" device name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nov::CompiledModel compiled_model = core.compile_model(\"model.xml\", \"AUTO\");\n```\n\n----------------------------------------\n\nTITLE: Set Manual Affinities in Heterogeneous Execution with OpenVINO (C++)\nDESCRIPTION: This snippet shows how to manually set operation affinities within an OpenVINO model using C++. It iterates through the model's operations, assigning specific operations to the 'GPU' and others to the 'CPU'. This requires the OpenVINO runtime library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/hetero-execution.rst#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"path_to_your_model.xml\");\n\n    // [set_manual_affinities]\n    for (const auto& node : model->get_ops()) {\n        std::string layer_name = node->get_friendly_name();\n        if (layer_name.find(\"fully_connected\") != std::string::npos) {\n            node->get_rt_info()[\"affinity\"] = \"GPU\";\n        } else {\n            node->get_rt_info()[\"affinity\"] = \"CPU\";\n        }\n    }\n    // [set_manual_affinities]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Batching via BATCH Plugin Python\nDESCRIPTION: This Python snippet demonstrates how to enable batching by using the BATCH plugin with `ov::Core::compile_model()`.  It utilizes the `compile_model_batch_plugin` fragment from the specified Python file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n# [compile_model_batch_plugin]\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model=model, device_name=\"BATCH:GPU\")\n# [compile_model_batch_plugin]\n```\n\n----------------------------------------\n\nTITLE: Adding a New pybind11 Submodule\nDESCRIPTION: This C++ code snippet shows how to add a new submodule named `mymodule` to the main OpenVINO™ module using pybind11. The `m.def_submodule` function registers the submodule with a docstring. This is the starting point for exposing C++ functionalities through the Python API.  The `m` variable represents the main OpenVINO™ module.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\npy::module mymodule = m.def_submodule(\"mymodule\", \"My first feature - openvino.runtime.mymodule\");\n```\n\n----------------------------------------\n\nTITLE: Wrap NT Handle with RemoteTensor - OpenVINO™ C++\nDESCRIPTION: This code snippet shows how to wrap an existing NT handle (Windows) with an OpenVINO™ RemoteTensor. The RemoteTensor then allows the OpenVINO™ runtime to access and manipulate the memory associated with the NT handle. The code requires a valid NT handle to be passed in.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/npu-device/remote-tensor-api-npu-plugin.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nvoid* nt_handle = get_native_handle(); // Acquire NT handle\nsize_t tensor_size = get_tensor_size(); // Get buffer size for the tensor\nauto remote_tensor = remote_context.create_tensor(remote_context.get_params(), tensor_size, nt_handle);\n```\n\n----------------------------------------\n\nTITLE: Configuring Fallback Devices with Heterogeneous Execution in C++\nDESCRIPTION: This C++ snippet demonstrates configuring fallback devices using the OpenVINO Heterogeneous execution mode. The GPU device is set to enable profiling data and uses the default execution precision, while the CPU device is configured to perform inference in fp32. The devices are specified by priority.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/hetero-execution.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nvoid configure_fallback_devices() {\n    // ! [configure_fallback_devices]\n    ov::Core core;\n    std::string model_path = \"model.xml\";\n    std::vector<std::string> device_priorities = {\"GPU\", \"CPU\"};\n    ov::AnyMap config;\n    config[\"GPU.enable_profiling\"] = \"true\";\n    config[\"CPU.hint.inference_precision\"] = \"fp32\";\n    ov::CompiledModel compiled_model = core.compile_model(model_path, device_priorities, config);\n    // ! [configure_fallback_devices]\n}\n```\n\n----------------------------------------\n\nTITLE: Run Quantization with Accuracy Control - OpenVINO\nDESCRIPTION: This snippet demonstrates how to perform quantization with accuracy control for an OpenVINO model using `nncf.quantize_with_accuracy_control()`. It requires a model, calibration dataset, validation dataset, and validation function.  The `max_drop` parameter defines the maximum acceptable accuracy drop.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/quantizing-with-accuracy-control.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# [quantization]\nimport nncf\nfrom openvino.runtime import Core\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\n\n\ndef dataset_fn(model):\n    # actual dataset\n    return None\n\n\ndef validate(compiled_model, validation_dataset):\n    # actual validation\n    return 0.5\n\n\ncalibration_dataset = dataset_fn(model)\nvalidation_dataset = dataset_fn(model)\n\nquantized_model = nncf.quantize_with_accuracy_control(model=model,\n                                                       calibration_dataset=calibration_dataset,\n                                                       validation_dataset=validation_dataset,\n                                                       validation_fn=validate,\n                                                       max_drop=0.01)\n# [quantization]\n```\n\n----------------------------------------\n\nTITLE: Model Quantization using NNCF\nDESCRIPTION: This code snippet demonstrates how to quantize a model using the Neural Network Compression Framework (NNCF). It involves importing necessary libraries, creating a calibration dataset, and using the `nncf.quantize` function to create a quantized model. The quantized model is then compiled with the OpenVINO backend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport nncf\nimport openvino.torch\nimport torch\n\ncalibration_loader = torch.utils.data.DataLoader(...)\n\ndef transform_fn(data_item):\n    images, _ = data_item\n    return images\ncalibration_dataset = nncf.Dataset(calibration_loader, transform_fn)\n# Model quantization\nquantized_model = nncf.quantize(model, calibration_dataset)\n\nquantized_model = torch.compile(quantized_model, backend=\"openvino\")\n```\n\n----------------------------------------\n\nTITLE: Installing Python Requirements for OpenVINO™ API\nDESCRIPTION: These shell commands clone the OpenVINO repository, update submodules, and install the required Python packages using pip.  It installs requirements for the core bindings, testing, and, if `-DENABLE_WHEEL=ON` is used, wheel development.  Requires git and pip to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/build.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone https://github.com/openvinotoolkit/openvino.git\ncd openvino\ngit submodule update --init --recursive\npip install -r src/bindings/python/requirements.txt\npip install -r src/bindings/python/requirements_test.txt\n```\n\nLANGUAGE: Shell\nCODE:\n```\npip install -r src/bindings/python/wheel/requirements-dev.txt\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Model using torch.export\nDESCRIPTION: This code demonstrates converting a PyTorch model to OpenVINO using `torch.export`. It creates a ResNet50 model, exports it using `torch.export`, and then converts the exported model to OpenVINO using `openvino.convert_model`. The snippet requires `torchvision`, `torch` and `openvino` libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-pytorch.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torch.export import export\nfrom openvino import convert_model\n\nmodel = resnet50(weights=ResNet50_Weights.DEFAULT)\nmodel.eval()\nexported_model = export(model, (torch.randn(1, 3, 224, 224),))\nov_model = convert_model(exported_model)\n```\n\n----------------------------------------\n\nTITLE: Set Pipeline Parallelism for Heterogeneous Execution with OpenVINO (Python)\nDESCRIPTION: This snippet demonstrates setting pipeline parallelism for heterogeneous execution in OpenVINO using Python.  It sets the model distribution policy to PIPELINE_PARALLEL and specifies the device priorities. This requires the openvino package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/hetero-execution.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom openvino.runtime import Core, properties\n\ncore = Core()\nmodel = core.read_model(\"path_to_your_model.xml\")\n\n# [set_pipeline_parallelism]\ncompiled_model = core.compile_model(model, \"HETERO:dGPU,iGPU,CPU\",\n                                       config={\"HINT_MODEL_DISTRIBUTION_POLICY\": \"PIPELINE_PARALLEL\"})\n# The same, but with API\ncompiled_model = core.compile_model(model, \"HETERO:dGPU,iGPU,CPU\",\n                                       config=properties.hint.model_distribution_policy(\"PIPELINE_PARALLEL\"))\n# [set_pipeline_parallelism]\n```\n\n----------------------------------------\n\nTITLE: NV12 Preprocessing with Single Plane (Multiple Batches) in OpenVINO (C++)\nDESCRIPTION: Shows how to process NV12 data with a single plane in multiple batches using OpenVINO. This code snippet illustrates handling multiple NV12 images within a batch for inference. Requires OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_39\n\nLANGUAGE: cpp\nCODE:\n```\n//! [batched_case]\nstd::vector<ov::Tensor> input_tensors;\nfor (size_t i = 0; i < batch_size; ++i) {\n    input_tensors.emplace_back(ov::element::u8, {height + height / 2, width}, input_data + i * height * width * 3 / 2);\n}\ninfer_request.set_tensor(input_port, input_tensors);\n//! [batched_case]\n```\n\n----------------------------------------\n\nTITLE: Convert MobileNet Model with Static Input Shape in Python\nDESCRIPTION: This Python code snippet demonstrates how to convert a TensorFlow MobileNet model using `openvino.convert_model` and specify a static input shape of [2, 300, 300, 3] for the input layer. It imports the `openvino` library and then calls `ov.convert_model` with the model file name and the desired input shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/setting-input-shapes.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model(\"MobileNet.pb\", input=[2, 300, 300, 3])\n```\n\n----------------------------------------\n\nTITLE: Defining Complex Layout with ov::Layout in Python\nDESCRIPTION: This snippet demonstrates defining a complex layout in Python using `ov::Layout` with square brackets, assigning descriptive names to each dimension (N, C, H, W). This syntax allows for more readable and descriptive dimension labeling.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlayout = ov.Layout(\"[N, C, H, W]\")\n```\n\n----------------------------------------\n\nTITLE: Get default RemoteContext from CompiledModel (C)\nDESCRIPTION: This snippet shows how to retrieve the default `ov::RemoteContext` from a compiled OpenVINO model (`ov_compiled_model_t`) using the GPU plugin's C API. Requires the OpenVINO runtime and intel_gpu ocl headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_11\n\nLANGUAGE: c\nCODE:\n```\n//! [default_context_from_model]\n#include <openvino/runtime.h>\n#include <openvino/runtime/intel_gpu/ocl/ocl.h>\n\nvoid get_default_context_from_model(ov_compiled_model_t* model) {\n    // default context from compiled model\n    ov_remote_context remote_context = ov_compiled_model_get_context(model);\n    ov_remote_context_free(remote_context);\n}\n//! [default_context_from_model]\n```\n\n----------------------------------------\n\nTITLE: Convert Keras H5 with Custom Layer (TF2) to SavedModel and Save\nDESCRIPTION: This snippet shows how to load a Keras H5 model with a custom layer in TensorFlow 2, specify the custom layer object, and then save it in the SavedModel format using the `tf.saved_model.save` function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_3\n\nLANGUAGE: py\nCODE:\n```\nimport tensorflow as tf\nfrom custom_layer import CustomLayer\nmodel = tf.keras.models.load_model('model.h5', custom_objects={'CustomLayer': CustomLayer})\ntf.saved_model.save(model,'model')\n```\n\n----------------------------------------\n\nTITLE: \"Postponed Return\" Inference in OpenVINO (Python)\nDESCRIPTION: Demonstrates how to avoid the overhead of `OVDict` return from synchronous calls by using the `compiled_model.infer()` method without assigning the result. This is useful when only a part of the output data is needed, data is not required immediately, or data return is not needed at all.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-advanced-inference.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncompiled_model({\"input_tensor_name_1\": input_tensor_1, \"input_tensor_name_2\": input_tensor_2})\n# Data can be extracted later from InferRequest via .get_tensor()\n```\n\n----------------------------------------\n\nTITLE: TensorFlow: Run Inference\nDESCRIPTION: Executes inference on the quantized TensorFlow model. The sample input is passed through the quantized model to get the inference results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nresults = quantized_model(input_tensor)\nprint(results)\n\n```\n\n----------------------------------------\n\nTITLE: Adding OpenVINO APT Repository to Snap\nDESCRIPTION: This snippet shows how to add the OpenVINO APT repository to the snap's snapcraft.yaml file using the `package-repositories` keyword. It specifies the repository type, components, suites, key ID, and URL.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/integrate-openvino-with-ubuntu-snap.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\npackage-repositories:\n  - type: apt\n   components: [main]\n   suites: [ubuntu20]\n   key-id: E9BF0AFC46D6E8B7DA5882F1BAC6F0C353D04109\n   url: https://apt.repos.intel.com/openvino/2024\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Environment Variables on Linux\nDESCRIPTION: This script sets up the OpenVINO environment variables on a Linux system. It sources the `setupvars.sh` script located in the OpenVINO installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\nsource  <INSTALL_DIR>/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: Convert Torchvision Model to OpenVINO IR (Python)\nDESCRIPTION: This code snippet demonstrates how to convert a ResNet50 model from Torchvision to OpenVINO IR using the `openvino.convert_model` function. It loads the model, prepares sample input data, converts the model, and then either saves it to OpenVINO IR format or compiles and runs inference directly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nimport torch\nfrom torchvision.models import resnet50\n\nmodel = resnet50(weights='DEFAULT')\n\n# prepare input_data\ninput_data = torch.rand(1, 3, 224, 224)\n\nov_model = ov.convert_model(model, example_input=input_data)\n\n###### Option 1: Save to OpenVINO IR:\n\n# save model to OpenVINO IR for later use\nov.save_model(ov_model, 'model.xml')\n\n###### Option 2: Compile and infer with OpenVINO:\n\n# compile model\ncompiled_model = ov.compile_model(ov_model)\n\n# run inference\nresult = compiled_model(input_data)\n```\n\n----------------------------------------\n\nTITLE: Verify OpenVINO Installation\nDESCRIPTION: This command executes a Python script that imports the `Core` class from the `openvino` module and prints the available devices. If the installation was successful, the list of available devices will be displayed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-pip.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npython -c \"from openvino import Core; print(Core().available_devices)\"\n```\n\n----------------------------------------\n\nTITLE: Process Inference Results in C++\nDESCRIPTION: This C++ snippet shows how to obtain and process the inference results using the OpenVINO C++ API, typically by accessing output tensors from the `InferRequest` object after performing the inference. This assumes the `InferRequest` has already been created and inference has been run.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_34\n\nLANGUAGE: cpp\nCODE:\n```\n//! [part6]\nInferRequest infer_request = executable_network.create_infer_request();\n\ninfer_request.infer();\n\nauto output_tensor = infer_request.get_output_tensor(0);\nauto output_shape = output_tensor.get_shape();\n//! [part6]\n```\n\n----------------------------------------\n\nTITLE: start_pipeline() Method C++\nDESCRIPTION: This code snippet shows the implementation of the `start_pipeline()` method. It executes a pipeline synchronously using the `m_executable` object, initiating the actual computation within the backend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/synch-inference-request.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nvoid InferRequest::start_pipeline() {\n    // Executes a pipeline synchronously using m_executable object\n    m_executable->run();\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Inference Flow C API\nDESCRIPTION: This snippet outlines the core OpenVINO C API calls required for basic inference: creating a core object, reading and compiling a model, creating an inference request, configuring input tensors, and retrieving output tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/c/hello_classification/README.md#_snippet_1\n\nLANGUAGE: C\nCODE:\n```\n``ov_core_create``,\n``ov_core_read_model``,\n``ov_core_compile_model``,\n``ov_compiled_model_create_infer_request``,\n``ov_infer_request_set_input_tensor_by_index``,\n``ov_infer_request_get_output_tensor_by_index``\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies on Linux using pip\nDESCRIPTION: This snippet installs the necessary dependencies for using OpenVINO GenAI on a Linux system. It creates a virtual environment, activates it, and then installs the required packages including nncf, onnx, optimum-intel, openvino, openvino-tokenizers, and openvino-genai.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_0\n\nLANGUAGE: Console\nCODE:\n```\npython3 -m venv npu-env\nnpu-env/bin/activate\npip install  nncf==2.14.1 onnx==1.17.0 optimum-intel==1.22.0\npip install openvino==2025.1 openvino-tokenizers==2025.1 openvino-genai==2025.1\n```\n\n----------------------------------------\n\nTITLE: Export LLM to OpenVINO with Full Precision (fp16)\nDESCRIPTION: This example exports the Llama-2-7b-chat-hf LLM to OpenVINO IR format with fp16 precision, using the `optimum-cli export openvino` command.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/genai-model-preparation.rst#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\noptimum-cli export openvino --model meta-llama/Llama-2-7b-chat-hf --weight-format fp16 ov_llama_2\n```\n\n----------------------------------------\n\nTITLE: Install Optimum Intel dependencies\nDESCRIPTION: Installs the necessary dependencies for Optimum Intel, including OpenVINO and NNCF. This command uses pip to install the optimum package with the openvino and nncf extras.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npip install optimum[openvino,nncf]\n```\n\n----------------------------------------\n\nTITLE: Querying Target Runtime Devices in Python\nDESCRIPTION: This Python snippet illustrates how to query the runtime target devices used by AUTO for inference. The code snippet is within `docs/articles_en/assets/snippets/ov_auto.py`, section `[part7]`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndocs/articles_en/assets/snippets/ov_auto.py\n```\n\n----------------------------------------\n\nTITLE: Convert Flax Module to OpenVINO Model in Python\nDESCRIPTION: This snippet showcases how to convert a Flax `Module` object to an OpenVINO model using the `convert_model()` method in Python.  A simple Dense Flax module is defined and an instance is created and passed to the convert_model. The model is then compiled using `compile_model()` for execution on the specified device (AUTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_25\n\nLANGUAGE: py\nCODE:\n```\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nimport openvino as ov\n\n# let user have some Flax module\nclass SimpleDenseModule(nn.Module):\n    features: int\n\n    @nn.compact\n    def __call__(self, x):\n\n```\n\n----------------------------------------\n\nTITLE: Model Creation in C++ (OpenVINO)\nDESCRIPTION: This C++ snippet, referenced by `samples/cpp/model_creation_sample/main.cpp`, demonstrates how to create a model from weights and perform inference using the OpenVINO Runtime. It requires the OpenVINO library and a LeNet model weights file (`.bin`). The application takes the path to the weights file and the device name as command-line arguments and outputs inference results to the console.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/model-creation.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n.. doxygensnippet:: samples/cpp/model_creation_sample/main.cpp\n   :language: cpp\n```\n\n----------------------------------------\n\nTITLE: Reshape by Input Port (Object) - C++\nDESCRIPTION: This C++ snippet shows how to reshape an OpenVINO model by specifying the input port using an `ov::Output<ov::Node>` object.  The `reshape` method is called with a map where the key is the input port and the value is the new `ov::PartialShape`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/changing-input-shape.rst#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\n\n    // ! [obj_to_shape]\n    ov::Output<ov::Node> input_port = model->input(0); // Get the input port object\n    ov::PartialShape new_shape = {1, 3, 256, 256};\n    model->reshape({{input_port, new_shape}});\n    // ! [obj_to_shape]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Importing and Using New Module\nDESCRIPTION: This code snippet shows how to import and use the newly created Python module `openvino.mymodule`. It creates an instance of `MyClass` from that module and calls its `say_hello` method, demonstrating basic usage of the extended API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.mymodule as ov_module\n\nobj = ov_module.MyClass()\nobj.say_hello()\n>>> \"Hello! Let's work on OV together!\"\n```\n\n----------------------------------------\n\nTITLE: Load and Compress to INT4 with custom config\nDESCRIPTION: Loads a model with INT4 weight quantization using OVWeightQuantizationConfig.  Demonstrates loading from Hugging Face and pre-converted IR, as well as customizing quantization parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel import OVModelForCausalLM, OVWeightQuantizationConfig\nimport nncf\n\nmodel = OVModelForCausalLM.from_pretrained(\n    model_id,\n    export=True,\n    quantization_config=OVWeightQuantizationConfig(bits=4),\n)\n\n# or if the model has been already converted\nmodel = OVModelForCausalLM.from_pretrained(\n    model_path,\n    quantization_config=OVWeightQuantizationConfig(bits=4),\n)\n\n# use custom parameters for weight quantization\nmodel = OVModelForCausalLM.from_pretrained(\n    model_path,\n    quantization_config=OVWeightQuantizationConfig(bits=4, asym=True, ratio=0.8, dataset=\"ptb\"),\n)\n\n# save the model after optimization\nmodel.save_pretrained(optimized_model_path)\n```\n\n----------------------------------------\n\nTITLE: CMake Integration for C\nDESCRIPTION: This CMake snippet is designed for C applications using OpenVINO. It locates the OpenVINO package, specifies include directories for header files, and links the necessary OpenVINO libraries to the executable. It requires the OpenVINO environment variables to be configured.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_40\n\nLANGUAGE: cpp\nCODE:\n```\n#! [cmake:integration_example_c]\ncmake_minimum_required(VERSION 3.13)\nproject(openvino_example C)\n\nfind_package(OpenVINO REQUIRED)\n\nadd_executable(${PROJECT_NAME} main.c)\ntarget_include_directories(${PROJECT_NAME} PUBLIC ${OpenVINO_INCLUDE_DIRS})\ntarget_link_libraries(${PROJECT_NAME} ${OpenVINO_LIBRARIES})\n#! [cmake:integration_example_c]\n```\n\n----------------------------------------\n\nTITLE: Define Model Inputs Property (TypeScript)\nDESCRIPTION: This code defines the `inputs` property of the `Model` interface as an array of `Output` objects.  This property provides access to the input tensors of the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ninputs: Output[]\n```\n\n----------------------------------------\n\nTITLE: Converting paddle.hapi.model.Model to OpenVINO IR\nDESCRIPTION: This code converts a PaddlePaddle model of type `paddle.hapi.model.Model` to OpenVINO IR format. It requires the `paddlepaddle` and `openvino` packages. It defines input specifications for the model before conversion and saves the resulting OpenVINO IR.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-paddle.rst#_snippet_4\n\nLANGUAGE: py\nCODE:\n```\nimport paddle\nimport openvino as ov\n\n# create a paddle.hapi.model.Model format model\nresnet50 = paddle.vision.models.resnet50()\nx = paddle.static.InputSpec([1,3,224,224], 'float32', 'x')\ny = paddle.static.InputSpec([1,1000], 'float32', 'y')\n\nmodel = paddle.Model(resnet50, x, y)\n\n# convert to OpenVINO IR format\nov_model = ov.convert_model(model)\n\nov.save_model(ov_model, \"resnet50.xml\")\n```\n\n----------------------------------------\n\nTITLE: Integrating Preprocessing Steps into the Model (C++)\nDESCRIPTION: This snippet demonstrates how to build the model after defining the preprocessing steps and retrieve the final model in C++ using `ppp.build()`. The built model will automatically incorporate the specified preprocessing steps into its execution graph. It shows the PrePostProcessor configuration for debugging purposes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nppp.input().model().set_layout(\"NCHW\");\nmodel = ppp.build();\nstd::cout << ppp << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Set Core Property Then Compile (C++)\nDESCRIPTION: This snippet demonstrates how to set a global property using `ov::Core::set_property` before compiling a model in C++. The example sets a global inference precision hint for the `CPU` device. Requires an initialized `ov::Core` object and a valid model path.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nov::Core core;\nstd::string model_path = \"path_to_model.xml\";\nstd::string device_name = \"CPU\";\ncore.set_property(device_name, ov::hint::inference_precision(ov::element::f32));\nauto model = core.read_model(model_path);\nauto compiled_model = core.compile_model(model, device_name);\n```\n\n----------------------------------------\n\nTITLE: Compiling Model on Specific GPU C++\nDESCRIPTION: This C++ snippet demonstrates how to compile a model on a specific GPU device (identified by ID) using `ov::Core::compile_model()`. It utilizes the `compile_model_gpu_with_id` fragment from the specified C++ file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n// [compile_model_gpu_with_id]\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"GPU.1\");\n// [compile_model_gpu_with_id]\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCL ICD for GPU on Linux\nDESCRIPTION: This command installs the OpenCL ICD (Installable Client Driver) on Linux, which is required to use a GPU device for OpenVINO inference.  It is necessary to enable OpenVINO to communicate with the GPU.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conda.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nconda install ocl-icd-system\n```\n\n----------------------------------------\n\nTITLE: Get default RemoteContext from Core (C++)\nDESCRIPTION: This snippet shows how to retrieve the default `ov::RemoteContext` from the OpenVINO `ov::Core` object using the GPU plugin's C++ API. This is useful when you want to use the plugin's internal context. Requires the OpenVINO runtime and intel_gpu ocl headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n//! [default_context_from_core]\n#include <openvino/runtime/core.hpp>\n#include <openvino/runtime/intel_gpu/ocl/ocl.hpp>\n\nvoid get_default_context_from_core() {\n    // default context from plugin\n    ov::Core core;\n    auto remote_context = core.get_default_context(\"GPU\").as<ov::intel_gpu::ocl::ocl_context>();\n    (void)remote_context;\n}\n//! [default_context_from_core]\n```\n\n----------------------------------------\n\nTITLE: Build and install OneTBB\nDESCRIPTION: Clones the OneTBB repository, creates build and install directories, configures the OneTBB CMake project with Android-specific settings (ABI, platform, STL, toolchain file), builds the project in parallel, and installs it to the specified install directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_android.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# Clone OneTBB™ repository \ngit clone --recursive https://github.com/oneapi-src/oneTBB $OPV_HOME_DIR/one-tbb\n# Create build and install directory \nmkdir $OPV_HOME_DIR/one-tbb-build $OPV_HOME_DIR/one-tbb-install\n# Configure OneTBB™ CMake project \ncmake -S $OPV_HOME_DIR/one-tbb \\\n        -B $OPV_HOME_DIR/one-tbb-build \\\n        -DCMAKE_BUILD_TYPE=Release \\\n        -DCMAKE_INSTALL_PREFIX=$OPV_HOME_DIR/one-tbb-install \\\n        -DCMAKE_TOOLCHAIN_FILE=$CURRENT_CMAKE_TOOLCHAIN_FILE \\\n        -DANDROID_ABI=$CURRENT_ANDROID_ABI \\\n        -DANDROID_PLATFORM=$CURRENT_ANDROID_PLATFORM \\\n        -DANDROID_STL=$CURRENT_ANDROID_STL \\\n        -DTBB_TEST=OFF \\\n        -DCMAKE_SHARED_LINKER_FLAGS=\"-Wl,--undefined-version\" \n# Build OneTBB™ project \ncmake --build $OPV_HOME_DIR/one-tbb-build --parallel\n# Install OneTBB™ project \ncmake --install $OPV_HOME_DIR/one-tbb-build\n```\n\n----------------------------------------\n\nTITLE: Process Inference Results in C++\nDESCRIPTION: This snippet demonstrates how to process inference results obtained from an OpenVINO model using the C++ API.  It shows how to access the output tensors and extract meaningful data from them. The snippet is expected to work within the context of a larger OpenVINO application.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_31\n\nLANGUAGE: cpp\nCODE:\n```\n//! [part5]\nInferRequest infer_request = executable_network.create_infer_request();\n\ninfer_request.infer();\n//! [part5]\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (TensorFlow) with OpenVINO in Python\nDESCRIPTION: This code compiles a model in TensorFlow format using the `core.compile_model()` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel_tf = core.compile_model(\"path_to_model.pb\", \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Model Conversion with OpenVINO (CLI)\nDESCRIPTION: This command demonstrates how to convert a model to OpenVINO's Intermediate Representation (IR) format using the `ovc` command-line tool. It takes the path to the model as input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/throughput-benchmark.rst#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\novc ./models/googlenet-v1\n```\n\n----------------------------------------\n\nTITLE: Example of Snippet Parameter Dump Usage\nDESCRIPTION: This snippet provides an example of how to use the OV_SNIPPETS_DUMP_BRGEMM_PARAMS environment variable to dump snippet parameters to a CSV file named 'brgemm.csv'. The 'binary ...' part represents the execution of the OpenVINO application or binary.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/docs/debug_capabilities/parameters_dump.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nOV_SNIPPETS_DUMP_BRGEMM_PARAMS=\"path=brgemm.csv\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Throughput Benchmark Execution (Python)\nDESCRIPTION: This command executes the throughput_benchmark.py script with a specified model.  The device to run the model on can optionally be specified; the default device is CPU.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/throughput-benchmark.rst#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\npython throughput_benchmark.py <path_to_model> <device_name>(default: CPU)\n```\n\n----------------------------------------\n\nTITLE: benchmark_app warm-up inference output example (sh)\nDESCRIPTION: This code snippet shows the output from `benchmark_app` for the initial warm-up inference, which can show a high latency due to the auto-batching timeout.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\n[ INFO ] First inference took 1000.18ms\n```\n\n----------------------------------------\n\nTITLE: Enable Performance Hints in Python\nDESCRIPTION: This snippet shows how to enable performance hints for an application using OpenVINO in Python. It relies on external code defined in `docs/articles_en/assets/snippets/ov_auto.py` within the `[part3]` section.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndocs/articles_en/assets/snippets/ov_auto.py\n```\n\n----------------------------------------\n\nTITLE: Disabling Winograd Convolutions - Bash\nDESCRIPTION: This code snippet demonstrates how to disable Winograd convolutions in OpenVINO using the `compile_model` method with the `GPU_DISABLE_WINOGRAD_CONVOLUTION` configuration option set to `True`. Disabling Winograd convolutions can resolve inaccuracy and performance issues on GPU for fine-tuned models. It takes the compiled OpenVINO model, device name, and a configuration dictionary as parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncompiled_model = core.compile_model(ov_model, device_name=devStr1, config={ \"GPU_DISABLE_WINOGRAD_CONVOLUTION\": True })\n```\n\n----------------------------------------\n\nTITLE: Wrap cl_image with RemoteTensor (C++)\nDESCRIPTION: This C++ snippet demonstrates how to wrap an existing OpenCL image (cl_image) into an OpenVINO RemoteTensor using the GPU plugin.  It shows the necessary steps to create a RemoteTensor object from a cl_image, allowing for seamless integration of OpenCL image data within an OpenVINO inference pipeline.  No specific dependencies are listed beyond the OpenVINO and OpenCL libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\n// example usage\n{\n    cl::Image image;\n    // fill image with data\n\n    auto remote_blob = context.create_tensor(image);\n}\n```\n\n----------------------------------------\n\nTITLE: Linking OpenVINO libraries directly with MSVC (sh)\nDESCRIPTION: This snippet shows how to link the static OpenVINO libraries using the Microsoft Visual Studio compiler and the `/WHOLEARCHIVE` flag.  All libraries from `<install_root>/runtime/lib` directory must be specified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/static_libaries.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n/WHOLEARCHIVE:<ov_library 0> /WHOLEARCHIVE:<ov_library 1> ...\n```\n\n----------------------------------------\n\nTITLE: Adding OpenVINO Sample with CMake\nDESCRIPTION: This snippet utilizes the `ov_add_sample` function to configure the build of the 'model_creation_sample'. It defines the sample's name, source files (`main.cpp`), header files (`model_creation_sample.hpp`), and its dependencies (`format_reader`, `ie_samples_utils`).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/model_creation_sample/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_sample(NAME model_creation_sample\n              SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/main.cpp\"\n              HEADERS \"${CMAKE_CURRENT_SOURCE_DIR}/model_creation_sample.hpp\"\n              DEPENDENCIES format_reader ie_samples_utils)\n```\n\n----------------------------------------\n\nTITLE: Canceling Inference Request in Python\nDESCRIPTION: This Python code demonstrates how to cancel a running inference request using `ov::InferRequest::cancel`.  Requires OpenVINO and a compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\ncompiled_model = ie.compile_model(model, \"CPU\")\ninfer_request = compiled_model.create_infer_request()\n\ninput_tensor = infer_request.get_input_tensor()\ninput_tensor.data[:] = np.random.normal(0, 1, input_tensor.shape)\n\ninfer_request.start_async()\ninfer_request.cancel()\n```\n\n----------------------------------------\n\nTITLE: Creating OpenVINO Core Instance with Config C\nDESCRIPTION: This code snippet demonstrates how to create an OpenVINO Core instance using a custom XML configuration file. It takes the path to the XML file as input and creates a core object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_17\n\nLANGUAGE: C\nCODE:\n```\nchar *xml_config_file=\"/localdisk/plugins/my_custom_cfg.xml\";\nov_core_t* core;\nov_status_e status = ov_core_create_with_config(xml_config_file, &core);\n```\n\n----------------------------------------\n\nTITLE: Getting OpenVINO API Version in Python\nDESCRIPTION: This snippet retrieves the OpenVINO API version. This is helpful for ensuring compatibility and debugging.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/python/benchmark/sync_benchmark/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nopenvino.__version__\n```\n\n----------------------------------------\n\nTITLE: Parametrized MyTensor Creation Test Python\nDESCRIPTION: This test case verifies the creation of a `MyTensor` object with different source types (list and `ov.Tensor`). It uses pytest's `parametrize` decorator to run the same test logic with different input values. It asserts that the tensor is not `None` and checks its size.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.parametrize((\"source\"), [\n    ([1, 2, 3]),\n    (ov.Tensor(np.array([4, 5 ,6]).astype(np.float32))),\n])\ndef test_mytensor_creation(source):\n    tensor = ov.MyTensor(source)\n\n    assert tensor is not None\n    assert tensor.get_size() == 3\n```\n\n----------------------------------------\n\nTITLE: Visit Attributes in C++\nDESCRIPTION: This snippet demonstrates how to override the `visit_attributes` method in C++ for a custom OpenVINO operation. This method enables serialization and deserialization of operation attributes. An `AttributeVisitor` is passed to the method, and the implementation is expected to walk over all the attributes in the op using the type-aware `on_attribute` helper.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-openvino-operations.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n    bool visit_attributes(ov::AttributeVisitor& visitor) override {\n        return true;\n    }\n```\n\n----------------------------------------\n\nTITLE: Convert Keras 3 Model to OpenVINO IR with convert_model (Python)\nDESCRIPTION: This code snippet demonstrates how to convert a Keras 3 model to the OpenVINO Intermediate Representation (IR) format using the `convert_model` function after exporting it to a TensorFlow SavedModel. It requires the `keras_hub` and `openvino` libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-keras.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport keras_hub\nimport openvino as ov\n\nmodel = keras_hub.models.BertTextClassifier.from_preset(\n    \"bert_base_en_uncased\",\n    num_classes=4,\n    preprocessor=None,\n)\n\n# export to SavedModel\nmodel.export(\"bert_base\")\n\n# convert to OpenVINO model\nov_model = ov.convert_model(\"bert_base\")\n```\n\n----------------------------------------\n\nTITLE: GatherElements Example 1 (axis=0)\nDESCRIPTION: Demonstrates GatherElements operation with axis=0. The output is generated by gathering elements from the 'data' tensor using indices from the 'indices' tensor along the 0th axis.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-elements-6.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndata = [\n    [1, 2],\n    [3, 4],\n]\nindices = [\n    [0, 1],\n    [0, 0],\n]\naxis = 0\noutput = [\n    [1, 4],\n    [1, 2],\n]\n```\n\n----------------------------------------\n\nTITLE: Disabling constant folding for PyTorch 2 export quantization\nDESCRIPTION: This code snippet demonstrates how to disable constant folding in quantization when using PyTorch version 2.3.0 or later to benefit from optimizations in the OpenVINO backend during quantization of a model. This is achieved by passing `fold_quantize=False` parameter into the `convert_pt2e` function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nconverted_model = convert_pt2e(prepared_model, fold_quantize=False)\n```\n\n----------------------------------------\n\nTITLE: Importing OpenVINO properties - Python\nDESCRIPTION: This Python snippet shows the necessary imports at the beginning of code snippets that use OpenVINO properties. This is used as a header in other snippets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.runtime import Core, Properties, get_version, \n    hint, opt, device_id, enable_profiling, num_streams, affinity, \n    inference_precision, execution_mode, scheduler, performance_hint, \n    log_level, enable_statistics, cache_dir, numa_node_assign_policy, \n    shareable_constant_buffer, constant_cache, device, enable_mmap, \n    compiled_model_property, execution_devices, hint,\n    intel_auto, properties\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Resizing Image (Auto Dimensions) Python\nDESCRIPTION: Demonstrates how to resize an image using the OpenVINO preprocessing API in Python, omitting target width/height when the original model has known spatial dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n# 1) Set input tensor information. We apply preprocessing\n#    demanding input tensor to have 'u8' type and 'NHWC' layout.\nppp.input().tensor().set_element_type(ov.Type.u8).set_layout(ov.Layout('NHWC'))\n# 2) Adding explicit preprocessing steps\nppp.input().preprocess().resize(ov.ResizeAlgorithm.RESIZE_LINEAR)\n# 3) Set input model information. We assume that the original model has 'NCHW' layout\nppp.input().model().set_layout(ov.Layout('NCHW'))\n# 4) Apply preprocessing modifying the original model\nmodel = ppp.build()\n```\n\n----------------------------------------\n\nTITLE: Including a Subdirectory in CMake\nDESCRIPTION: This CMake command includes the CMakeLists.txt file located in the 'src' subdirectory in the current build process. This command is essential for multi-directory CMake projects, allowing each subdirectory to define its own build instructions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/jax/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Dataset for LLM Compression with NNCF\nDESCRIPTION: This code snippet demonstrates how to generate a synthetic dataset for compressing a large language model using the NNCF library. It utilizes the `transformers` library for tokenization and model loading and then uses NNCF's `generate_text_data` function to create the synthetic dataset. A transform function is required to process the dataset which is not included in this snippet. The generated dataset is then used to compress the model's weights.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom nncf import Dataset\nfrom nncf.data import generate_text_data\nfrom functools import partial\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Example: Generating synthetic dataset\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\nhf_model = AutoModelForCausalLM.from_pretrained(\n     MODEL_ID, export=True, load_in_8bit=False\n)\n\n# Synthetic-based compression\nsynthetic_dataset = nncf.data.generate_text_data(hf_model, tokenizer, dataset_size=100)\nquantization_dataset = nncf.Dataset(\n    synthetic_dataset,\n    transform_fn # See the example in NNCF repo to learn how to make transform_fn.\n)\n\nmodel = compress_weights(\n    model,\n    mode=CompressWeightsMode.INT4_ASYM,\n    group_size=64,\n    ratio=1.0,\n    dataset=quantization_dataset,\n    awq=True,\n    scale_estimation=True\n)  # The model is openvino.Model.\n```\n\n----------------------------------------\n\nTITLE: Manual Constant Folding (C++)\nDESCRIPTION: This code snippet shows an example of manual constant folding which is often preferable to `ov::pass::ConstantFolding()` because it is faster. It demonstrates creating a Constant node, performing an operation with it, and then replacing the original operation with the result.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n//! [manual_constant_folding]\nstd::vector<float> data = { 1.0f, 2.0f, 3.0f };\nauto const node = ov::op::v0::Constant::create(ov::element::f32, ov::Shape{ 3 }, data);\nauto relu = std::make_shared<ov::op::v0::Relu>(node);\n\nov::Output<ov::Node> out = relu->output(0);\nstd::vector<float> expected = { 1.0f, 2.0f, 3.0f };\nauto const result_node = ov::op::v0::Constant::create(ov::element::f32, ov::Shape{ 3 }, expected);\nout.replace(result_node);\n\n//! [manual_constant_folding]\n```\n\n----------------------------------------\n\nTITLE: BatchNormInference with 2D Input Tensor XML\nDESCRIPTION: Demonstrates the structure of a BatchNormInference layer in OpenVINO's XML format using a 2D input tensor.  It specifies the input and output port dimensions, along with the epsilon value for numerical stability. Required inputs are data, gamma, beta, mean, and variance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/batch-norm-inference-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"BatchNormInference\" ...>\n    <data epsilon=\"9.99e-06\" />\n    <input>\n        <port id=\"0\">  <!-- input -->\n            <dim>10</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"1\">  <!-- gamma -->\n            <dim>128</dim>\n        </port>\n        <port id=\"2\">  <!-- beta -->\n            <dim>128</dim>\n        </port>\n        <port id=\"3\">  <!-- mean -->\n            <dim>128</dim>\n        </port>\n        <port id=\"4\">  <!-- variance -->\n            <dim>128</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"5\">\n            <dim>10</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Create Intel Directory\nDESCRIPTION: Creates the Intel directory in Program Files (x86) using the command line. This directory is used to store the OpenVINO installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-windows.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmkdir \"C:\\Program Files (x86)\\Intel\"\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Target in CMake\nDESCRIPTION: Adds an executable target named `ov_capi_test` using the collected source and header files.  This specifies how the executable is built from the source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(${TARGET_NAME} ${SOURCES} ${HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Reshape by Input Name - C++\nDESCRIPTION: This C++ snippet shows how to reshape an OpenVINO model by specifying the input by its name. The reshape method updates the input shape according to the provided name and PartialShape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/changing-input-shape.rst#_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\n\n    // ! [name_to_shape]\n    std::string input_name = \"input\"; // Name of the input\n    ov::PartialShape new_shape = {1, 3, 256, 256};\n    model->reshape({{input_name, new_shape}});\n    // ! [name_to_shape]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Auto Batching with compile_model in Python\nDESCRIPTION: This Python code snippet demonstrates how to disable Auto-Batching by setting `ov::hint::allow_auto_batching` to `False` in addition to `ov::hint::performance_mode`. This ensures that auto-batching is not triggered even when throughput performance mode is specified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model, device_name=\"GPU\", config={\n    ov.hint.performance_mode: ov.hint.PerformanceMode.THROUGHPUT,\n    ov.hint.allow_auto_batching: False\n})\n```\n\n----------------------------------------\n\nTITLE: Build and install OpenVINO\nDESCRIPTION: Clones the OpenVINO repository, creates build and install directories, configures the OpenVINO CMake project with Android-specific settings (ABI, platform, STL, toolchain file, and OneTBB directory), builds the project in parallel, and installs it to the specified install directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_android.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n# Clone OpenVINO™ repository \ngit clone --recursive https://github.com/openvinotoolkit/openvino $OPV_HOME_DIR/openvino\n# Create build and install directory \nmkdir $OPV_HOME_DIR/openvino-build $OPV_HOME_DIR/openvino-install\n# Configure OpenVINO™ CMake project \ncmake -S $OPV_HOME_DIR/openvino \\\n        -B $OPV_HOME_DIR/openvino-build \\\n        -DCMAKE_INSTALL_PREFIX=$OPV_HOME_DIR/openvino-install \\\n        -DCMAKE_TOOLCHAIN_FILE=$CURRENT_CMAKE_TOOLCHAIN_FILE \\\n        -DANDROID_ABI=$CURRENT_ANDROID_ABI \\\n        -DANDROID_PLATFORM=$CURRENT_ANDROID_PLATFORM \\\n        -DANDROID_STL=$CURRENT_ANDROID_STL \\\n        -DTBB_DIR=$OPV_HOME_DIR/one-tbb-install/lib/cmake/TBB\n# Build OpenVINO™ project \ncmake --build $OPV_HOME_DIR/openvino-build --parallel\n# Install OpenVINO™ project \ncmake --install $OPV_HOME_DIR/openvino-build\n```\n\n----------------------------------------\n\nTITLE: Compiling TensorFlow Model in OpenVINO using C\nDESCRIPTION: This C code snippet demonstrates compiling a TensorFlow model with OpenVINO. It calls `ov_core_compile_model_from_file` to compile 'saved_model.pb' and stores the result in `compiled_model`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_4\n\nLANGUAGE: c\nCODE:\n```\nov_compiled_model_t* compiled_model = NULL;\nov_core_compile_model_from_file(core, \"saved_model.pb\", \"AUTO\", 0, &compiled_model);\n```\n\n----------------------------------------\n\nTITLE: Convert and Save Model using OpenVINO API\nDESCRIPTION: This code snippet demonstrates how to convert a model to the OpenVINO Intermediate Representation (IR) format using the `openvino.convert_model` function and then save the converted model using `openvino.save_model`. The code imports the `openvino` library, converts the model, optionally adjusts the model with pre-post processing, and then saves the model to an XML file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/conversion-parameters.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\nov_model = ov.convert_model('path_to_your_model')\n# or, when model is a Python model object\nov_model = ov.convert_model(model)\n\n# Optionally adjust model by embedding pre-post processing here...\n\nov.save_model(ov_model, 'model.xml')\n```\n\n----------------------------------------\n\nTITLE: Remote Tensor Usage (Python)\nDESCRIPTION: Shows how to create and use a remote tensor using `ov::RemoteContext` in Python to work with remote device memory. This is useful for scenarios where the input data resides on a remote device, allowing for efficient data transfer and processing.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# Get remote context from device\nremote_context = core.get_context(device_name, remote_device_id)\n\n# Create remote tensor\nremote_tensor = ov.Tensor(remote_context, input_tensor.shape, input_tensor.dtype)\n\n# Create input tensor with data copies to remote_tensor\ninput_tensor = ov.Tensor(remote_context, input_tensor.shape, input_tensor.dtype, np.random.normal(size=input_tensor.shape))\ninfer_request.infer({input_tensor : remote_tensor})\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO using vcpkg\nDESCRIPTION: This command installs the complete OpenVINO package using vcpkg. It builds all necessary packages and dependencies from source, which may take a significant amount of time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-vcpkg.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nvcpkg install openvino\n```\n\n----------------------------------------\n\nTITLE: Downloading and Installing OpenVINO (Ubuntu 22.04)\nDESCRIPTION: These commands download the OpenVINO Runtime archive for Ubuntu 22.04, extract it, and move the extracted directory to `/opt/intel`. It uses `curl` to download the archive, `tar` to extract it, and `sudo mv` to move the extracted folder with root privileges.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino/packages/2025.1/linux/openvino_toolkit_ubuntu22_2025.1.0.18503.6fec06580ab_x86_64.tgz --output openvino_2025.1.0.tgz\ntar -xf openvino_2025.1.0.tgz\nsudo mv openvino_toolkit_ubuntu22_2025.1.0.18503.6fec06580ab_x86_64 /opt/intel/openvino_2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Model Conversion with OpenVINO (Python)\nDESCRIPTION: This Python code snippet demonstrates how to convert a model to OpenVINO's Intermediate Representation (IR) format using the `ov.convert_model` function. It accepts the path to the model or a Python model object as input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/throughput-benchmark.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openvino as ov\n\nov_model = ov.convert_model('./models/googlenet-v1')\n# or, when model is a Python model object\nov_model = ov.convert_model(googlenet-v1)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Set Color Format (C++)\nDESCRIPTION: The `ov::preprocess::InputTensorInfo::set_color_format` API is used to change the color format of the input data. This allows you to specify the color format of the input tensor, ensuring that the data is correctly interpreted by the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_nv12_input_classification/README.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n``ov::preprocess::InputTensorInfo::set_color_format``\n```\n\n----------------------------------------\n\nTITLE: Canceling Inference Request in C++\nDESCRIPTION: This C++ code shows how to cancel a running inference request using `ov::InferRequest::cancel`. Requires OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\nov::InferRequest infer_request = compiled_model.create_infer_request();\n\nov::Tensor input_tensor = infer_request.get_input_tensor();\nstd::vector<float> input_data(input_tensor.get_size());\nstd::generate(input_data.begin(), input_data.end(), []() { return static_cast<float>(rand()) / RAND_MAX; });\nstd::memcpy(input_tensor.data(), input_data.data(), input_data.size() * sizeof(float));\n\ninfer_request.start_async();\ninfer_request.cancel();\n```\n\n----------------------------------------\n\nTITLE: Reshape Model with New Batch Size - Python\nDESCRIPTION: This snippet demonstrates how to use the `reshape` method in Python to set a new batch size for an OpenVINO model. It assumes the model is already loaded and ready for reshaping. The `reshape` method updates input shapes and propagates them to the outputs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/changing-input-shape.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\n\n# ! [picture_snippet]\nnew_shape = [1, 3, 256, 256]  # New shape with batch size 1 and spatial size 256x256.\nmodel.reshape(new_shape)\n# ! [picture_snippet]\n```\n\n----------------------------------------\n\nTITLE: Model Compilation with OpenVINO Core in C++\nDESCRIPTION: This snippet demonstrates how to compile a model for the NPU device using the OpenVINO Core API. The `compile_model` function takes the model, device name (\"NPU\"), and optional configuration as input and returns a `CompiledModel` object. The compiled model can then be used to create inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/README.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nov::CompiledModel compiled_model = core.compile_model(model, \"NPU\" [, config]);\n```\n\n----------------------------------------\n\nTITLE: Convert OCR Model with Ordered Input Shapes via CLI\nDESCRIPTION: This command-line snippet converts an ONNX OCR model using the `ovc` tool, specifying shapes for two inputs based on their order.  The `--input` argument is used to define the shapes as comma-separated lists, enclosed in brackets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/setting-input-shapes.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\novc ocr.onnx --input [3,150,200,1],[3]\n```\n\n----------------------------------------\n\nTITLE: Adjust Spatial Dimensions with Reshape - Python\nDESCRIPTION: This snippet adjusts the spatial dimensions of an input image in Python using the `reshape` method. It requires the OpenVINO library and assumes a model is loaded. It demonstrates reshaping with a single input by passing the new shape directly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/changing-input-shape.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\n\n# ! [simple_spatials_change]\n# Example: adjust input spatial dimensions to 512x512\nmodel.reshape([1, 3, 512, 512])\n# ! [simple_spatials_change]\n```\n\n----------------------------------------\n\nTITLE: Run OpenVINO™ Python API Tests\nDESCRIPTION: Executes the OpenVINO™ Python API tests using the `pytest` framework. This command runs all tests located in the `tests/` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npytest tests/\n```\n\n----------------------------------------\n\nTITLE: Fix Automatic Affinities for Heterogeneous Execution with OpenVINO (Python)\nDESCRIPTION: This Python snippet demonstrates how to fix automatically assigned affinities in OpenVINO's heterogeneous execution. It iterates through the model's operations, ensuring that if an operation's affinity is automatically set to 'CPU', it remains 'CPU'. This is useful for optimizing memory transfers. The openvino package is required.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/hetero-execution.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom openvino.runtime import Core\n\ncore = Core()\nmodel = core.read_model(\"path_to_your_model.xml\")\n\n# [fix_automatic_affinities]\nfor node in model.get_ops():\n    if node.get_rt_info().get('affinity') == 'CPU':\n        node.get_rt_info()['affinity'] = 'CPU'\n# [fix_automatic_affinities]\n```\n\n----------------------------------------\n\nTITLE: Getting Tensor from Inference Request in C++\nDESCRIPTION: This snippet illustrates how to retrieve a specific tensor from an inference request using its name. The `get_tensor` method returns an `ov::Tensor` object, providing access to the tensor's data for populating inputs or reading outputs. This is the preferred method as it avoids unnecessary memory copies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/README.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nov::Tensor requestTensor = inferRequest.get_tensor(tensor_name);\n```\n\n----------------------------------------\n\nTITLE: Pattern with Predicate (C++)\nDESCRIPTION: This code snippet demonstrates how to use a predicate to construct a more complex pattern and how to manually match a pattern on a given node.  It also shows how to clear the Matcher's state using the `m->clear_state()` method after manual matching to avoid conflicts in subsequent matching attempts.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/matcher-pass.rst#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n// [pattern:predicate_example]\n#include <openvino/core/node.hpp>\n#include <openvino/opsets/opset10.hpp>\n#include <openvino/pass/pattern/op/any_input.hpp>\n\n#include \"openvino/pass/graph_rewrite.hpp\"\n\nusing namespace std;\nusing namespace ov::opset10;\n\n{\n    auto any_input = ov::pass::pattern::any_input();\n    auto constant = make_shared<Constant>(element::f32, Shape{1}, vector<float>{1.0f});\n    auto mul = make_shared<Multiply>(any_input, constant);\n\n    ov::pass::Matcher m(mul);\n    auto result = m.match(mul);\n    m.clear_state();\n\n    ov::pass::pattern::Matcher pass_m(mul, [](const Output<Node>& output) { return true; });\n\n    auto callback = [](ov::pass::Matcher& m) {\n        return true;\n    };\n}\n// [pattern:predicate_example]\n```\n\n----------------------------------------\n\nTITLE: PrePostProcessor Interface Definition TypeScript\nDESCRIPTION: Defines the PrePostProcessor interface with methods for building the preprocessor and accessing input/output information. The `build()` method creates the preprocessor, `input()` configures input, and `output()` configures output. `idxOrTensorName` parameter (optional) allows specifying input or output by index or name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PrePostProcessor.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface PrePostProcessor {\n    build(): PrePostProcessor;\n    input(idxOrTensorName?): InputInfo;\n    output(idxOrTensorName?): OutputInfo;\n}\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Accessing Input by Index Python\nDESCRIPTION: Demonstrates how to access a specific input of a model by its index using the `ov::preprocess::PrePostProcessor` in Python.  This is used when dealing with models with multiple inputs and the inputs are to be addressed by index.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n# Get input Tensor by index and set input tensor information.\n# We apply preprocessing demanding input tensor to have 'u8' type and 'NCHW' layout.\nppp.input(1).tensor().set_element_type(ov.Type.u8).set_layout(ov.Layout('NCHW'))\n# 2) Adding explicit preprocessing steps\nppp.input(1).preprocess().convert_element_type(ov.Type.f32).convert_layout(ov.Layout('NHWC'))\n# 3) Set input model information.\nppp.input(1).model().set_layout(ov.Layout('NHWC'))\n# 4) Apply preprocessing modifying the original model\nmodel = ppp.build()\n```\n\n----------------------------------------\n\nTITLE: 2D GroupConvolution XML Example\nDESCRIPTION: This XML snippet illustrates the setup for a 2D GroupConvolution layer within OpenVINO. It details the dimensions of input and output tensors, alongside attributes such as dilations, padding, and strides, demonstrating how to configure a 2D convolution operation using grouped channels.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/group-convolution-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"GroupConvolution\" ...>\n    <data dilations=\"1,1\" pads_begin=\"2,2\" pads_end=\"2,2\" strides=\"1,1\" auto_pad=\"explicit\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>12</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>5</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Allocate cl::Buffer (C++)\nDESCRIPTION: This C++ snippet shows how to allocate an OpenCL buffer using the OpenVINO GPU plugin. The allocated buffer can be used as input/output for kernels executed on the GPU. Requires OpenCL and OpenVINO libraries. Using cl::Buffer allows integration of existing OpenCL code with OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\n// example usage\n{\n    size_t buffer_size;\n    cl::Buffer buffer(context.get_queue().get_context(), CL_MEM_READ_WRITE, buffer_size);\n    auto remote_blob = context.create_tensor(buffer);\n}\n```\n\n----------------------------------------\n\nTITLE: Build C OpenVINO Samples\nDESCRIPTION: This command executes the build_samples.sh script located in the C samples directory of the OpenVINO installation. It compiles the C sample applications.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-zypper.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n/usr/share/openvino/samples/c/build_samples.sh\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: This snippet sets the include directories for the target `${PROJECT_NAME}`. It includes the directories for `node-api-headers`, `node-addon-api`, and the parent directory of the current source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${PROJECT_NAME} PRIVATE\n    \"${node-api-headers_SOURCE_DIR}/include\"\n    \"${node-addon-api_SOURCE_DIR}\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/..\"\n)\n```\n\n----------------------------------------\n\nTITLE: CumSum Example 4: Exclusive Reverse Summation in OpenVINO (XML)\nDESCRIPTION: This XML example demonstrates the CumSum operation with exclusive summation (exclusive=\"1\") and reverse direction (reverse=\"1\"). It calculates the reverse exclusive cumulative sum of the input tensor [1., 2., 3., 4., 5.] along axis 0, resulting in the output tensor [14., 12., 9., 5., 0.].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/cumsum-3.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"CumSum\" exclusive=\"1\" reverse=\"1\">\n    <input>\n        <port id=\"0\">     < -- input value is: [1., 2., 3., 4., 5.] -->\n            <dim>5</dim>\n        </port>\n        <port id=\"1\"/>     < -- axis value is: 0 -->\n    </input>\n    <output>\n        <port id=\"2\">     < -- output value is: [14., 12., 9., 5., 0.] -->\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Getting OpenVINO Version C API\nDESCRIPTION: Demonstrates how to retrieve the OpenVINO API version using the `ov_get_openvino_version` function. This function allows users to determine the version of the OpenVINO runtime being used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/c/hello_classification/README.md#_snippet_0\n\nLANGUAGE: C\nCODE:\n```\n``ov_get_openvino_version``\n```\n\n----------------------------------------\n\nTITLE: Set Include Directories CMake\nDESCRIPTION: Defines include directories for the target.  The `PUBLIC` include directory is added to the include path of any target that links against this library. The `PRIVATE` include directory is only used when compiling this library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/onnx_common/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(ONNX_COMMON_INCLUDE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/include)\nset(ONNX_COMMON_SRC_DIR ${CMAKE_CURRENT_SOURCE_DIR}/src)\n\ntarget_include_directories(${TARGET_NAME}\n    PUBLIC $<BUILD_INTERFACE:${ONNX_COMMON_INCLUDE_DIR}>\n           $<INSTALL_INTERFACE:${FRONTEND_INSTALL_INCLUDE}>\n    PRIVATE ${ONNX_COMMON_SRC_DIR})\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch ResNet50 Model with Example Input\nDESCRIPTION: This snippet shows how to convert a PyTorch ResNet50 model to an OpenVINO model, providing an `example_input` to improve the quality of the converted model. It utilizes random input data of shape (1, 3, 224, 224). It requires the `torchvision`, `torch`, and `openvino` libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-pytorch.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision\nimport torch\nimport openvino as ov\n\nmodel = torchvision.models.resnet50(weights='DEFAULT')\nov_model = ov.convert_model(model, example_input=torch.rand(1, 3, 224, 224))\n```\n\n----------------------------------------\n\nTITLE: Define ov_element_type enum in C\nDESCRIPTION: This enum defines the supported element types for tensors in OpenVINO. It covers a wide range of numerical types, including boolean, floating-point, and integer types, as well as a string type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_10\n\nLANGUAGE: C\nCODE:\n```\ntypedef enum {\n\n    UNDEFINED = 0U,  //!< Undefined element type\n\n    DYNAMIC,         //!< Dynamic element type\n\n    BOOLEAN,         //!< boolean element type\n\n    BF16,            //!< bf16 element type\n\n    F16,             //!< f16 element type\n\n    F32,             //!< f32 element type\n\n    F64,             //!< f64 element type\n\n    I4,              //!< i4 element type\n\n    I8,              //!< i8 element type\n\n    I16,             //!< i16 element type\n\n    I32,             //!< i32 element type\n\n    I64,             //!< i64 element type\n\n    U1,              //!< binary element type\n\n    U2,              //!< u2 element type\n\n    U3,              //!< u3 element type\n\n    U4,              //!< u4 element type\n\n    U6,              //!< u6 element type\n\n    U8,              //!< u8 element type\n\n    U16,             //!< u16 element type\n\n    U32,             //!< u32 element type\n\n    U64,             //!< u64 element type\n\n    NF4,             //!< nf4 element type\n\n    F8E4M3,          //!< f8e4m3 element type\n\n    F8E5M3,          //!< f8e5m2 element type\n\n    STRING,          //!< string element type\n\n    F4E2M1,          //!< f4e2m1 element type\n\n    F8E8M0,          //!< f8e8m0 element type\n\n} ov_element_type_e;\n```\n\n----------------------------------------\n\nTITLE: NV12 to Grey Conversion (Single Batch) in OpenVINO (C++)\nDESCRIPTION: Demonstrates NV12 to Grayscale conversion using OpenVINO in a single batch scenario. The provided input data in NV12 format is converted to grayscale by the OpenVINO model. Requires OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_37\n\nLANGUAGE: cpp\nCODE:\n```\n//! [single_batch]\nauto input_tensor = ov::Tensor(ov::element::u8, {height + height / 2, width}, input_data);\nauto input_port = compiled_model.input();\ninfer_request.set_tensor(input_port, input_tensor);\n//! [single_batch]\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Environment Variables on Windows\nDESCRIPTION: This script sets up the OpenVINO environment variables on a Windows system. It executes the `setupvars.bat` batch file located in the OpenVINO installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_17\n\nLANGUAGE: bat\nCODE:\n```\n<INSTALL_DIR>\\setupvars.bat\n```\n\n----------------------------------------\n\nTITLE: Inference with Quantized Model - ONNX\nDESCRIPTION: This snippet demonstrates running inference with a quantized ONNX model using the ONNX Runtime. It loads the ONNX model and creates an inference session, then executes inference with a random input tensor and prints the results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/quantizing-with-accuracy-control.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# [inference]\nimport numpy as np\nimport onnx\nfrom onnx.checker import check_model\nimport onnxruntime\n\nmodel = onnx.load_model(\"model.onnx\")\ncheck_model(model)\n\nsess = onnxruntime.InferenceSession(model.SerializeToString())\noutput_name = sess.get_outputs()[0].name\ninput_name = sess.get_inputs()[0].name\nshape = sess.get_inputs()[0].shape\n\ninput_data = np.random.normal(0, 1, shape).astype(np.float32)\nres = sess.run([output_name], {input_name: input_data})[0]\nprint(res)\n# [inference]\n```\n\n----------------------------------------\n\nTITLE: Reshape by Input Port (Object) - Python\nDESCRIPTION: This Python snippet shows how to reshape an OpenVINO model by specifying the input port using an `openvino.runtime.Output` object as the dictionary key. The dictionary value represents the new shape as a `PartialShape`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/changing-input-shape.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\n\n# ! [obj_to_shape]\ninput_port = model.input(0)  # Get the input port object\nnew_shape = ov.PartialShape([1, 3, 256, 256])\nmodel.reshape({input_port: new_shape})\n# ! [obj_to_shape]\n```\n\n----------------------------------------\n\nTITLE: Export VLM to OpenVINO with Full Precision (fp16)\nDESCRIPTION: This example exports the openbmb/MiniCPM-V-2_6 VLM model to OpenVINO IR format with fp16 precision, enabling trust for remote code execution with `--trust-remote-code`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/genai-model-preparation.rst#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\noptimum-cli export openvino --model openbmb/MiniCPM-V-2_6 --trust-remote-code –weight-format fp16 ov_MiniCPM-V-2_6\n```\n\n----------------------------------------\n\nTITLE: Configuring NPU Compilation Mode Parameters in OpenVINO\nDESCRIPTION: This example shows how to configure compilation mode parameters for the OpenVINO NPU plugin, specifically setting the `optimization-level` and `performance-hint-override`. This allows for fine-grained control over the model compilation process. This configuration is then passed to the `compile_model` function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/README.md#_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nmap<str, str> config = {ov::intel_npu::compilation_mode_params.name(), ov::Any(\"optimization-level=1 performance-hint-override=latency\")};\ncompile_model(model, config);\n```\n\n----------------------------------------\n\nTITLE: Configuring prompt and response length in C++\nDESCRIPTION: This C++ snippet configures the maximum prompt length and minimum response length for the LLMPipeline. It uses ov::AnyMap to define the pipeline configuration and passes it to the LLMPipeline constructor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nov::AnyMap pipeline_config = { { \"MAX_PROMPT_LEN\",  1024 }, { \"MIN_RESPONSE_LEN\", 512 } };\nov::genai::LLMPipeline pipe(model_path, \"NPU\", pipeline_config);\n```\n\n----------------------------------------\n\nTITLE: ReLU Kernel Implementation in OpenCL C\nDESCRIPTION: This OpenCL C kernel implements a ReLU (Rectified Linear Unit) activation function. It retrieves input values from `input0`, applies the ReLU operation (optionally with a leaky ReLU slope), and writes the result to `output`.  The kernel utilizes pre-defined macros like `INPUT0_TYPE`, `OUTPUT0_TYPE`, `INPUT0_PITCHES`, `OUTPUT0_PITCHES`, `INPUT0_OFFSET`, `OUTPUT0_OFFSET`, and `neg_slope` (for Leaky ReLU) provided by the OpenVINO framework.  The code calculates the linear index for both input and output tensors using their pitches and offsets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-gpu-operations.rst#_snippet_5\n\nLANGUAGE: c\nCODE:\n```\n#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n__kernel void example_relu_kernel(\n    const __global INPUT0_TYPE*  input0,\n          __global OUTPUT0_TYPE* output)\n{\n    const uint idx  = get_global_id(0);\n    const uint idy  = get_global_id(1);\n    const uint idbf = get_global_id(2); // batches*features, as OpenCL supports 3D nd-ranges only\n    const uint feature = idbf % OUTPUT0_DIMS[1];\n    const uint batch   = idbf / OUTPUT0_DIMS[1];\n    //notice that pitches are in elements, not in bytes!\n    const uint in_id  = batch*INPUT0_PITCHES[0] + feature*INPUT0_PITCHES[1]   + idy*INPUT0_PITCHES[2]  + idx*INPUT0_PITCHES[3]  + INPUT0_OFFSET;\n    const uint out_id = batch*OUTPUT0_PITCHES[0] + feature*OUTPUT0_PITCHES[1]  + idy*OUTPUT0_PITCHES[2]  + idx*OUTPUT0_PITCHES[3]  + OUTPUT0_OFFSET;\n\n    INPUT0_TYPE value = input0[in_id];\n    // neg_slope (which is non-zero for leaky ReLU) is put automatically as #define, refer to the config xml\n    output[out_id] = value < 0 ? value * neg_slope : value;\n}\n```\n\n----------------------------------------\n\nTITLE: TorchFX: Create Calibration Dataset\nDESCRIPTION: Creates an `nncf.Dataset` for TorchFX models. The `transform_fn` extracts the input tensor from each dataset sample, making the data compatible with the quantization API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset = nncf.Dataset(torch_dataset, transform_fn)\n\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for the Target\nDESCRIPTION: This snippet adds include directories to the target's include path. Specifically, it adds the `include` directory located in the current source directory and also the current source directory itself as build interface include directories, which makes these available to dependent targets as well.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/plugin/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}\n    PRIVATE\n        $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\n    PRIVATE\n        $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/>\n)\n```\n\n----------------------------------------\n\nTITLE: Load converted model in OpenVINO representation\nDESCRIPTION: Loads a converted OpenVINO model directly from the disk using OVModelForCausalLM.from_pretrained.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel_id = \"llama_openvino\"\nmodel = OVModelForCausalLM.from_pretrained(model_id)\n```\n\n----------------------------------------\n\nTITLE: Importing necessary modules for OpenVINO transformations\nDESCRIPTION: This snippet imports necessary headers from the openvino library. These headers are required for creating and manipulating OpenVINO models and operations for transformation patterns.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <gtest/gtest.h>\n#include \"openvino/opsets/opset13.hpp\"\n#include \"openvino/core/model.hpp\"\n#include \"openvino/core/partial_shape.hpp\"\n#include \"openvino/core/dimension.hpp\"\n#include \"openvino/frontend/frontend.hpp\"\n#include \"openvino/frontend/manager.hpp\"\n#include \"openvino/offline_transformations/pass/compress_model_transformation.hpp\"\n#include \"openvino/offline_transformations/pass/low_latency.hpp\"\n#include \"openvino/core/preprocess/pre_post_process.hpp\"\n#include <openvino/op/if.hpp>\n#include <openvino/op/loop.hpp>\n#include <openvino/op/tensor_iterator.hpp>\n\n#include <openvino/op/normalize_l2.hpp>\n#include <openvino/op/embedding_segments_sum.hpp>\n#include <openvino/op/embedding_bag_offsets_sum.hpp>\n#include <openvino/op/embedding_bag_packed_sum.hpp>\n#include <openvino/op/roi_align.hpp>\n\n#include <openvino/op/lstm_sequence.hpp>\n#include <openvino/op/gru_sequence.hpp>\n#include <openvino/op/rnn_sequence.hpp>\n\n#include <openvino/op/fft.hpp>\n\n#include <transformations/rt_info/fused_names_attribute.hpp>\n\n#include <openvino/pass/manager.hpp>\n#include <transformations/init_node_info.hpp>\n#include <transformations/utils/utils.hpp>\n#include <openvino/core/type/element_type.hpp>\n\n#include <map>\n#include <string>\n#include <vector>\n#include <memory>\n#include <fstream>\n#include <algorithm>\n#include <iostream>\n\nusing namespace std;\n```\n\n----------------------------------------\n\nTITLE: Create an Advanced Model (C++)\nDESCRIPTION: This snippet shows how to create a more advanced model with multiple outputs in OpenVINO using C++.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n// [ov:create_advanced_model]\n```\n\n----------------------------------------\n\nTITLE: Get Element Type of Port in OpenVINO (C)\nDESCRIPTION: This function retrieves the tensor type (element type) of a port. It takes a pointer to an `ov_output_const_port_t` and returns the tensor type in the `ov_element_type_e* tensor_type` parameter. The function returns a status code indicating success or failure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_39\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_port_get_element_type(const ov_output_const_port_t* port, ov_element_type_e* tensor_type)\n```\n\n----------------------------------------\n\nTITLE: Exclude Layers by Regular Expression\nDESCRIPTION: Excludes layers from quantization based on a regular expression pattern that matches layer names.  This is useful for excluding groups of layers with similar naming conventions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\nregex = '.*layer_.*'\nncf.quantize(model, dataset, ignored_scope=nncf.IgnoredScope(patterns=regex))\n\n```\n\n----------------------------------------\n\nTITLE: Create Stateful OpenVINO Model (Python)\nDESCRIPTION: This Python snippet showcases how to create a stateful OpenVINO model from scratch using `ov::SinkVector`.  `Assign` nodes should point to `Model` to avoid deletion during graph transformations. Requires the openvino and numpy packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\nfrom openvino.runtime import Model, op\n\n\ndef stateful_model() -> Model:\n    \"\"\"Create stateful model from scratch\"\"\"\n    dtype = np.float32\n    # Create an empty model\n    parameter = op.Parameter(dtype, ov.Shape([1, 10]), name=\"parameter\")\n    add = op.Add(parameter, op.Constant(np.array([1]), dtype))\n    result = op.Result(add, name=\"result\")\n\n    # Create state variable (read-value) and assign ops\n    read_value = op.ReadValue(np.array([0], dtype=dtype), \"state\")\n    add_state = op.Add(read_value.output(0), result)\n    assign = op.Assign(add_state, \"state\")\n\n    model = Model([result, assign], [parameter])\n    return model\n```\n\n----------------------------------------\n\nTITLE: Reshape by Input Index - C++\nDESCRIPTION: This C++ snippet shows how to reshape an OpenVINO model by specifying the input by its index using `map<size_t, ov::PartialShape>`. The reshape method updates the input shape according to the specified index and shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/changing-input-shape.rst#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\n\n    // ! [idx_to_shape]\n    size_t input_index = 0; // Index of the input\n    ov::PartialShape new_shape = {1, 3, 256, 256};\n    model->reshape({{input_index, new_shape}});\n    // ! [idx_to_shape]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Export Diffusion Model to OpenVINO with Full Precision (fp16)\nDESCRIPTION: This example exports the stabilityai/stable-diffusion-xl-base-1.0 diffusion model to OpenVINO IR format with fp16 precision, using the `optimum-cli export openvino` command.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/genai-model-preparation.rst#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\noptimum-cli export openvino --model stabilityai/stable-diffusion-xl-base-1.0 --weight-format fp16 ov_SDXL\n```\n\n----------------------------------------\n\nTITLE: Waiting for Inference Completion in C++\nDESCRIPTION: This C++ code waits for an asynchronous inference request to complete using `ov::InferRequest::wait`.  It starts the inference asynchronously and then calls wait to block until completion. Requires OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\nov::InferRequest infer_request = compiled_model.create_infer_request();\n\nov::Tensor input_tensor = infer_request.get_input_tensor();\nstd::vector<float> input_data(input_tensor.get_size());\nstd::generate(input_data.begin(), input_data.end(), []() { return static_cast<float>(rand()) / RAND_MAX; });\nstd::memcpy(input_tensor.data(), input_data.data(), input_data.size() * sizeof(float));\n\ninfer_request.start_async();\ninfer_request.wait();\n```\n\n----------------------------------------\n\nTITLE: Enabling Snappy Compression for TensorFlow Frontend\nDESCRIPTION: This CMake conditional block enables Snappy compression for the TensorFlow frontend if 'ENABLE_SNAPPY_COMPRESSION' is set. It links the 'openvino::snappy' library and defines the 'ENABLE_SNAPPY_COMPRESSION' compile definition for the 'openvino_tensorflow_frontend' target. This allows the frontend to handle TensorFlow models compressed with Snappy.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_SNAPPY_COMPRESSION)\n    target_link_libraries(openvino_tensorflow_frontend PRIVATE openvino::snappy)\n    target_compile_definitions(openvino_tensorflow_frontend PRIVATE ENABLE_SNAPPY_COMPRESSION)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Speech Recognition with Whisper in Python\nDESCRIPTION: This Python code snippet shows how to perform speech recognition using the OpenVINO GenAI WhisperPipeline. It reads a WAV file, initializes the pipeline, generates text from the audio, and prints the results, including timestamps for each chunk.  It requires the `openvino_genai` and `librosa` libraries. The input is a WAV file path, and the output is the transcribed text with timestamps.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino_genai\nimport librosa\n\n\n            def read_wav(filepath):\n                raw_speech, samplerate = librosa.load(filepath, sr=16000)\n                return raw_speech.tolist()\n\n\n            def infer(model_dir: str, wav_file_path: str):\n                device = \"CPU\"  # GPU or NPU can be used as well.\n                pipe = openvino_genai.WhisperPipeline(model_dir, device)\n\n                # The pipeline expects normalized audio with a sampling rate of 16kHz.\n                raw_speech = read_wav(wav_file_path)\n                result = pipe.generate(\n                    raw_speech,\n                    max_new_tokens=100,\n                    language=\"<|en|>\",\n                    task=\"transcribe\",\n                    return_timestamps=True,\n                )\n\n                print(result)\n\n                for chunk in result.chunks:\n                    print(f\"timestamps: [{chunk.start_ts}, {chunk.end_ts}] text: {chunk.text}\")\n```\n\n----------------------------------------\n\nTITLE: Creating Remote Context in OpenVINO Plugin (C++)\nDESCRIPTION: This code snippet shows how to implement the `Plugin::create_context()` method to return an `ov::RemoteContext`. If remote contexts are not supported, the plugin should throw an exception. This function is essential for plugins supporting offload to remote hardware.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\nRemoteContext::Ptr TemplatePlugin::create_context(const ParamMap& params) const override {\n    // In the simplest case, we can just return a new instance of the context\n    return std::make_shared<TemplateContext>(*this, params);\n}\n```\n\n----------------------------------------\n\nTITLE: Get OpenVINO Model in Python\nDESCRIPTION: This snippet demonstrates how to obtain an OpenVINO Model in Python. It loads the model from a specified path using `core.read_model()`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ncore = ov.Core()\nmodel = core.read_model(\"path_to_your_model.xml\")\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Inference for Hiding Latency in OpenVINO (Python)\nDESCRIPTION: Shows how to use asynchronous calls with `InferRequest.start_async` to hide latency. This non-blocking call releases the GIL, allowing other computations to proceed while inference is in progress.  Requires the use of `infer_request.wait()` to retrieve the results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-advanced-inference.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninfer_request = compiled_model.create_infer_request()\ninfer_request.start_async({\"input_tensor_name_1\": input_tensor_1, \"input_tensor_name_2\": input_tensor_2})\ninfer_request.wait()\nresults = infer_request.get_tensors()\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Get CPU Device Name (Python)\nDESCRIPTION: This code demonstrates how to get the full name of the CPU device using `ov::Core::get_property` in Python.  It retrieves the device name and prints it.  It requires an initialized OpenVINO `Core` object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ncore = ov.Core()\ndevice_name = \"CPU\"\ndevice_full_name = core.get_property(device_name, \"device_name\")\nprint(f\"{device_name} full name: {device_full_name}\")\n```\n\n----------------------------------------\n\nTITLE: StridedSlice Layer Configuration with Ellipsis Mask (OpenVINO XML)\nDESCRIPTION: This XML snippet configures a StridedSlice layer in OpenVINO. It demonstrates the use of the `ellipsis_mask` attribute in conjunction with `begin_mask`, `end_mask`, and `new_axis_mask` to achieve a specific array slicing operation equivalent to `array[2:, ..., np.newaxis, :10]`. The input dimensions and the expected output dimensions are described in the ports.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/strided-slice-1.rst#_snippet_7\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"StridedSlice\" ...>\n    <data begin_mask=\"0,0,1,1\" end_mask=\"1,1,0,0\" new_axis_mask=\"0,0,1\" shrink_axis_mask=\"0\" ellipsis_mask=\"0,1\"/>\n    <input>\n        <port id=\"0\">\n            <dim>10</dim> <!-- first dim -->\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim> <!-- last dim -->\n        </port>\n        <port id=\"1\">\n            <dim>3</dim> <!-- begin: [2, 1, 10, 10] - second dimension marked as ellipsis. third dimension marked as a new axis -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim> <!-- end: [123, 1, 10, 5] -->\n        </port>\n        <port id=\"3\">\n            <dim>3</dim> <!-- stride: [1, -1, 1, 1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>8</dim> <!-- first dim modified, begin = 2, end = 10 -->\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim> <!-- ellipsis skipped over 8 dimensions -->\n            <dim>10</dim> <!-- 8 = 10 - (4 - 1 - 1) -->\n            <dim>10</dim> <!-- 10 - rank(input), 4 - rank(begin), 1 - new_axis_mask -->\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>1</dim> <!-- new dimension from new_axis_mask, 'consumes' the penultimate slicing arguments -->\n            <dim>5</dim> <!-- last dim modified, begin = 0, end = 5 -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Get Default RemoteContext from Core - OpenVINO™ C++\nDESCRIPTION: This code snippet demonstrates how to obtain the default RemoteContext from the ov::Core object. This context is then used to create ov::RemoteTensor objects for shared memory access between the application and the NPU plugin. No specific dependencies are needed beyond the OpenVINO™ runtime library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/npu-device/remote-tensor-api-npu-plugin.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nauto core = ov::Core{};\nauto remote_context = core.get_default_context(\"NPU\");\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenVINO Notebooks\nDESCRIPTION: This code snippet clones the OpenVINO notebooks repository from GitHub, navigates into the cloned directory, upgrades pip, and installs the necessary Python packages specified in the requirements.txt file. This prepares the environment with the required dependencies for running the OpenVINO notebooks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/openvinotoolkit/openvino_notebooks.git\ncd openvino_notebooks\n# Install OpenVINO and OpenVINO notebook Requirements\npython -m pip install --upgrade pip\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Benchmarking asl-recognition model on CPU in latency mode (Python)\nDESCRIPTION: This snippet demonstrates how to run the OpenVINO Benchmark Tool to measure the latency of the 'asl-recognition' model on a CPU. It uses the `benchmark_app` command with the `-m` option to specify the model path, `-d` to select the CPU device, and `-hint` to set the performance hint to latency. The model is expected to be in the OpenVINO Intermediate Representation (IR) format ('.xml' file).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app -m omz_models/intel/asl-recognition-0004/FP16/asl-recognition-0004.xml -d CPU -hint latency\n```\n\n----------------------------------------\n\nTITLE: ExperimentalDetectronDetectionOutput Layer Configuration XML\nDESCRIPTION: This XML snippet configures the ExperimentalDetectronDetectionOutput layer with specific attributes such as class agnostic box regression, delta weights, max delta log wh, max detections per image, NMS threshold, number of classes, post NMS count, and score threshold. It defines the input and output ports with their corresponding dimensions and precisions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/experimental-detectron-detection-output-6.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ExperimentalDetectronDetectionOutput\" version=\"opset6\">\n    <data class_agnostic_box_regression=\"false\" deltas_weights=\"10.0,10.0,5.0,5.0\" max_delta_log_wh=\"4.135166645050049\" max_detections_per_image=\"100\" nms_threshold=\"0.5\" num_classes=\"81\" post_nms_count=\"2000\" score_threshold=\"0.05000000074505806\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1000</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1000</dim>\n            <dim>324</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1000</dim>\n            <dim>81</dim>\n        </port>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"4\" precision=\"FP32\">\n            <dim>100</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"5\" precision=\"I32\">\n            <dim>100</dim>\n        </port>\n        <port id=\"6\" precision=\"FP32\">\n            <dim>100</dim>\n        </port>\n        <port id=\"7\" precision=\"I32\">\n            <dim>100</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Modify String Tensor Elements (Python)\nDESCRIPTION: Modifies the contents of a string tensor by assigning a new list of strings to the `str_data` attribute.  Also shows doing the same using `bytes_data`.  Requires the `openvino` package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/string-tensors.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Unicode strings:\ntensor.str_data = ['one', 'two', 'three']\n# Do NOT use tensor.str_data[i] to set a new value, it won't update the tensor content\n\n# Encoded strings:\ntensor.bytes_data = [b'one', b'two', 'three']\n# Do NOT use tensor.bytes_data[i] to set a new value, it won't update the tensor content\n```\n\n----------------------------------------\n\nTITLE: Working with Node Ports in OpenVINO (C++)\nDESCRIPTION: This code snippet demonstrates how to work with input and output ports of OpenVINO operations (nodes). It shows how to access the node to which a port belongs, including its shape, type, consumers for output ports, and the producer node for input ports.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov:ports_example]\nauto input_node_output = op->input_value(0);\nauto output_node_input = op->output(0);\n\nauto input_node = input_node_output.get_node_shared_ptr();\nauto output_node = output_node_input.get_node();\n//! [ov:ports_example]\n```\n\n----------------------------------------\n\nTITLE: Quantize Model for QAT - TensorFlow\nDESCRIPTION: Quantizes a TensorFlow model using NNCF for Quantization-Aware Training. This step is essential before fine-tuning to simulate quantization effects during training.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/quantization-aware-training.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnncf.quantize(model)\n```\n\n----------------------------------------\n\nTITLE: Model Conversion using OpenVINO (Python)\nDESCRIPTION: This Python code snippet shows how to convert a model to OpenVINO Intermediate Representation (IR) format using the `openvino.convert_model` function. It takes a path to the original model as input and converts it to an OpenVINO model object, which can then be used for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/sync-benchmark.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openvino as ov\n\nov_model = ov.convert_model('./models/googlenet-v1')\n# or, when model is a Python model object\nov_model = ov.convert_model(googlenet-v1)\n```\n\n----------------------------------------\n\nTITLE: Serialize a Model to IR (Python)\nDESCRIPTION: This snippet demonstrates how to serialize an OpenVINO model to an Intermediate Representation (IR) file using Python. Serialization is useful for debugging purposes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# [ov:serialize]\n```\n\n----------------------------------------\n\nTITLE: Get Const Input Port (Single Input Model) in OpenVINO (C)\nDESCRIPTION: This function retrieves a const input port from an OpenVINO model, specifically designed for models with a single input. It takes a pointer to a `ov_model_t` and returns a pointer to the `ov_output_const_port_t` representing the input port. The function returns a status code indicating success or failure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_24\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_model_const_input(const ov_model_t* model, ov_output_const_port_t** input_port);\n```\n\n----------------------------------------\n\nTITLE: Setting Compilation Mode Parameters for NPU\nDESCRIPTION: This code snippet demonstrates how to set the `ov::intel_npu::compilation_mode_params` property to control model compilation for the NPU. It creates a map with the desired configuration options, such as optimization level and performance hint override, and passes it to the `compile_model` function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/npu-device.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nmap<str, str> config = {ov::intel_npu::compilation_mode_params.name(), ov::Any(\"optimization-level=1 performance-hint-override=latency\")};\n\ncompile_model(model, config);\n```\n\n----------------------------------------\n\nTITLE: Example Custom Layer Configuration - XML\nDESCRIPTION: This XML configuration defines a custom layer named \"ReLU\" of type \"SimpleGPU\" with version 1. It specifies the kernel source file, a define for the negative slope, input and output tensor configurations, compiler options, and work sizes. This file needs to be loaded by the OpenVINO application for the custom layer to function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-gpu-operations.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<CustomLayer name=\"ReLU\" type=\"SimpleGPU\" version=\"1\">\n  <Kernel entry=\"example_relu_kernel\">\n    <Source filename=\"custom_layer_kernel.cl\"/>\n    <Define name=\"neg_slope\" type=\"float\" param=\"negative_slope\" default=\"0.0\"/>\n  </Kernel>\n  <Buffers>\n    <Tensor arg-index=\"0\" type=\"input\" port-index=\"0\" format=\"BFYX\"/>\n    <Tensor arg-index=\"1\" type=\"output\" port-index=\"0\" format=\"BFYX\"/>\n  </Buffers>\n  <CompilerOptions options=\"-cl-mad-enable\"/>\n  <WorkSizes global=\"X,Y,B*F\"/>\n</CustomLayer>\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINO Runtime with CMake\nDESCRIPTION: This command demonstrates how to find the OpenVINO Runtime library using CMake's `find_package` function.  It requires the `OpenVINO_DIR` variable to be set correctly. Then it creates two executables and links them against the OpenVINO runtime. The first links against the C++ runtime, the second links against the C runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DOpenVINO_DIR=<INSTALLDIR>/runtime/cmake .\n```\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenVINO REQUIRED)\nadd_executable(ov_app main.cpp)\ntarget_link_libraries(ov_app PRIVATE openvino::runtime)\n\nadd_executable(ov_c_app main.c)\ntarget_link_libraries(ov_c_app PRIVATE openvino::runtime::c)\n```\n\n----------------------------------------\n\nTITLE: Using AnyInput with a predicate\nDESCRIPTION: This Python snippet demonstrates using AnyInput with a predicate (lambda function).  The predicate ensures that the input has a rank of 4 (i.e., the input's shape has 4 dimensions).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.tools.ovc.composite.pattern import AnyInput\n\ndef any_input_predicate():\n    any_input_node = AnyInput(lambda node: node.get_output_shape(0).rank().get_length() == 4)\n\n    return any_input_node\n```\n\n----------------------------------------\n\nTITLE: Getting OpenVINO Version C\nDESCRIPTION: This code snippet retrieves the version information of the OpenVINO runtime, including the description and build number. It requires a pointer to an `ov_version_t` struct to store the version data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_18\n\nLANGUAGE: C\nCODE:\n```\nov_version_t version = {.description = NULL, .buildNumber = NULL};\nov_get_openvino_version(&version);\nprintf(\"description : %s \\n\", version.description);\nprintf(\"build number: %s \\n\", version.buildNumber);\nov_version_free(&version);\n```\n\n----------------------------------------\n\nTITLE: Benchmarking with benchmark_app\nDESCRIPTION: This command uses the benchmark_app tool to test the performance of a hardware plugin with different performance hints and device configurations.  It takes the device, hint, and model as input. It requires the benchmark_app tool and a model compatible with the hardware plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/integration.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app -d ${device} -hint ${hint} -m <any model works on HW plugin>\n```\n\n----------------------------------------\n\nTITLE: Defining Input Tensor Information (Python)\nDESCRIPTION: This snippet demonstrates how to define the format of the user's input data using the `ov::preprocess::PrePostProcessor::input` method in Python. It sets the precision to `U8`, the shape to `{1,480,640,3}`, the layout to `NHWC`, and the color format to `BGR`.  This prepares the PrePostProcessor for handling data in this specific format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nppp.input().tensor()\n    .set_element_type(ov.Type.u8)\n    .set_shape([1, 480, 640, 3])\n    .set_layout(ov.Layout('NHWC'))\n    .set_color_format(ov.ColorFormat.BGR)\n```\n\n----------------------------------------\n\nTITLE: Shared Test Directory\nDESCRIPTION: This snippet shows the directories for shared test definitions and GPU plugin-specific test instances.  Shared tests are defined in one location and instantiated with plugin-specific parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/source_code_structure.md#_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\nsrc/tests/functional/plugin/shared                        <--- test definitions\nsrc/tests/functional/plugin/gpu/shared_tests_instances    <--- instances for GPU plugin\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Frontend Layer Tests\nDESCRIPTION: This command executes the layer tests to verify PyTorch operation support in the frontend. It uses pytest to run Python-based tests that compare the inference results of PyTorch and OpenVINO models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/pytorch/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest layer_tests/pytorch_tests\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Inference in C++\nDESCRIPTION: This snippet demonstrates how to execute inference asynchronously using the `start_async` and `wait` methods of the `InferRequest` object. The `start_async` method starts the inference in a non-blocking manner, and `wait` (optional) blocks until the inference is complete. A user-provided callback can be used instead of `wait`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/README.md#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\ninferRequest.start_async();\ninferRequest.wait(); // optional, in case user callback is not provided\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This snippet sets the target name for the model creation sample to 'model_creation_sample'. This name is used as the executable name when the project is built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/model_creation_sample/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME \"model_creation_sample\")\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Convert Element Type (C++)\nDESCRIPTION: The `ov::preprocess::PreProcessSteps::convert_element_type` API is used to convert the element type of the input data. This allows you to change the data type of the input tensor, ensuring compatibility with the model's expected input type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_nv12_input_classification/README.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n``ov::preprocess::PreProcessSteps::convert_element_type``\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies (Raspbian)\nDESCRIPTION: Installs the required dependencies for building OpenVINO Runtime on Raspbian using apt-get. This includes git, cmake, scons, and build-essential.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_raspbian.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get install -y git cmake scons build-essential\n```\n\n----------------------------------------\n\nTITLE: Make Stateful Model Using Tensor Names in Command Line\nDESCRIPTION: This snippet demonstrates how to apply the MakeStateful transformation to an OpenVINO model using tensor names in command line. It utilizes the `--transform` argument with the `MakeStateful` transformation, specifying the parameter/result tensor name pairs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n--input_model <INPUT_MODEL> --transform \"MakeStateful[param_res_names={'tensor_name_1':'tensor_name_4','tensor_name_3':'tensor_name_6'}]\"\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark App with Performance Hints\nDESCRIPTION: This code snippet demonstrates how to run the OpenVINO benchmark application with performance hints for throughput or latency prioritization. The `-hint` parameter is used to specify whether to prioritize throughput (`tput`) or latency. It requires the model and device to be specified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/performance-benchmarks/getting-performance-numbers.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# for throughput prioritization\nbenchmark_app -hint tput -m <model> -d <device>\n# for latency prioritization\nbenchmark_app -hint latency -m <model> -d <device>\n```\n\n----------------------------------------\n\nTITLE: Accessing Results from AsyncInferQueue Requests in OpenVINO Python\nDESCRIPTION: This snippet demonstrates how to access results from `InferRequest` objects within an `AsyncInferQueue` after calling `wait_all` in the OpenVINO Python API. After `wait_all` completes, jobs and their data can be safely accessed using the job's ID, allowing for retrieval of output data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model)\n\n# Create an AsyncInferQueue with 4 infer requests\ninfer_queue = ov.AsyncInferQueue(compiled_model, 4)\n\ninput_data = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n\n# Start asynchronous inference for all the requests\nfor i in range(len(infer_queue)):\n    infer_queue.start_async({compiled_model.inputs[0]: input_data})\n\n# Wait for all the requests to finish\ninfer_queue.wait_all()\n\n# Access the results\nfor i in range(len(infer_queue)):\n    results = infer_queue.get_result(i)\n    print(results)\n```\n\n----------------------------------------\n\nTITLE: Set Pipeline Parallelism for Heterogeneous Execution with OpenVINO (C++)\nDESCRIPTION: This C++ snippet configures pipeline parallelism for heterogeneous execution in OpenVINO. It sets the `ov::hint::model_distribution_policy` to `PIPELINE_PARALLEL` when compiling the model. The OpenVINO runtime library is required.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/hetero-execution.rst#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"path_to_your_model.xml\");\n\n    // [set_pipeline_parallelism]\n    ov::CompiledModel compiled_model = core.compile_model(model, \"HETERO:dGPU,iGPU,CPU\",\n                                                           ov::hint::model_distribution_policy(ov::hint::ModelDistributionPolicy::PIPELINE_PARALLEL));\n    // [set_pipeline_parallelism]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Convert Keras Model to OpenVINO\nDESCRIPTION: This snippet shows how to convert a Keras model loaded from `tf.keras.applications` to an OpenVINO model using `openvino.convert_model`. Requires the `tensorflow` package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_15\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nmodel = tf.keras.applications.ResNet50(weights=\"imagenet\")\nov_model = ov.convert_model(model)\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library for OpenVINO Core\nDESCRIPTION: This snippet creates an object library `openvino_core_obj` from the source files and public headers. An object library contains the compiled object files but is not linked into a library until it is linked into another target. This can improve build times and code reuse.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(openvino_core_obj OBJECT ${LIBRARY_SRC} ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Compiling Model on Specific Tile Python\nDESCRIPTION: This Python snippet demonstrates how to compile a model on a specific tile of a GPU device (identified by ID and tile ID) using `ov::Core::compile_model()`. It utilizes the `compile_model_gpu_with_id_and_tile` fragment from the specified Python file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# [compile_model_gpu_with_id_and_tile]\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model=model, device_name=\"GPU.1.0\")\n# [compile_model_gpu_with_id_and_tile]\n```\n\n----------------------------------------\n\nTITLE: Wrap Model with NNCF - PyTorch\nDESCRIPTION: This code snippet demonstrates how to wrap the original model with the NNCF object using the `create_compressed_model()` API in PyTorch. This API will transform the model and add required operations for compression. The model can then be used normally.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncompression_ctrl, model = create_compressed_model(model, nncf_config)\n```\n\n----------------------------------------\n\nTITLE: NMSRotated Layer Definition XML\nDESCRIPTION: This XML snippet defines a NMSRotated layer with specific input and output port dimensions, data types, and attributes like sort_result_descending and output_type. It demonstrates how to configure the layer within an OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/nms-rotated-13.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"NMSRotated\" ... >\n    <data sort_result_descending=\"true\" output_type=\"i64\" clockwise=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>100</dim>\n            <dim>5</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>\n            <dim>5</dim>\n            <dim>100</dim>\n        </port>\n        <port id=\"2\"/> <!-- 10 -->\n        <port id=\"3\"/>\n        <port id=\"4\"/>\n    </input>\n    <output>\n        <port id=\"6\" precision=\"I64\">\n            <dim>150</dim> <!-- min(100, 10) * 3 * 5 -->\n            <dim>3</dim>\n        </port>\n        <port id=\"7\" precision=\"FP32\">\n            <dim>150</dim> <!-- min(100, 10) * 3 * 5 -->\n            <dim>3</dim>\n        </port>\n        <port id=\"8\" precision=\"I64\">\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Initialize NV12 two-plane preprocessing (C)\nDESCRIPTION: This C code snippet initializes preprocessing for NV12 two-plane video input in OpenVINO. It defines the necessary preprocessing steps for models consuming NV12 data. Requires OpenVINO library and the C API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_31\n\nLANGUAGE: c\nCODE:\n```\nov_preprocess_input_info_set_element_type(input_info, OPENVINO_TYPE_U8, &except);\nov_preprocess_input_tensor_info_set_color_format(input_tensor_info,\n                                                                 NV12_TWO_PLANES,\n                                                                 &except);\nov_preprocess_input_tensor_info_set_memory_type(input_tensor_info, SURFACE, &except);\nov_preprocess_input_model_info_set_layout(input_model_info, nhwc_layout, &except);\nov_preprocess_input_process_info_preprocess_convert_color(input_process_info, BGR, &except);\nmodel = ov_preprocess_build(preprocessor, &except);\n```\n\n----------------------------------------\n\nTITLE: Wrap DMA-BUF FD with RemoteTensor - OpenVINO™ C++\nDESCRIPTION: This code snippet demonstrates wrapping a DMA-BUF file descriptor (Linux) with an OpenVINO™ RemoteTensor.  This enables OpenVINO™ to use memory allocated and managed by other processes or devices through DMA-BUF. The code depends on having a valid DMA-BUF file descriptor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/npu-device/remote-tensor-api-npu-plugin.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nint dma_buf_fd = get_dmabuf_fd(); // Acquire dma_buf fd\nsize_t tensor_size = get_tensor_size(); // Get buffer size for the tensor\nauto remote_tensor = remote_context.create_tensor(remote_context.get_params(), tensor_size, dma_buf_fd);\n```\n\n----------------------------------------\n\nTITLE: Shape with Defined Rank and Static Dimensions C++\nDESCRIPTION: These code snippets demonstrate how to create PartialShape and Shape objects with defined rank and fully defined dimensions in OpenVINO using C++.\nThey initialize `ov::PartialShape` and `ov::Shape` objects with specific static dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/docs/shape_propagation.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nov::PartialShape({1, 3, 400, 400});  // rank == 4\nov::Shape({1, 3, 400, 400});  // rank == 4\nov::PartialShape({5});  // rank == 1, one-dimensional tensor with five values in it\nov::PartialShape({});  // rank == 0, scalar -- zero-dimensional tensor with single value in it\nov::PartialShape({1});  // rank == 1, one-dimensional tensor with single value in it\nov::PartialShape({1, 3, 0, 0});  // rank == 4, four-dimensional tensor with no value in it empty tensor\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties and Compile Definitions for Frontend Library in CMake\nDESCRIPTION: This snippet sets target properties to disable interprocedural optimization, defines the frontend library suffix based on the OS and version, and sets compile definitions for the frontend library prefix and suffix.  It defines the library name and versioning scheme. Conditional logic handles variations for Apple platforms.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n# TODO: fix lto\nset_target_properties(${TARGET_NAME}_obj PROPERTIES\n    INTERPROCEDURAL_OPTIMIZATION_RELEASE OFF)\n\nset(FRONTEND_LIB_SUFFIX \"${FRONTEND_NAME_SUFFIX}${OV_BUILD_POSTFIX}\")\nif(APPLE)\n    set(FRONTEND_LIB_SUFFIX \"${FRONTEND_LIB_SUFFIX}${OpenVINO_VERSION_SUFFIX}${CMAKE_SHARED_LIBRARY_SUFFIX}\")\nelse()\n    set(FRONTEND_LIB_SUFFIX \"${FRONTEND_LIB_SUFFIX}${CMAKE_SHARED_LIBRARY_SUFFIX}${OpenVINO_VERSION_SUFFIX}\")\nendif()\n\ntarget_compile_definitions(${TARGET_NAME}_obj PRIVATE\n    FRONTEND_LIB_PREFIX=\"${CMAKE_SHARED_LIBRARY_PREFIX}${FRONTEND_NAME_PREFIX}\"\n    FRONTEND_LIB_SUFFIX=\"${FRONTEND_LIB_SUFFIX}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Compilers for OpenVINO on Linux\nDESCRIPTION: This command installs necessary compilers (cmake, c-compiler, cxx-compiler, make) and sets environment variables for linking OpenVINO libraries from Conda on Linux.  This allows you to compile applications that use OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conda.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nconda install cmake c-compiler cxx-compiler make\nconda env config vars set LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Detokenize Output\nDESCRIPTION: This code snippet demonstrates how to detokenize the output tokens using a tokenizer object. It takes the accumulated tokens and converts them back into human-readable text. It then prints the original prompt and the generated text.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ntext_result = detokenizer(tokens_result)[\"string_output\"]\nprint(f\"Prompt:\\n{text_input[0]}\")\nprint(f\"Generated:\\n{text_result[0]}\")\n```\n\n----------------------------------------\n\nTITLE: Add TensorFlow Hub Model to Precommit YAML\nDESCRIPTION: Adds a new test for a model from the TensorFlow Hub by adding a line to the `precommit.yml` file. This line specifies the model name and its link, separated by a comma. This registers the model for end-to-end validation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/e2e_tests/README.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nmovenet/singlepose/lightning,https://www.kaggle.com/models/google/movenet/frameworks/tensorFlow2/variations/singlepose-lightning/versions/4\n```\n\n----------------------------------------\n\nTITLE: Compiling TensorFlow Lite Model in OpenVINO using Python\nDESCRIPTION: This code snippet demonstrates compiling a TensorFlow Lite model in OpenVINO using Python. It compiles the '<INPUT_MODEL>.tflite' file for execution on the 'AUTO' device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_8\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\ncompiled_model = ov.compile_model(\"<INPUT_MODEL>.tflite\", \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Create RemoteContext from cl_queue (C)\nDESCRIPTION: This snippet demonstrates how to create an `ov::RemoteContext` from an existing OpenCL command queue (`cl_command_queue`) using the GPU plugin's C API. It initializes the OpenVINO core and retrieves the extension. Requires the OpenVINO runtime and intel_gpu ocl headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_4\n\nLANGUAGE: c\nCODE:\n```\n//! [context_from_cl_queue]\n#include <openvino/runtime.h>\n#include <openvino/runtime/intel_gpu/ocl/ocl.h>\n\n#if defined(OPENVINO_USE_OPENCL)\n#include <CL/cl.h>\n#endif\n\nvoid create_context_from_cl_queue(cl_command_queue queue) {\n    // context from user-defined OpenCL queue\n    ov_core_t* core = ov_core_create();\n    ov_remote_context remote_context = ov_core_get_extension_ocl_queue(core, queue);\n    ov_remote_context_free(remote_context);\n    ov_core_free(core);\n}\n//! [context_from_cl_queue]\n```\n\n----------------------------------------\n\nTITLE: CLI Model Conversion\nDESCRIPTION: This command-line instruction demonstrates how to convert a model using the OpenVINO Model Conversion API (ovc).  It takes the path to the model as input and converts it into the OpenVINO Intermediate Representation (IR) format (.xml + .bin).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-reshape-ssd.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\novc ./test_data/models/mobilenet-ssd\n```\n\n----------------------------------------\n\nTITLE: INT4 Asymmetric Weight Compression with NNCF\nDESCRIPTION: This code snippet demonstrates how to compress model weights using the INT4 Asymmetric mode (INT4_ASYM) in NNCF. This mode aims to balance speed and accuracy by quantizing weights asymmetrically with a non-fixed zero point.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression/4-bit-weight-quantization.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom nncf import compress_weights\nfrom nncf import CompressWeightsMode\n\ncompressed_model = compress_weights(model, mode=CompressWeightsMode.INT4_ASYM)\n```\n\n----------------------------------------\n\nTITLE: Query Device Information C++\nDESCRIPTION: This C++ snippet queries the OpenVINO Runtime for available devices and prints their supported metrics and plugin configuration parameters. It leverages the Query Device API to retrieve device-specific properties.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-query-device.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <iostream>\n#include <openvino/openvino.hpp>\n\nint main() {\n    std::cout << \"Hello Query Device Sample\" << std::endl;\n\n    // Create OpenVINO Core object\n    ov::Core core;\n\n    // Query available devices\n    std::vector<std::string> availableDevices = core.get_available_devices();\n\n    std::cout << \"[ INFO ] Available devices:\" << std::endl;\n    for (const auto& deviceName : availableDevices) {\n        std::cout << \"[ INFO ] \" << deviceName << std::endl;\n        std::cout << \"[ INFO ] \\tSUPPORTED_PROPERTIES:\" << std::endl;\n        auto supportedProperties = core.get_property(deviceName, ov::supported_properties);\n        for (const auto& propertyName : supportedProperties) {\n            try {\n                auto propertyValue = core.get_property(deviceName, propertyName.name());\n                std::cout << \"[ INFO ] \\t\\t\" << propertyName.name() << \" : \" << propertyValue.as<std::string>() << std::endl;\n            } catch (const std::exception& e) {\n                std::cerr << \"[ ERROR ] Cannot get property \" << propertyName.name() << \" for device \" << deviceName << \": \" << e.what() << std::endl;\n            }\n        }\n    }\n\n    return 0;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Uniting Matcher Passes into GraphRewrite (C++)\nDESCRIPTION: This example demonstrates how multiple matcher passes can be united into a single GraphRewrite pass.  This is useful for efficient execution of multiple related transformations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api.rst#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n//! [matcher_pass:manager2]\nov::pass::Manager manager;\nmanager.register_pass<ov::pass::GraphRewrite>()->\n    add_matcher<TemplateMatcher>();\nmanager.run_on_model(model);\n//! [matcher_pass:manager2]\n```\n\n----------------------------------------\n\nTITLE: Convert a Hugging Face model to OpenVINO IR using optimum-cli\nDESCRIPTION: Demonstrates how to use the optimum-cli tool to convert a Hugging Face model to the OpenVINO IR format. This command-line interface provides a convenient way to perform the conversion.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\noptimum-cli export openvino --model <MODEL_NAME> <NEW_MODEL_NAME>\n```\n\n----------------------------------------\n\nTITLE: Check if Model is Dynamic in OpenVINO (C)\nDESCRIPTION: This function checks whether an OpenVINO model contains any operations with dynamic shapes. It takes a pointer to the `ov_model_t` and returns `true` if the model has dynamic shapes, and `false` otherwise.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_32\n\nLANGUAGE: C\nCODE:\n```\nbool ov_model_is_dynamic(const ov_model_t* model)\n```\n\n----------------------------------------\n\nTITLE: Adding PaddlePaddle Frontend with CMake\nDESCRIPTION: This CMake snippet uses the `ov_add_frontend` function to configure the PaddlePaddle frontend within the OpenVINO project. It sets the name to 'paddle', specifies that it's a linkable frontend, requires Protocol Buffers (both full and lite), provides a description, and links against specified libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_frontend(NAME paddle\n                LINKABLE_FRONTEND\n                PROTOBUF_REQUIRED\n                PROTOBUF_LITE\n                FILEDESCRIPTION \"FrontEnd to load and convert PaddlePaddle file format\"\n                LINK_LIBRARIES openvino::util openvino::core::dev)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Files in CMake\nDESCRIPTION: This CMake snippet installs the Python source files located in `${OpenVINOPython_SOURCE_DIR}/src/openvino` into the `${OV_CPACK_PYTHONDIR}` directory. It excludes \"test_utils\" and \"torchvision/requirements.txt\" and assigns the component `${OV_CPACK_COMP_PYTHON_OPENVINO}_${pyversion}`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY ${OpenVINOPython_SOURCE_DIR}/src/openvino\n        DESTINATION ${OV_CPACK_PYTHONDIR}\n        COMPONENT ${OV_CPACK_COMP_PYTHON_OPENVINO}_${pyversion}\n        ${OV_CPACK_COMP_PYTHON_OPENVINO_EXCLUDE_ALL}\n        USE_SOURCE_PERMISSIONS\n        PATTERN \"test_utils\" EXCLUDE\n        PATTERN \"torchvision/requirements.txt\" EXCLUDE)\n```\n\n----------------------------------------\n\nTITLE: OpenCL Context Sharing with OpenVINO from Compiled Model (C++)\nDESCRIPTION: Demonstrates how to retrieve the OpenCL context from a compiled OpenVINO model object and use it to execute OpenCL kernels.  This example shows retrieving the context and queue and demonstrates how to configure cl::CommandQueue using the context and queue retrieved from OpenVINO. Requires OpenVINO runtime and OpenCL.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_42\n\nLANGUAGE: cpp\nCODE:\n```\n//! [context_sharing_get_from_ov]\n// Get context from compiled model.\nauto context = compiled_model.get_context();\n// Get OpenCL queue from context.\nauto queue = context.get_property(ov::intel_gpu::queue());\ncl::CommandQueue user_queue(context, queue);\n//Fill buffer via OpenCL.\nuser_queue.enqueueFill(cl_buffer, pattern, 0, size);\nuser_queue.finish();\n\n// Pass shared buffer to the InferRequest.\ninfer_request.set_tensor(input_tensor_name, tensor);\n\n// Do inference.\ninfer_request.infer();\n//! [context_sharing_get_from_ov]\n```\n\n----------------------------------------\n\nTITLE: Shape with Defined Rank and Undefined Dimensions C++\nDESCRIPTION: These code snippets show how to create PartialShape objects with a defined rank but undefined dimensions in OpenVINO using C++.\nThey initialize `ov::PartialShape` objects with specific ranks, where some or all dimensions are dynamic.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/docs/shape_propagation.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov::PartialShape({1, 3, Dimension::dynamic(), Dimension::dynamic()});  // rank == 4, two static dimensions and two fully dynamic dimension\nov::PartialShape({Dimension::dynamic(), Dimension::dynamic()});  // rank == 2, all dimensions are fully dynamic\nov::PartialShape::dynamic(5); // rank == 5, all dimensions are fully dynamic\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Python Class Inheriting from pybind Class\nDESCRIPTION: This code snippet defines a custom Python class `MyTensor` that inherits from a pybind-generated class `MyTensorBase`. It overrides the `say_hello` method to handle different argument types and return different values accordingly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n# Inherit from pybind implementation everything is preserved\nclass MyTensor(MyTensorBase):\n    \"\"\"MyTensor created as part of tutorial, it overrides pure-pybind class.\"\"\"\n    # Function name must be aligned with pybind one!\n    def say_hello(self, arg=None):\n        \"\"\"Say hello to the world!\n\n        :param arg: Argument of the function.\n        :type arg: Union[str, int], optional\n        \"\"\"\n        # If None invoke pybind implementation\n        if arg is None:\n            super().say_hello()\n            return None\n        # If string invoke pybind implementation and return 0\n        elif type(arg) is str:\n            super().say_hello(arg)\n            return 0\n        # If int convert argument, invoke pybind implementation\n        # and return string\n        elif type(arg) is int:\n            # Additionally if less than 3, return itself plus one\n            if arg < 3:\n                return arg + 1\n            super().say_hello(str(arg))\n            return \"Converted int to string!\"\n        # Otherwise raise an error\n        raise TypeError(\"Unsupported argument type!\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Conda Environment\nDESCRIPTION: This command creates a new Conda environment named `py310` with Python version 3.10. This isolates the OpenVINO installation and dependencies from other projects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conda.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nconda create --name py310 python=3.10\n```\n\n----------------------------------------\n\nTITLE: Setting Callback for Inference in Python\nDESCRIPTION: This Python code demonstrates setting a callback function to be executed upon inference completion using `ov::InferRequest::set_callback`. Requires OpenVINO and a compiled model.  The callback receives the infer_request as an argument.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ndef callback(infer_request):\n    print(\"Inference complete!\")\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\ncompiled_model = ie.compile_model(model, \"CPU\")\ninfer_request = compiled_model.create_infer_request()\n\ninput_tensor = infer_request.get_input_tensor()\ninput_tensor.data[:] = np.random.normal(0, 1, input_tensor.shape)\n\ninfer_request.set_callback(callback)\ninfer_request.start_async()\n```\n\n----------------------------------------\n\nTITLE: Node Operations: Get Layer Name in OpenVINO (C++)\nDESCRIPTION: The `ov::Output::get_any_name` API is used to retrieve the name of a layer within the OpenVINO model. This function is helpful for identifying and manipulating specific layers during inference or model analysis.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_nv12_input_classification/README.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n``ov::Output::get_any_name``\n```\n\n----------------------------------------\n\nTITLE: Clone with New Inputs in Python\nDESCRIPTION: This snippet demonstrates how to override the `clone_with_new_inputs` method in Python for a custom OpenVINO operation.  This method creates a copy of the operation with new inputs, enabling graph manipulation routines to connect the operation to different nodes during optimization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-openvino-operations.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n    def clone_with_new_inputs(self, new_args):\n        \"\"\"Creates a copy of the node with new inputs.\n\n        :param new_args: New inputs.\n        :return: A copy of the node with new inputs.\n        \"\"\"\n        # The new_args list is validated by the framework, so we don't need to\n        # validate it here.\n        return ExampleOp(new_args[0], self.get_attribute('val'))\n```\n\n----------------------------------------\n\nTITLE: Importing necessary modules for OpenVINO transformations\nDESCRIPTION: This snippet imports necessary modules from the openvino and openvino.opset13 libraries. These imports are required for creating and manipulating OpenVINO models and operations for transformation patterns.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.tools.ovc.support_api.mxnet_helper import get_mxnet_opset_version\nimport openvino.runtime as ov\nfrom openvino.runtime import PartialShape, Dimension\nfrom openvino.runtime import Model\nfrom openvino.opset13 import relu, sigmoid, add, constant, parameter, result\nfrom openvino.opset13 import reshape, transpose, gather, concat, split, tile, squeeze, unsqueeze\nfrom openvino.opset13 import mvn, gelu, softplus, reduce_sum, subtract, multiply, divide, exp, log\nfrom openvino.opset13 import log_softmax, hsigmoid, bucketize, round, prelu, space_to_depth\nfrom openvino.opset13 import depth_to_space, fake_quantize, convert, group_convolution, extract_image_patches\nfrom openvino.opset13 import lstm_cell, minimum, maximum, mod, erf, floor, ceiling, elu\nfrom openvino.opset13 import embedding_segments_sum, embedding_bag_offsets_sum, embedding_bag_packed_sum\nfrom openvino.opset13 import range, one_hot, cum_sum, equal, not_equal, greater, greater_equal, less, less_equal, select\nfrom openvino.opset13 import is_finite, is_inf, is_nan\nfrom openvino.opset13 import pad, convolution, group_convolution, deformable_convolution\nfrom openvino.opset13 import avg_pool, max_pool, adaptive_avg_pool, adaptive_max_pool, roi_pooling, psroi_pooling\nfrom openvino.opset13 import batch_to_space, space_to_batch, strided_slice, topk, reorg_yolo, non_max_suppression\nfrom openvino.opset13 import non_zero, irfft, rfft, hamming_window, hann_window, ctc_greedy_decoder, ctc_loss\nfrom openvino.opset13 import read_value, assign, reverse_sequence, scatter_elements_update, scatter_update\nfrom openvino.opset13 import gather_nd, scatter_nd_update, deformable_convolution, convolution_backprop_data\nfrom openvino.opset13 import selu, asin, acos, atan, sinh, cosh, tanh, asinh, acosh, atanh\nfrom openvino.opset13 import atan2, broadcast, compress, detection_output, embedding_lookup, grn, if_op, log2, log10\nfrom openvino.opset13 import nv12_to_bgr, nv12_to_rgb, psroi_align, quantized_convolution, quantized_group_convolution\nfrom openvino.opset13 import proposal, reduce_l1, reduce_l2, region_yolo, reshape, reverse\nfrom openvino.opset13 import roll, sigmoid_crossentropy, sinh, space_to_depth, split, tf_idf_vectorizer\nfrom openvino.opset13 import variadic_split\nfrom openvino.opset13 import get_constant_from_source, Interpolate, lrn, normalize_l2, pooling\nfrom openvino.opset13 import thresholded_relu, DepthToSpace, EmbeddingSegmentSum, GatherElements, Sqrt\nfrom openvino.opset13 import ShuffleChannels, LSTMSequence, GRUSequence, RNNSequence\nfrom openvino.opset13 import Loop, TensorIterator, ROIAlign, ConvertLike, BroadcastLike\nfrom openvino.opset13 import FFT, IFFT\n\nfrom openvino.frontend import FrontEndManager\nfrom openvino.offline_transformations import compress_model_transformation, ApplyLowLatencyTransformation\n\nfrom openvino.tools.ovc.cmd_parser import OVArgumentParser\nfrom openvino.tools.ovc.convert import InputModel\n\nfrom openvino.tools.ovc.utils import get_ov_friendly_name\nimport numpy as np\nimport sys\n```\n\n----------------------------------------\n\nTITLE: Compiling Model on Specific Tile C++\nDESCRIPTION: This C++ snippet demonstrates how to compile a model on a specific tile of a GPU device (identified by ID and tile ID) using `ov::Core::compile_model()`. It utilizes the `compile_model_gpu_with_id_and_tile` fragment from the specified C++ file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n// [compile_model_gpu_with_id_and_tile]\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"GPU.1.0\");\n// [compile_model_gpu_with_id_and_tile]\n```\n\n----------------------------------------\n\nTITLE: Launching All Notebooks with Jupyter\nDESCRIPTION: This command launches Jupyter Lab, opening the file browser in the `notebooks` directory.  Users can then select and open any notebook within the directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/run-notebooks.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njupyter lab notebooks\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO Node.js via npm\nDESCRIPTION: This command installs the OpenVINO Node.js package from the npm registry. It allows you to use the OpenVINO Runtime C++ API subset in a Node.js environment. This package runs only in Node.js and requires Node.js and npm to be installed on the system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-npm.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnpm install openvino-node\n```\n\n----------------------------------------\n\nTITLE: Run Benchmark App (Python)\nDESCRIPTION: This command runs the OpenVINO benchmark application with default options on a model specified by its XML file. The application will load the model and perform inference on CPU for 60 seconds using randomly generated data inputs. It displays the benchmark parameters and reports the minimum, average, and maximum inference latency and the average throughput upon completion.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app -m model.xml\n```\n\n----------------------------------------\n\nTITLE: Configuring Fallback Devices with Heterogeneous Execution in Python\nDESCRIPTION: This Python snippet demonstrates how to configure fallback devices using the OpenVINO Heterogeneous execution mode. It sets the GPU device to enable profiling data and uses the default execution precision, while the CPU device is configured to perform inference in fp32. This allows for leveraging the GPU for primary execution and falling back to the CPU for unsupported operations or when the GPU is overloaded.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/hetero-execution.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef configure_fallback_devices():\n    import openvino.runtime as ov\n\n    core = ov.Core()\n    # ! [configure_fallback_devices]\n    device_priorities = ['GPU', 'CPU']\n    config = {}\n    config['GPU.enable_profiling'] = 'true'\n    config['CPU.hint.inference_precision'] = 'fp32'\n    compiled_model = core.compile_model('model.xml', device_priorities, config=config)\n    # ! [configure_fallback_devices]\n```\n\n----------------------------------------\n\nTITLE: TensorFlow: Create Calibration Dataset\nDESCRIPTION: Creates a calibration dataset for TensorFlow models using `nncf.Dataset`. The transformation function prepares the dataset by extracting the input tensor, ensuring it is suitable for model inference during quantization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndataset = nncf.Dataset(tf_dataset, transform_fn)\n\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Accessing Input by Index C++\nDESCRIPTION: Demonstrates how to access a specific input of a model by its index using the `ov::preprocess::PrePostProcessor` in C++. This is used when dealing with models with multiple inputs and the inputs are to be addressed by index.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n// Get input Tensor by index and set input tensor information.\n// We apply preprocessing demanding input tensor to have 'u8' type and 'NCHW' layout.\nppp.input(1).tensor().set_element_type(ov::element::u8).set_layout(\"NCHW\");\n// 2) Adding explicit preprocessing steps\nppp.input(1).preprocess().convert_element_type(ov::element::f32).convert_layout(\"NHWC\");\n// 3) Set input model information.\nppp.input(1).model().set_layout(\"NHWC\");\n// 4) Apply preprocessing modifying the original model\nmodel = ppp.build();\n```\n\n----------------------------------------\n\nTITLE: Using Pass Manager (C++)\nDESCRIPTION: This code snippet demonstrates the basic usage of `ov::pass::Manager`, which is a container class that can store and execute a list of transformations. It registers and applies transformation passes on a model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api.rst#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n//! [matcher_pass:manager3]\nov::pass::Manager manager;\nmanager.register_pass<ov::pass::Validate>();\nmanager.run_on_model(model);\n//! [matcher_pass:manager3]\n```\n\n----------------------------------------\n\nTITLE: Setting Shape of Input Tensor in TypeScript\nDESCRIPTION: This code snippet shows the `setShape` method of the InputTensorInfo interface. This method allows setting the shape (dimensions) of the input tensor. It accepts an array of numbers representing the shape as a parameter and returns an `InputTensorInfo` object for chaining.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InputTensorInfo.rst#_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nsetShape(shape): InputTensorInfo\n```\n\n----------------------------------------\n\nTITLE: Load a saved OpenVINO IR model\nDESCRIPTION: Loads a pre-converted OpenVINO IR model from a specified directory. This is more efficient than converting the model on-the-fly every time it is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-optimum-intel.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = OVModelForCausalLM.from_pretrained(\"ov_model\")\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO AUTO Plugin Unit Tests\nDESCRIPTION: This snippet shows the cmake command to enable building of unit tests for the OpenVINO project. It sets the `CMAKE_BUILD_TYPE` to `Release` and `ENABLE_TESTS` to `ON` which is a prerequisite for building the unit tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/tests.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncmake -DCMAKE_BUILD_TYPE=Release \\\n       -DENABLE_TESTS=ON \\\n```\n\n----------------------------------------\n\nTITLE: Running benchmark_app with CPU Fallback Enabled\nDESCRIPTION: This snippet demonstrates running `benchmark_app` with CPU fallback enabled by loading the configuration from `config.json` using `-load_config ./config.json`. The configuration file contains the `ENABLE_STARTUP_FALLBACK` set to `YES`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/tests.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nopenvino/bin/intel64/Release$ ./benchark_app -m openvino/src/core/tests/models/ir/add_abc.xml -d AUTO -load_config ./config.json\n```\n\n----------------------------------------\n\nTITLE: Plugin Constructor C++\nDESCRIPTION: This snippet showcases the plugin constructor implementation. It emphasizes the importance of device availability checks and driver initialization within the constructor.  It also calls `set_device_name()` to register the device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nPlugin::Plugin(const std::shared_ptr<ov::runtime::IPluginService>& service, const std::string& device_name) {\n    //!\n    // Add devices to the plugin\n    //\n    set_device_name(device_name);\n    m_service = service;\n\n    //!\n    // Create backend\n    //\n    m_backend = ov::runtime::Backend::create(\"REFERENCE\");\n\n    //!\n    // Create task executor for waiting on completion of device tasks\n    //\n    m_waitExecutor = m_backend->create_task_executor({\"name\", \"TemplatePlugin_Wait\"});\n\n    IE_ASSERT(m_waitExecutor);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Pad Layer Definition - Edge Mode - XML\nDESCRIPTION: Defines a Pad layer in OpenVINO's XML format, configured with 'edge' pad mode. It specifies input and output port dimensions and the pads_begin and pads_end values. This example demonstrates padding a 1x3x32x40 tensor with specific padding values derived from the edge of the tensor, resulting in a 2x8x37x48 output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-1.rst#_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Pad\" ...>\n    <data pad_mode=\"edge\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>40</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>     <!-- pads_begin = [0, 5, 2, 1]  -->\n        </port>\n        <port id=\"2\">\n            <dim>4</dim>     <!-- pads_end = [1, 0, 3, 7] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>2</dim>     <!-- 2 = 0 + 1 + 1 = pads_begin[0] + input.shape[0] + pads_end[0] -->\n            <dim>8</dim>     <!-- 8 = 5 + 3 + 0 = pads_begin[1] + input.shape[1] + pads_end[1] -->\n            <dim>37</dim>    <!-- 37 = 2 + 32 + 3 = pads_begin[2] + input.shape[2] + pads_end[2] -->\n            <dim>48</dim>    <!-- 48 = 1 + 40 + 7 = pads_begin[3] + input.shape[3] + pads_end[3] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Prepare Validation Function - ONNX\nDESCRIPTION: This example shows how to construct a validation function for ONNX models.  It receives an ONNX model and a validation dataset, executes inference (simulated here), and returns an accuracy metric (a placeholder value of 0.5). This provides a template for implementing your own accuracy validation logic.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/quantizing-with-accuracy-control.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# [validation]\nimport onnx\nfrom onnx.checker import check_model\n\nmodel = onnx.load_model(\"model.onnx\")\ncheck_model(model)\n\n\n\ndef validate(model, validation_dataset):\n    # actual validation\n    return 0.5\n\n# [validation]\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Inference with a GPTQ Model\nDESCRIPTION: This code snippet demonstrates how to load a 4-bit GPTQ model from Hugging Face using the `optimum` library and run inference with it. It requires the `optimum[openvino]` and `auto-gptq` packages to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression/4-bit-weight-quantization.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom optimum.intel.openvino import OVModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline\n\n# Load model from Hugging Face already optimized with GPTQ\nmodel_id = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\nmodel = OVModelForCausalLM.from_pretrained(model_id, export=True)\n\n# Inference\ntokenizer = AutoTokenizer.from_pretrained(model_id)\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nphrase = \"The weather is\"\nresults = pipe(phrase)\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Custom Streamer in Python\nDESCRIPTION: This Python code demonstrates how to create a custom streamer class in Python inheriting from `ov_genai.StreamerBase`. The `put` method allows custom processing of each token ID. The return value is a boolean indicating if generation should be stopped.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport openvino_genai as ov_genai\n\nclass CustomStreamer(ov_genai.StreamerBase):\n   def __init__(self, tokenizer):\n      ov_genai.StreamerBase.__init__(self)\n      self.tokenizer = tokenizer\n   def put(self, token_id) -> bool:\n      # Decode tokens and process them.\n      # Streamer returns a flag indicating whether generation should be stopped.\n      # In Python, `return` can be omitted. In that case, the function will return None\n      # which will be converted to False, meaning that generation should continue.\n      # return stop_flag\n   def end(self):\n      # Decode tokens and process them.\n\npipe = ov_genai.LLMPipeline(model_path, \"CPU\")\npipe.generate(\"The Sun is yellow because\", streamer=CustomStreamer(), max_new_tokens=100)\n```\n\n----------------------------------------\n\nTITLE: Validate and Infer Types in C++\nDESCRIPTION: This snippet demonstrates how to override the `validate_and_infer_types` method in C++ for a custom OpenVINO operation.  This method validates operation attributes and calculates output shapes using attributes of the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-openvino-operations.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n    void validate_and_infer_types() override {\n        OPENVINO_ASSERT(get_input_size() == 1, \"Expected 1 input, but got \", get_input_size());\n        set_output_type(0, get_input_element_type(0), get_input_partial_shape(0));\n    }\n```\n\n----------------------------------------\n\nTITLE: Setting Threading Interface\nDESCRIPTION: Sets the threading interface for the target using the `ov_set_threading_interface_for` macro. This configures how the target will handle threading and parallelism.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_41\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Launching a Single Notebook with Jupyter\nDESCRIPTION: This command launches a specific Jupyter notebook, such as the Monodepth notebook, using Jupyter Lab. It requires Jupyter Lab to be installed and accessible in the system's PATH.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/run-notebooks.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\njupyter lab notebooks/vision-monodepth/vision-monodepth.ipynb\n```\n\n----------------------------------------\n\nTITLE: Using Extended Helper Function\nDESCRIPTION: This code demonstrates how to use the newly added `top1_index` function through the `openvino.helpers` module. It showcases a simple usage scenario, computing the index of the highest value in a list.  It assumes the OpenVINO project has been rebuilt after adding the custom module.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.helpers as ov_helpers\n\nov_helpers.top1_index([0.7, 2.99, 3.0, -1.0])\n>>> 2\n```\n\n----------------------------------------\n\nTITLE: NV12 Preprocessing with Single Plane (Single Batch) in OpenVINO (C++)\nDESCRIPTION: Demonstrates how to preprocess NV12 data using a single plane in a single batch scenario within OpenVINO. This snippet likely involves setting input tensors for the inference request, enabling OpenVINO to process the NV12 data directly. Requires OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_36\n\nLANGUAGE: cpp\nCODE:\n```\n//! [single_batch]\nauto input_tensor = ov::Tensor(ov::element::u8, {height + height / 2, width}, input_data);\nauto input_port = compiled_model.input();\ninfer_request.set_tensor(input_port, input_tensor);\n//! [single_batch]\n```\n\n----------------------------------------\n\nTITLE: Windows-Specific Settings\nDESCRIPTION: This snippet sets Windows-specific build configurations, including delay loading for NODE.EXE, setting the path to node.lib, defining the source file for delay loading, and defining the node-lib.def file. It also sets the DELAYIMP_LIB.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n    set(CMAKE_SHARED_LINKER_FLAGS /DELAYLOAD:NODE.EXE)\n    set(CMAKE_JS_LIB ${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/node.lib)\n    set(CMAKE_JS_SRC ${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/win_delay_load_hook.cc)\n\n    set(CMAKE_JS_NODELIB_DEF ${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/node-lib.def)\n    set(CMAKE_JS_NODELIB_TARGET ${CMAKE_JS_LIB})\n    set(DELAYIMP_LIB delayimp.lib)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Extending Existing API Modules\nDESCRIPTION: This snippet demonstrates how to add a new directory `custom_module` inside `openvino/helpers` to extend existing API. It includes creating `__init__.py` and other python files to organize the new functionality. This allows for adding custom helpers and functionalities to the OpenVINO™ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nopenvino/\n├── frontend/\n├── helpers/                        <-- Working directory\n│   ├── __init__.py\n│   ├── custom_module/              <-- New directory\n│   │   ├── __init__.py             <-- New file\n│   │   ├── custom_helpers.py       <-- New file\n│   │   └── packing.py\n│   ├── ...\n│   ├── runtime/\n│   ├── test_utils/\n│   └── __init__.py\n└── utils.py\n```\n\n----------------------------------------\n\nTITLE: ONNX: Create Calibration Dataset\nDESCRIPTION: Creates an instance of the nncf.Dataset class using an ONNX dataset. The transformation function prepares the dataset sample by extracting only the input data for model inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndataset = nncf.Dataset(onnx_dataset, transform_fn)\n\n```\n\n----------------------------------------\n\nTITLE: Save Checkpoint - TensorFlow\nDESCRIPTION: Saves a model checkpoint during QAT in TensorFlow using NNCF. This allows you to preserve the training state for later use or deployment. The checkpoint saves the NNCF-wrapped model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/quantization-aware-training.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnncf.save_checkpoint(model, checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Freeing Model Memory in OpenVINO (C)\nDESCRIPTION: This function releases the memory allocated for an OpenVINO model object. The user must provide a pointer to a valid `ov_model_t` structure. The function returns a status code indicating the success or failure of the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_23\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_model_free(ov_model_t* model)\n```\n\n----------------------------------------\n\nTITLE: Identity Operation Header Definition (C++)\nDESCRIPTION: Defines the header for a template extension Identity operation in C++.  This snippet showcases the declaration of the `Identity` class within the `TemplateExtension` namespace. It serves as a placeholder for real custom operations during extension development.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/frontend-extensions.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nclass Identity : public ov::op::Op {\npublic:\n    OPENVINO_OP(\"Identity\");\n    Identity() = default;\n    Identity(const Output<Node>& arg);\n\n    void validate_and_infer_types() override;\n\n    std::shared_ptr<Node> clone_with_new_inputs(const OutputVector& new_args) const override;\n\n    bool visit_attributes(AttributeVisitor& visitor) override;\n};\n```\n\n----------------------------------------\n\nTITLE: Add OpenVINO Plugin\nDESCRIPTION: This snippet uses the custom CMake function `ov_add_plugin` to configure the plugin. It specifies the plugin name, device name, sources, and other options like version definitions and clang-format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_plugin(NAME ${TARGET_NAME}\n              DEVICE_NAME \"HETERO\"\n              PSEUDO_DEVICE\n              SOURCES ${SOURCES} ${HEADERS}\n              VERSION_DEFINES_FOR src/version.cpp\n              ADD_CLANG_FORMAT)\n```\n\n----------------------------------------\n\nTITLE: Fine-tune Quantized Model - TensorFlow\nDESCRIPTION: Fine-tunes a quantized TensorFlow model with a small learning rate to mitigate accuracy degradation caused by quantization. This process involves training with simulated quantization during both forward and backward passes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/quantization-aware-training.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_loop_fn(model, optimizer, loss_fn, train_dataset, metrics, config.steps_per_epoch)\n    val_loop_fn(model, loss_fn, val_dataset, metrics, config.validation_steps)\n```\n\n----------------------------------------\n\nTITLE: TopK Layer Configuration Example\nDESCRIPTION: This code snippet presents an example of how to configure a TopK layer in OpenVINO. It defines the layer type, attributes (axis, mode, sort, index_element_type), input port dimensions, and output port dimensions. This example demonstrates how to define the behavior of the TopK operation within a neural network.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/top-k-3.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"TopK\" ... >\n      <data axis=\"1\" mode=\"max\" sort=\"value\" index_element_type=\"i64\"/>\n      <input>\n          <port id=\"0\">\n              <dim>6</dim>\n              <dim>12</dim>\n              <dim>10</dim>\n              <dim>24</dim>\n          </port>\n          <port id=\"1\">\n              <!-- k = 3 -->\n          </port>\n      <output>\n          <port id=\"2\">\n              <dim>6</dim>\n              <dim>3</dim>\n              <dim>10</dim>\n              <dim>24</dim>\n          </port>\n          <port id=\"3\">\n              <dim>6</dim>\n              <dim>3</dim>\n              <dim>10</dim>\n              <dim>24</dim>\n          </port>\n      </output>\n  </layer>\n```\n\n----------------------------------------\n\nTITLE: Including Subdirectories with CMake\nDESCRIPTION: This snippet uses the `add_subdirectory` command in CMake to include the 'utils' and 'format_reader' subdirectories in the build process. Each subdirectory is expected to contain its own CMakeLists.txt file that defines how it should be built. This allows for modularization of the project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(utils)\nadd_subdirectory(format_reader)\n```\n\n----------------------------------------\n\nTITLE: GroupNormalization XML Configuration\nDESCRIPTION: This XML snippet demonstrates the configuration of a GroupNormalization layer in OpenVINO.  It specifies the `epsilon` and `num_groups` attributes, along with input and output port dimensions. The `epsilon` attribute provides numerical stability during normalization, and `num_groups` determines how the channels are divided into groups for normalization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/group-normalization-12.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"GroupNormalization\">\n    <data epsilon=\"1e-5\" num_groups=\"4\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>12</dim>\n            <dim>100</dim>\n            <dim>100</dim>\n        </port>\n        <port id=\"1\">\n            <dim>12</dim> <!-- 12 scale values, 1 for each channel -->\n        </port>\n        <port id=\"2\">\n            <dim>12</dim> <!-- 12 bias values, 1 for each channel -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>3</dim>\n            <dim>12</dim>\n            <dim>100</dim>\n            <dim>100</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Initializing Preprocessing with GPU Plugin (C++)\nDESCRIPTION: This C++ code snippet demonstrates initializing preprocessing within the OpenVINO GPU plugin. It involves using `ov::preprocess::InputTensorInfo::set_memory_type()` to specify the memory type for input tensors, using either `ov::intel_gpu::memory_type::surface` or `ov::intel_gpu::memory_type::buffer`, providing the plugin with a hint about the expected type of input tensors, facilitating optimized kernel generation, specifically when using NV12 format with two planes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\n# The code snippet is not available in the provided text.\n```\n\n----------------------------------------\n\nTITLE: C++ Inference Execution\nDESCRIPTION: This command line instruction shows how to run the hello_reshape_ssd executable. It requires the path to the model (.xml), the path to the image, and the device name (e.g., GPU) as command-line arguments to perform inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-reshape-ssd.rst#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nhello_reshape_ssd ./models/person-detection-retail-0013.xml person_detection.bmp GPU\n```\n\n----------------------------------------\n\nTITLE: Disable Deprecated Warnings for Intel LLVM CMake\nDESCRIPTION: This snippet disables deprecated warnings if the compiler being used is Intel LLVM. This can be useful for suppressing warnings related to deprecated features or functions, especially when dealing with legacy code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(OV_COMPILER_IS_INTEL_LLVM)\n    ov_disable_deprecated_warnings()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Upgrade PIP\nDESCRIPTION: This command upgrades the pip package installer to the latest version.  Upgrading pip ensures access to the most recent features and bug fixes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-pip.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython -m pip install --upgrade pip\n```\n\n----------------------------------------\n\nTITLE: Adding TensorFlow Frontend with CMake\nDESCRIPTION: This CMake function adds the TensorFlow frontend to the OpenVINO project. It specifies dependencies like protobuf and links necessary libraries such as 'openvino::core::dev' and 'openvino::frontend::tensorflow_common'. It defines the frontend's file description.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_frontend(NAME tensorflow\n                PROTOBUF_REQUIRED\n                LINKABLE_FRONTEND\n                FILEDESCRIPTION \"FrontEnd to load and convert TensorFlow file format\"\n                LINK_LIBRARIES openvino::core::dev openvino::frontend::tensorflow_common)\n```\n\n----------------------------------------\n\nTITLE: Creating Symbolic Link to OpenVINO Directory\nDESCRIPTION: This set of commands creates a symbolic link named `openvino_2025` to the actual OpenVINO installation directory. This allows for easier referencing of the directory.  `sudo ln -s` creates the symbolic link with elevated privileges. The `cd` command before the link creation ensures the link is created in /opt/intel.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\ncd /opt/intel\n\nsudo ln -s openvino_2025.1.0 openvino_2025\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Inference C++\nDESCRIPTION: Illustrates how to perform asynchronous inference using `ov::InferRequest::start_async` along with setting a callback function using `ov::InferRequest::set_callback`. This allows for non-blocking inference execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/benchmark/throughput_benchmark/README.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n``ov::InferRequest::start_async``,\n``ov::InferRequest::set_callback``\n```\n\n----------------------------------------\n\nTITLE: Benchmark App Command with GPU - Shell\nDESCRIPTION: This command line instruction demonstrates how to run the OpenVINO benchmark_app tool with a specific device (GPU) to measure its inference performance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app -m ../public/alexnet/FP32/alexnet.xml -d GPU -niter 128\n```\n\n----------------------------------------\n\nTITLE: Creating Tensor with Default Allocator in OpenVINO (C)\nDESCRIPTION: This function constructs an OpenVINO tensor using a specified element type and shape. It allocates internal host storage using the default allocator. The function takes the tensor element type, shape, and a pointer to a tensor pointer as input. It returns a status code indicating success or failure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_44\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_tensor_create(const ov_element_type_e type, const ov_shape_t shape, ov_tensor_t** tensor)\n```\n\n----------------------------------------\n\nTITLE: Run OpenVINO Python sample\nDESCRIPTION: Runs the `hello_query_device.py` Python sample application. Requires the OpenVINO Runtime and Python API to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\npython3 /usr/share/openvino/samples/python/hello_query_device/hello_query_device.py\n```\n\n----------------------------------------\n\nTITLE: Building C++ Samples on Linux and macOS\nDESCRIPTION: These commands navigate to the C++ samples directory and execute the `build_samples.sh` script. This script compiles the sample applications for Linux and macOS systems.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncd <INSTALLDIR>/samples/cpp\n./build_samples.sh\n```\n\n----------------------------------------\n\nTITLE: Activate Python Virtual Environment\nDESCRIPTION: Activates the created virtual environment. This ensures that subsequent pip commands install packages into the virtual environment, not globally.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsource venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Install OpenCL packages on Ubuntu 20.04\nDESCRIPTION: Installs OpenCL packages on Ubuntu 20.04 LTS. It adds the Intel graphics repository, updates the package list, installs necessary packages, and adds the user to the render group. Requires curl, gpg, and apt package manager.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/configurations-intel-gpu.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\napt-get update && apt-get install -y --no-install-recommends curl gpg gpg-agent && \\\ncurl https://repositories.intel.com/graphics/intel-graphics.key | gpg --dearmor --output /usr/share/keyrings/intel-graphics.gpg && \\\necho 'deb [arch=amd64 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/graphics/ubuntu focal-legacy main' | tee  /etc/apt/sources.list.d/intel.gpu.focal.list && \\\napt-get update\napt-get update && apt-get install -y --no-install-recommends intel-opencl-icd intel-level-zero-gpu level-zero\nsudo usermod -a -G render $LOGNAME\n```\n\n----------------------------------------\n\nTITLE: Running Hello Classification Sample with Node.js\nDESCRIPTION: This command executes the hello_classification.js script with the specified model file, image path, and device. It uses the model v3-small_224_1.0_float.xml, the image coco.jpg, and the AUTO device. The script is located in the current directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/js/node/hello_classification/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnode hello_classification.js ../../assets/models/v3-small_224_1.0_float.xml ../../assets/images/coco.jpg AUTO\n```\n\n----------------------------------------\n\nTITLE: Getting Core Property C\nDESCRIPTION: This code snippet gets the value of a specified property for a device on the OpenVINO core. A property key and the device name should be passed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_22\n\nLANGUAGE: C\nCODE:\n```\nov_core_t* core = nullptr;\nov_core_create(&core);\nconst char* key = ov_property_key_hint_performance_mode;\nconst char* mode = \"LATENCY\";\nov_core_set_property(core, \"CPU\", key, mode);\nchar* ret = nullptr;\nov_core_get_property(core, \"CPU\", key, &ret);\nov_free(ret);\n...\nov_core_free(core);\n```\n\n----------------------------------------\n\nTITLE: Convert JAX/Flax via TensorFlow SavedModel (Python)\nDESCRIPTION: This code shows the process of converting a JAX/Flax model to OpenVINO via TensorFlow SavedModel. It involves exporting the JAX/Flax model to SavedModel using `jax2tf.convert` and then converting the SavedModel to OpenVINO using `ov.convert_model`. It uses `flax.linen`, `jax`, `jax.experimental.jax2tf`, `jax.numpy`, `openvino`, and `tensorflow`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-jax.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport flax.linen as nn\nimport jax\nimport jax.experimental.jax2tf as jax2tf\nimport jax.numpy as jnp\nimport openvino as ov\nimport openvino as ov\nimport tensorflow as tf\n\n# let user have some Flax module\nclass SimpleModule(nn.Module):\n    features: int\n\n    @nn.compact\n    def __call__(self, x):\n        return nn.Dense(features=self.features)(x)\n\nflax_module = SimpleModule(features=4)\n\n# prepare parameters to initialize the module\n# they can be also loaded using pickle, flax.serialization\nexample_input = jnp.ones((2, 3))\nkey = jax.random.PRNGKey(0)\nparams = flax_module.init(key, example_input)\nmodule = flax_module.bind(params)\n\n# 1. Export to SavedModel\n# create TF function and wrap it into TF Module\ntf_function = tf.function(jax2tf.convert(flax_module, native_serialization=False), autograph=False,\n                           input_signature=[tf.TensorSpec(shape=[2, 3], dtype=tf.float32)])\ntf_module = tf.Module()\ntf_module.f = tf_function\ntf.saved_model.save(tf_module, './saved_model')\n\n# 2. Convert to OpenVINO\nov_model = ov.convert_model('./saved_model')\n```\n\n----------------------------------------\n\nTITLE: Creating a simple model and pattern with WrapType\nDESCRIPTION: This snippet demonstrates how to create a simple OpenVINO model and a pattern using WrapType for the Relu node. WrapType allows for more flexible pattern matching by wrapping specific node types.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.tools.ovc.composite.pattern import WrapType\n\ndef create_simple_model_and_pattern_wrap_type():\n    # Model creation\n    shape = [1, 3, 224, 224]\n    parameter_node = parameter(shape, dtype=np.float32, name=\"input\")\n    add_node = add(parameter_node, constant(1.0, dtype=np.float32))\n    relu_node = relu(add_node)\n    result_node = result(relu_node, name=\"result\")\n\n    model = Model([result_node], [parameter_node])\n\n    # Pattern creation\n    parameter_node_pattern = parameter(shape, dtype=np.float32, name=\"input\")\n    add_node_pattern = add(parameter_node_pattern, constant(1.0, dtype=np.float32))\n    relu_node_pattern = WrapType(relu())\n\n    pattern = [parameter_node_pattern, add_node_pattern, relu_node_pattern]\n\n    return model, pattern\n```\n\n----------------------------------------\n\nTITLE: Basic MyTensor Usage Python\nDESCRIPTION: Demonstrates basic usage of the `MyTensor` class in Python, including construction from a list and an OpenVINO `Tensor`, as well as calling the `get_size` and `say_hello` methods. Also tests the `get_smile` function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport openvino._pyopenvino.mymodule as mymodule\nfrom openvino.runtime import Tensor, Type\n\na = mymodule.MyTensor([1,2,3])\na.get_size()\n>>> 3\na.say_hello()\n>>> Hello there!\n>>> 1.0\n>>> 2.0\n>>> 3.0\n\nt = Tensor(Type.f32, [5])\nt.data[:] = 1\nb = mymodule.MyTensor(t)\nb.get_size()\n>>> 5\nb.say_hello()\n>>> Hello there!\n>>> 1.0\n>>> 1.0\n>>> 1.0\n>>> 1.0\n>>> 1.0\n\nmymodule.get_smile()\n>>> :)\n```\n\n----------------------------------------\n\nTITLE: Synchronous Benchmark Implementation (C++)\nDESCRIPTION: This C++ snippet is the main part of the synchronous benchmark sample. It measures the performance of a given OpenVINO model using synchronous inference. The code compiles a model for a specified device, generates random input data, performs synchronous inference repeatedly, and reports performance statistics.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/sync-benchmark.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nsamples/cpp/benchmark/sync_benchmark/main.cpp\n```\n\n----------------------------------------\n\nTITLE: Set Static Input Shape (C++)\nDESCRIPTION: This C++ code snippet demonstrates how to reshape a model to a static input shape before compiling it, potentially improving performance if the input shape is consistent across inferences. The reshape method on the model is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\n    model->reshape({{\"input_0\", ov::Shape{1, 3, 224, 224}}});\n    ov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Inference Request in Python\nDESCRIPTION: This code creates an inference request object from a compiled model using the `create_infer_request()` method. The inference request is used to set input tensors, execute the model, and retrieve the results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ninfer_request = compiled_model_ir.create_infer_request()\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Convert Color (C++)\nDESCRIPTION: The `ov::preprocess::PreProcessSteps::convert_color` API is used to perform color conversion on the input data. This API allows for changes to be made to the color format of the input tensor, such as NV12 to RGB/BGR etc., ensuring compatibility with the model's expected input format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_nv12_input_classification/README.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n``ov::preprocess::PreProcessSteps::convert_color``\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO with CMake\nDESCRIPTION: This command builds the OpenVINO project using CMake.  It specifies the Release configuration and uses parallel compilation to speed up the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\ncmake --build . --config Release --parallel $(sysctl -n hw.ncpu)\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version\nDESCRIPTION: This command specifies the minimum required version of CMake for the project. It ensures that the CMake version used to build the project is at least 3.13.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/samples_tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\n```\n\n----------------------------------------\n\nTITLE: Enabling Transformation Pass Profiling in OpenVINO\nDESCRIPTION: This snippet demonstrates how to enable profiling of transformation passes in OpenVINO using the `OV_ENABLE_PROFILE_PASS` environment variable. Setting the variable to \"true\" enables visualizations, while specifying a file path saves the execution times to a file. No dependencies are explicitly required, other than the OpenVINO environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/docs/debug_capabilities/transformation_statistics_collection.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OV_ENABLE_PROFILE_PASS=true\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport OV_ENABLE_PROFILE_PASS=\"/path/to/save/profiling/results\"\n```\n\n----------------------------------------\n\nTITLE: Configure and Compile Project with CMake\nDESCRIPTION: These commands configure and compile a project using CMake, integrating with OpenVINO through the `conan_toolchain.cmake` file generated by Conan.  The toolchain file provides the necessary compiler and linker settings for OpenVINO. The configuration specifies a Release build type, and the compilation uses parallel processing for faster build times.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conan.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_TOOLCHAIN_FILE=<path to conan_toolchain.cmake> -DCMAKE_BUILD_TYPE=Release -S <path to CMakeLists.txt of your project> -B <build dir>\ncmake --build <build dir> --parallel\n```\n\n----------------------------------------\n\nTITLE: Creating Plugin Instance in OpenVINO (C++)\nDESCRIPTION: This code snippet demonstrates the use of the `OV_DEFINE_PLUGIN_CREATE_FUNCTION` macro to export a function that creates a plugin instance.  This is the entry point for OpenVINO to load the plugin. It is required for every OpenVINO plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\n// Plugin entry point\nOV_DEFINE_PLUGIN_CREATE_FUNCTION(TemplatePlugin)\n\n```\n\n----------------------------------------\n\nTITLE: Fine-tune Pruned Model - PyTorch\nDESCRIPTION: This code snippet shows how to fine-tune the pruned model in PyTorch, ensuring that the training schedule and learning rate are similar to those used for the original model. This is crucial for maintaining accuracy after pruning.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nloss, output = model(input_batch)\nloss.backward()\noptimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Reading TensorFlow Lite Model in OpenVINO using Python\nDESCRIPTION: This code snippet reads a TensorFlow Lite model into OpenVINO using Python. It initializes the OpenVINO Core, reads the '<INPUT_MODEL>.tflite' file, and compiles the resulting OpenVINO model for execution on the 'AUTO' device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_7\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\ncore = ov.Core()\nov_model = core.read_model(\"<INPUT_MODEL>.tflite\")\ncompiled_model = ov.compile_model(ov_model, \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: C++ Sample Output\nDESCRIPTION: This is an example output from running the C++ version of the hello_reshape_ssd sample. It displays information about the OpenVINO Runtime version, the loaded model, input and output shapes, and the detected objects with their confidence and coordinates. The resulting image is saved to a file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-reshape-ssd.rst#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n[ INFO ] OpenVINO Runtime version ......... <version>\n[ INFO ] Build ........... <build>\n[ INFO ]\n[ INFO ] Loading model files: \\models\\person-detection-retail-0013.xml\n[ INFO ] model name: ResMobNet_v4 (LReLU) with single SSD head\n[ INFO ]     inputs\n[ INFO ]         input name: data\n[ INFO ]         input type: f32\n[ INFO ]         input shape: {1, 3, 320, 544}\n[ INFO ]     outputs\n[ INFO ]         output name: detection_out\n[ INFO ]         output type: f32\n[ INFO ]         output shape: {1, 1, 200, 7}\nReshape network to the image size = [960x1699]\n[ INFO ] model name: ResMobNet_v4 (LReLU) with single SSD head\n[ INFO ]     inputs\n[ INFO ]         input name: data\n[ INFO ]         input type: f32\n[ INFO ]         input shape: {1, 3, 960, 1699}\n[ INFO ]     outputs\n[ INFO ]         output name: detection_out\n[ INFO ]         output type: f32\n[ INFO ]         output shape: {1, 1, 200, 7}\n[0,1] element, prob = 0.716309,    (852,187)-(983,520)\nThe resulting image was saved in the file: hello_reshape_ssd_output.bmp\n\nThis sample is an API example, for any performance measurements please use the dedicated benchmark_app tool\n```\n\n----------------------------------------\n\nTITLE: Configure Theme Logo in conf.py\nDESCRIPTION: This snippet shows how to configure the theme logo by setting the `html_logo` variable in the `conf.py` file. Replace `<path to the logo file>` with the actual path to your logo image.  The path should be relative to the documentation's root directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/openvino_sphinx_theme/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nhtml_logo = <path to the logo file>\n```\n\n----------------------------------------\n\nTITLE: Add GPG key to system keyring\nDESCRIPTION: Adds the downloaded GPG key to the system's trusted keyring. This allows the system to verify the authenticity of packages from the Intel OpenVINO repository. Requires `gnupg` to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsudo gpg --output /etc/apt/trusted.gpg.d/intel.gpg --dearmor GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB\n```\n\n----------------------------------------\n\nTITLE: Defining the OpenVINO Core Interface\nDESCRIPTION: Defines the Core interface, which represents the main entry point for interacting with the OpenVINO runtime. This interface includes methods for model compilation, device management, and property manipulation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface Core {\n    addExtension(libraryPath): void;\n    compileModel(model, deviceName, config?): Promise<CompiledModel>;\n    compileModel(modelPath, deviceName, config?): Promise<CompiledModel>;\n    compileModelSync(model, deviceName, config?): CompiledModel;\n    compileModelSync(modelPath, deviceName, config?): CompiledModel;\n    getAvailableDevices(): string[];\n    getProperty(propertyName): OVAny;\n    getProperty(deviceName, propertyName): OVAny;\n    getVersions(deviceName): {\n        [deviceName: string]: {\n            buildNumber: string;\n            description: string;\n        };\n    };\n    importModel(modelStream, device, config?): Promise<CompiledModel>\n    importModelSync(modelStream, device, config?): CompiledModel;\n    queryModel(model, deviceName, properties?): string[];\n    readModel(modelPath, weightsPath?): Promise<Model>;\n    readModel(model, weights): Promise<Model>;\n    readModel(modelBuffer, weightsBuffer?): Promise<Model>;\n    readModelSync(modelPath, weightsPath?): Model;\n    readModelSync(model, weights): Model;\n    readModelSync(modelBuffer, weightsBuffer?): Model;\n    setProperty(properties): void;\n    setProperty(deviceName, properties): void;\n}\n```\n\n----------------------------------------\n\nTITLE: Running Hello Classification Sample with Heterogeneous Execution\nDESCRIPTION: This shell snippet demonstrates how to use the Heterogeneous execution with the `-d` option when running OpenVINO sample programs. It runs the hello_classification sample, specifying HETERO:GPU,CPU as the device to use, which prioritizes GPU execution and falls back to CPU if necessary.  It also passes the path to the model file and the image file to classify.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/hetero-execution.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\n./hello_classification <path_to_model>/squeezenet1.1.xml <path_to_pictures>/picture.jpg HETERO:GPU,CPU\n```\n\n----------------------------------------\n\nTITLE: Asymmetric Quantization Customization in OpenVINO C++\nDESCRIPTION: This C++ code snippet illustrates how to customize asymmetric quantization support in Low Precision Transformations (LPT) using transformation callbacks, enabling fine-grained control over quantization behavior.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/advanced-guides/low-precision-transformations.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nauto& transformation = *transformation;\n\n```\n\n----------------------------------------\n\nTITLE: Releasing GIL in OpenVINO Python Functions\nDESCRIPTION: This snippet highlights that some functions in the OpenVINO Python API release the Global Interpreter Lock (GIL) while running work-intensive code, allowing for improved parallelism in Python applications using threads. This improves performance for multi-threaded Python applications using OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# This snippet describes the context of GIL releasing, no actual code is presented\n# Refer to the list of functions in the documentation for more details.\n```\n\n----------------------------------------\n\nTITLE: Freeing Tensor Memory in OpenVINO (C)\nDESCRIPTION: This function frees the memory associated with an OpenVINO tensor. It takes a pointer to the tensor as input. The function does not return any value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_50\n\nLANGUAGE: C\nCODE:\n```\nvoid ov_tensor_free(ov_tensor_t* tensor)\n```\n\n----------------------------------------\n\nTITLE: Uninstall Specific OpenVINO Runtime Version\nDESCRIPTION: This command uninstalls a specific version of the OpenVINO Runtime using the YUM package manager.  Replace `<VERSION>.<UPDATE>.<PATCH>` with the desired version number. It requires sudo privileges to remove software packages and their dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nsudo yum autoremove openvino-<VERSION>.<UPDATE>.<PATCH>\n```\n\n----------------------------------------\n\nTITLE: Downloading and Installing OpenVINO (CentOS 7)\nDESCRIPTION: These commands download the OpenVINO Runtime archive for CentOS 7, extract it, and move the extracted directory to `/opt/intel`. It uses `curl` to download the archive, `tar` to extract it, and `sudo mv` to move the extracted folder with root privileges.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino/packages/2025.1/linux/openvino_toolkit_centos7_2025.1.0.18503.6fec06580ab_x86_64.tgz --output openvino_2025.1.0.tgz\ntar -xf openvino_2025.1.0.tgz\nsudo mv openvino_toolkit_centos7_2025.1.0.18503.6fec06580ab_x86_64 /opt/intel/openvino_2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Install NNCF package\nDESCRIPTION: Installs the NNCF package into the created Python virtual environment. This package provides the necessary tools for neural network compression, including quantization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install nncf\n```\n\n----------------------------------------\n\nTITLE: Build Docs Function Definition (CMake)\nDESCRIPTION: This function encapsulates the logic for building OpenVINO documentation using Doxygen and Sphinx. It finds required packages (Doxygen, LATEX), defines directories for source and output, specifies preprocessing scripts, and configures Doxygen and Sphinx settings. Custom targets and commands are then added to preprocess the documentation, generate API references, and build the final documentation output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(build_docs)\n    find_package(Doxygen REQUIRED dot)\n    find_package(LATEX REQUIRED)\n\n    set(DOCS_BUILD_DIR \"${CMAKE_CURRENT_BINARY_DIR}\")\n    set(DOCS_SOURCE_DIR \"${OpenVINO_SOURCE_DIR}/docs\")\n    set(ARTICLES_EN_DIR \"${OpenVINO_SOURCE_DIR}/docs/articles_en\")\n    set(SCRIPTS_DIR \"${DOCS_SOURCE_DIR}/scripts\")\n\n    # Preprocessing scripts\n    set(REMOVE_XML_SCRIPT \"${SCRIPTS_DIR}/remove_xml.py\")\n    set(FILE_HELPER_SCRIPT \"${SCRIPTS_DIR}/filehelper.py\")\n    set(ARTICLES_HELPER_SCRIPT \"${SCRIPTS_DIR}/articles_helper.py\")\n    set(COPY_IMAGES_SCRIPT \"${SCRIPTS_DIR}/copy_images.py\")\n    set(DOXYGEN_MAPPING_SCRIPT \"${SCRIPTS_DIR}/create_mapping.py\")\n    set(DOCS_MAPPING_SCRIPT \"${SCRIPTS_DIR}/create_doc_mapping.py\")\n    set(BREATHE_APIDOC_SCRIPT \"${SCRIPTS_DIR}/apidoc.py\")\n    set(OV_INSTALLATION_SCRIPT \"${SCRIPTS_DIR}/install_appropriate_openvino_version.py\")\n\n    # Doxygen/Sphinx setup\n    set(DOXYGEN_XML_OUTPUT \"${DOCS_BUILD_DIR}/xml\")\n    set(SPHINX_SETUP_DIR \"${DOCS_SOURCE_DIR}/sphinx_setup\")\n    set(SPHINX_SOURCE_DIR \"${DOCS_BUILD_DIR}/sphinx_source\")\n    set(SPHINX_OUTPUT \"${DOCS_BUILD_DIR}/_build\")\n\n    list(APPEND commands COMMAND ${CMAKE_COMMAND} -E cmake_echo_color --green \"STARTED preprocessing OpenVINO articles\")\n    list(APPEND commands COMMAND ${Python3_EXECUTABLE} ${ARTICLES_HELPER_SCRIPT}\n        --filetype=rst\n        --input_dir=${ARTICLES_EN_DIR}\n        --output_dir=${SPHINX_SOURCE_DIR}\n        --exclude_dir=${SPHINX_SOURCE_DIR})\n    list(APPEND commands COMMAND ${CMAKE_COMMAND} -E cmake_echo_color --green \"FINISHED preprocessing OpenVINO articles\")\n\n    if(${ENABLE_CPP_API})\n        # Doxygen config\n        set(DOXYFILE_SOURCE \"${DOCS_SOURCE_DIR}/Doxyfile.config\")\n        set(DOXYFILE_BUILD \"${DOCS_BUILD_DIR}/Doxyfile.config\")\n        configure_file(${DOXYFILE_SOURCE} ${DOXYFILE_BUILD} @ONLY)\n        list(APPEND commands COMMAND ${CMAKE_COMMAND} -E cmake_echo_color --green \"STARTED preprocessing OpenVINO C/C++ API reference\")\n        list(APPEND commands COMMAND ${Python3_EXECUTABLE} ${REMOVE_XML_SCRIPT} ${DOXYGEN_XML_OUTPUT})\n        list(APPEND commands COMMAND ${DOXYGEN_EXECUTABLE} ${DOXYFILE_BUILD})\n        list(APPEND post_commands COMMAND ${Python3_EXECUTABLE} ${BREATHE_APIDOC_SCRIPT} ${DOXYGEN_XML_OUTPUT} -o \"${SPHINX_SOURCE_DIR}/api/c_cpp_api\" -m -T -p openvino -g class,group,struct,union,enum)\n        list(APPEND commands COMMAND ${CMAKE_COMMAND} -E cmake_echo_color --green \"FINISHED preprocessing OpenVINO C/C++ API reference\")\n    endif()\n\n    if(${ENABLE_PYTHON_API} OR ${ENABLE_GENAI_API})\n        list(APPEND commands COMMAND ${CMAKE_COMMAND} -E cmake_echo_color --green \"STARTED preprocessing OpenVINO Python API\")\n        list(APPEND commands COMMAND ${Python3_EXECUTABLE} ${OV_INSTALLATION_SCRIPT}\n        --ov_dir=${SPHINX_SETUP_DIR}\n        --python=${Python3_EXECUTABLE}\n        --enable_genai=${ENABLE_GENAI_API})\n        list(APPEND commands COMMAND ${CMAKE_COMMAND} -E cmake_echo_color --green \"FINISHED preprocessing OpenVINO Python API\")\n    endif()\n\n    if(${ENABLE_OVMS})\n        list(APPEND commands COMMAND ${CMAKE_COMMAND} -E cmake_echo_color --green \"STARTED preprocessing OVMS\")\n        list(APPEND commands COMMAND ${Python3_EXECUTABLE} ${FILE_HELPER_SCRIPT}\n        --filetype=md\n        --input_dir=${OVMS_DOCS_DIR}\n        --output_dir=${SPHINX_SOURCE_DIR}/model-server\n        --exclude_dir=${SPHINX_SOURCE_DIR})\n        list(APPEND commands COMMAND ${CMAKE_COMMAND} -E cmake_echo_color --green \"FINISHED preprocessing OVMS\")\n    endif()\n\n\n    # Preprocess docs\n    add_custom_target(preprocess_docs\n                      COMMAND ${CMAKE_COMMAND} -E remove_directory ${SPHINX_SOURCE_DIR}\n                      ${commands}\n                      WORKING_DIRECTORY ${DOCS_BUILD_DIR}\n                      VERBATIM)\n\n    add_custom_command(TARGET preprocess_docs\n                       POST_BUILD\n                       COMMAND ${Python3_EXECUTABLE} ${COPY_IMAGES_SCRIPT} ${DOXYGEN_XML_OUTPUT} ${SPHINX_SOURCE_DIR}\n                       COMMAND ${Python3_EXECUTABLE} ${DOXYGEN_MAPPING_SCRIPT} ${DOXYGEN_XML_OUTPUT} ${DOCS_BUILD_DIR} ${OpenVINO_SOURCE_DIR}/../\n                       COMMAND ${Python3_EXECUTABLE} ${DOCS_MAPPING_SCRIPT} ${DOCS_BUILD_DIR} ${ARTICLES_EN_DIR}\n                       COMMAND ${CMAKE_COMMAND} -E copy_directory ${SPHINX_SETUP_DIR} ${SPHINX_SOURCE_DIR}\n                       ${post_commands}\n                       VERBATIM)\n\n    # Build docs\n    add_custom_target(sphinx_docs\n                      DEPENDS preprocess_docs\n                      COMMAND ${CMAKE_COMMAND} -E cmake_echo_color --green \"STARTED sphinx documentation build\"\n                      COMMAND sphinx-build -j auto -w ${DOCS_BUILD_DIR}/sphinx.log -b html ${SPHINX_SOURCE_DIR} ${SPHINX_OUTPUT}\n                      COMMAND ${CMAKE_COMMAND} -E cmake_echo_color --green \"FINISHED sphinx documentation build\"\n                      WORKING_DIRECTORY ${SPHINX_SOURCE_DIR}\n                      VERBATIM)\n\n    set_target_properties(preprocess_docs sphinx_docs PROPERTIES FOLDER docs)\n\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Download and unpack Android NDK package\nDESCRIPTION: Downloads and unpacks the Android NDK (Native Development Kit) to the specified work directory. It then renames the unpacked directory and sets the ANDROID_NDK_PATH environment variable to point to the NDK installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_android.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nwget https://dl.google.com/android/repository/android-ndk-r26d-linux.zip --directory-prefix $OPV_HOME_DIR\n\nunzip $OPV_HOME_DIR/android-ndk-r26d-linux.zip -d $OPV_HOME_DIR\nmv $OPV_HOME_DIR/android-ndk-r26d $OPV_HOME_DIR/android-ndk\nexport ANDROID_NDK_PATH=$OPV_HOME_DIR/android-ndk\n```\n\n----------------------------------------\n\nTITLE: Convert TensorFlow Module to OpenVINO\nDESCRIPTION: This snippet shows how to convert a TensorFlow Module to OpenVINO. It requires specifying the input shapes using the `input` parameter when converting the model. Requires the `tensorflow` package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_17\n\nLANGUAGE: py\nCODE:\n```\nimport tensorflow as tf\nimport openvino as ov\n\nclass MyModule(tf.Module):\n   def __init__(self, name=None):\n      super().__init__(name=name)\n      self.constant1 = tf.constant(5.0, name=\"var1\")\n```\n\n----------------------------------------\n\nTITLE: Suppressing min/max Macros on Windows\nDESCRIPTION: This snippet suppresses the definition of `min` and `max` as macros on Windows by adding the `NOMINMAX` definition to the target's compile definitions. This prevents potential conflicts with standard library functions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/vectorized/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif (WIN32)\n    # Prevents defining min/max as macros\n    target_compile_definitions(${TARGET_NAME} PRIVATE NOMINMAX)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Build OpenVINO Tokenizers from Source\nDESCRIPTION: Builds and installs OpenVINO Tokenizers from source. This requires OpenVINO to be installed and configured. The setupvars.sh script sets the necessary environment variables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nsource path/to/installed/openvino/setupvars.sh\n```\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/openvinotoolkit/openvino_tokenizers.git\n```\n\nLANGUAGE: shell\nCODE:\n```\ncd openvino_tokenizers\n```\n\nLANGUAGE: shell\nCODE:\n```\npip install --no-deps .\n```\n\n----------------------------------------\n\nTITLE: Initialize OpenVINO Environment\nDESCRIPTION: Initializes the OpenVINO environment by sourcing the `setupvars.sh` script. This script sets up the necessary environment variables for using OpenVINO. Replace `~/openvino/build/install` with the correct installation prefix.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/dump_check/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# suppose CMAKE_INSTALL_PREFIX=~/openvino/build/install\nsource ~/openvino/build/install/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: Benchmark App Help Message (Python)\nDESCRIPTION: This code snippet displays the help message for the OpenVINO benchmark application when run using Python. It showcases all available command-line options, their purposes, and accepted values. This is useful for understanding the application's configurable parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\n[Step 1/11] Parsing and validating input arguments\n[ INFO ] Parsing input parameters\nusage: benchmark_app.py [-h [HELP]] [-i PATHS_TO_INPUT [PATHS_TO_INPUT ...]] -m PATH_TO_MODEL [-d TARGET_DEVICE]\n                                    [-hint {throughput,cumulative_throughput,latency,none}] [-niter NUMBER_ITERATIONS] [-max_irate MAXIMUM_INFERENCE_RATE] [-t TIME] [-b BATCH_SIZE] [-shape SHAPE]\n                                    [-data_shape DATA_SHAPE] [-layout LAYOUT] [-extensions EXTENSIONS] [-c PATH_TO_CLDNN_CONFIG] [-cdir CACHE_DIR] [-lfile [LOAD_FROM_FILE]]\n                                    [-api {sync,async}] [-nireq NUMBER_INFER_REQUESTS] [-nstreams NUMBER_STREAMS] [-inference_only [INFERENCE_ONLY]]\n                                    [-infer_precision INFER_PRECISION] [-ip {bool,f16,f32,f64,i8,i16,i32,i64,u8,u16,u32,u64}]\n                                    [-op {bool,f16,f32,f64,i8,i16,i32,i64,u8,u16,u32,u64}] [-iop INPUT_OUTPUT_PRECISION] [--mean_values [R,G,B]] [--scale_values [R,G,B]]\n                                    [-nthreads NUMBER_THREADS] [-pin {YES,NO}] [-latency_percentile LATENCY_PERCENTILE]\n                                    [-report_type {no_counters,average_counters,detailed_counters}] [-report_folder REPORT_FOLDER] [-pc [PERF_COUNTS]]\n                                    [-pcsort {no_sort,sort,simple_sort}] [-pcseq [PCSEQ]] [-exec_graph_path EXEC_GRAPH_PATH] [-dump_config DUMP_CONFIG] [-load_config LOAD_CONFIG]\n\n            Options:\n              -h [HELP], --help [HELP]\n                                    Show this help message and exit.\n\n              -i PATHS_TO_INPUT [PATHS_TO_INPUT ...], --paths_to_input PATHS_TO_INPUT [PATHS_TO_INPUT ...]\n                                    Optional. Path to a folder with images and/or binaries or to specific image or binary file.It is also allowed to map files to model inputs:\n                                    input_1:file_1/dir1,file_2/dir2,input_4:file_4/dir4 input_2:file_3/dir3 Currently supported data types: bin, npy. If OPENCV is enabled, this\n                                    functionalityis extended with the following data types: bmp, dib, jpeg, jpg, jpe, jp2, png, pbm, pgm, ppm, sr, ras, tiff, tif.\n\n              -m PATH_TO_MODEL, --path_to_model PATH_TO_MODEL\n                                    Required. Path to an .xml/.onnx file with a trained model or to a .blob file with a trained compiled model.\n\n              -d TARGET_DEVICE, --target_device TARGET_DEVICE\n                                    Optional. Specify a target device to infer on (the list of available devices is shown below). Default value is CPU. Use '-d HETERO:<comma\n                                    separated devices list>' format to specify HETERO plugin. Use '-d MULTI:<comma separated devices list>' format to specify MULTI plugin. The\n                                    application looks for a suitable plugin for the specified device.\n\n              -hint {throughput,cumulative_throughput,latency,none}, --perf_hint {throughput,cumulative_throughput,latency,none}\n                                    Optional. Performance hint (latency or throughput or cumulative_throughput or none). Performance hint allows the OpenVINO device to select the\n                                    right model-specific settings. 'throughput': device performance mode will be set to THROUGHPUT. 'cumulative_throughput': device performance\n                                    mode will be set to CUMULATIVE_THROUGHPUT. 'latency': device performance mode will be set to LATENCY. 'none': no device performance mode will\n                                    be set. Using explicit 'nstreams' or other device-specific options, please set hint to 'none'\n\n              -niter NUMBER_ITERATIONS, --number_iterations NUMBER_ITERATIONS\n                                    Optional. Number of iterations. If not specified, the number of iterations is calculated depending on a device.\n\n              -t TIME, --time TIME  Optional. Time in seconds to execute topology.\n\n              -api {sync,async}, --api_type {sync,async}\n                                    Optional. Enable using sync/async API. When hint is throughput, default value is async. When hint is latency, default value is sync.\n\n\n            Input shapes:\n              -b BATCH_SIZE, --batch_size BATCH_SIZE\n                                    Optional. Batch size value. If not specified, the batch size value is determined from Intermediate Representation\n\n              -shape SHAPE          Optional. Set shape for input. For example, \"input1[1,3,224,224],input2[1,4]\" or \"[1,3,224,224]\" in case of one input size. This parameter\n                                    affect model Parameter shape, can be dynamic. For dynamic dimesions use symbol `?`, `-1` or range `low.. up`.\n\n              -data_shape DATA_SHAPE\n                                    Optional. Optional if model shapes are all static (original ones or set by -shape).Required if at least one input shape is dynamic and input\n                                    images are not provided.Set shape for input tensors. For example, \"input1[1,3,224,224][1,3,448,448],input2[1,4][1,8]\" or\n                                    \"[1,3,224,224][1,3,448,448] in case of one input size.\n\n              -layout LAYOUT        Optional. Prompts how model layouts should be treated by application. For example, \"input1[NCHW],input2[NC]\" or \"[NCHW]\" in case of one input\n                                    size.\n\n\n            Advanced options:\n              -extensions EXTENSIONS, --extensions EXTENSIONS\n                                    Optional. Path or a comma-separated list of paths to libraries (.so or .dll) with extensions.\n\n              -c PATH_TO_CLDNN_CONFIG, --path_to_cldnn_config PATH_TO_CLDNN_CONFIG\n                                    Optional. Required for GPU custom kernels. Absolute path to an .xml file with the kernels description.\n\n              -cdir CACHE_DIR, --cache_dir CACHE_DIR\n                                    Optional. Enable model caching to specified directory\n\n              -lfile [LOAD_FROM_FILE], --load_from_file [LOAD_FROM_FILE]\n                                    Optional. Loads model from file directly without read_model.\n\n              -nireq NUMBER_INFER_REQUESTS, --number_infer_requests NUMBER_INFER_REQUESTS\n                                    Optional. Number of infer requests. Default value is determined automatically for device.\n\n              -nstreams NUMBER_STREAMS, --number_streams NUMBER_STREAMS\n```\n\n----------------------------------------\n\nTITLE: Throughput Benchmark Execution (C++)\nDESCRIPTION: This command executes the compiled throughput_benchmark application with a specified model.  The device to run the model on can optionally be specified; the default device is CPU.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/throughput-benchmark.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nthroughput_benchmark <path_to_model> <device_name>(default: CPU)\n```\n\n----------------------------------------\n\nTITLE: Query HETERO Device Priorities (Python)\nDESCRIPTION: This code shows how to query the `HETERO` device priority using `ov::Core::get_property` in Python. It retrieves the device priority for the specified device and prints it. Requires an initialized OpenVINO `Core` object and a valid device name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ncore = ov.Core()\ndevice_name = \"HETERO\"\npriority = core.get_property(device_name, \"device_priorities\")\nprint(f\"{device_name} priority: {priority}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Model Asynchronously\nDESCRIPTION: Asynchronously imports a previously exported compiled model from a model stream. This allows reusing compiled models across different sessions or applications. The config parameter allows specifying properties relevant only for this load operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimportModel(modelStream, device, config?): Promise<CompiledModel>\n```\n\n----------------------------------------\n\nTITLE: Disabling Graph Transformations\nDESCRIPTION: This snippet shows how graph transformations are disabled using the transformations option with comma-separated tokens to specify the transformations to disable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/feature_disabling.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ntransformations=<comma_separated_tokens>\n```\n\n----------------------------------------\n\nTITLE: Enable Denormals Optimization (C++)\nDESCRIPTION: This C++ code demonstrates how to enable denormals optimization for the CPU plugin using the `ov::intel_cpu::denormals_optimization` property. The property is set when compiling the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\n    ov::CompiledModel compiled_model = core.compile_model(model, \"CPU\", {{ov::intel_cpu::denormals_optimization, true}});\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Dump IR with transformations option OpenVINO (sh)\nDESCRIPTION: This example demonstrates how to use the transformations option with OV_CPU_DUMP_IR to dump IR before and after specific transformation stages. It shows how to specify a directory for the dumped IR files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/graph_serialization.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_DUMP_IR=\"transformations\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Create a Simple Model (Python)\nDESCRIPTION: This snippet shows how to create a simple model with a single output in OpenVINO using Python.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n# [ov:create_simple_model]\n```\n\n----------------------------------------\n\nTITLE: Reshape OpenVINO Model in C++\nDESCRIPTION: This snippet demonstrates how to reshape an OpenVINO Model in C++, usually for models with sequence length inputs. It changes the input shape to set the sequence dimension to exactly 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nmodel->reshape({input_layer->get_friendly_name(), {1, 100}});\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Logging\nDESCRIPTION: This environment variable enables debug logging. Setting it to `-` outputs logs to standard output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nOV_CPU_DEBUG_LOG=-\n```\n\n----------------------------------------\n\nTITLE: Create NV12 surface (C)\nDESCRIPTION: This C snippet shows how to create an NV12 surface for video processing using the OpenVINO GPU plugin. Requires OpenCL and OpenVINO libraries. This allows direct consumption of hardware video decoder output, enabling efficient video processing within OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_24\n\nLANGUAGE: c\nCODE:\n```\n// example usage\n{\n    size_t width;\n    size_t height;\n\n    cl_mem y_plane;\n    cl_mem uv_plane;\n    // fill surfaces with data\n\n    auto remote_blob = context.create_tensor_nv12(height, width, y_plane, uv_plane);\n}\n```\n\n----------------------------------------\n\nTITLE: Compile PaddlePaddle model using compile_model in C\nDESCRIPTION: This snippet shows how to compile a PaddlePaddle model using the OpenVINO C API. The model is compiled from a file for execution on the specified device (AUTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_22\n\nLANGUAGE: c\nCODE:\n```\nov_compiled_model_t* compiled_model = NULL;\nov_core_compile_model_from_file(core, \"<INPUT_MODEL>.pdmodel\", \"AUTO\", 0, &compiled_model);\n```\n\n----------------------------------------\n\nTITLE: Benchmarking ASL Model with OpenVINO in C++ (sh)\nDESCRIPTION: This command, intended to be executed within a C++ context, benchmarks the 'asl-recognition-0004' model using the OpenVINO benchmark_app. It specifies the model path, the CPU as the target device, and the input shape. The '-data_shape' argument defines a sequence of shapes to use for the input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\n./benchmark_app -m omz_models/intel/asl-recognition-0004/FP16/asl-recognition-0004.xml -d CPU -shape [-1,3,16,224,224] -data_shape [1,3,16,224,224][2,3,16,224,224][4,3,16,224,224] -pcseq\n```\n\n----------------------------------------\n\nTITLE: Create NNCF Configuration - TensorFlow 2\nDESCRIPTION: This code snippet demonstrates how to create an NNCF configuration object for filter pruning in TensorFlow 2. It involves defining model-related parameters (input_info) and optimization method parameters (compression). The config defines parameters of the compression algorithm.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnncf_config_dict = {\n    \"input_info\": {\"sample_size\": [1, 224, 224, 3]},\n    \"compression\": {\n        \"algorithm\": \"filter_pruning\",\n        \"params\": {\n            \"pruning_init\": 0.05,\n            \"pruning_target\": 0.5,\n            \"pruning_steps\": 1000\n        }\n    }\n}\nnncf_config = NNCFConfig.from_dict(nncf_config_dict)\n```\n\n----------------------------------------\n\nTITLE: Export Pruned Model - PyTorch\nDESCRIPTION: This code snippet demonstrates how to export the pruned model to ONNX format in PyTorch for further inference. Exporting to ONNX makes the model compatible with OpenVINO and other inference engines.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntorch.onnx.export(model, dummy_input, \"model.onnx\", verbose=False)\n```\n\n----------------------------------------\n\nTITLE: RNN Cell Formula (C++)\nDESCRIPTION: This C++-style code block illustrates the formula for calculating the hidden state (Ht) within the RNNCell-3 operation. It shows the matrix multiplications, transposes, and activation function involved in the computation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/rnn-cell-3.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nFormula:\n  *  - matrix multiplication\n  ^T - matrix transpose\n  f  - activation function\n    Ht = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Wbi + Rbi)\n```\n\n----------------------------------------\n\nTITLE: Get Partial Shape in OpenVINO (Python)\nDESCRIPTION: This snippet shows how to get a partial shape of a node in OpenVINO using Python. Partial shapes can represent static or dynamic shapes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [ov:partial_shape]\n```\n\n----------------------------------------\n\nTITLE: Allocate Remote Tensor with Level Zero Host Memory - OpenVINO™ C++\nDESCRIPTION: This snippet demonstrates how to allocate a RemoteTensor with Level Zero host memory using the remote context. The NPU plugin allocates the memory on the Level Zero host. The allocated memory can then be accessed and used by the NPU plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/npu-device/remote-tensor-api-npu-plugin.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nov::Tensor t = remote_context.create_host_tensor(ov::element::f32, {1, 3, 224, 224});\n```\n\n----------------------------------------\n\nTITLE: Removing Jupyter Kernel\nDESCRIPTION: This command removes a specified Jupyter kernel. In this case, it removes the kernel associated with the `openvino_env` virtual environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/run-notebooks.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\njupyter kernelspec remove openvino_env\n```\n\n----------------------------------------\n\nTITLE: Exporting Model Blob with OpenVINO GenAI (C++)\nDESCRIPTION: This C++ snippet demonstrates how to configure the OpenVINO GenAI pipeline to export the compiled model as a blob. It sets the `EXPORT_BLOB` option to \"YES\" and specifies the path to save the blob using the `BLOB_PATH` option. This allows you to save the compiled model for later use. Requires the OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\nov::AnyMap pipeline_config = { { \"EXPORT_BLOB\",  \"YES\" }, { \"BLOB_PATH\",  \".npucache\\\\compiled_model.blob\" } };\nov::genai::LLMPipeline pipe(model_path, \"NPU\", pipeline_config);\n```\n\n----------------------------------------\n\nTITLE: Working with OVDict in OpenVINO Python\nDESCRIPTION: This snippet demonstrates how to work with `OVDict`, a special data structure returned by synchronous calls in the OpenVINO Python API. `OVDict` is a \"frozen dictionary\" that allows accessing elements in various ways, such as by string or integer keys.  It shows how to access elements from the `OVDict`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model)\n\ninput_data = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n\n# Perform synchronous inference\nresults = compiled_model(input_data)\n\n# Accessing result by index\nprint(results[0])\n\n# Accessing result by output tensor name\noutput_key = compiled_model.outputs[0]\nprint(results[output_key])\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for GPU Tensor Dumps (Bash)\nDESCRIPTION: This snippet demonstrates how to configure environment variables to enable GPU tensor dumping for specified layers in OpenVINO. It requires enabling `ENABLE_DEBUG_CAPS` during CMake configuration. The `OV_GPU_DUMP_TENSORS_PATH` specifies the directory for storing dump files, `OV_GPU_DUMP_LAYER_NAMES` lists the layers to dump, and `OV_GPU_DUMP_TENSORS` specifies which tensors (input or output) to save.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_debug_utils.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# As a prerequisite, enable ENABLE_DEBUG_CAPS from cmake configuration.\nexport OV_GPU_DUMP_TENSORS_PATH=path/to/dir\nexport OV_GPU_DUMP_LAYER_NAMES=\"layer_name_to_dump1 layer_name_to_dump2\"\nexport OV_GPU_DUMP_TENSORS=out              # only out tensors should be saved\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Helper Function\nDESCRIPTION: This code snippet defines a custom helper function `top1_index` that takes a list of results as input and returns the index of the maximum value.  It's a simple example of adding new functionality to the OpenVINO™ API. This helper function can be used to determine the top prediction index from a list of output probabilities.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef top1_index(results: list) -> int:\n    return results.index(max(results))\n```\n\n----------------------------------------\n\nTITLE: Looping Over Available GPU Devices in C++\nDESCRIPTION: This C++ snippet illustrates how to loop over all available devices of the 'GPU' type within the OpenVINO framework. The purpose is to configure the inference runtime to utilize only devices of the GPU type. It relies on the OpenVINO Runtime API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n//! [part3]\nstd::string device_string;\nstd::string supported_devices_string;\nfor (auto&& available_device : core.get_available_devices()) {\n    if (available_device.find(\"GPU\") != std::string::npos) {\n        if (!device_string.empty()) {\n            device_string += \",\";\n            supported_devices_string += \",\";\n        }\n        device_string += available_device;\n        supported_devices_string += available_device;\n    }\n}\n//! [part3]\n```\n\n----------------------------------------\n\nTITLE: Linear Interpolation 4D Tensor\nDESCRIPTION: Performs linear interpolation on a 4D tensor. It calculates coordinates in the input tensor corresponding to each output pixel. It utilizes height_scale and width_scale for resizing.  It also clamps the coordinates to the input dimensions and computes the weighted average based on distances.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n           for y in range(0, output_height):\n               in_y = self.get_original_coordinate(y, height_scale, output_height, input_height)\n               y_original[y] = in_y\n               in_y = max(0, min(in_y, input_height - 1))\n               in_y1[y] = max(0, min(int(in_y), input_height - 1))\n               in_y2[y] = min(in_y1[y] + 1, input_height - 1)\n               dy1[y] = abs(in_y - in_y1[y])\n               dy2[y] = abs(in_y - in_y2[y])\n\n               if in_y1[y] == in_y2[y]:\n                   dy1[y] = 0.5\n                   dy2[y] = 0.5\n\n           for x in range(0, output_width):\n               in_x = self.get_original_coordinate(x, width_scale, output_width, input_width);\n               x_original[x] = in_x\n               in_x = max(0.0, min(in_x, input_width - 1));\n\n               in_x1[x] = min(in_x, input_width - 1);\n               in_x2[x] = min(in_x1[x] + 1, input_width - 1);\n\n               dx1[x] = abs(in_x - in_x1[x]);\n               dx2[x] = abs(in_x - in_x2[x]);\n               if in_x1[x] == in_x2[x]:\n                   dx1[x] = 0.5\n                   dx2[x] = 0.5\n\n           for n in range(0, batch_size):\n               for c in range(0, num_channels):\n                   for y in range(0, output_height):\n                       for x in range(0, output_width):\n                           x11 = reshaped_data[n, c, in_y1[y], in_x1[x]]\n                           x21 = reshaped_data[n, c, in_y1[y], in_x2[x]]\n                           x12 = reshaped_data[n, c, in_y2[y], in_x1[x]]\n                           x22 = reshaped_data[n, c, in_y2[y], in_x2[x]]\n                           temp = dx2[x] * dy2[y] * x11 + dx1[x] * dy2[y] * x21 + dx2[x] * dy1[y] * x12 + dx1[x] * dy1[y] * x22\n                           result[n, c, y, x] = temp\n```\n\n----------------------------------------\n\nTITLE: Apply LowLatency2 Transformation in Python\nDESCRIPTION: This snippet shows how to apply the LowLatency2 transformation in Python. It utilizes `ov.pass.LowLatency2()` to automatically detect and replace pairs of Parameter and Result operations with Assign and ReadValue operations within TensorIterator/Loop nodes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nov.pass.LowLatency2().run_on_model(model)\n```\n\n----------------------------------------\n\nTITLE: Range Operation Example (Positive Step) - XML\nDESCRIPTION: This XML snippet demonstrates the Range operation with a positive step. It shows how to define the input ports for start, stop, and step values, and the expected output dimension. The start value is 2, the stop value is 23, and the step value is 3, resulting in the sequence [2, 5, 8, 11, 14, 17, 20].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/range-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Range\">\n    <input>\n        <port id=\"0\">  <!-- start value: 2 -->\n        </port>\n        <port id=\"1\">  <!-- stop value: 23 -->\n        </port>\n        <port id=\"2\">  <!-- step value: 3 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>7</dim> <!-- [ 2,  5,  8, 11, 14, 17, 20] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Evaluate in C++\nDESCRIPTION: This snippet demonstrates how to override the `evaluate` method in C++ for a custom OpenVINO operation. If your operation contains ``evaluate`` method you also need to override the ``has_evaluate`` method, this method allows to get information about availability of ``evaluate`` method for the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-openvino-operations.rst#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n//Evaluate and has_evaluate methods are not presented in Identity class\n//Here is example how it can be done for custom operation\nclass AddConst : public ov::op::Op {\npublic:\n    OPENVINO_OP(\"AddConst\", \"template\");\n\n    AddConst(const ov::Output<ov::Node>& arg) : ov::op::Op({arg}) {\n        constructor_validate_and_infer_types();\n    }\n\n    std::shared_ptr<ov::Node> clone_with_new_inputs(const ov::OutputVector& new_args) const override {\n        OPENVINO_ASSERT(new_args.size() == 1, \"Expected 1 element in new_args, but got \", new_args.size());\n        return std::make_shared<AddConst>(new_args.at(0));\n    };\n\n    void validate_and_infer_types() override {\n        OPENVINO_ASSERT(get_input_size() == 1, \"Expected 1 input, but got \", get_input_size());\n        set_output_type(0, get_input_element_type(0), get_input_partial_shape(0));\n    }\n\n    bool visit_attributes(ov::AttributeVisitor& visitor) override {\n        return true;\n    }\n\n    bool evaluate(ov::TensorVector& outputs, const ov::TensorVector& inputs) const {\n        if (inputs.empty()) {\n            return false;\n        }\n\n        auto in_data = inputs[0].data<float>();\n        auto out_data = outputs[0].data<float>();\n\n        for (size_t i = 0; i < inputs[0].get_size(); ++i) {\n            out_data[i] = in_data[i] + 1;\n        }\n        return true;\n    }\n\n    bool has_evaluate() const override {\n        return true;\n    }\n};\n\n```\n\n----------------------------------------\n\nTITLE: Install OpenCL packages on RedHat UBI 8\nDESCRIPTION: Installs OpenCL and Level Zero packages on RedHat UBI 8 using yum. It requires access to yum repositories. It also installs the OpenCL ICD Loader via rpm.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/configurations-intel-gpu.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nyum install intel-opencl level-zero intel-level-zero-gpu intel-igc-core intel-igc-cm intel-gmmlib intel-ocloc\n```\n\nLANGUAGE: sh\nCODE:\n```\nrpm -ivh http://mirror.centos.org/centos/8-stream/AppStream/x86_64/os/Packages/ocl-icd-2.2.12-1.el8.x86_64.rpm\n```\n\n----------------------------------------\n\nTITLE: Minimal Python API Configuration in CMake\nDESCRIPTION: This CMake snippet calls the custom function `ov_python_minimal_api` with the project name as an argument. This function likely configures a minimal Python API for the pyopenvino module, potentially setting up basic functionalities or interfaces.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_python_minimal_api(${PROJECT_NAME})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Performance of Heterogeneous Execution with performance counts\nDESCRIPTION: This shell snippet shows an example of the output when using performance data (the `-pc` option) to get the performance data on each subgraph when running in heterogeneous execution mode.  It shows the realTime, cpu time and execType (execution type) for each layer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/hetero-execution.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nsubgraph1: 1. input preprocessing (mean data/HDDL):EXECUTED layerType:          realTime: 129   cpu: 129  execType:\nsubgraph1: 2. input transfer to DDR:EXECUTED                layerType:          realTime: 201   cpu: 0    execType:\nsubgraph1: 3. HDDL execute time:EXECUTED                    layerType:          realTime: 3808  cpu: 0    execType:\nsubgraph1: 4. output transfer from DDR:EXECUTED             layerType:          realTime: 55    cpu: 0    execType:\nsubgraph1: 5. HDDL output postprocessing:EXECUTED           layerType:          realTime: 7     cpu: 7    execType:\nsubgraph1: 6. copy to IE blob:EXECUTED                      layerType:          realTime: 2     cpu: 2    execType:\nsubgraph2: out_prob:          NOT_RUN                       layerType: Output   realTime: 0     cpu: 0    execType: unknown\nsubgraph2: prob:              EXECUTED                      layerType: SoftMax  realTime: 10    cpu: 10   execType: ref\nTotal time: 4212 microseconds\n```\n\n----------------------------------------\n\nTITLE: IDFT Layer XML Configuration (3D input, with signal_size)\nDESCRIPTION: This XML snippet configures an IDFT layer with a 3D input tensor, incorporating the signal_size input to define the output dimensions. The example showcases how to provide signal size information alongside input data and axes specifications.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/idft-7.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>\t<!-- [0, 1] -->\n        </port>\n        <port id=\"2\">\n            <dim>2</dim>\t<!-- [512, 100] -->\n        </port>\n    <output>\n        <port id=\"3\">\n            <dim>512</dim>\n            <dim>100</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: create_infer_request() Implementation C++\nDESCRIPTION: This snippet shows an alternate implementation to create the infer request. The actual logic of creating the infer request is implemented in create_infer_request_impl().\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/compiled-model.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nov::InferRequest CompiledModel::create_infer_request() {\n    return create_infer_request_impl({});\n}\n```\n\n----------------------------------------\n\nTITLE: Excluding Devices - Python\nDESCRIPTION: This Python snippet demonstrates how to exclude a device (CPU in this case) from the device candidate list when compiling a model using the AUTO device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ncompiled_model = core.compile_model(model=model, device_name=\"AUTO:-CPU\")\n```\n\n----------------------------------------\n\nTITLE: Quantize-only FQ U8\nDESCRIPTION: This snippet represents a quantize-only FakeQuantize operation for unsigned 8-bit integers (U8). It shows the formula for quantizing the input value 'x' with an input scale 'S_i', clamping the result to the range [0, 255]. It assumes that the input low is 0, S_o = 1 and Z_o = 0\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/fake_quantize.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nq'_{U8} = clamp(round(x*\\frac{1}{S_i}), 0, 255)\n```\n\n----------------------------------------\n\nTITLE: MyTensor Class Definition C++\nDESCRIPTION: Defines a simple `MyTensor` class with a constructor that takes an `ov::Tensor`, a destructor, and a `get_size` method that returns the size of the underlying tensor. The `_tensor` member is a public `ov::Tensor`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nclass MyTensor {\npublic:\n    // Constructing MyTensor is done with already created Tensor\n    MyTensor(ov::Tensor& t) : _tensor(t) {};\n\n    ~MyTensor() = default;\n\n    // Gets size from Tensor and return it\n    size_t get_size() const {\n        return _tensor.get_size();\n    }\n\n    // Public member that allows all operations on Tensor\n    ov::Tensor _tensor;\n};\n```\n\n----------------------------------------\n\nTITLE: Setting String Data for Tensor in OpenVINO (C)\nDESCRIPTION: This function sets string data for a given OpenVINO tensor. It takes a pointer to the tensor, a pointer to a string array, and the size of the array as input. The function returns a status code to indicate success or failure of the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_49\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_tensor_set_string_data(ov_tensor_t* tensor, const char** string_array, size_t array_size)\n```\n\n----------------------------------------\n\nTITLE: Disabling LPT transformations\nDESCRIPTION: This example demonstrates how to disable only the LPT (Low Precision Transformation) graph transformations by using the 'transformations=lpt' option.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/feature_disabling.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_DISABLE=\"transformations=lpt\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Overall GPU Graph Data Structure Diagram (Mermaid)\nDESCRIPTION: This Mermaid diagram illustrates the relationships between the key classes and structures involved in representing a computational graph for GPU execution in OpenVINO. It includes classes such as `primitive`, `program_node`, `topology`, `program`, `network`, and their respective implementations and instances.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/basic_data_structures.md#_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nclassDiagram\ndirection LR\npooling  --<| primitive_base\nconvolution --<| primitive_base\nclass primitive_base{<<PType>>}\nprimitive_base --<| primitive\nprimitive --o program_node\nprimitive --o topology\nclass typed_program_node {<<convolution>>}\ntyped_program_node --<| typed_program_node_base\nclass typed_program_node_base{<<PType>>}\ntyped_program_node_base --<| program_node\nprogram_node --o program\nclass primitive_type {\n+create_node\n+create_instance\n+choose_mpl}\nprogram --> topology\nprogram ..<| primitive_type : create_node()\\nchoose_impl()\nconvolution_impl --<| typed_primitive_impl_ocl\nfully_connected_impl --<| typed_primitive_impl_ocl\nconvolution_onednn --<| typed_primitive__onednn_impl\npooling_onednn --<| typed_primitive__onednn_impl\nclass typed_primitive_impl_ocl {<<PType>>}\ntyped_primitive_impl_ocl --<| typed_primitive_impl\nclass typed_primitive__onednn_impl {<<PType>>}\ntyped_primitive__onednn_impl --<| typed_primitive_impl\nclass typed_primitive_impl {<<PType>>}\ntyped_primitive_impl --<| primitive_impl\nprimitive_impl --o primitive_inst\nprimitive_impl --o program_node\nclass typed_primitive_inst {<<convolution>>}\nclass `typed_primitive-inst` {<<pooling>>}\ntyped_primitive_inst --<| typed_primitive_inst_base\n`typed_primitive-inst` --<| typed_primitive_inst_base\nclass typed_primitive_inst_base {<<PType>>}\ntyped_primitive_inst_base --<| primitive_inst\nprimitive_inst --> program_node\nprimitive_inst --o network\nnetwork --> program\nnetwork ..<| primitive_type : create_instance\nclass primitive_type_base {<<PType>>}\nprimitive_type_base --<| primitive_type\nprimitive_type_base ..<| typed_program_node\nprimitive_type_base --o primitive_base: 0.1\nclass implementation_map {<<PType>>\\nget(typed_program_node<Ptype>): factory_type}\nprimitive_type_base ..<| implementation_map : get()\nprimitive_type_base ..<| typed_primitive_inst\na1 o-- a2 : Aggregation\nb1 --> b2 : Association\nc1 --<| c2 : Inheritance\nd1 ..> d2 : Dependency\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Dependency\nDESCRIPTION: Configures the OpenCV dependency, requiring version 3.0 or greater.  If OpenCV is found and meets the version requirement, it defines a compile definition and links the opencv_core library to the target; otherwise, it issues a warning.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/benchmark_app/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenCV QUIET COMPONENTS core)\nif(NOT OpenCV_FOUND OR NOT OpenCV_VERSION VERSION_GREATER_EQUAL 3)\n    message(WARNING \"OpenCV ver. 3.0+ is not found, ${TARGET_NAME} will be built without OpenCV support. Set OpenCV_DIR\")\nelse()\n    target_compile_definitions(${TARGET_NAME} PRIVATE USE_OPENCV)\n    target_link_libraries(${TARGET_NAME} PRIVATE opencv_core)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Downloading and Installing OpenVINO (Ubuntu 20.04)\nDESCRIPTION: These commands download the OpenVINO Runtime archive for Ubuntu 20.04, extract it, and move the extracted directory to `/opt/intel`. It uses `curl` to download the archive, `tar` to extract it, and `sudo mv` to move the extracted folder with root privileges.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino/packages/2025.1/linux/openvino_toolkit_ubuntu20_2025.1.0.18503.6fec06580ab_x86_64.tgz --output openvino_2025.1.0.tgz\ntar -xf openvino_2025.1.0.tgz\nsudo mv openvino_toolkit_ubuntu20_2025.1.0.18503.6fec06580ab_x86_64 /opt/intel/openvino_2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Building the OpenVINO Template Plugin with CMake\nDESCRIPTION: This snippet shows how to build the OpenVINO Template Plugin using CMake and Make. It assumes the OpenVINO core has already been built. The `ENABLE_TEMPLATE_REGISTRATION` option registers the plugin in `plugin.xml` and enables the install target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cd <openvino_dir>\n$ mkdir <openvino_dir>/build\n$ cd <openvino_dir>/build\n$ cmake -DENABLE_TESTS=ON -DENABLE_FUNCTIONAL_TESTS=ON ..\n$ make -j8\n$ cd <template_plugin_dir>\n$ mkdir <template_plugin_dir>/build\n$ cd <template_plugin_dir>/build\n$ cmake -DOpenVINODeveloperPackage_DIR=<openvino_dir>/build -DENABLE_TEMPLATE_REGISTRATION=ON ..\n$ make -j8\n```\n\n----------------------------------------\n\nTITLE: Hello Reshape SSD Node.js Arguments\nDESCRIPTION: This describes the arguments for the Hello Reshape SSD Node.js script.  It requires specifying the *path_to_model_file*, *path_to_img*, and *device* as arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/js/node/hello_reshape_ssd/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnode hello_reshape_ssd.js *path_to_model_file* *path_to_img* *device*\n```\n\n----------------------------------------\n\nTITLE: Running Image Classification Async Sample with Node.js\nDESCRIPTION: This command executes the classification_sample_async.js script to perform image classification. It requires specifying the path to the model file (-m), the path to the input image(s) (-i), and the target device (-d). The AUTO device option automatically selects the optimal device for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/js/node/classification_sample_async/README.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nnode classification_sample_async.js -m ../../assets/models/v3-small_224_1.0_float.xml -i ../../assets/images/coco.jpg -i ../../assets/images/coco_hollywood.jpg -d AUTO\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenVINO™ installation with Python\nDESCRIPTION: This Python snippet verifies a successful OpenVINO™ installation by importing the `Core` class from the `openvino` module and printing the available devices. If OpenVINO is correctly installed, it will output a list of available devices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/troubleshooting-install-config.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython -c \"from openvino import Core; print(Core().available_devices)\"\n```\n\n----------------------------------------\n\nTITLE: AsyncInferRequest Constructor (C++)\nDESCRIPTION: The constructor defines the device pipeline (m_pipeline) for asynchronous inference. It sets up tasks for preprocessing, device execution and postprocessing, distributing them between CPU and device task executors to enable parallel execution and improve overall inference performance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/asynch-inference-request.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nAsyncInferRequest::AsyncInferRequest(\n    const std::shared_ptr<CompiledModel>& compiled_model,\n    const PrePostProcessor& inputs_preprocessor,\n    const std::shared_ptr<ov::threading::ITaskExecutor>& wait_executor)\n    : ov::IAsyncInferRequest({compiled_model->get_core()->get_callback_executor()}),\n      m_inputs_preprocessor(inputs_preprocessor),\n      m_compiled_model(compiled_model),\n      m_cancel_callback([] { return false; }),\n      m_wait_executor(wait_executor) {\n    m_pipeline.emplace_back(m_compiled_model.lock()->get_request_executor(), [this] {\n        infer_preprocess_and_start_pipeline();\n    });\n    m_pipeline.emplace_back(m_wait_executor, [this] {\n        wait_pipeline();\n    });\n    m_pipeline.emplace_back(m_compiled_model.lock()->get_callback_executor(), [this] {\n        infer_postprocess();\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies based on Enabled Features\nDESCRIPTION: This CMake snippet conditionally adds dependencies to the test target based on whether Intel CPU or GPU support is enabled. If `ENABLE_INTEL_CPU` is true, it adds `openvino_intel_cpu_plugin` as a dependency. Similarly, if `ENABLE_INTEL_GPU` is true, it adds `openvino_intel_gpu_plugin` as a dependency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/tests/functional/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_INTEL_CPU)\n    add_dependencies(${TARGET_NAME} openvino_intel_cpu_plugin)\nendif()\nif(ENABLE_INTEL_GPU)\n    add_dependencies(${TARGET_NAME} openvino_intel_gpu_plugin)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Interpolate Layer Configuration in XML - OpenVINO\nDESCRIPTION: This XML snippet configures the Interpolate layer in OpenVINO, demonstrating the use of 'scales' for shape calculation, padding settings, and the 'bicubic_pillow' interpolation mode. It showcases the input and output port definitions, including their dimensions, and the expected values in the scales and axes input tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-11.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Interpolate\" ...>\n    <data shape_calculation_mode=\"scales\" pads_begin=\"0\" pads_end=\"0\" mode=\"bicubic_pillow\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>2</dim>\n            <dim>48</dim>\n            <dim>80</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>\t<!--The values in this input are [24, 160] -->\n        </port>\n        <port id=\"2\">\n            <dim>2</dim>\t<!--The values in this input are [0.5, 2.0] -->\n        </port>\n        <port id=\"3\">\n            <dim>2</dim>\t<!--The values in this input are [2, 3] (axes). -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\"  precision=\"FP32\">\n            <dim>1</dim>\n            <dim>2</dim>\n            <dim>24</dim>\n            <dim>160</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: AsyncInferRequest Cancel Method (C++)\nDESCRIPTION: The cancel method provides a way to interrupt the asynchronous inference request execution. It sets a callback that allows to interrupt the execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/asynch-inference-request.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nvoid AsyncInferRequest::set_cancel_callback(std::function<bool()> callback) {\n    m_cancel_callback = callback;\n}\n```\n\n----------------------------------------\n\nTITLE: Compile Model with CUMULATIVE_THROUGHPUT Hint in Python\nDESCRIPTION: This snippet shows how to compile a model using the AUTO plugin with a specific device priority (GPU, then CPU) and the CUMULATIVE_THROUGHPUT performance hint in Python. This hint is used to maximize throughput across multiple devices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\ncompiled_model = core.compile_model(model, \"AUTO:GPU,CPU\", {hints.performance_mode: hints.PerformanceMode.CUMULATIVE_THROUGHPUT})\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Space Before Function Parentheses\nDESCRIPTION: This rule enforces consistent spacing before function parentheses in JavaScript and TypeScript code. Named and anonymous functions should not have a space before parentheses, while async arrow functions should. The configuration is `space-before-function-paren: ['error', { named: 'never', anonymous: 'never', asyncArrow: 'always' }]`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_15\n\nLANGUAGE: JavaScript\nCODE:\n```\nspace-before-function-paren: ['error', { named: 'never', anonymous: 'never', asyncArrow: 'always' }]\n```\n\n----------------------------------------\n\nTITLE: Creating New Python Module\nDESCRIPTION: This snippet demonstrates the directory structure and file creation for adding a new Python module to the OpenVINO™ namespace.  It shows how to organize a new module `mymodule` within the `openvino` package. The `__init__.py` files are crucial for Python to recognize the directories as packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nopenvino/               <-- Main package/namespace\n├── __init__.py         <-- Unified file between all packages\n└── mymodule/           <-- This is your new module and its contents\n    ├── __init__.py\n    ├── ...\n    └── myclass.py\n```\n\n----------------------------------------\n\nTITLE: OpenVINO: Quantize Model\nDESCRIPTION: Applies 8-bit quantization to the OpenVINO model using the provided dataset. This process calibrates the model for reduced precision inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nquantized_model = nncf.quantize(model=model, calibration_dataset=dataset)\n\n```\n\n----------------------------------------\n\nTITLE: Loading Preprocessed Model - C++\nDESCRIPTION: Demonstrates loading a preprocessed OpenVINO IR model from a file in C++. The application loading the model avoids the original preprocessing steps, which increases the performance. Additionally, the device configuration and model caching options are specified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details/integrate-save-preprocessing-use-case.rst#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nvoid save_load() {\n    ov::Core core;\n    // the preprocessed model is loaded, preprocessing is not required\n    std::shared_ptr<ov::Model> model = core.read_model(\"ov_model.xml\");\n    // Set device preferences and enable model caching\n    ov::CompiledModel compiled_model = core.compile_model(model, \"CPU\", {\"CACHE_DIR\": \"cache\"});\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCL Dependency\nDESCRIPTION: Configures the OpenCL dependency based on the `ENABLE_INTEL_GPU` and `SAMPLES_ENABLE_OPENCL` options.  It finds the OpenCL package and header files. If found, it sets compile definitions and links the OpenCL library to the target; otherwise, it issues a warning.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/benchmark_app/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(DEFINED ENABLE_INTEL_GPU AND NOT ENABLE_INTEL_GPU)\n    # Intel GPU plugin is turned off explicitly\n    option(SAMPLES_ENABLE_OPENCL \"Use OpenCL in benchmark_app\" OFF)\nelse()\n    option(SAMPLES_ENABLE_OPENCL \"Use OpenCL in benchmark_app\" ON)\nendif()\n\nif(SAMPLES_ENABLE_OPENCL)\n    find_package(OpenCL QUIET)\n    if(NOT OpenCL_FOUND)\n        message(WARNING \"OpenCL is disabled or not found, ${TARGET_NAME} will be built without OpenCL support. Install OpenCL.\")\n    endif()\n\n    set(opencl_header_search_params\n        HINTS\n            ${opencl_root_hints}\n        PATHS\n            ENV \"PROGRAMFILES(X86)\"\n            ENV AMDAPPSDKROOT\n            ENV INTELOCLSDKROOT\n            ENV NVSDKCOMPUTE_ROOT\n            ENV CUDA_PATH\n            ENV ATISTREAMSDKROOT\n            ENV OCL_ROOT\n        PATH_SUFFIXES\n            \"include\"\n            \"OpenCL/common/inc\"\n            \"AMD APP/include\")\n\n    find_path(OpenCL_HPP_INCLUDE_DIR\n        NAMES\n            CL/opencl.hpp OpenCL/opencl.hpp\n        ${opencl_header_search_params})\n\n    find_path(CL2_HPP_INCLUDE_DIR\n        NAMES\n            CL/cl2.hpp OpenCL/cl2.hpp\n        ${opencl_header_search_params})\n\n    if(OpenCL_FOUND AND (OpenCL_HPP_INCLUDE_DIR OR CL2_HPP_INCLUDE_DIR))\n        set(OpenCL_DEFINITIONS HAVE_GPU_DEVICE_MEM_SUPPORT)\n\n        # Append OpenCL CPP headers to C headers and use both\n        if(OpenCL_HPP_INCLUDE_DIR)\n            list(APPEND OpenCL_HEADERS ${OpenCL_HPP_INCLUDE_DIR})\n            # the macro below is defined when opencl.hpp is found to suppress deprecation message from cl2.hpp\n            list(APPEND OpenCL_DEFINITIONS OV_GPU_USE_OPENCL_HPP)\n        endif()\n        if(CL2_HPP_INCLUDE_DIR)\n            list(APPEND OpenCL_HEADERS ${CL2_HPP_INCLUDE_DIR})\n        endif()\n\n        # cmake cannot set properties for imported targets\n        get_target_property(opencl_target OpenCL::OpenCL ALIASED_TARGET)\n        if(NOT TARGET ${opencl_target})\n            set(opencl_target OpenCL::OpenCL)\n        endif()\n\n        set_property(TARGET ${opencl_target} APPEND PROPERTY\n            INTERFACE_INCLUDE_DIRECTORIES ${OpenCL_HEADERS})\n        set_property(TARGET ${opencl_target} APPEND PROPERTY\n            INTERFACE_COMPILE_DEFINITIONS ${OpenCL_DEFINITIONS})\n\n        target_link_libraries(${TARGET_NAME} PRIVATE OpenCL::OpenCL)\n    else()\n        message(WARNING \"OpenCL CPP header is not found, ${TARGET_NAME} will be built without OpenCL support and you will not be able to use the '-use_device_mem' option. Please, install '<apt | yum> install opencl-headers' to enable the option\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Export OV Model with INT4 Weight Format using CLI\nDESCRIPTION: Exports an OpenVINO model from a Hugging Face model using the `optimum-cli` command, compressing the weights to INT4 format with AWQ quantization. This command includes options for AWQ, scale estimation, dataset selection, group size, and ratio.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression.rst#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\noptimum-cli export openvino --model microsoft/Phi-3.5-mini-instruct --weight-format int4 --awq --scale-estimation --dataset wikitext2 --group-size 64 --ratio 1.0 ov_phi-3.5-mini-instruct\n```\n\n----------------------------------------\n\nTITLE: StridedSlice Clamping Example in XML\nDESCRIPTION: Illustrates clamping behavior in the StridedSlice layer. The example performs slicing on a 2D array with begin and end values that are out of bounds.  The begin and end values are clamped to the valid range. It is equivalent to array[2:3, 2:1:-1].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/strided-slice-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StridedSlice\" ...>\n    <data/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- begin: [1234, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>2</dim> <!-- end: [1234, 4321] -->\n        </port>\n        <port id=\"3\">\n            <dim>2</dim> <!-- stride: [1, -1] - second slicing is in reverse-->\n        </port>\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>1</dim> <!-- begin clamped to 2, end clamped to 3, element ids: [2] -->\n            <dim>1</dim> <!-- begin clamped to 2, end clamped to 1, element ids: [2] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Running Image Classification Sample (Python, Windows)\nDESCRIPTION: This command runs the `classification_sample_async.py` sample with a specified model (`googlenet-v1.xml`), input image (`dog.bmp`), and target device (`CPU`) on a Windows system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_30\n\nLANGUAGE: bat\nCODE:\n```\npython classification_sample_async.py -m %USERPROFILE%\\Documents\\ir\\googlenet-v1.xml -i %USERPROFILE%\\Downloads\\dog.bmp -d CPU\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Node.js package with npm\nDESCRIPTION: This command installs the OpenVINO Node.js package from npm. It downloads and installs the necessary dependencies to use OpenVINO within a Node.js environment, allowing for deep learning model deployment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install openvino-node\n```\n\n----------------------------------------\n\nTITLE: Add OpenVINO Tokenizers Extension (Windows)\nDESCRIPTION: Adds the OpenVINO Tokenizers extension to the OpenVINO Core object in C++ for Windows, enabling the use of the tokenizers in inference pipelines.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ncore.add_extension(\"openvino_tokenizers.dll\")\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Tokenizers\nDESCRIPTION: Installs the openvino-tokenizers package using pip. This installs the core library needed for tokenization and detokenization using OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npip install openvino-tokenizers\n```\n\n----------------------------------------\n\nTITLE: Set Performance Mode to Best Performance (Python)\nDESCRIPTION: This Python snippet demonstrates how to configure the OpenVINO GenAI pipeline for best performance by setting the `GENERATE_HINT` option to `BEST_PERF`. This optimizes the pipeline for maximum throughput at the expense of increased compilation time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npipeline_config = { \"GENERATE_HINT\": \"BEST_PERF\" }\npipe = ov_genai.LLMPipeline(model_path, \"NPU\", pipeline_config)\n```\n\n----------------------------------------\n\nTITLE: Enabling Faster Builds\nDESCRIPTION: Calls a custom cmake function, `ov_build_target_faster`, which configures the build for faster compilation, potentially using unity builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME}\n    UNITY\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating Preprocessing Steps into the Model (Python)\nDESCRIPTION: This snippet demonstrates how to build the model after defining the preprocessing steps, retrieve the configuration for debugging, and get the final model in Python using `ppp.build()`.  The built model will automatically incorporate the specified preprocessing steps into its execution graph.  The `print(ppp)` statement is for debugging, it shows the PrePostProcessor configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nppp.input().model().set_layout(ov.Layout('NCHW'))\nmodel = ppp.build()\nprint(ppp)\n```\n\n----------------------------------------\n\nTITLE: Save Checkpoint - PyTorch\nDESCRIPTION: Saves a model checkpoint during QAT using NNCF's API. This is important for resuming training or deploying the model later. NNCF wraps the original model, so a specific API is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/quantization-aware-training.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnncf.save_checkpoint(model, f=\"{checkpoint_path}/model_quantized.pth\")\n```\n\n----------------------------------------\n\nTITLE: Preprocessing C++\nDESCRIPTION: Demonstrates preprocessing steps using OpenVINO API, including setting element type, layout, and spatial static shape for input tensors, as well as resizing and building the preprocessor. These steps are essential for adapting the input image to the model's requirements.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_classification/README.md#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n``ov::preprocess::InputTensorInfo::set_element_type``\n```\n\nLANGUAGE: C++\nCODE:\n```\n``ov::preprocess::InputTensorInfo::set_layout``\n```\n\nLANGUAGE: C++\nCODE:\n```\n``ov::preprocess::InputTensorInfo::set_spatial_static_shape``\n```\n\nLANGUAGE: C++\nCODE:\n```\n``ov::preprocess::PreProcessSteps::resize``\n```\n\nLANGUAGE: C++\nCODE:\n```\n``ov::preprocess::InputModelInfo::set_layout``\n```\n\nLANGUAGE: C++\nCODE:\n```\n``ov::preprocess::OutputTensorInfo::set_element_type``\n```\n\nLANGUAGE: C++\nCODE:\n```\n``ov::preprocess::PrePostProcessor::build``\n```\n\n----------------------------------------\n\nTITLE: Converting TensorFlow TopKV2 Operation in OpenVINO\nDESCRIPTION: This code snippet shows how to convert the TensorFlow `TopKV2` operation using the OpenVINO framework extension.  It leverages `NamedOutputVector` for assigning names to outputs, enabling both indexed and named access. Refer to TensorFlow's `tf.raw_ops` documentation to determine output names for accurate mapping.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/frontend-extensions.rst#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nOPENVINO_FRONTEND_EXTENSION(frontend_extension_tf, frontend_extension_tf_TopK)\n{\n    using namespace ov::frontend;\n    add_operation(\"TopKV2\",\n        [&](const NodeContext& node)\n        {\n            NamedOutputVector outputs;\n            outputs[\"values\"] = {node.get_output(0)};\n            outputs[\"indices\"] = {node.get_output(1)};\n            return outputs;\n        });\n}\n```\n\n----------------------------------------\n\nTITLE: Proposal Layer Configuration in XML\nDESCRIPTION: This XML snippet shows an example configuration of the Proposal layer, specifying the data attributes such as base_size, feat_stride, min_size, nms_thresh, normalize, post_nms_topn, pre_nms_topn, ratio, and scale. It also defines the input and output ports with their respective dimensions and precision.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/proposal-4.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Proposal\" ... >\n    <data base_size=\"16\" feat_stride=\"8\" min_size=\"16\" nms_thresh=\"1.0\" normalize=\"0\" post_nms_topn=\"1000\" pre_nms_topn=\"1000\" ratio=\"1\" scale=\"1,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>7</dim>\n            <dim>4</dim>\n            <dim>28</dim>\n            <dim>28</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>8</dim>\n            <dim>28</dim>\n            <dim>28</dim>\n        </port>\n        <port id=\"2\">\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP32\">\n            <dim>7000</dim>\n            <dim>5</dim>\n        </port>\n        <port id=\"4\" precision=\"FP32\">\n            <dim>7000</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO GenAI from Archive - macOS x86_64\nDESCRIPTION: These commands download and extract the OpenVINO GenAI archive for macOS (x86_64 architecture). The `curl` command downloads the tarball, and the `tar` command extracts its contents.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-genai.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino_genai/packages/2025.1/macos/openvino_genai_macos_12_6_2025.1.0.0_x86_64.tar.gz --output openvino_genai_2025.1.0.0.tgz\ntar -xf openvino_genai_2025.1.0.0.tgz\n```\n\n----------------------------------------\n\nTITLE: Convert TensorFlow Function to OpenVINO\nDESCRIPTION: This snippet shows how to convert a TensorFlow function decorated with `@tf.function` to an OpenVINO model using `openvino.convert_model`. The `input_signature` argument is crucial for defining the input shapes and types. Requires the `tensorflow` package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_14\n\nLANGUAGE: py\nCODE:\n```\n@tf.function(\n   input_signature=[tf.TensorSpec(shape=[1, 2, 3], dtype=tf.float32),\n                        tf.TensorSpec(shape=[1, 2, 3], dtype=tf.float32)])\ndef func(x, y):\n   return tf.nn.sigmoid(tf.nn.relu(x + y))\n\nimport openvino as ov\nov_model = ov.convert_model(func)\n```\n\n----------------------------------------\n\nTITLE: Setting Link Options\nDESCRIPTION: This snippet sets linker options based on the operating system's architecture. On Apple, it suppresses undefined symbols and flattens the namespace. On AArch64 and ARM, it ignores unresolved symbols.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif(APPLE)\n    target_link_options(${PROJECT_NAME} PRIVATE -Wl,-undefined,suppress,-flat_namespace)\nelif(AARCH64 OR ARM)\n    target_link_options(${PROJECT_NAME} PRIVATE -Wl,--unresolved-symbols=ignore-all)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Performance Counters Example\nDESCRIPTION: This shell code block shows an example of the output from the performance counters log.  It highlights the `execType` field containing `brgemm_avx512_amx_sparse_I8`, which indicates that the sparse weights decompression feature was applied to the `MatMul_1800` layer using the Intel AMX extension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\nMatMul_1800         EXECUTED         layerType: FullyConnected         execType: brgemm_avx512_amx_sparse_I8 realTime (ms): 0.050000  cpuTime (ms): 0.050000\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Requests Hint in Python\nDESCRIPTION: This Python snippet shows how to set the `ov::hint::num_requests` configuration key to limit the batch size for the GPU and the number of inference streams for the CPU. It optimizes performance for applications processing a limited number of video streams (e.g., 4). It assumes the `core` object and `model` object are already defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/high-level-performance-hints.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncompiled_model = core.compile_model(model=model, device_name=\"CPU\", config={\n    ov.hint.performance_mode: ov.hint.PerformanceMode.THROUGHPUT,\n    ov.hint.num_requests: 4\n})\n```\n\n----------------------------------------\n\nTITLE: Downloading, Extracting and Moving OpenVINO x86 Archive\nDESCRIPTION: These commands download the OpenVINO Runtime archive file for macOS (x86, 64-bit), extract the files, rename the extracted folder, and move it to the `/opt/intel/openvino_2025.1.0` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-macos.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino/packages/2025.1/macos/openvino_toolkit_macos_12_6_2025.1.0.18503.6fec06580ab_x86_64.tgz --output openvino_2025.1.0.tgz\ntar -xf openvino_2025.1.0.tgz\nsudo mv openvino_toolkit_macos_12_6_2025.1.0.18503.6fec06580ab_x86_64 /opt/intel/openvino_2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Profiling and Data Collection (Linux)\nDESCRIPTION: This snippet demonstrates how to set environment variables for CPU cache sizes and use `sea_runtool.py` with `benchmark_app` to collect profiling data. The script uses `numactl` to bind the benchmark to specific cores for accurate profiling. This data is later used to generate the conditional compilation package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nexport OV_CC_L1_CACHE_SIZE=<L1 cache size>\nexport OV_CC_L2_CACHE_SIZE=<L2 cache size>\nexport OV_CC_L3_CACHE_SIZE=<L3 cache size>\npython thirdparty/itt_collector/runtool/sea_runtool.py --bindir ${OPENVINO_LIBRARY_DIR} -o ${MY_MODEL_RESULT} ! sde -spr -- numactl -C 0-$core_num ./benchmark_app -niter 1 -nireq 1 -m ${MY_MODEL}.xml\n```\n\n----------------------------------------\n\nTITLE: Create Python Virtual Environment (Windows)\nDESCRIPTION: This command creates a Python virtual environment named `openvino_env` in the current directory. Virtual environments help isolate project dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-pip.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython -m venv openvino_env\n```\n\n----------------------------------------\n\nTITLE: Tensor Operations in Python\nDESCRIPTION: This snippet shows how to retrieve the shape and data of a tensor using `get_shape` and `data`. This is useful for understanding the structure of the tensor and accessing its underlying data buffer for modification or inspection.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/python/benchmark/sync_benchmark/README.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nopenvino.runtime.Tensor.get_shape\n```\n\nLANGUAGE: Python\nCODE:\n```\nopenvino.runtime.Tensor.data\n```\n\n----------------------------------------\n\nTITLE: Include OpenVINO CMake Module\nDESCRIPTION: This snippet includes the cmake/openvino.cmake module, which likely contains OpenVINO-specific CMake functions and settings. This module may define custom build targets, dependencies, or other configurations required by the OpenVINO project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(cmake/openvino.cmake)\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Priority in C++\nDESCRIPTION: This C++ snippet shows how to configure model priority for the Auto-Device plugin.  The implementation details are provided within the `[part4]` section of `docs/articles_en/assets/snippets/AUTO4.cpp`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\ndocs/articles_en/assets/snippets/AUTO4.cpp\n```\n\n----------------------------------------\n\nTITLE: Checking iGPU Enablement (Linux)\nDESCRIPTION: This command checks if the integrated GPU (iGPU) is enabled by reading the contents of a specific file in the `/sys` filesystem. A value of `1` indicates that the iGPU is enabled. The command uses `cat` to print the file contents.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_driver_troubleshooting.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ cat /sys/devices/pci0000\\:00/0000\\:00\\:02.0/enable\n1\n```\n\n----------------------------------------\n\nTITLE: Markup Pipeline Configuration C++\nDESCRIPTION: This code snippet demonstrates how to configure and run the low precision markup pipeline in OpenVINO using the C++ API. It specifies the precision restrictions for the convolution operations and then runs the markup transformations.  The pipeline aims to optimize the model for inference performance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/advanced-guides/low-precision-transformations/step2-markup.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstd::vector<ov::pass::low_precision::MarkupElement> markupElements = {\n    {\n        ov::pass::low_precision::MarkupTarget::Convolution, /*convolution*/\n        {0}, /*input port 0*/\n        {ov::element::u8} /* precision U8*/\n    },\n    {\n        ov::pass::low_precision::MarkupTarget::Convolution, /*convolution*/\n        {1}, /*input port 1*/\n        {ov::element::i8} /* precision I8*/\n    }\n};\n\nconfiguration.lowPrecision.markup.params.emplace(lowPrecisionTransformation::name(), ov::pass::low_precision::MarkupParams(markupElements));\nconfiguration.lowPrecision.operations = {\"Convolution\"};\n\npassManager.register_pass<ov::pass::low_precision::LowPrecision>(configuration.lowPrecision);\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Input by name of CompiledModel (TypeScript)\nDESCRIPTION: This code shows the declaration of the `input` method within the `CompiledModel` interface to retrieve an input by name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\ninput(name): Output\n```\n\n----------------------------------------\n\nTITLE: Getting Tensor Byte Size in OpenVINO (C)\nDESCRIPTION: This function retrieves the byte size of an OpenVINO tensor. It takes a pointer to the tensor and a pointer to a size_t variable as input. The function returns a status code indicating success or failure in retrieving the tensor byte size.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_47\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_tensor_get_byte_size(const ov_tensor_t* tensor, size_t* byte_size)\n```\n\n----------------------------------------\n\nTITLE: Running Conformance Tests in Parallel using run_parallel.py (Python)\nDESCRIPTION: This snippet demonstrates how to use the `run_parallel.py` tool to execute conformance tests in parallel. It shows how to specify the path to the conformanceTests executable, input folders, device, and other arguments to control the test execution. Arguments after the `--` symbol are forwarded to the `conformanceTests` target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/README.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\npython3 run_parallel.py -e=/path/to/openvino/bin/intel64/Debug/conformanceTests -d .\n--gtest_filter=*Add*:*BinaryConv* -- --input_folders=/path/to/ir_1,/path/to/ir_2 --device=CPU\n--report_unique_name --output_folder=/path/to/temp_output_report_folder\n```\n\n----------------------------------------\n\nTITLE: Setting Interprocedural Optimization\nDESCRIPTION: This snippet sets the interprocedural optimization property for the target library in Release mode based on the value of `ENABLE_LTO`.  If LTO is enabled, this improves optimization across different compilation units.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Einsum Inner Product Example C++\nDESCRIPTION: This example demonstrates how Einsum computes the inner product of two 1D tensors using the Einstein summation convention.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/einsum-7.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\na1 = [1.0, 2.0, 3.0]\na2 = [4.0, 5.0, 6.0]\nequation = \"i,i->\"\noutput = 32.0\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO Samples with Make (Linux)\nDESCRIPTION: This command uses the `make` utility to build the OpenVINO samples based on the generated Makefiles. The `--parallel` flag enables parallel compilation for faster build times.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ncmake --build . --parallel\n```\n\n----------------------------------------\n\nTITLE: GatherND Element Example\nDESCRIPTION: Demonstrates how GatherND retrieves individual elements from a data tensor using indices.  The 'indices' tensor specifies the coordinates to extract from the 'data' tensor. The output is a new tensor containing the gathered elements.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-8.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nindices = [[0, 0],\n              [1, 0]]\ndata    = [[1, 2],\n              [3, 4]]\noutput  = [1, 3]\n```\n\n----------------------------------------\n\nTITLE: Primitive Instance Class Definition in C++\nDESCRIPTION: Defines the structure of the `primitive_inst` class, representing an instance of a primitive within the network. It holds a reference to the corresponding program node, the primitive implementation, dependencies, and memory objects (output and intermediate). The `execute` method triggers the execution of the primitive.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/basic_data_structures.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nclass primitive_inst {\n...\n    program_node const& _node;\n    std::unique_ptr<primitive_impl> _impl;\n    std::vector<std::shared_ptr<primitive_inst>> _deps;\n    std::vector<std::shared_ptr<primitive_inst>> _exec_deps;\n    memory::ptr _output;\n    std::vector<memory::cptr> _intermediates_memory;\n\n    event::ptr execute(const std::vector<event::ptr>& events);\n    memory::ptr allocate_output();\n...\n};\n```\n\n----------------------------------------\n\nTITLE: Create String Tensor from NumPy Array (Python)\nDESCRIPTION: Creates an OpenVINO tensor using a NumPy array initialized with Unicode or byte strings. Requires the `numpy` package. Shows using both unicode strings and byte strings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/string-tensors.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ntensor = ov.Tensor(np.array(['text', 'more text', 'even more text']))\ntensor = ov.Tensor(np.array([b'text', b'more text', b'even more text']))\n```\n\n----------------------------------------\n\nTITLE: Using Optional with a predicate\nDESCRIPTION: This Python snippet shows how to use Optional with a predicate. The predicate validates that the number of consumers for the wrapped node is 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.tools.ovc.composite.pattern import WrapType, Optional\n\ndef optional_predicate():\n    # Creating nodes\n    relu_node = WrapType(\"opset13.Relu\")\n    optional_node = Optional(relu_node, lambda node: len(node.get_output_target_inputs(0)) == 1)\n\n    return optional_node\n```\n\n----------------------------------------\n\nTITLE: Search OpenVINO packages using apt-cache\nDESCRIPTION: Searches available OpenVINO packages in the apt cache using `apt-cache search`. It allows the user to find available versions for installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt-cache search openvino\n```\n\n----------------------------------------\n\nTITLE: Cascade of Models in OpenVINO with Python\nDESCRIPTION: This Python code demonstrates how to use `ov::InferRequest` to create a cascade of models, where the output tensor from the first model is set as the input tensor for the second model. Requires OpenVINO and compiled models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nie = Core()\nmodel_1 = ie.read_model(\"model_1.xml\")\ncompiled_model_1 = ie.compile_model(model_1, \"CPU\")\ninfer_request_1 = compiled_model_1.create_infer_request()\n\nmodel_2 = ie.read_model(\"model_2.xml\")\ncompiled_model_2 = ie.compile_model(model_2, \"CPU\")\ninfer_request_2 = compiled_model_2.create_infer_request()\n\ninput_tensor_1 = infer_request_1.get_input_tensor()\ninput_tensor_1.data[:] = np.random.normal(0, 1, input_tensor_1.shape)\n\ninfer_request_1.infer()\n\noutput_tensor_1 = infer_request_1.get_output_tensor()\ninfer_request_2.set_input_tensor(output_tensor_1)\n```\n\n----------------------------------------\n\nTITLE: Using AnyInput with a predicate\nDESCRIPTION: This C++ snippet demonstrates using AnyInput with a predicate (lambda function). The predicate ensures that the input has a rank of 4 (i.e., the input's shape has 4 dimensions).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nshared_ptr<ov::Node> any_input_predicate() {\n    auto any_input_node = make_shared<AnyInput>([](const Output<Node>& output) {\n        return output.get_shape().rank() == 4;\n    });\n\n    return any_input_node;\n}\n```\n\n----------------------------------------\n\nTITLE: Packing Data for Low Precision Element Types in OpenVINO Python\nDESCRIPTION: This snippet demonstrates how to pack data for low-precision element types (u1, u4, i4) in the OpenVINO Python API. To create an input tensor with such element types, you may need to pack your data in a new NumPy array, where the byte size matches the original input size.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\n# Assuming the input shape is (1, 1, 4, 4) and element type is u4\nshape = (1, 1, 4, 4)\nelement_type = np.uint8  # Use uint8 for u4\n\n# Original data (example)\ndata = np.array([list(range(16))], dtype=np.uint8).reshape(shape)\n\n# Pack the data\npacked_data = np.packbits(data, axis=None).reshape(shape[:2] + tuple(np.array(shape[2:]) // 2))\n```\n\n----------------------------------------\n\nTITLE: Get Default RemoteContext from CompiledModel - OpenVINO™ C++\nDESCRIPTION: This code snippet illustrates how to retrieve the default RemoteContext from a compiled model. This allows for memory sharing between the model's inputs/outputs and external memory buffers. It assumes that a compiled model has already been created using the NPU plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/npu-device/remote-tensor-api-npu-plugin.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nauto compiled_model = core.compile_model(\"model.xml\", \"NPU\");\nauto remote_context = compiled_model.get_context();\n```\n\n----------------------------------------\n\nTITLE: Compile ONNX model using compile_model in Python\nDESCRIPTION: This snippet shows how to compile an ONNX model directly using the `compile_model()` method in Python. The model is compiled for execution on the specified device (AUTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_14\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\ncompiled_model = ov.compile_model(\"<INPUT_MODEL>.onnx\", \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Update ZYPPER Repository\nDESCRIPTION: This command updates the official factory repository using zypper to get the latest software releases. It ensures that the package manager has the most up-to-date information about available packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-zypper.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nsudo zypper refresh\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Layer Configuration (Example 2)\nDESCRIPTION: Defines the ScatterElementsUpdate layer's input and output ports, specifying the dimensions for each port. The input ports include 'values', 'indices', and 'updates'. The output port defines the precision as FP32.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"ScatterElementsUpdate\">\n        <input>\n            <port id=\"0\">\n                <dim>1000</dim>\n                <dim>256</dim>\n                <dim>7</dim>\n                <dim>7</dim>\n            </port>\n            <port id=\"1\">\n                <dim>125</dim>\n                <dim>20</dim>\n                <dim>7</dim>\n                <dim>6</dim>\n            </port>\n            <port id=\"2\">\n                <dim>125</dim>\n                <dim>20</dim>\n                <dim>7</dim>\n                <dim>6</dim>\n            </port>\n            <port id=\"3\">     <!-- values: [0] -->\n                <dim>1</dim>\n            </port>\n        </input>\n        <output>\n            <port id=\"4\" precision=\"FP32\">\n                <dim>1000</dim>\n                <dim>256</dim>\n                <dim>7</dim>\n                <dim>7</dim>\n            </port>\n        </output>\n    </layer>\n```\n\n----------------------------------------\n\nTITLE: Get Input Port by Index in OpenVINO (C)\nDESCRIPTION: This function gets an input port of an OpenVINO model by its index. It requires a pointer to the `ov_model_t`, the input tensor index as a `size_t`, and returns a pointer to the `ov_output_port_t` representing the input port. A status code is returned.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_27\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_model_input_by_index(const ov_model_t* model, const size_t index, ov_output_port_t** input_port)\n```\n\n----------------------------------------\n\nTITLE: Install Python and dependencies on macOS with Brew\nDESCRIPTION: This code snippet uses Homebrew to install Python 3.9, protobuf, and ffmpeg on macOS. Protobuf and FFMPEG are optional but recommended dependencies for OpenVINO notebooks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nbrew install python@3.9\nbrew install protobuf\n\n# optional but recommended\nbrew install ffmpeg\n```\n\n----------------------------------------\n\nTITLE: Create Stateful OpenVINO Model (C++)\nDESCRIPTION: This C++ snippet demonstrates how to create a stateful OpenVINO model from scratch. `Assign` nodes should also point to `Model` to prevent deletion during graph transformations. Requires the OpenVINO runtime library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nov::Model stateful_model() {\n    auto type = ov::element::f32;\n    auto shape = ov::Shape{1, 10};\n    auto parameter = std::make_shared<ov::op::v0::Parameter>(type, shape);\n    parameter->set_friendly_name(\"parameter\");\n\n    auto constant = ov::op::v0::Constant::create(type, {1}, {1.0f});\n    auto add = std::make_shared<ov::op::v1::Add>(parameter, constant);\n    auto result = std::make_shared<ov::op::v0::Result>(add);\n    result->set_friendly_name(\"result\");\n\n    auto initial_value = ov::op::v0::Constant::create(type, {1}, {0.0f});\n    auto read_value = std::make_shared<ov::op::v3::ReadValue>(initial_value, \"state\");\n    auto add_state = std::make_shared<ov::op::v1::Add>(read_value, result);\n    auto assign = std::make_shared<ov::op::v3::Assign>(add_state, \"state\");\n\n    ov::SinkVector sinks{assign};\n    auto model = std::make_shared<ov::Model>(ov::ResultVector{result}, ov::ParameterVector{parameter}, sinks);\n    return model;\n}\n```\n\n----------------------------------------\n\nTITLE: Enable Python API Build\nDESCRIPTION: Options to enable building the Python API for OpenVINO during the CMake configuration step. Requires libpython3-dev:armhf and python3-pip packages installed via apt-get.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_raspbian.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n -DENABLE_PYTHON=ON \\\n   -DPython3_EXECUTABLE=/usr/bin/python3.8\n```\n\n----------------------------------------\n\nTITLE: Example of Generated Conditional Compilation Header\nDESCRIPTION: This code block shows an example of the generated `conditional_compilation_gen.h` file. It contains preprocessor definitions that enable or disable specific code regions based on the statistics collected during the analysis phase. A value of `1` indicates that the code region was used and should be included, while the absence of a definition implies that the code region was unused and can be excluded.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n#pragma once\n\n#define tbb_bind_TBBbindSystemTopology 1\n#define tbb_bind_task_arena_initialize 1\n\n#define ov_opset_opset1_Parameter 1\n#define ov_opset_opset1_Constant 1\n#define ov_opset_opset1_Convolution 1\n#define ov_opset_opset1_Add 1\n#define ov_opset_opset1_Relu 1\n#define ov_opset_opset1_GroupConvolution 1\n#define ov_opset_opset3_ShapeOf 1\n#define ov_opset_opset1_Squeeze 1\n#define ov_opset_opset4_Range 1\n#define ov_opset_opset1_ReduceMean 1\n#define ov_opset_opset1_Softmax 1\n#define ov_opset_opset1_Result 1\n\n#define ov_op_v0_Parameter_visit_attributes 1\n#define ov_op_v0_Parameter_validate_and_infer_types 1\n#define ov_op_v0_Parameter_clone_with_new_inputs 1\n#define ov_op_v0_Constant_visit_attributes 1\n#define ov_op_v0_Constant_clone_with_new_inputs 1\n#define ov_op_v1_Convolution_visit_attributes 1\n#define ov_op_v1_Convolution_validate_and_infer_types 1\n#define ov_op_v1_Convolution_clone_with_new_inputs 1\n#define ov_op_v0_util_BinaryElementwiseArithmetic_visit_attributes 1\n#define ov_op_v1_Add_visit_attributes 1\n#define ov_op_v0_util_BinaryElementwiseArithmetic_validate_and_infer_types 1\n#define ov_op_v1_Add_clone_with_new_inputs 1\n#define ov_op_v0_Relu_visit_attributes 1\n#define ov_op_util_UnaryElementwiseArithmetic_validate_and_infer_types 1\n#define ov_op_v0_Relu_clone_with_new_inputs 1\n#define ov_op_v1_GroupConvolution_visit_attributes 1\n#define ov_op_v1_GroupConvolution_validate_and_infer_types 1\n#define ov_op_v1_GroupConvolution_clone_with_new_inputs 1\n#define ov_op_v3_ShapeOf_visit_attributes 1\n#define ov_op_v3_ShapeOf_validate_and_infer_types 1\n#define ov_op_v3_ShapeOf_clone_with_new_inputs 1\n#define ov_op_v0_Squeeze_visit_attributes 1\n...\n```\n\n----------------------------------------\n\nTITLE: MVN Kernel Implementation Example in C++\nDESCRIPTION: This code defines the `MVNKernelRef` class, which inherits from `MVNKernelBase`, representing a specific kernel implementation for the MVN operation. It specifies the kernel name, provides methods for retrieving kernel data and supported parameters, defines JIT constants, and lists supported fused operations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_kernels.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nclass MVNKernelRef : public MVNKernelBase {\npublic:\n    MVNKernelRef() : MVNKernelBase(\"mvn_gpu_ref\") {} // mvn_gpu_ref is the name of the file with kernel template in cl_kernels/ folder without .cl extension\n    // Returns the kernel specified for input parameters if the implementation can process it\n    KernelsData GetKernelsData(const Params& params) const override;\n    // Returns `ParamsKey` for current implementation for quick applicability check\n    ParamsKey GetSupportedKey() const override;\n\nprotected:\n    // Specifies additional jit constants for kernel template specification\n    JitConstants GetJitConstants(const mvn_params& params, DispatchData dispatchData) const override;\n    // The list of supported fused operations\n    std::vector<FusedOpType> GetSupportedFusedOps() const override {\n        return {\n            FusedOpType::ACTIVATION,\n            FusedOpType::QUANTIZE,\n            FusedOpType::ELTWISE,\n            FusedOpType::SCALE\n        };\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Create String Tensor with Initial Data (C++)\nDESCRIPTION: Creates an OpenVINO tensor using a C++ vector of strings. The tensor shares the same memory as the strings vector, avoiding data duplication. Requires the `openvino/openvino.hpp` header.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/string-tensors.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <vector>\n#include <string>\n#include <openvino/openvino.hpp>\n\nstd::vector<std::string> strings = {\"text\", \"more text\", \"even more text\"};\nov::Tensor tensor(ov::element::string, ov::Shape{strings.size()}, &strings[0]);\n```\n\n----------------------------------------\n\nTITLE: Get/Set Tensor by Port in Python\nDESCRIPTION: This Python code shows how to get and set tensors by port using `get_tensor(port)` and `set_tensor(port, tensor)`. Requires OpenVINO and a compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\ncompiled_model = ie.compile_model(model, \"CPU\")\ninfer_request = compiled_model.create_infer_request()\n\ninput_port = model.input(0)\ninput_tensor = infer_request.get_tensor(input_port)\ninput_tensor.data[:] = np.random.normal(0, 1, input_tensor.shape)\n\ninfer_request.infer()\n\noutput_port = model.output(0)\noutput_tensor = infer_request.get_tensor(output_port)\nprint(output_tensor.data)\n```\n\n----------------------------------------\n\nTITLE: Creating Library and Linking Dependencies CMake\nDESCRIPTION: This snippet defines the source files, creates the C API library, and links it to other OpenVINO libraries.  It uses `file(GLOB)` to find the source files and headers, then it creates the library using `add_library` and links against `openvino` and `openvino::util` using `target_link_libraries`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/*.h ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp)\nfile(GLOB_RECURSE HEADERS ${OpenVINO_C_API_SOURCE_DIR}/include/openvino/*.h)\n\n# create library\nadd_library(${TARGET_NAME} ${LEGACY_HEADERS} ${HEADERS} ${SOURCES})\nadd_library(openvino::runtime::c ALIAS ${TARGET_NAME})\n\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino openvino::util)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO™ using PIP with mirror source\nDESCRIPTION: This snippet shows how to install the openvino-dev package using pip and specifying a mirror source to resolve potential download issues, especially for users in China. It includes adding a trusted host parameter if the URL is http.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/troubleshooting-install-config.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install openvino-dev -i https://mirrors.aliyun.com/pypi/simple/\n```\n\n----------------------------------------\n\nTITLE: Update Dispatch Data for Shape Agnostic Kernel\nDESCRIPTION: This snippet demonstrates how to define the update_dispatch_data_func for a shape-agnostic kernel implementation. This function is called for every execution to update the global workgroup, local workgroup, and skip_execution flag based on the input parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/dynamic_shape/dynamic_impl.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nkd.update_dispatch_data_func = [this](const Params& params, KernelData& kd) {\n    const auto& prim_params = static_cast<const activation_params&>(params);\n    auto dispatchData = SetDefault(prim_params);\n    OPENVINO_ASSERT(kd.kernels.size() == 1, \"[GPU] Invalid kernels size for update dispatch data func\");\n    kd.kernels[0].params.workGroups.global = dispatchData.gws;\n    kd.kernels[0].params.workGroups.local = dispatchData.lws;\n    kd.kernels[0].skip_execution = KernelData::SkipKernelExecution(prim_params);\n};\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Mixed Pads Example - C++\nDESCRIPTION: Demonstrates padding with a mix of positive and negative padding values.  Illustrates how the different pad modes affect the output when some dimensions are cropped and others are padded.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\npads_begin = [2, -1]\npads_end = [-1, 3]\n\nDATA =\n[[1,  2,  3,  4]\n[5,  6,  7,  8]\n[9, 10, 11, 12]]\nShape(3, 4)\n```\n\n----------------------------------------\n\nTITLE: Model Inputs Retrieval C++\nDESCRIPTION: Demonstrates retrieving the inputs of a compiled model using `ov::CompiledModel::inputs`. This is crucial for understanding the model's input requirements.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/benchmark/throughput_benchmark/README.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n``ov::CompiledModel::inputs``\n```\n\n----------------------------------------\n\nTITLE: Running benchmark_app with environment variables\nDESCRIPTION: This snippet demonstrates how to run the `benchmark_app` with environment variables to enable debug options such as verbose output and dumping tensors to a specified path. This is useful for quick debugging without modifying the application code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_debug_utils.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n$ OV_VERBOSE=1 ./benchmark_app ...                       # Run benchmark_app with OV_VERBOSE option\n$ OV_GPU_DUMP_TENSORS_PATH=\"dump/\" ./benchmark_app ...   # Run benchmark_app and store intermediate buffers into dump/ directory.\n```\n\n----------------------------------------\n\nTITLE: Protopipe Help Command\nDESCRIPTION: This command executes the Protopipe executable with the `-h` flag, which prints the usage message and displays available command-line options. This is used to verify that Protopipe has been built and installed correctly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\n> protopipe.exe -h\n```\n\n----------------------------------------\n\nTITLE: Install Specific OpenVINO Runtime Version Example\nDESCRIPTION: This command installs the OpenVINO Runtime version 2025.1.0 using the YUM package manager. It requires sudo privileges to install software packages. This is an example demonstrating how to install a specific OpenVINO version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nsudo yum install openvino-2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Conditional SHL Library Setting\nDESCRIPTION: If 'ENABLE_SHL_FOR_CPU' is enabled, this sets the 'SHL_LIBRARY' variable to 'shl'. The library is later linked to the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_SHL_FOR_CPU)\n    set(SHL_LIBRARY \"shl\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Install Optimum Intel Package\nDESCRIPTION: Installs the Optimum Intel package, which provides tools for optimizing and running models with OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install optimum[openvino]\n```\n\n----------------------------------------\n\nTITLE: Profiling OpenCL Kernel Execution with cliloader (Bash)\nDESCRIPTION: This snippet demonstrates how to profile device timing for kernel execution using `cliloader` with the opencl-intercept-layer. The `CLI_DevicePerformanceTiming` environment variable is set to 1 to enable profiling.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_debug_utils.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# Profile device timing for kernel execution\n$ CLI_DevicePerformanceTiming=1 /path/to/cliloader /path/to/benchmark_app ...\n```\n\n----------------------------------------\n\nTITLE: FakeQuantize Definition\nDESCRIPTION: This snippet defines the FakeQuantize operation. It describes how the input value 'x' is mapped to an output based on its relationship to input_low, input_high, output_low, output_high, and levels. The output is determined by whether x is less than or equal to min(input_low, input_high), greater than max(input_low, input_high), or within the range (input_low, input_high].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/fake_quantize.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nif x <= min(input_low, input_high):\n    output = output_low\nelif x > max(input_low, input_high):\n    output = output_high\nelse:\n    # input_low < x <= input_high\n    output = round((x - input_low) / (input_high - input_low) * (levels-1)) / (levels-1) * (output_high - output_low) + output_low\n```\n\n----------------------------------------\n\nTITLE: Add OpenVINO Tokenizers Extension (MacOS)\nDESCRIPTION: Adds the OpenVINO Tokenizers extension to the OpenVINO Core object in C++ for MacOS, enabling the use of the tokenizers in inference pipelines.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ncore.add_extension(\"libopenvino_tokenizers.dylib\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Average Counters via Environment Variable (sh)\nDESCRIPTION: This snippet shows how to enable the collection of per-node average counters in OpenVINO by setting the `OV_CPU_AVERAGE_COUNTERS` environment variable. The `<filename>` specifies the output file for the counter data, followed by the binary that you want to analyze.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/average_counters.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_AVERAGE_COUNTERS=<filename> binary ...\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Log Level in Shell\nDESCRIPTION: Sets the `OPENVINO_LOG_LEVEL` environment variable to control the verbosity of OpenVINO logs. A lower number corresponds to less verbose logging (e.g., 0 for no logs, 1 for errors only).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection/debugging-auto-device.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nWhen defining it via the variable,\na number needs to be used instead of a log level name, e.g.:\n\nLinux\nexport OPENVINO_LOG_LEVEL=0\n\nWindows\nset OPENVINO_LOG_LEVEL=0\n```\n\n----------------------------------------\n\nTITLE: C++ Project Structure\nDESCRIPTION: This C++ snippet provides a basic project structure example.  It represents the main function and assumes that the necessary OpenVINO libraries are linked correctly. This example shows the basic structure of a C++ OpenVINO application.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_37\n\nLANGUAGE: cpp\nCODE:\n```\n//! [part7]\n#include <iostream>\n#include <openvino/openvino.hpp>\n\nint main() {\n    try {\n        // -------- Step 1. Initialize OpenVINO runtime core\n        ov::Core core;\n\n        // -------- Step 2. Read a model\n        auto model = core.read_model(\"path_to_your_model/your_model.xml\");\n\n        // -------- Step 3. Set up input\n        ov::Shape input_shape = model->input().get_shape();\n\n        // -------- Step 4. Configure preprocessing\n\n        // -------- Step 5. Compile the model\n        ov::CompiledModel compiled_model = core.compile_model(model, \"AUTO\");\n\n        // -------- Step 6. Create an inference request\n        ov::InferRequest infer_request = compiled_model.create_infer_request();\n\n        // -------- Step 7. Prepare input data\n\n        // -------- Step 8. Measure inference latency\n\n        // -------- Step 9. Process inference results\n\n        std::cout << \"Inference completed successfully.\" << std::endl;\n        return 0;\n    } catch (const std::exception& ex) {\n        std::cerr << \"An exception occurred: \" << ex.what() << std::endl;\n        return 1;\n    }\n}\n//! [part7]\n```\n\n----------------------------------------\n\nTITLE: Detecting Dynamic Output Shapes - C++\nDESCRIPTION: This C++ code snippet illustrates how to detect whether the output shape of a compiled OpenVINO model is dynamic by checking the `is_dynamic()` property of the partial shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_17\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n#include <iostream>\n\nint main() {\n    // ! [ov_dynamic_shapes:detect_dynamic]\n    ov::Core core;\n    auto model = core.read_model(\"model.xml\");\n    ov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\n\n    ov::PartialShape output_shape = compiled_model.output(0).get_partial_shape();\n\n    if (output_shape.is_dynamic())\n        std::cout << \"Output shape is dynamic\" << std::endl;\n    else\n        std::cout << \"Output shape is static\" << std::endl;\n    // ! [ov_dynamic_shapes:detect_dynamic]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: OpenVINO IR XML Example\nDESCRIPTION: This code snippet demonstrates the structure of an OpenVINO Intermediate Representation (IR) XML file. It defines the model's layers, including Parameter, Const, Convolution, ReLU, and Result layers, along with their attributes, inputs, and outputs. It also shows the connections between the layers through edges, describing the data flow.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<?xml version=\"1.0\" ?>\n<net name=\"model_file_name\" version=\"10\">\n   <layers>\n      <layer id=\"0\" name=\"input\" type=\"Parameter\" version=\"opset1\">\n            <data element_type=\"f32\" shape=\"1,3,32,100\"/> <!-- attributes of operation -->\n            <output>\n               <!-- description of output ports with type of element and tensor dimensions -->\n               <port id=\"0\" precision=\"FP32\">\n                  <dim>1</dim>\n                  <dim>3</dim>\n                  <dim>32</dim>\n                  <dim>100</dim>\n               </port>\n            </output>\n      </layer>\n      <layer id=\"1\" name=\"conv1/weights\" type=\"Const\" version=\"opset1\">\n            <!-- Const is only operation from opset1 that refers to the IR binary file by specifying offset and size in bytes relative to the beginning of the file. -->\n            <data element_type=\"f32\" offset=\"0\" shape=\"64,3,3,3\" size=\"6912\"/>\n            <output>\n               <port id=\"1\" precision=\"FP32\">\n                  <dim>64</dim>\n                  <dim>3</dim>\n                  <dim>3</dim>\n                  <dim>3</dim>\n               </port>\n            </output>\n      </layer>\n      <layer id=\"2\" name=\"conv1\" type=\"Convolution\" version=\"opset1\">\n            <data auto_pad=\"same_upper\" dilations=\"1,1\" output_padding=\"0,0\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"1,1\"/>\n            <input>\n               <port id=\"0\">\n                  <dim>1</dim>\n                  <dim>3</dim>\n                  <dim>32</dim>\n                  <dim>100</dim>\n               </port>\n               <port id=\"1\">\n                  <dim>64</dim>\n                  <dim>3</dim>\n                  <dim>3</dim>\n                  <dim>3</dim>\n               </port>\n            </input>\n            <output>\n               <port id=\"2\" precision=\"FP32\">\n                  <dim>1</dim>\n                  <dim>64</dim>\n                  <dim>32</dim>\n                  <dim>100</dim>\n               </port>\n            </output>\n      </layer>\n      <layer id=\"3\" name=\"conv1/activation\" type=\"ReLU\" version=\"opset1\">\n            <input>\n               <port id=\"0\">\n                  <dim>1</dim>\n                  <dim>64</dim>\n                  <dim>32</dim>\n                  <dim>100</dim>\n               </port>\n            </input>\n            <output>\n               <port id=\"1\" precision=\"FP32\">\n                  <dim>1</dim>\n                  <dim>64</dim>\n                  <dim>32</dim>\n                  <dim>100</dim>\n               </port>\n            </output>\n      </layer>\n      <layer id=\"4\" name=\"output\" type=\"Result\" version=\"opset1\">\n            <input>\n               <port id=\"0\">\n                  <dim>1</dim>\n                  <dim>64</dim>\n                  <dim>32</dim>\n                  <dim>100</dim>\n               </port>\n            </input>\n      </layer>\n   </layers>\n   <edges>\n      <!-- Connections between layer nodes: based on ids for layers and ports used in the descriptions above -->\n      <edge from-layer=\"0\" from-port=\"0\" to-layer=\"2\" to-port=\"0\"/>\n      <edge from-layer=\"1\" from-port=\"1\" to-layer=\"2\" to-port=\"1\"/>\n      <edge from-layer=\"2\" from-port=\"2\" to-layer=\"3\" to-port=\"0\"/>\n      <edge from-layer=\"3\" from-port=\"1\" to-layer=\"4\" to-port=\"0\"/>\n   </edges>\n   <meta_data>\n      <!-- This section that is not related to a topology; contains auxiliary information that serves for the debugging purposes. -->\n      <MO_version value=\"2022.3\"/>\n      <cli_parameters>\n            <blobs_as_inputs value=\"True\"/>\n            <caffe_parser_path value=\"DIR\"/>\n            <data_type value=\"float\"/>\n\n            ...\n\n            <!-- Omitted a long list of CLI options that always are put here by MO for debugging purposes. -->\n\n      </cli_parameters>\n   </meta_data>\n</net>\n```\n\n----------------------------------------\n\nTITLE: Setting Log Level Programmatically (Python)\nDESCRIPTION: This Python code snippet demonstrates how to set the OpenVINO log level using the `ov.log.Level` enum. This allows for controlling the verbosity of OpenVINO's debug messages. The `compile_model` method or `set_property` method can be used to overwrite the environment variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection/debugging-auto-device.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#placeholder to enable tabset in generated file, do not remove\n# [part6]:\nimport openvino.runtime as ov\n\ncore = ov.Core()\ncore.set_property({'LOG_LEVEL': 'LOG_DEBUG'})\ncore.compile_model(\"path_to_model.xml\", \"CPU\")\n\ncore.set_property(\"CPU\", {'LOG_LEVEL': 'LOG_DEBUG'})\n```\n\n----------------------------------------\n\nTITLE: MulticlassNonMaxSuppression Layer Configuration in OpenVINO\nDESCRIPTION: This code snippet demonstrates the configuration of a MulticlassNonMaxSuppression layer within an OpenVINO model. It shows the layer definition, input and output port specifications, and data attributes, including the sort result order and output type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/multiclass-non-max-suppression-8.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"MulticlassNonMaxSuppression\" ... >\n    <data sort_result=\"score\" output_type=\"i64\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>100</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>\n            <dim>5</dim>\n            <dim>100</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"5\" precision=\"FP32\">\n            <dim>-1</dim> <!-- \"-1\" means a undefined dimension calculated during the model inference -->\n            <dim>6</dim>\n        </port>\n        <port id=\"6\" precision=\"I64\">\n            <dim>-1</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"7\" precision=\"I64\">\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO Runtime via Conda\nDESCRIPTION: This command installs the OpenVINO Runtime package from the `conda-forge` channel with a specific version (2025.1.0).  The `conda-forge` channel provides community-maintained packages.  Specifying the version ensures a consistent and reproducible installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conda.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nconda install -c conda-forge openvino=2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Convert OCR Model with Ordered Input Shapes in Python\nDESCRIPTION: This Python code snippet converts an ONNX OCR model using `openvino.convert_model`, specifying shapes for two inputs based on their order in the model.  It imports the `openvino` library and then calls `ov.convert_model` with the model file name and the input shapes as a tuple of lists.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/setting-input-shapes.rst#_snippet_4\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model(\"ocr.onnx\", input=([3,150,200,1], [3]))\n```\n\n----------------------------------------\n\nTITLE: Convert OCR Model with Dynamic Input Shapes in Python\nDESCRIPTION: This Python code snippet demonstrates how to convert an ONNX OCR model using `openvino.convert_model` and specify dynamic batch dimension for inputs `data` and `seq_len`, using `-1` to indicate a dynamic dimension. It imports the `openvino` library and then calls `ov.convert_model` with the model file name and the input shapes as a list of tuples.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/setting-input-shapes.rst#_snippet_6\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model(\"ocr.onnx\", input=[(\"data\", [-1, 150, 200, 1]), (\"seq_len\", [-1])])\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for x86_64 build\nDESCRIPTION: This command configures the CMake build for x86_64 architecture on Apple Silicon. It adds `-DCMAKE_OSX_ARCHITECTURES=x86_64` and removes `-DENABLE_SYSTEM_*` options to prevent linking against arm64 system libraries when building x86_64 binaries for Rosetta.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\ncmake -G \"Ninja Multi-Config\" -DCMAKE_OSX_ARCHITECTURES=x86_64 ..\n```\n\n----------------------------------------\n\nTITLE: Stable TopK Output\nDESCRIPTION: Shows the single correct output when the stable attribute is set to true, ensuring a deterministic result.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/top-k-11.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\noutput_values  = [5, 3, 1, 2]\noutput_indices = [0, 1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO with CMake for Xuantie toolchain\nDESCRIPTION: This snippet configures the OpenVINO build using CMake for a RISC-V target with vectorized primitives using the Xuantie GNU toolchain. It specifies the build type, installation prefix, toolchain file, and the root directory of the toolchain.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ncmake .. \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DCMAKE_INSTALL_PREFIX=<openvino_install_path> \\\n  -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/<toolchain_file> \\\n  -DRISCV_TOOLCHAIN_ROOT=<xuantie_install_path>\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenVINO Conda Environment\nDESCRIPTION: This code snippet creates a Conda environment named 'openvino_env' with Python 3.9, activates the environment, installs the ipykernel package, and sets the PATH variable to include the environment's binaries.  This ensures that the OpenVINO environment is properly configured for running notebooks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name openvino_env python=3.9\nconda activate openvino_env\nconda install ipykernel\nset PATH=\"/anaconda/envs/openvino_env/bin;%PATH%\"\n```\n\n----------------------------------------\n\nTITLE: BatchNormInference with 4D Input Tensor XML\nDESCRIPTION: Illustrates the XML structure for a BatchNormInference layer with a 4D input tensor in OpenVINO. The configuration includes the epsilon attribute and specifies the dimensions for the input and output ports, and the dimensions of gamma, beta, mean and variance inputs. The epsilon attribute contributes to avoiding division by zero.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/batch-norm-inference-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"BatchNormInference\" ...>\n    <data epsilon=\"9.99e-06\" />\n    <input>\n        <port id=\"0\">  <!-- input -->\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">  <!-- gamma -->\n            <dim>3</dim>\n        </port>\n        <port id=\"2\">  <!-- beta -->\n            <dim>3</dim>\n        </port>\n        <port id=\"3\">  <!-- mean -->\n            <dim>3</dim>\n        </port>\n        <port id=\"4\">  <!-- variance -->\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"5\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining OpenVINO CompiledModel Interface (TypeScript)\nDESCRIPTION: This snippet defines the `CompiledModel` interface in TypeScript, which represents a compiled model in OpenVINO. It includes properties for accessing input and output tensors, as well as methods for getting properties, creating inference requests, exporting the model, setting properties and accessing inputs/outputs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface CompiledModel {\n    inputs: Output[];\n    outputs: Output[];\n    getProperty(propertyName): string | number | boolean;\n    createInferRequest(): InferRequest;\n    exportModelSync(): Buffer;\n    input(): Output;\n    input(index): Output;\n    input(name): Output;\n    output(): Output;\n    output(index): Output;\n    output(name): Output;\n    setProperty(properties: Record<string, OVAny>): void;\n}\n```\n\n----------------------------------------\n\nTITLE: Enable Verbose Mode\nDESCRIPTION: This snippet shows how to enable verbose mode for the OpenVINO CPU plugin using the `OV_CPU_VERBOSE` environment variable. The level specifies the amount of detail printed, with higher levels including more information.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/verbose.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_VERBOSE=<level> binary ...\n```\n\n----------------------------------------\n\nTITLE: Set Inference Precision - C++\nDESCRIPTION: This C++ code snippet demonstrates how to explicitly set the inference precision to `ov::element::f32` when compiling a model for CPU. This forces the CPU plugin to use FP32 precision even if the hardware supports half-precision. Dependency: OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n// [part2]\nauto model = core.read_model(\"model.xml\");\nauto compiled_model = core.compile_model(model, \"CPU\", ov::hint::inference_precision(ov::element::f32));\n\nauto inference_precision = compiled_model.get_property(ov::hint::inference_precision);\nstd::cout << \"Inference precision: \" << inference_precision.as<ov::element::Type>() << \"\\n\";\n// [part2]\n```\n\n----------------------------------------\n\nTITLE: Building ONNX Runtime with OpenVINO Support\nDESCRIPTION: This command builds ONNX Runtime with OpenVINO support enabled. It uses the 'build.sh' script with the configuration set to 'RelWithDebInfo', enables OpenVINO with CPU_FP32 precision, builds a shared library, and utilizes parallel compilation. Replace <ONNX_RUNTIME_REPO_DIR> with the path to the cloned ONNX Runtime repository.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\n<ONNX_RUNTIME_REPO_DIR>/build.sh --config RelWithDebInfo --use_openvino CPU_FP32 --build_shared_lib --parallel\n```\n\n----------------------------------------\n\nTITLE: Compile Model for NPU\nDESCRIPTION: This is an example command to compile a model for inference on an Intel Core Ultra NPU using the compile_tool. It specifies the path to the XML model file and the target device (NPU.3720).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/compile_tool/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./compile_tool -m <path_to_model>/model_name.xml -d NPU.3720\n```\n\n----------------------------------------\n\nTITLE: FakeQuantize Calculation\nDESCRIPTION: This Python code snippet demonstrates the core calculation performed by the FakeQuantize operation. It shows how the input value 'x' is clipped based on 'input_low' and 'input_high' values, and then quantized to the output range specified by 'output_low' and 'output_high' using the number of quantization 'levels'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/quantization/fake-quantize-1.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nif x <= min(input_low, input_high):\n    output = output_low\nelif x > max(input_low, input_high):\n    output = output_high\nelse:\n    # input_low < x <= input_high\n    output = round((x - input_low) / (input_high - input_low) * (levels-1)) / (levels-1) * (output_high - output_low) + output_low\n```\n\n----------------------------------------\n\nTITLE: Uninstall specific OpenVINO Runtime version\nDESCRIPTION: Uninstalls a specific version of the OpenVINO Runtime using the APT package manager. Uses `autoremove` to remove dependencies that are no longer needed. Requires the exact version number to be specified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt autoremove openvino-<VERSION>.<UPDATE>.<PATCH>\n```\n\n----------------------------------------\n\nTITLE: SliceScatter Example 2: Update every second value\nDESCRIPTION: This XML code demonstrates an example of using SliceScatter to update every second value over axis 1. The data tensor has shape (2, 5), the updates tensor has shape (2, 3), and the slice is defined by start=-25 (clamped to 0), stop=25 (clamped to 5), step=2, and axis=1. The start and stop parameters are clamped to the valid range of the dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-scatter-15.rst#_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"SliceScatter\">\n    <input>\n        <port id=\"0\" precision=\"FP32\">  <!-- data -->\n            <dim>2</dim>\n            <dim>5</dim>  <!-- values: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]] -->\n        </port>\n        <port id=\"1\" precision=\"FP32\">  <!-- updates -->\n            <dim>2</dim>\n            <dim>3</dim>  <!-- values: [[10, 20, 30], [40, 50, 60]] -->\n        </port>\n        <port id=\"2\" precision=\"I32\">  <!-- start -->\n            <dim>1</dim>  <!-- values: [-25], silently clamped to 0 -->\n        </port>\n        <port id=\"3\" precision=\"I32\">  <!-- stop -->\n            <dim>1</dim>  <!-- values: [25], silently clamped to 5 -->\n        </port>\n        <port id=\"4\" precision=\"I32\">  <!-- step -->\n            <dim>1</dim>  <!-- values: [2] -->\n        </port>\n        <port id=\"5\" precision=\"I32\">  <!-- axes -->\n            <dim>1</dim>  <!-- values: [1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"6\" precision=\"FP32\">\n            <dim>2</dim>\n            <dim>5</dim>  <!-- values: [[10, 1, 20, 3, 30], [40, 6, 50, 8, 60]] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Background Removal Script Usage in Node.js\nDESCRIPTION: This command provides the syntax for running the `vision_background_removal.js` script, including placeholders for the model file path, foreground image path, background image path, and the target device. The device can be specified as AUTO or a specific hardware device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/js/node/vision_background_removal/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnode vision_background_removal.js *path_to_model_file* *path_to_foreground_image* *path_to_background_image* *device*\n```\n\n----------------------------------------\n\nTITLE: Running benchmark_app with Latency Hint on AUTO plugin\nDESCRIPTION: This snippet demonstrates how to run the `benchmark_app` with the `-hint latency` option, targeting the AUTO plugin.  It uses a sample model file (`add_abc.xml`) and specifies the target device as `AUTO`. This configuration aims to optimize for minimal latency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/tests.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nopenvino/bin/intel64/Release$ ./benchark_app -m openvino/src/core/tests/models/ir/add_abc.xml -d AUTO -hint latency\n```\n\n----------------------------------------\n\nTITLE: Benchmarking ASL Model with OpenVINO (sh)\nDESCRIPTION: This command benchmarks the 'asl-recognition-0004' model using the OpenVINO benchmark_app. It specifies the model path, the CPU as the target device, and the input shape. The '-data_shape' argument defines a sequence of shapes to use for the input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_21\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app -m omz_models/intel/asl-recognition-0004/FP16/asl-recognition-0004.xml -d CPU -shape [-1,3,16,224,224] -data_shape [1,3,16,224,224][2,3,16,224,224][4,3,16,224,224] -pcseq\n```\n\n----------------------------------------\n\nTITLE: MaxPool with 4D input, 2D kernel, same_lower padding (sh)\nDESCRIPTION: Illustrates MaxPool operation on a 4D input tensor with a 2D kernel using 'same_lower' auto padding. The example provides the input tensor, strides, kernel size, rounding type, auto padding, and the resulting output tensor and indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-8.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[-1, 2, 3],\n               [4, 5, -6],\n               [-7, 8, 9]]]]\nstrides = [1, 1]\nkernel = [2, 2]\nrounding_type = \"floor\"\nauto_pad = \"same_lower\"\noutput0 = [[[[-1, 2, 3],\n                  [4, 5, 5]\n                  [4, 8, 9]]]]\noutput1 = [[[[0, 1, 2],\n                  [3, 4, 4]\n                  [3, 7, 8]]]]\n```\n\n----------------------------------------\n\nTITLE: Add Custom Command for fdupes Check - CMake\nDESCRIPTION: This CMake code adds a custom command (if not on Windows) to perform 'fdupes' checks on the built wheel. It defines the command, dependencies, working directory, and a comment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT CMAKE_HOST_WIN32)\n    set(fdupes_report ${CMAKE_CURRENT_BINARY_DIR}/fdupes_report.txt)\n    add_custom_command(OUTPUT \"${fdupes_report}\"\n        COMMAND ${CMAKE_COMMAND}\n            -D Python3_EXECUTABLE=${Python3_EXECUTABLE}\n            -D WORKING_DIRECTORY=${CMAKE_CURRENT_BINARY_DIR}\n            -D WHEEL_VERSION=${WHEEL_VERSION}\n            -D PACKAGE_FILE=${openvino_wheel_path}\n            -D REPORT_FILE=${fdupes_report}\n            -D CMAKE_SHARED_LIBRARY_SUFFIX=${CMAKE_SHARED_LIBRARY_SUFFIX}\n            -P \"${CMAKE_CURRENT_SOURCE_DIR}/fdupes_check.cmake\"\n        DEPENDS \"${openvino_wheel_path}\"\n        WORKING_DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}\"\n        COMMENT \"Run 'fdupes' checks for wheel ${openvino_wheel_name}\"\n        VERBATIM)\n    list(APPEND ie_wheel_deps ${fdupes_report})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Multiply Dimensions of Shape C++\nDESCRIPTION: This code snippet demonstrates how to multiply the first two dimensions of a shape using overloaded mathematical operators for the `ov::Dimension` class.\nIt retrieves the first two dimensions of a `ov::PartialShape` and multiplies them to get the product.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/docs/shape_propagation.md#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nov::PartialShape shape = ...;\nov::Rank = rank = shape.rank();\nif (rank.is_static() && rank.get_length() > 2)\n    ov::Dimension product_of_first_and_second_dims = shape[0] * shape[1];\n```\n\n----------------------------------------\n\nTITLE: Standalone Build: Build Single Image Test Tool\nDESCRIPTION: This snippet details the steps to build the Single Image Test Tool as a standalone application. It requires OpenVINO Runtime and OpenCV, and uses CMake to configure and build the project.  It specifies the OpenVINO and OpenCV directories during the cmake configuration step, and performs the build and installation steps. The variables `<openvino_install_dir>`, `<opencv_install_dir>`, and `<sit_source_dir>` need to be defined with the actual paths.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmkdir sit_build && cd sit_build\nsource <openvino_install_dir>/setupvars.sh\ncmake -DOpenVINO_DIR=<openvino_install_dir>/runtime/cmake -DOpenCV_DIR=<opencv_install_dir> <sit_source_dir>\ncmake --build . --config Release\ncmake --install . --prefix <sit_install_dir>\n```\n\n----------------------------------------\n\nTITLE: Build OpenVINO with Selective Build (ON)\nDESCRIPTION: This snippet shows how to rebuild OpenVINO with the `SELECTIVE_BUILD=ON` option, enabling the exclusion of inactive code regions based on the statistics collected earlier. The `SELECTIVE_BUILD_STAT` option specifies the location of the CSV statistics files.  Profiling is turned off by setting `DENABLE_PROFILING_ITT=OFF`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DSELECTIVE_BUILD=ON -DSELECTIVE_BUILD_STAT=${ABSOLUTE_PATH_TO_STATISTICS_FILES}/*.csv -DENABLE_PROFILING_ITT=OFF ..\ncmake --build .\n```\n\n----------------------------------------\n\nTITLE: Check Codestyle with flake8 Python\nDESCRIPTION: Checks the code style of the OpenVINO™ Python API using `flake8`. This command runs `flake8` on the specified source directory (`./src/openvino/`) using a custom configuration file (`setup.cfg`).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npython -m flake8 ./src/openvino/ --config=setup.cfg\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories (CMake)\nDESCRIPTION: This snippet sets the include directories for the `openvino_tensorflow_common` library. The `include` directory under the root directory is added as a public build interface, and the `src` directory under the root directory is added as a private include directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/src/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}\n    PUBLIC $<BUILD_INTERFACE:${root_dir}/include>\n    PRIVATE ${root_dir}/src)\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Python Package\nDESCRIPTION: This command installs the OpenVINO Python package, a prerequisite for running the BERT benchmark sample. This provides access to the OpenVINO runtime environment and associated APIs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/bert-benchmark.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npython -m pip install openvino\n```\n\n----------------------------------------\n\nTITLE: Example debug configuration in JSON format\nDESCRIPTION: This JSON snippet provides an example of a configuration file that can be used to set debug options for the GPU plugin. It demonstrates setting verbose output and performance counter options for a specific GPU device. Options set via environment variables have higher priority.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_debug_utils.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"GPU.1\":{\"OV_VERBOSE\":\"ON\",\"PERF_COUNT\":\"ON\"}}\n```\n\n----------------------------------------\n\nTITLE: Install Latest OpenVINO Runtime\nDESCRIPTION: This command installs the latest version of the OpenVINO Runtime using the YUM package manager. It requires sudo privileges to install software packages. This is the primary command for installing the OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nsudo yum install openvino\n```\n\n----------------------------------------\n\nTITLE: Getting NPU Plugin Properties with OpenVINO\nDESCRIPTION: This code snippet demonstrates how to retrieve properties of the NPU plugin at both the core and compiled model levels using OpenVINO's `get_property` method. The `plugin_properties` variable retrieves properties at the core level using the plugin name \"NPU\", while `model_properties` retrieves properties specific to a compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/README.md#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nplugin_properties = ov.get_property(\"NPU\", <property_name>);\n[...]\nmodel_properties = compiled_model.get_property(<property_name>);\n```\n\n----------------------------------------\n\nTITLE: Creating C++ Tensor from C shape\nDESCRIPTION: This C code snippet demonstrates how C shape data is converted to C++ objects before calling C++ interfaces, in this case, for creating a Tensor. The C shape is converted to the corresponding C++ class before the C++ interface is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/how_to_wrap_openvino_interfaces_with_c.md#_snippet_4\n\nLANGUAGE: C\nCODE:\n```\nhttps://github.com/openvinotoolkit/openvino/blob/d96c25844d6cfd5ad131539c8a0928266127b05a/src/bindings/c/src/ov_tensor.cpp#L41-L55\n```\n\n----------------------------------------\n\nTITLE: Unpacking Data from Low Precision Tensors in OpenVINO Python\nDESCRIPTION: This snippet demonstrates how to extract low-precision values from a tensor into a NumPy array using a helper function in the OpenVINO Python API. This is necessary to properly interpret the data when working with u1, u4, and i4 element types.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\n# Assuming the output tensor contains packed u4 data with shape (1, 1, 2, 2)\n# and the original shape of the data was (1, 1, 4, 4)\n\n# Create a dummy tensor (replace with your actual tensor)\npacked_data = np.array([[[[0b11001010, 0b01110001],[0b10101100, 0b00010111]]]], dtype=np.uint8)\ntensor = ov.Tensor(packed_data)\n\n# Unpack the data. Note: This is a simplified example and may need adjustments\n# based on the exact packing format.\nunpacked_data = np.unpackbits(tensor.data, axis=None).reshape((1, 1, 4, 4))\n\nprint(unpacked_data)\n```\n\n----------------------------------------\n\nTITLE: Configuring Individual Devices with Auto-Device plugin in C++\nDESCRIPTION: This C++ code snippet shows how to configure individual devices and create the Auto-Device plugin. This is implemented in the section `[part5]` of `docs/articles_en/assets/snippets/AUTO5.cpp`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\ndocs/articles_en/assets/snippets/AUTO5.cpp\n```\n\n----------------------------------------\n\nTITLE: Using Optional as the top node in a pattern\nDESCRIPTION: This Python snippet shows how to use Optional as the top node of a pattern, meaning the entire pattern may or may not exist in the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.tools.ovc.composite.pattern import WrapType, Optional\n\ndef pattern_optional_top():\n    # Creating nodes\n    relu_node = WrapType(\"opset13.Relu\")\n    optional_node = Optional(relu_node)\n\n    return optional_node\n```\n\n----------------------------------------\n\nTITLE: Tensor Operations C++\nDESCRIPTION: Gets the shape of a tensor using the OpenVINO API. Knowing the tensor shape is crucial for properly formatting input data and interpreting the output results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_classification/README.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n``ov::Tensor::get_shape``\n```\n\n----------------------------------------\n\nTITLE: Convert flax.linen.Module to OpenVINO (Python)\nDESCRIPTION: This example illustrates how to convert a `flax.linen.Module` to an OpenVINO model using `ov.convert_model`. It requires `flax.linen`, `jax`, `jax.numpy`, and `openvino`. The `example_input` parameter is necessary for tracing the model during conversion.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-jax.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nimport openvino as ov\n\n# let user have some Flax module\nclass SimpleModule(nn.Module):\n    features: int\n\n    @nn.compact\n    def __call__(self, x):\n        return nn.Dense(features=self.features)(x)\n\nmodule = SimpleModule(features=4)\n\n# create example_input used for training\nexample_input = jnp.ones((2, 3))\n\n# prepare parameters to initialize the module\n# they can be also loaded using pickle, flax.serialization\nkey = jax.random.PRNGKey(0)\nparams = module.init(key, example_input)\nmodule = module.bind(params)\n\nov_model = ov.convert_model(module, example_input=example_input)\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Specific OpenVINO Components\nDESCRIPTION: This command uninstalls a specific OpenVINO component from the `conda-forge` channel. Replacing `<component_name>` with the name of the component will remove that specific component from your environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conda.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nconda remove conda-forge::<component_name>\n```\n\n----------------------------------------\n\nTITLE: Tensor Layout Example: bfyx\nDESCRIPTION: Demonstrates the memory layout of a tensor with the 'bfyx' format, showing how elements are arranged in a linear memory buffer. The example uses a tensor with dimensions [b: 2; f: 2; y: 2; x: 2] and illustrates the mapping of tensor coordinates to memory indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_memory_formats.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ni = 0  => [b=0; f=0; y=0; x=0];\ni = 1  => [b=0; f=0; y=0; x=1];\n\ni = 2  => [b=0; f=0; y=1; x=0];\ni = 3  => [b=0; f=0; y=1; x=1];\n\ni = 4  => [b=0; f=1; y=0; x=0];\ni = 5  => [b=0; f=1; y=0; x=1];\n\ni = 6  => [b=0; f=1; y=1; x=0];\ni = 7  => [b=0; f=1; y=1; x=1];\n\ni = 8  => [b=1; f=0; y=0; x=0];\ni = 9  => [b=1; f=0; y=0; x=1];\n\ni = 10 => [b=1; f=0; y=1; x=0];\ni = 11 => [b=1; f=0; y=1; x=1];\n\ni = 12 => [b=1; f=1; y=0; x=0];\ni = 13 => [b=1; f=1; y=0; x=1];\n\ni = 14 => [b=1; f=1; y=1; x=0];\ni = 15 => [b=1; f=1; y=1; x=1];\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration with Selective Build (Linux)\nDESCRIPTION: This snippet shows how to configure the OpenVINO build using cmake with the `SELECTIVE_BUILD` option enabled. It specifies the path to the statistics CSV files and disables ITT profiling.  The second line builds the configured cmake project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DSELECTIVE_BUILD=ON -DSELECTIVE_BUILD_STAT=${ABSOLUTE_PATH_TO_STATISTICS_FILES}/*.csv -DENABLE_PROFILING_ITT=OFF ..\ncmake --build .\n```\n\n----------------------------------------\n\nTITLE: IsInf XML Layer Definition in OpenVINO\nDESCRIPTION: This XML snippet defines an IsInf layer in OpenVINO. It configures the layer to detect both positive and negative infinities, takes an FP32 tensor as input, and produces a boolean tensor as output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/isinf-10.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IsInf\" ...>\n    <data detect_negative=\"true\" detect_positive=\"true\"/>\n    <input>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>256</dim>\n            <dim>128</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"BOOL\">\n            <dim>256</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: BitwiseAnd output for uint8 tensor in Python\nDESCRIPTION: Shows the BitwiseAnd operation for uint8 tensors. It details the bitwise AND process, starting with the uint8 values, converting them to binary, performing the AND operation on the binary representations, and then converting the binary result back to uint8. The example uses `a` and `b` as input lists of uint8 values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-and-13.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# For given uint8 inputs:\na = [21, 120]\nb = [3, 37]\n# Create a binary representation of uint8:\n# binary a: [00010101, 01111000]\n# binary b: [00000011, 00100101]\n# Perform bitwise AND of corresponding elements in a and b:\n# [00000001, 00100000]\n# Convert binary values to uint8:\noutput = [1, 32]\n```\n\n----------------------------------------\n\nTITLE: Run Docker Image (CPU Only - Additional Memory)\nDESCRIPTION: Runs the Docker image allocating 8GB of shared memory for model training notebooks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_27\n\nLANGUAGE: console\nCODE:\n```\ndocker run -it -p 8888:8888 --shm-size 8G openvino_notebooks\n```\n\n----------------------------------------\n\nTITLE: Compiling Model Asynchronously (Model Path)\nDESCRIPTION: Asynchronously reads a model and creates a compiled model from a file path. This can be more efficient than reading the model separately and then compiling it. The config parameter allows specifying properties relevant only for this load operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ncompileModel(modelPath, deviceName, config?): Promise<CompiledModel>\n```\n\n----------------------------------------\n\nTITLE: ReLU Fusion Example (C++)\nDESCRIPTION: This code snippet demonstrates fusing a sequence of operations using the `register_new_node` method in MatcherPass.  It shows how to replace a sub-graph with a fused operation, and register the new node for further pattern matching.  The newly created nodes should be registered in topological order for proper processing by the GraphRewrite.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/matcher-pass.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n// [matcher_pass:relu_fusion]\n#include <openvino/core/rt_info.hpp>\n#include <openvino/opsets/opset10.hpp>\n#include <openvino/pass/matcher_pass.hpp>\n\n#include \"openvino/pass/graph_rewrite.hpp\"\n\nusing namespace std;\nusing namespace ov::opset10;\n\nnamespace {\nclass ReluFusion final : public ov::pass::MatcherPass {\npublic:\n    ReluFusion() {\n        MATCHER_SCOPE(ReluFusion);\n        // Pattern\n        auto input = ov::pattern::any_input();\n        auto relu = ov::pattern::wrap_type<Relu>({input});\n\n        // Matcher\n        ov::matcher_pass_callback callback = [=](ov::pass::Matcher& m) {\n            const auto& pattern_to_output = m.get_pattern_value_map();\n            auto relu_node = std::dynamic_pointer_cast<Relu>(pattern_to_output.at(relu).get_node_shared_ptr());\n            if (!relu_node) {\n                return false;\n            }\n            auto parent = relu_node->input_value(0).get_node_shared_ptr();\n            if (ov::is_type<Sigmoid>(parent)) {\n                return false;\n            }\n\n            // register new node\n            auto sigmoid = make_shared<Sigmoid>(relu_node->input_value(0));\n            sigmoid->set_friendly_name(relu_node->get_friendly_name());\n            ov::copy_runtime_info(relu_node, sigmoid);\n            replace_node(relu_node, sigmoid);\n            register_new_node(sigmoid);\n            return true;\n        };\n\n        auto m = std::make_shared<ov::pass::Matcher>(relu, \"ReluFusion\");\n        register_matcher(m, callback);\n    }\n};\n}  // namespace\n// [matcher_pass:relu_fusion]\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target in CMake\nDESCRIPTION: This code snippet adds a clang-format target that runs clang-format on the source code. The `ov_add_clang_format_target` macro likely comes from a custom CMake module within the OpenVINO project. It ensures consistent code formatting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/offline_transformations/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: BitwiseNot output for uint8 tensor in Python\nDESCRIPTION: Illustrates the BitwiseNot operation on a uint8 tensor. It converts the input integers to their binary representation, performs bitwise negation on each bit, and converts the result back to uint8. The input is an array of uint8 integers, and the output is the bitwise negation of each element.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-not-13.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# For given uint8 input:\ninput = [1, 3]\n# Create a binary representation of uint8:\n# [00000001, 00000011]\n# Perform bitwise negation:\n# [11111110, 11111100]\n# Convert back binary values to uint8:\noutput = [254, 252]\n```\n\n----------------------------------------\n\nTITLE: Example PyTorch Model (Python)\nDESCRIPTION: This snippet defines a simple PyTorch model with a custom `some_work` function that introduces a random operation. It showcases the creation of a model that can be used to demonstrate and debug accuracy differences between PyTorch and OpenVINO due to the `torch.randn_like` operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/pytorch/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass example_model(torch.nn.Module):\n    def some_work(self, x):\n        return torch.randn_like(x)\n\n    def forward(self, x):\n        y = x * x\n        z = self.some_work(x)\n        res = x + y + z\n        return res\n```\n\n----------------------------------------\n\nTITLE: Allocate USM device memory (C)\nDESCRIPTION: This C snippet demonstrates how to allocate Unified Shared Memory (USM) on the device using the OpenVINO GPU plugin. This allocates memory that resides on the GPU and is directly accessible by the GPU kernels. Requires OpenCL and OpenVINO libraries. This memory can be used for efficient GPU-based computations within the OpenVINO framework.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_26\n\nLANGUAGE: c\nCODE:\n```\n// example usage\n{\n    size_t buffer_size;\n    auto remote_blob = context.create_tensor(ov::element::f32, ov::Shape{buffer_size}, ov::intel_gpu::memory_type::usm_device);\n}\n```\n\n----------------------------------------\n\nTITLE: Enable Link Time Optimization (LTO)\nDESCRIPTION: This snippet enables Link Time Optimization (LTO) for the release build.  It sets the `INTERPROCEDURAL_OPTIMIZATION_RELEASE` property of the target to the value of `ENABLE_LTO`.  LTO can improve performance by optimizing across multiple source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Inverse Layer Definition (5D input)\nDESCRIPTION: This XML code defines an Inverse layer with a 5D input tensor, including three batch dimensions (5x4x3). The input and output ports are specified with precision FP32 and dimensions representing the batch sizes, rows, and columns of the square matrices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/inverse-14.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... name=\"Inverse\" type=\"Inverse\">\n    <data/>\n    <input>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>5</dim> <!-- batch size of 5 -->\n            <dim>4</dim> <!-- batch size of 4 -->\n            <dim>3</dim> <!-- batch size of 3 -->\n            <dim>2</dim> <!-- 2 rows of square matrix -->\n            <dim>2</dim> <!-- 2 columns of square matrix -->\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"FP32\" names=\"Inverse:0\">\n            <dim>5</dim> <!-- batch size of 5 -->\n            <dim>4</dim> <!-- batch size of 4 -->\n            <dim>3</dim> <!-- batch size of 3 -->\n            <dim>2</dim> <!-- 2 rows of square matrix -->\n            <dim>2</dim> <!-- 2 columns of square matrix -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: KeyType Hash and Equality Operator C++\nDESCRIPTION: This code defines the structure `KeyType` which is used as a key for the cache. It includes a `hash()` method to calculate the hash value of the key and an `operator==` to compare two keys for equality. These methods are crucial for efficient lookup and comparison within the cache.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/runtime_parameters_cache.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstruct KeyType {\n    size_t hash() const;\n    bool operator== () const;\n};\n```\n\n----------------------------------------\n\nTITLE: Emulate CPU SKU with SDE\nDESCRIPTION: This snippet uses Intel's SDE (Software Development Emulator) to emulate different CPU SKUs while running the `benchmark_app`. This allows collecting statistics for multiple SKUs without needing to run the application on each physical SKU.`${OPENVINO_LIBRARY_DIR}` should point to the location of OpenVINO libraries, `${MY_MODEL_RESULT}` is the directory to store the results, and `${MY_MODEL}.xml` is the model file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nfor cpu in spr adl tgl icl skl; do\n    python ../thirdparty/itt_collector/runtool/sea_runtool.py --bindir ${OPENVINO_LIBRARY_DIR} -o ${MY_MODEL_RESULT} ! sde -$cpu -- ./benchmark_app -niter 1 -nireq 1 -m ${MY_MODEL}.xml\ndone\n```\n\n----------------------------------------\n\nTITLE: Simple Pattern Example (C++)\nDESCRIPTION: This code snippet illustrates a simple pattern creation using OpenVINO opset operations. It shows how to create a `Parameter` operation with specified type and shape, and then use it as the root of the pattern.  The type and shape specified are only needed for creating the `Parameter` and are not used during pattern matching.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/matcher-pass.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// [pattern:simple_example]\n#include <openvino/core/node.hpp>\n#include <openvino/opsets/opset10.hpp>\n\n#include \"openvino/pass/graph_rewrite.hpp\"\n\nusing namespace std;\nusing namespace ov::opset10;\n\n{    // Parameter\n    auto input = make_shared<Parameter>(element::f32, Shape{1, 3, 64, 64});\n}\n// [pattern:simple_example]\n```\n\n----------------------------------------\n\nTITLE: Disabling all transformations except common\nDESCRIPTION: This example demonstrates disabling all transformations except 'common' by using transformations=all,-common. This provides fine-grained control over which transformations are disabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/feature_disabling.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_DISABLE=\"transformations=all,-common\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Reverse-1 Layer Configuration XML Example\nDESCRIPTION: This XML snippet demonstrates the configuration of a Reverse-1 layer in OpenVINO.  It specifies the input and output tensor dimensions, as well as the 'index' mode, indicating that the second input provides indices of the axes to be reversed. The first port is the data to be reversed, and the second port (port id=1) indicates what axes to revert.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/reverse-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Reverse\">\n    <data mode=\"index\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>10</dim>\n            <dim>100</dim>\n            <dim>200</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>   <!-- reverting along single axis -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>3</dim>\n            <dim>10</dim>\n            <dim>100</dim>\n            <dim>200</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard and Flags\nDESCRIPTION: This snippet configures the C++ standard to version 17 and disables language extensions. It also adds a compiler flag to enforce C++11 if the compiler is GNU.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset (CMAKE_CXX_STANDARD 17)\nset (CMAKE_CXX_EXTENSIONS OFF)\nset (CMAKE_CXX_STANDARD_REQUIRED ON)\nif (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n    set (CMAKE_CXX_FLAGS \"-std=c++11 ${CMAKE_CXX_FLAGS}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Define Target Name and Variables\nDESCRIPTION: Defines variables for the target name (`ov_npu_func_tests`), excluded functional tests directory, optional includes, and libraries. These variables are used later in the script to configure the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/functional/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_npu_func_tests)\nset(EXCLUDED_FUNC_TESTS_DIR \"\")\nset(OPTIONAL_FUNC_TESTS_INCLUDES \"\")\nset(OPTIONAL_FUNC_TESTS_LIBS \"\")\n\nset(SKIP_CONFIG \"skip_tests.xml\")\nset(SKIP_CONFIG_PATH ${CMAKE_CURRENT_SOURCE_DIR}/shared_tests_instances/${SKIP_CONFIG})\n```\n\n----------------------------------------\n\nTITLE: Setting Source File Properties in CMake\nDESCRIPTION: This snippet sets the `COMPILE_DEFINITIONS` property for the `file_util.cpp` source file, adding the `OpenVINO_VERSION` definition with the value of the `OpenVINO_VERSION` variable. This allows the source file to use this definition during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset_source_files_properties(\n    \"${CMAKE_CURRENT_SOURCE_DIR}/src/file_util.cpp\"\n    PROPERTIES COMPILE_DEFINITIONS OpenVINO_VERSION=\"${OpenVINO_VERSION}\")\n```\n\n----------------------------------------\n\nTITLE: NV12 VAAPI Surface Consumption (C++) with OpenVINO\nDESCRIPTION: Demonstrates how to directly consume an NV12 VAAPI video decoder surface on Linux with OpenVINO. This enables processing video frames decoded by VAAPI using OpenVINO models. Requires OpenVINO runtime and VAAPI.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_44\n\nLANGUAGE: cpp\nCODE:\n```\n//! [context_sharing_va]\n// Create remote context\nov::AnyMap context_properties;\ncontext_properties.insert({ov::intel_gpu::va_display(), display});\nauto remote_context = core.create_context(\"GPU\", context_properties);\n// Create remote tensor\nov::AnyMap tensor_properties;\ntensor_properties.insert({ov::intel_gpu::va_surface(), surface});\nauto remote_tensor = remote_context.create_tensor(ov::element::u8, ov::Shape{height + height / 2, width}, tensor_properties);\n\n// Set remote tensor to InferRequest\ninfer_request.set_tensor(input_tensor_name, remote_tensor);\ninfer_request.infer();\n//! [context_sharing_va]\n```\n\n----------------------------------------\n\nTITLE: Adding pyopenvino Subdirectory\nDESCRIPTION: This snippet adds the `src/pyopenvino` subdirectory to the build.  This subdirectory contains the source code for the Python bindings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(src/pyopenvino)\n```\n\n----------------------------------------\n\nTITLE: Tile Example 1: Repeats matches data shape in XML\nDESCRIPTION: Example demonstrating the Tile operation where the number of elements in the 'repeats' input matches the shape of the 'data' input. The XML configuration shows the input and output tensor dimensions after the Tile operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/tile-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Tile\">\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>  <!-- [1, 2, 3] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>2</dim>\n            <dim>6</dim>\n            <dim>12</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Registering an OpExtension in Python\nDESCRIPTION: This snippet demonstrates how to register a custom operation using `OpExtension` in Python. It directly maps the custom operation to an existing OpenVINO operation (`opset9.Add`), specifying the operation name, domain, and optional attributes like `auto_broadcast`. This allows for a more concise registration when a direct mapping is possible.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/how_to_add_op.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.frontend.onnx import OpExtension\n...\nfe.add_extension(OpExtension(\"opset9.Add\", \"CustomAdd\", \"org.openvinotoolkit\", {}, {\"auto_broadcast\": \"numpy\"}))\n```\n\n----------------------------------------\n\nTITLE: Installing numpy Python Library\nDESCRIPTION: This command installs the numpy Python library using pip and the `requirements.txt` file located in the OpenVINO installation directory. This step is only required when using the Python API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-macos.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncd /opt/intel/openvino_2025.1.0\npython3 -m pip install -r ./python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Define Optional Shape Info Arguments\nDESCRIPTION: This code snippet demonstrates how to define the OPTIONAL_SHAPE_INFO_ARG and OPTIONAL_SHAPE_INFO_TENSOR macros using JIT constants, which are used in shape agnostic kernels to pass shape information. The IS_DYNAMIC macro determines whether the shape info argument is included.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/dynamic_shape/dynamic_impl.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nif (params.is_shape_agnostic) {\n    jit.AddConstant(MakeJitConstant(\"IS_DYNAMIC\", 1));\n    jit.AddConstant(MakeJitConstant(\"OPTIONAL_SHAPE_INFO_ARG\", \"__global const int* shape_info,\"));\n    jit.AddConstant(MakeJitConstant(\"OPTIONAL_SHAPE_INFO_TENSOR\", \"shape_info,\"));\n} else {\n    jit.AddConstant(MakeJitConstant(\"OPTIONAL_SHAPE_INFO_ARG\", \"\"));\n    jit.AddConstant(MakeJitConstant(\"OPTIONAL_SHAPE_INFO_TENSOR\", \"\"));\n}\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions: Restoring Test Execution Time\nDESCRIPTION: This YAML snippet shows how to restore a previously saved cache using the `actions/cache/restore` action. It attempts to restore the cache associated with a specific key based on runner OS, architecture, and Git SHA. If an exact match isn't found, it tries to restore using the `restore-keys`, providing a fallback key without the SHA to find the closest match.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/caches.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nCPU_Functional_Tests:\n  name: CPU functional tests\n  ...\n  steps:\n    - name: Restore tests execution time\n      uses: actions/cache/restore@v3\n      with:\n        path: ${{ env.PARALLEL_TEST_CACHE }}\n        key: ${{ runner.os }}-${{ runner.arch }}-tests-functional-cpu-stamp-${{ github.sha }}\n        restore-keys: |\n          ${{ runner.os }}-${{ runner.arch }}-tests-functional-cpu-stamp\n    ...\n```\n\n----------------------------------------\n\nTITLE: Creating Interface Library for OpenVINO Core Dev API\nDESCRIPTION: This snippet creates an interface library `openvino_core_dev` and its alias `openvino::core::dev`. Interface libraries are used to propagate include directories and link dependencies without compiling any actual code. It also sets up the include directories and links necessary libraries using target_include_directories and target_link_libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(openvino_core_dev INTERFACE)\nadd_library(openvino::core::dev ALIAS openvino_core_dev)\n\ntarget_include_directories(openvino_core_dev INTERFACE\n    $<BUILD_INTERFACE:${OV_CORE_INCLUDE_PATH}>\n    $<BUILD_INTERFACE:${OpenVINO_SOURCE_DIR}/src/core/dev_api>\n    $<BUILD_INTERFACE:${OpenVINO_SOURCE_DIR}/src/frontends/common/include>\n    $<BUILD_INTERFACE:${OpenVINO_SOURCE_DIR}/src/common/transformations/include>\n    $<BUILD_INTERFACE:${OpenVINO_SOURCE_DIR}/src/common/low_precision_transformations/include>)\n\ntarget_include_directories(openvino_core_dev SYSTEM INTERFACE\n    $<BUILD_INTERFACE:$<$<TARGET_EXISTS:xbyak::xbyak>:$<TARGET_PROPERTY:xbyak::xbyak,INTERFACE_INCLUDE_DIRECTORIES>>>)\n\ntarget_link_libraries(openvino_core_dev INTERFACE openvino::itt openvino::util)\n\nset_target_properties(openvino_core_dev PROPERTIES EXPORT_NAME core::dev)\n```\n\n----------------------------------------\n\nTITLE: Running C NV12 Classification Sample\nDESCRIPTION: This command executes the C version of the `hello_nv12_input_classification_c` sample. It takes the model file path, the image file path, the image size, and the target device (e.g., CPU) as arguments. The sample loads the model, preprocesses the NV12 image, performs inference, and outputs classification results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-nv12-input-classification.rst#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nhello_nv12_input_classification_c ./models/alexnet.xml ./images/cat.yuv 300x300 CPU\n```\n\n----------------------------------------\n\nTITLE: Install npm Dependencies (Bash)\nDESCRIPTION: This command installs the npm dependencies required for the `openvino-node` package. This ensures that all necessary external libraries and modules are available for the package to function correctly. It also includes transpiling TypeScript to Javascript code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Finishing Stream for Buffer Dump C++\nDESCRIPTION: Before dumping layer in/out buffers for debugging, the `get_stream().finish()` function must be called to ensure synchronization with kernel execution. This function ensures that all operations in the stream are completed before accessing the buffer, preventing potential race conditions or inconsistent data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/execution_of_inference.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nget_stream().finish()\n```\n\n----------------------------------------\n\nTITLE: Einsum Matrix-Vector Multiplication Example C++\nDESCRIPTION: This example shows how Einsum performs matrix-vector multiplication using the Einstein summation convention.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/einsum-7.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nA = [[1.0, 2.0, 3.0],\n     [1.0, 2.0, 3.0]]\nb = [4.0, 5.0, 6.0]\nequation = \"ij,j->i\"\noutput = [32.0, 32.0]\n```\n\n----------------------------------------\n\nTITLE: Run Memory Test Script\nDESCRIPTION: This snippet demonstrates how to execute the memory test using a dedicated Python script. It specifies the path to the test executable, a model file, and the target device (CPU).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/run_memorytest.py <install_path>/tests/memtest_infer -m model.xml -d CPU\n```\n\n----------------------------------------\n\nTITLE: Constructor and Function Definitions C++\nDESCRIPTION: Defines the constructors and functions for the `MyTensor` class using pybind11. Includes constructors taking an `ov::Tensor` and a `std::vector<float>`, as well as a `get_size` method. A `say_hello` method is also defined to print a message and the contents of the tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n// This initialize use already implemented C++ constructor \ncls.def(py::init<ov::Tensor&>(),\n        py::arg(\"tensor\"),\n        R\"(\n            MyTensor's constructor.\n\n            :param tensor: `Tensor` to create new `MyTensor` from.\n            :type tensor: openvino.runtime.Tensor\n        )\");\n\n// This initialize use custom constructor, implemented via lambda \ncls.def(py::init([](std::vector<float>& list) {\n                auto tensor = ov::Tensor(ov::element::f32, ov::Shape({list.size()}));\n                std::memcpy(tensor.data(), &list[0], list.size() * sizeof(float));\n                return MyTensor(tensor);\n            }),\n        py::arg(\"list\"),\n        R\"(\n            MyTensor's constructor.\n\n            :param list: List to create new `MyTensor` from.\n            :type list: list\n        )\");\n\n// Binds class function directly\ncls.def(\"get_size\", &MyTensor::get_size,\n        R\"(\n            Get MyTensor's size.\n        )\");\n\n// Adds function only on pybind's layer -- this function will become exclusive to Python API\ncls.def(\"say_hello\", [](const MyTensor& self) {\n            py::print(\"Hello there!\");\n            for (size_t i = 0; i < self.get_size(); i++) {\n                py::print(self._tensor.data<float>() + i);\n            }\n        },\n        R\"(\n            Say hello and print contents of Tensor.\n        )\");\n```\n\n----------------------------------------\n\nTITLE: GatherND Slice Example\nDESCRIPTION: Illustrates how GatherND retrieves slices (rows or columns) from a data tensor based on the provided indices.  Here, 'indices' selects entire rows from the 'data' tensor, resulting in an output tensor composed of these rows.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-8.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nindices = [[1], [0]]\ndata    = [[1, 2],\n              [3, 4]]\noutput  = [[3, 4],\n              [1, 2]]\n```\n\n----------------------------------------\n\nTITLE: TensorFlow Module to OpenVINO\nDESCRIPTION: Converts a custom TensorFlow module to an OpenVINO model using `ov.convert_model`. This example defines a simple TensorFlow module and then converts it to an OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nself.constant2 = tf.constant(1.0, name=\"var2\")\ndef __call__(self, x):\n   return self.constant1 * x + self.constant2\n```\n\nLANGUAGE: python\nCODE:\n```\nmodel = MyModule(name=\"simple_module\")\nov_model = ov.convert_model(model, input=[-1])\n```\n\n----------------------------------------\n\nTITLE: Enabling NPU Turbo Mode\nDESCRIPTION: This code snippet shows how to enable turbo mode for the NPU device using the `ov::intel_npu::turbo` property. Turbo mode provides a hint to the system to maintain the maximum NPU frequency and memory throughput within the platform TDP limits, potentially improving performance. Two methods are shown: setting the property globally and passing it to `compile_model`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/npu-device.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\ncore.set_property(\"NPU\", ov::intel_npu::turbo(true));\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncore.compile_model(ov_model, \"NPU\", {ov::intel_npu::turbo(true)});\n```\n\n----------------------------------------\n\nTITLE: Git Commit List Command Example\nDESCRIPTION: Example of a git command used to retrieve a list of commits within a specified range. The output is formatted to include only the commit hashes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/commit_slider/README.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"commitList\" : {\n    \"getCommitListCmd\" : \"git log start_hash..end_hash --boundary --pretty=\\\"%h\\\"\"\n}\n```\n\n----------------------------------------\n\nTITLE: CompiledModel Class Header Declaration C++\nDESCRIPTION: This code snippet shows the declaration of a compiled model class, inheriting from ov::ICompiledModel. It includes fields for tracking request IDs, configurations, the transformed ov::Model, and whether the model was loaded from cache.  It also includes virtual method overrides required by the OpenVINO Plugin API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/compiled-model.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n/**\n * \\brief Template plugin compiled model\n */\nclass CompiledModel : public ov::ICompiledModel {\npublic:\n    /**\n     * \\brief Compiled model constructor\n     * \\param model Input model\n     * \\param plugin Plugin pointer\n     * \\param cfg Map of properties, to configure compiled model\n     */\n    CompiledModel(const std::shared_ptr<ov::Model>& model, const std::shared_ptr<Plugin>& plugin, const ov::AnyMap& cfg = {});\n\n    /**\n     * \\brief Creates a new inference request\n     * \\return A new inference request\n     */\n    ov::InferRequest create_infer_request() override;\n\n    /**\n     * \\brief Creates a new inference request\n     * \\return A new inference request\n     */\n    ov::InferRequest create_infer_request_impl(const ov::AnyMap& request_cfg) override;\n\n    /**\n     * \\brief Exports compiled model to the output stream\n     * \\param model_stream Output stream to write compiled model to\n     */\n    void export_model(std::ostream& model_stream) const override;\n\n    /**\n     * \\brief Returns a property of compiled model with a corresponding name\n     * \\param name Name of property to get\n     * \\return Property value\n     */\n    ov::Any get_property(const std::string& name) const override;\n\n    /**\n     * \\brief Sets properties for the compiled model.\n     * \\param properties Map of properties\n     */\n    void set_property(const ov::AnyMap& properties) override;\n\n    /**\n     * \\brief Returns a runtime model\n     * \\return shared pointer to a runtime model\n     */\n    std::shared_ptr<const ov::Model> get_runtime_model() const override;\n\nprivate:\n    size_t m_request_id = 0;\n    ov::AnyMap m_cfg;\n    std::shared_ptr<ov::Model> m_model;\n    bool m_loaded_from_cache = false;\n};\n```\n\n----------------------------------------\n\nTITLE: Synchronous Inference with Input Data TypeScript\nDESCRIPTION: Performs inference synchronously using provided input data. Input data can be passed as an object with input names as keys and Tensors or TypedArrays as values, or as an array of Tensors or TypedArrays.  The TypedArrays are automatically wrapped into Tensors. Returns an object with output names and Tensor values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\ninfer(inputData): {\n    [outputName: string]: Tensor;\n}\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries and Setting Properties in CMake\nDESCRIPTION: This snippet finds the `Threads` package and links it to the target. It also sets the threading interface and adds an API validator post-build step. Finally, it sets target properties, specifically `INTERPROCEDURAL_OPTIMIZATION_RELEASE`, based on the `ENABLE_LTO` flag to enable Link Time Optimization. It's important to call `ov_set_threading_interface_for` after linking the threading library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(Threads REQUIRED)\ntarget_link_libraries(${TARGET_NAME} PRIVATE Threads::Threads)\n\nov_set_threading_interface_for(${TARGET_NAME})\n\n# must be called after all target_link_libraries\nov_add_api_validator_post_build_step(TARGET ${TARGET_NAME})\n\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Create conanfile.txt for OpenVINO Project\nDESCRIPTION: This code block shows the content of a `conanfile.txt` file, which declares the `openvino` dependency and specifies CMake generators and layout. This file is used by Conan to manage dependencies and build settings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conan.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n[requires]\nopenvino/2025.1.0\n[generators]\nCMakeDeps\nCMakeToolchain\n[layout]\ncmake_layout\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading OpenVINO IR Model\nDESCRIPTION: This code snippet demonstrates how to convert a TensorFlow model to OpenVINO IR, save it to a file, and then load it back for inference. It first converts a ResNet50 model to IR, saves it as 'model.xml', and then loads and compiles the saved model using `ov.read_model` and `ov.compile_model`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n# Run once\n\nimport openvino as ov\nimport tensorflow as tf\n\n# 1. Convert model created with TF code\nmodel = tf.keras.applications.resnet50.ResNet50(weights=\"imagenet\")\nov_model = ov.convert_model(model)\n\n# 2. Save model as OpenVINO IR\nov.save_model(ov_model, 'model.xml', compress_to_fp16=True) # enabled by default\n\n# Repeat as needed\n\nimport openvino as ov\n\n# 3. Load model from file\ncore = ov.Core()\nov_model = core.read_model(\"model.xml\")\n\n# 4. Compile model from memory\ncompiled_model = ov.compile_model(ov_model)\n```\n\n----------------------------------------\n\nTITLE: Run Application under ITT Collector on Different ISAs\nDESCRIPTION: This snippet shows how to run an application (in this case, `benchmark_app`) under the ITT collector (`sea_runtool.py`) to collect statistics on code usage for a specific model on different ISAs (Instruction Set Architectures).  `${OPENVINO_LIBRARY_DIR}` should point to the location of OpenVINO libraries, `${MY_MODEL_RESULT}` is the directory to store the results, and `${MY_MODEL}.xml` is the model file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npython thirdparty/itt_collector/runtool/sea_runtool.py --bindir ${OPENVINO_LIBRARY_DIR} -o ${MY_MODEL_RESULT} ! ./benchmark_app -niter 1 -nireq 1 -m ${MY_MODEL}.xml\n```\n\n----------------------------------------\n\nTITLE: Conditional Job Execution based on Smart CI Output (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to use the output of the Smart CI job to conditionally execute another job. The TensorFlow_Hub_Models_Tests job only runs if the 'TF_FE' component requires testing, as determined by the Smart CI job's output. The needs keyword specifies that this job depends on the Build and Smart_CI jobs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nTensorFlow_Hub_Models_Tests:\n  needs: [Build, Smart_CI]\n  ...\n  if: fromJSON(needs.smart_ci.outputs.affected_components).TF_FE.test\n  steps:\n    - ...\n```\n\n----------------------------------------\n\nTITLE: Set Inference Precision - Python\nDESCRIPTION: This Python code snippet demonstrates how to explicitly set the inference precision to `ov::element::f32` when compiling a model for CPU. This forces the CPU plugin to use FP32 precision even if the hardware supports half-precision. Dependency: OpenVINO Python API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_6\n\nLANGUAGE: py\nCODE:\n```\n# [part2]\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model=model,\n                                        device_name=\"CPU\",\n                                        config={ov.hint.inference_precision : ov.element.f32})\n\ninference_precision = compiled_model.get_property(ov.hint.inference_precision)\nprint(f\"Inference precision: {inference_precision}\\n\")\n# [part2]\n```\n\n----------------------------------------\n\nTITLE: ISTFT Example: 3D input, 1D output, center=false, signal_length provided (XML)\nDESCRIPTION: This example demonstrates the ISTFT operation with a 3D input, generating a 1D output signal. The 'center' attribute is 'false', and the 'signal_length' is explicitly provided as an input.  This shows how to control the output signal length directly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/istft-16.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ISTFT\" ... >\n    <data center=\"false\" ... />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>16</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n        </port>\n        <port id=\"2\"></port> <!-- frame_size value: 11 -->\n        <port id=\"3\"></port> <!-- frame_step value: 3 -->\n        <port id=\"4\"></port> <!-- signal_length value: 64 -->\n    </input>\n    <output>\n        <port id=\"5\">\n            <dim>64</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Reflect Mode Example (Positive Pads) - C++\nDESCRIPTION: Shows padding with the reflect mode, where padded values are a reflection of the input data tensor. Note that values on the edges are not duplicated.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\npads_begin = [0, 1]\npads_end = [2, 3]\n\nDATA =\n[[1,  2,  3,  4]\n[5,  6,  7,  8]\n[9, 10, 11, 12]]\n\npad_mode = \"reflect\"\n\nOUTPUT =\n[[  2,  1,  2,  3,  4,  3,  2,  1 ]\n[  6,  5,  6,  7,  8,  7,  6,  5 ]\n[ 10,  9, 10, 11, 12, 11, 10,  9 ]\n[  6,  5,  6,  7,  8,  7,  6,  5 ]\n[  2,  1,  2,  3,  4,  3,  2,  1 ]]\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies\nDESCRIPTION: Adds a dependency on the `pyopenvino` target. This ensures that `pyopenvino` is built before `test_utils_api`. This is crucial as the module relies on pyopenvino's functionalities.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_dependencies(${TARGET_NAME} pyopenvino)\n```\n\n----------------------------------------\n\nTITLE: Setting Linker Flags for GNU Compiler\nDESCRIPTION: This snippet sets target properties for the GNU compiler to suppress specific warnings related to maybe-uninitialized variables and stringop-overflow during LTO builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n    set_target_properties(${TARGET_NAME} PROPERTIES LINK_FLAGS_RELEASE \"-Wno-error=maybe-uninitialized -Wno-maybe-uninitialized -Wno-stringop-overflow\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Log Specific Matchers\nDESCRIPTION: This snippet showcases how to enable logging for specific matchers by setting the `OV_MATCHERS_TO_LOG` environment variable. Multiple matcher names can be provided, separated by commas, alongside the `OV_MATCHER_LOGGING` environment variable. This allows for focused debugging of particular matchers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/docs/debug_capabilities/matcher_logging.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nOV_MATCHER_LOGGING=true OV_MATCHERS_TO_LOG=EliminateSplitConcat,MarkDequantization ./your_amazing_program\n```\n\n----------------------------------------\n\nTITLE: PyTorch Inference with OpenVINO\nDESCRIPTION: This Python code demonstrates how to load a PyTorch model, convert it to OpenVINO format, compile it for CPU, and perform inference. It requires the openvino, torch, and torchvision libraries. The example uses a shufflenet_v2_x1_0 model from torchvision.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openvino as ov\nimport torch\nimport torchvision\n\n# load PyTorch model into memory\nmodel = torch.hub.load(\"pytorch/vision\", \"shufflenet_v2_x1_0\", weights=\"DEFAULT\")\n\n# convert the model into OpenVINO model\nexample = torch.randn(1, 3, 224, 224)\nov_model = ov.convert_model(model, example_input=(example,))\n\n# compile the model for CPU device\ncore = ov.Core()\ncompiled_model = core.compile_model(ov_model, 'CPU')\n\n# infer the model on random data\noutput = compiled_model({0: example.numpy()})\n```\n\n----------------------------------------\n\nTITLE: Install Memory Tests with CMake\nDESCRIPTION: This snippet shows how to install the built memory tests using CMake's install command.  It takes the build directory and an installation path as arguments, copying the compiled test executables to the specified location.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nсmake install <build_dir> --prefix <install_path>\n```\n\n----------------------------------------\n\nTITLE: Paddle Frontend Architecture Diagram (Mermaid)\nDESCRIPTION: This diagram illustrates the Paddle Frontend architecture, showing the flow of data from the Paddle Model to the ov::Model. It highlights the interaction between the Paddle Frontend, OpenVINO Frontend API, and Core APIs, including the load_impl and convert functions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/docs/paddle_frontend_architecture.md#_snippet_0\n\nLANGUAGE: Mermaid\nCODE:\n```\nflowchart TB\n    fw_model[(Paddle Model)]\n    style fw_model fill:#427cb0\n    \n    protobuf([protobuf])\n    subgraph frontend [ov::frontend::paddle::FrontEnd]\n        load_impl[\"load_impl()\"]\n    end\n    fw_model--as stream-->load_impl\n    load_impl--load stream-->protobuf\n    protobuf--parsed object-->load_impl\n    \n    \n    subgraph input_model [ov::frontend::paddle::InputModel]\n        convert[\"convert()\"]\n    end\n    \n    load_impl--create-->input_model\n    \n    ov_model[ov::Model]\n    \n    convert--recursively parse all operations from the model-->ov_model\n```\n\n----------------------------------------\n\nTITLE: Einsum Broadcasting Example C++\nDESCRIPTION: This example demonstrates how Einsum operates with broadcasting two operands using the Einstein summation convention.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/einsum-7.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nA = [[1.0, 2.0, 3.0],\n     [4.0, 5.0, 6.0],\n     [7.0, 8.0, 9.0]]\nB = [0.5]\nequation = \"a...,...->a...\"\noutput = [[0.5, 1.0, 1.5],\n            [2.0, 2.5, 3.0],\n            [3.5, 4.0, 4.5]]\n```\n\n----------------------------------------\n\nTITLE: Set Batch Size - C++\nDESCRIPTION: This C++ snippet shows how to use the `set_batch` method to change the batch dimension of an OpenVINO model. It requires including the OpenVINO Inference Engine library. The set_batch method changes the batch dimension of the model to the specified value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/changing-input-shape.rst#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\n\n    // ! [set_batch]\n    size_t new_batch_size = 32;\n    model->set_batch(new_batch_size);\n    // ! [set_batch]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: GatherND Slice Gathering Example\nDESCRIPTION: This example shows how GatherND gathers slices from the data tensor based on the indices. Instead of individual elements, entire slices of the data tensor are selected and combined to produce the output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-5.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nindices = [[1], [0]]\ndata    = [[1, 2],\n              [3, 4]]\noutput  = [[3, 4],\n              [1, 2]]\n```\n\n----------------------------------------\n\nTITLE: Running Inference on NPU with Blob\nDESCRIPTION: This snippet demonstrates how to run inference on the NPU using the `single-image-test.exe` application with a pre-compiled blob file. It also uses a configuration file (`mobilenet-v2.conf`), enables result comparison (`--run_test`), sets the log level, specifies the comparison mode (`classification`), and defines the top-K and probability tolerance parameters. The command tests the model against previously collected results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/README.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nsingle-image-test.exe \\\n        --network \\\n        mobilenet-v2.blob \\\n        --input \\\n        validation-set/224x224/watch.bmp \\\n        --ip \\\n        FP16 \\\n        --op \\\n        FP16 \\\n        --device \\\n        NPU \\\n        --config \\\n        mobilenet-v2.conf \\\n        --run_test \\\n        -log_level \\\n        LOG_ERROR \\\n        --mode \\\n        classification \\\n        --top_k \\\n        1 \\\n        --prob_tolerance \\\n        0.6 \\\n        --color_format \\\n        RGB \\\n        --il \\\n        NCHW \\\n        --ol \\\n        NC \\\n        --iml \\\n        NCHW \\\n        --oml \\\n        NC\n```\n\n----------------------------------------\n\nTITLE: Set Input Tensor by Index and Tensor Object TypeScript\nDESCRIPTION: Sets the input tensor for the InferRequest by index. The `idx` parameter specifies the index of the input tensor. The tensor's type and shape must match the model's input requirements. If `idx` is out of range, an exception is thrown. This method returns void.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nsetInputTensor(idx, tensor): void\n```\n\n----------------------------------------\n\nTITLE: Set cpuUtils Link Libraries and Includes\nDESCRIPTION: This snippet sets the link libraries and include paths for the 'cpuUtils' library. It adds 'openvino::runtime' as a required library. It also conditionally adds 'arm_compute::arm_compute' and 'dnnl' libraries along with their include paths if OV_CPU_WITH_ACL and OV_CPU_WITH_DNNL are enabled respectively. It also links xbyak_riscv if RISCV64 is defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/functional/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(CPU_UTILS_LINK_LIBRARIES openvino::runtime)\nset(CPU_UTILS_INCLUDE_PATHS)\nif(OV_CPU_WITH_ACL)\n    list(APPEND CPU_UTILS_LINK_LIBRARIES arm_compute::arm_compute)\n    list(APPEND CPU_UTILS_INCLUDE_PATHS $<TARGET_PROPERTY:arm_compute::arm_compute,SOURCE_DIR>)\nendif()\nif(OV_CPU_WITH_DNNL)\n    list(APPEND CPU_UTILS_LINK_LIBRARIES dnnl)\n    list(APPEND CPU_UTILS_INCLUDE_PATHS $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/thirdparty/onednn/src)\nendif()\nif(RISCV64)\n    list(APPEND CPU_UTILS_LINK_LIBRARIES xbyak_riscv)\nendif()\ntarget_link_libraries(cpuUtils PRIVATE ${CPU_UTILS_LINK_LIBRARIES})\ntarget_include_directories(cpuUtils PUBLIC ${CPU_UTILS_INCLUDE_PATHS})\n```\n\n----------------------------------------\n\nTITLE: Installing a Single OpenVINO Component\nDESCRIPTION: This command installs the `libopenvino-intel-cpu-plugin` component from the `conda-forge` channel. This plugin enables OpenVINO to utilize Intel CPUs for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conda.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nconda install conda-forge::libopenvino-intel-cpu-plugin\n```\n\n----------------------------------------\n\nTITLE: ONNX Linear Interpolation (5D)\nDESCRIPTION: Implements the ONNX-style linear interpolation, which is specifically designed for 3D or 5D tensors. This function asserts that the axes are either {2, 3, 4} or {0, 1, 2}. The implementation includes reshaping the input data for 3D tensors, and calculates indices and weights for interpolation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef onnx_linear_interpolation5D(self, input_data):\n    rank = len(self.input_shape)\n    assert rank in [3, 5], \"mode 'linear_onnx' supports only 3D or 5D tensors\"\n    assert set(self.axes) == {2, 3, 4} or set(self.axes) == {0, 1, 2}, \\\n        \"mode 'linear_onnx' supports only case when axes = {2, 3, 4} or axes = {0, 1, 2}\"\n\n    result = np.zeros(self.output_shape)\n\n    if rank == 3:\n        reshaped_data = np.reshape(input_data, (1, 1, self.input_shape[0], self.input_shape[1], self.input_shape[2]))\n        result = np.reshape(result,  (1, 1, self.output_shape[0], self.output_shape[1], self.output_shape[2]))\n    else:\n        reshaped_data = input_data\n\n    input_shape = np.array(reshaped_data.shape).astype(np.int64)\n    output_shape = np.array(result.shape).astype(np.int64)\n\n    batch_size = input_shape[0];\n    num_channels = input_shape[1];\n    input_depth = input_shape[2];\n    input_height = input_shape[3];\n    input_width = input_shape[4];\n    output_depth = output_shape[2];\n    output_height = output_shape[3];\n    output_width = output_shape[4];\n\n    depth_scale = self.scales[0];\n    height_scale = self.scales[1];\n    width_scale = self.scales[2];\n\n    z_original = np.zeros(output_depth).astype(np.float)\n    y_original = np.zeros(output_height).astype(np.float)\n    x_original = np.zeros(output_width).astype(np.float)\n\n    in_z1 = np.zeros(output_depth).astype(np.int64)\n    in_z2 = np.zeros(output_depth).astype(np.int64)\n    in_y1 = np.zeros(output_height).astype(np.int64)\n    in_y2 = np.zeros(output_height).astype(np.int64)\n    in_x1 = np.zeros(output_width).astype(np.int64)\n    in_x2 = np.zeros(output_width).astype(np.int64)\n\n    dz1 = np.zeros(output_depth).astype(np.float)\n    dz2 = np.zeros(output_depth).astype(np.float)\n\n    dy1 = np.zeros(output_height).astype(np.float)\n    dy2 = np.zeros(output_height).astype(np.float)\n\n    dx1 = np.zeros(output_width).astype(np.float)\n    dx2 = np.zeros(output_width).astype(np.float)\n\n    for z in range(0, output_depth):\n        in_z = self.get_original_coordinate(z, depth_scale, output_depth, input_depth)\n        z_original[z] = in_z\n        in_z = max(0, min(in_z, input_depth - 1))\n        in_z1[z] = max(0, min(int(in_z), input_depth - 1))\n        in_z2[z] = min(in_z1[z] + 1, input_depth - 1)\n        dz1[z] = abs(in_z - in_z1[z])\n        dz2[z] = abs(in_z - in_z2[z])\n\n        if in_z1[z] == in_z2[z]:\n            dz1[z] = 0.5\n```\n\n----------------------------------------\n\nTITLE: Dumping Memory Allocation History in GPU Plugin C++\nDESCRIPTION: This code snippet demonstrates how to dump the memory allocation history in the OpenVINO GPU plugin using the environment variable `OV_VERBOSE=2`, given that OpenVINO is built with `ENABLE_DEBUG_CAPS=ON`. It shows the output format, which includes the allocated memory size, allocation type, current total allocated memory, and the peak memory allocation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/memory_allocation_gpu_plugin.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n...\nGPU_Debug: Allocate 58982400 bytes of usm_host allocation type (current=117969612; max=117969612)\nGPU_Debug: Allocate 44621568 bytes of usm_device allocation type (current=44626380; max=44626380)\nGPU_Debug: Allocate 44236800 bytes of usm_host allocation type (current=162206412; max=162206412)\nGPU_Debug: Allocate 14873856 bytes of usm_device allocation type (current=59500236; max=59500236)\n...\n```\n\n----------------------------------------\n\nTITLE: Class Binding with pybind11 C++\nDESCRIPTION: Binds the `MyTensor` class to a pybind11 module named `mymodule`.  A shared pointer is used for memory management. Also adds a documentation string to the class.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n// Add class to the module\npy::class_<MyTensor, std::shared_ptr<MyTensor>> cls(mymodule, \"MyTensor\");\n\ncls.doc() = \"These are my first class bindings!\"\n```\n\n----------------------------------------\n\nTITLE: Enable Sparse Weights Decompression in Python\nDESCRIPTION: This Python snippet demonstrates how to enable the sparse weights decompression feature in OpenVINO by setting the `sparse_weights_decompression_rate` property before compiling the model. The property controls the sparse rate threshold for applying the feature. A value of 1 disables the feature, while values closer to 0 enable it for more operations. The property must be set before `compile_model()` is called.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n###\n# Example of sparse weights decompression rate property usage\n###\n\nimport openvino.runtime as ov\n\ncore = ov.Core()\n\n# the property must be set before calling compile_model()\ncore.set_property({\"CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE\" : 0.8})\n\nmodel = core.read_model(\"path_to_your_model/your_model.xml\")\n\ncompiled_model = core.compile_model(model, \"CPU\")\n\nresults = compiled_model()\n\n```\n\n----------------------------------------\n\nTITLE: Mapping to Standard Relu Operation (C++)\nDESCRIPTION: Illustrates mapping a custom operation ('MyRelu') to a standard OpenVINO 'Relu' operation in C++. This allows leveraging existing optimized implementations for custom operations with equivalent functionality.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/frontend-extensions.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nCore core;\ncore.add_extension(std::make_shared<ov::frontend::OpExtension<ov::op::v0::Relu>>(\"MyRelu\"));\nauto model = core.read_model(\"model.xml\");\n```\n\n----------------------------------------\n\nTITLE: Configuring oneDNN GPU Backend with CMake\nDESCRIPTION: This snippet checks if oneDNN is enabled for GPU and then configures the corresponding OpenVINO backend. It utilizes `ov_gpu_add_backend_target` to define and link the necessary libraries. The `ENABLE_ONEDNN_FOR_GPU` variable must be set to enable this functionality.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/onednn/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT ENABLE_ONEDNN_FOR_GPU)\n    return()\nendif()\n\nset(TARGET_NAME \"openvino_intel_gpu_onednn_obj\")\n\nov_gpu_add_backend_target(\n    NAME ${TARGET_NAME}\n    LINK_LIBRARIES onednn_gpu_tgt\n)\n```\n\n----------------------------------------\n\nTITLE: Set LTO Properties CMake\nDESCRIPTION: This snippet sets the `INTERPROCEDURAL_OPTIMIZATION_RELEASE` property for the `openvino_snippets` target, enabling Link-Time Optimization (LTO) in release builds, if `ENABLE_LTO` is enabled. LTO can improve the performance of the library by optimizing across function boundaries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Benchmarking asl-recognition model on CPU in latency mode (C++, repeated)\nDESCRIPTION: This snippet demonstrates how to run the OpenVINO Benchmark Tool in C++ to measure the latency of the 'asl-recognition' model on a CPU. It uses the `./benchmark_app` command with the `-m` option to specify the model path, `-d` to select the CPU device, and `-hint` to set the performance hint to latency. The model is expected to be in the OpenVINO Intermediate Representation (IR) format ('.xml' file). This is a repeated example.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_20\n\nLANGUAGE: sh\nCODE:\n```\n./benchmark_app -m omz_models/intel/asl-recognition-0004/FP16/asl-recognition-0004.xml -d CPU -hint latency\n```\n\n----------------------------------------\n\nTITLE: Quantize-only FQ I8\nDESCRIPTION: This snippet shows the quantize-only FakeQuantize operation for signed 8-bit integers (I8). The input value 'x' is quantized with an input scale 'S_i', and the result is clamped to the range [-128, 127]. In this optimized case Zi=128 and outputLow = -128 are optimized to Zi=0\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/fake_quantize.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nq'_{I8} = clamp(round(x*\\frac{1}{S_i}), -128, 127)\n```\n\n----------------------------------------\n\nTITLE: Python Asynchronous Inference Example\nDESCRIPTION: This is a console command example that demonstrates how to run the classification_sample_async.py script with a specified model and input image, using the GPU device for inference. It shows the basic syntax for executing the Python sample.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/image-classification-async.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npython classification_sample_async.py -m ./models/alexnet.xml -i ./test_data/images/banana.jpg ./test_data/images/car.bmp -d GPU\n```\n\n----------------------------------------\n\nTITLE: Registering a Conversion Extension in Python\nDESCRIPTION: This code snippet shows how to register a custom operation using `ConversionExtension` in Python. It defines a `custom_add` function that takes a `NodeContext`, retrieves the inputs, creates an `Add` operation, and returns its output. The `ConversionExtension` then associates this function with the custom operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/how_to_add_op.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.frontend.onnx import ConversionExtension\n...\ndef custom_add(node: NodeContext):\n    input_1 = node.get_input(0)\n    input_2 = node.get_input(1)\n    add = ops.add(input_1, input_2)\n    return [add.output(0)]\n\nfe.add_extension(ConversionExtension(\"CustomAdd\", \"org.openvinotoolkit\", custom_add))\n```\n\n----------------------------------------\n\nTITLE: Getting Tensor Data in TypeScript\nDESCRIPTION: Illustrates the `getData` method for retrieving the tensor's data as a `TypedArray` subclass, matching the tensor's element type. This method returns a new `TypedArray` instance containing a copy of the data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Tensor.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ngetData(): SupportedTypedArray;\n```\n\n----------------------------------------\n\nTITLE: Making OpenVINO AUTO Plugin Unit Tests\nDESCRIPTION: This snippet shows the make command to build the `ov_auto_unit_tests` target after cmake is configured with `ENABLE_TESTS=ON`.  The resulting executable is typically found in the *bin* directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/tests.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake ov_auto_unit_tests\n```\n\n----------------------------------------\n\nTITLE: Linking Platform-Specific Libraries\nDESCRIPTION: This snippet conditionally links platform-specific libraries. On Windows, it links 'setupapi', while on non-Android UNIX systems, it links 'pthread'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n  target_link_libraries(${TARGET_NAME} PRIVATE setupapi)\nelif((NOT ANDROID) AND (UNIX))\n  target_link_libraries(${TARGET_NAME} PRIVATE pthread)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Importing Model Blob with OpenVINO GenAI (Python)\nDESCRIPTION: This snippet demonstrates how to configure the OpenVINO GenAI pipeline to import a pre-compiled model from a blob file. It specifies the path to the blob file using the `BLOB_PATH` option. This allows you to load a previously compiled model, avoiding recompilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\npipeline_config = { \"BLOB_PATH\": \".npucache\\\\compiled_model.blob\" }\npipe = ov_genai.LLMPipeline(model_path, \"NPU\", pipeline_config)\n```\n\n----------------------------------------\n\nTITLE: Import NNCF API - PyTorch\nDESCRIPTION: This code snippet demonstrates how to import the necessary NNCF modules in a PyTorch training script to enable filter pruning. These imports are essential for using NNCF's compression capabilities and integrating them into the model training process. The NNCF package must be installed prior to running the script.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nncf import NNCFConfig\nfrom nncf import create_compressed_model\nfrom nncf import load_state\n```\n\n----------------------------------------\n\nTITLE: Get Shape of Port in OpenVINO (C)\nDESCRIPTION: This function retrieves the shape of a port object in OpenVINO. It takes a pointer to `ov_output_port_t` and returns the tensor shape through `ov_shape_t`. A status code is returned, indicating the result of the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_36\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_port_get_shape(const ov_output_port_t* port, ov_shape_t* tensor_shape)\n```\n\n----------------------------------------\n\nTITLE: Defining Plugin Configuration Header C++\nDESCRIPTION: This snippet presents the structure of the plugin configuration class. It showcases the configuration parameters used by the plugin, such as device ID, performance counts, stream executor configuration, and other performance-related settings. These configurations influence how the plugin interacts with the underlying hardware and optimizes model execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nclass Configuration {\npublic:\n    Configuration();\n    Configuration(const Configuration& rhs);\n    Configuration(const ov::AnyMap& config, const std::shared_ptr<ov::threading::IStreamsExecutor>& stream_executor);\n\n    ov::Any Get(const std::string& name) const;\n\n    std::shared_ptr<ov::threading::IStreamsExecutor> get_streams_executor() const { return _streams_executor; }\n\n    ov::AnyMap properties() const;\n\n    int32_t _device_id;\n    bool _perf_counts;\n    ov::hint::PerformanceMode _performance_hint;\n    bool _enable_profiling;\n    bool _disable_transformations;\n    bool _exclusive_async_requests;\n\nprivate:\n    std::shared_ptr<ov::threading::IStreamsExecutor> _streams_executor;\n    ov::AnyMap _user_properties;\n};\n\n```\n\n----------------------------------------\n\nTITLE: Convert OCR Model with Bounded Dynamic Dimensions via CLI\nDESCRIPTION: This command-line snippet converts an ONNX OCR model using the `ovc` tool and specifies a boundary (1..3) for the batch dimension of inputs `data` and `seq_len`. The `--input` argument is used to define the shapes with the boundary indicated by `1..3`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/setting-input-shapes.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\novc ocr.onnx --input data[1..3,150,200,1],seq_len[1..3]\n```\n\n----------------------------------------\n\nTITLE: Add Subdirectories for Source Code\nDESCRIPTION: Adds subdirectories containing source code for the runtime, kernel selector, and graph components of the GPU plugin to the build process. These directories contain the core implementation of the plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(src/runtime)\nadd_subdirectory(src/kernel_selector)\nadd_subdirectory(src/graph)\n```\n\n----------------------------------------\n\nTITLE: Setting NPU Plugin Properties with OpenVINO\nDESCRIPTION: This code snippet demonstrates how to set properties of the NPU plugin at both the core and compiled model levels using OpenVINO's `set_property` method. The properties are passed as a key-value pair enclosed in curly braces.  The `ov.set_property` method sets the property on the OpenVINO core, while `compiled_model.set_property` sets it on the compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/README.md#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nov.set_property(\"NPU\", {{Key, Value}});\n[...]\ncompiled_model.set_property({{Key, Value}});\n```\n\n----------------------------------------\n\nTITLE: Create Virtual Environment (Windows)\nDESCRIPTION: This command creates a virtual environment named 'openvino_env' in the current directory using Python's venv module. It isolates the project's dependencies from the system-wide Python installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\npython -m venv openvino_env\n```\n\n----------------------------------------\n\nTITLE: Eliminating a Node in OpenVINO (C++)\nDESCRIPTION: This code snippet demonstrates how to eliminate a node from an OpenVINO model using `ov::replace_output_update_name`.  This method handles the complexities of node removal, preserving the friendly name and runtime info if the replacement is successful.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov:eliminate_node]\nov::replace_output_update_name(op->output(0), input);\n//! [ov:eliminate_node]\n```\n\n----------------------------------------\n\nTITLE: Another Requirements File with Constraints\nDESCRIPTION: This example demonstrates another requirements file using the same constraints file as before. The `-c` flag includes the constraints file, specifying the package versions. The requirements file lists only package names, relying on the constraints file for version management. Note that a package is only installed if it is explicitly listed in the requirements file, even if a version is specified in the constraints file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/requirements_management.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n# requirements_tensorflow.txt\n\n-c main/constraints.txt  # versions are already defined here!\ncoverage\npylint\npyenchant\ntest-generator\n```\n\n----------------------------------------\n\nTITLE: Getting Device Property (Device Name and Property Name)\nDESCRIPTION: Retrieves a device property based on the specified device name and property name. This allows querying properties specific to a particular device. The property value is returned as OVAny.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\ngetProperty(deviceName, propertyName): OVAny\n```\n\n----------------------------------------\n\nTITLE: ModelPass Template Transformation Header C++\nDESCRIPTION: This code snippet demonstrates the template for creating a ModelPass transformation class in C++. It shows how to inherit from ov::pass::ModelPass and override the run_on_model method to implement the transformation logic. The transformation operates on an ov::Model object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/model-pass.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nclass TemplateModelPass : public ov::pass::ModelPass {\npublic:\n    OPENVINO_RTTI(\"TemplateModelPass\", \"0\");\n    TemplateModelPass() = default;\n\n    bool run_on_model(const std::shared_ptr<ov::Model>& model) override {\n        // Add transformation code here\n        return false;\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: RegionYolo Layer Configuration for YOLO V2 in XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a RegionYolo layer for YOLO V2. It specifies the anchors, axis, classes, coords, and num attributes, as well as the input and output port dimensions. The `do_softmax` attribute is set to 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/region-yolo-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<!-- YOLO V2 Example -->\n<layer type=\"RegionYolo\" ... >\n    <data anchors=\"1.08,1.19,3.42,4.41,6.63,11.38,9.42,5.11,16.62,10.52\" axis=\"1\" classes=\"20\" coords=\"4\" do_softmax=\"1\" end_axis=\"3\" num=\"5\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>125</dim>\n            <dim>13</dim>\n            <dim>13</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>21125</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing Xcode command line tools\nDESCRIPTION: This command installs the Clang compiler and other command-line tools from Xcode, which are required for building OpenVINO.  It's a prerequisite for compiling the OpenVINO runtime on macOS.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n% xcode-select --install\n```\n\n----------------------------------------\n\nTITLE: Link Threads Library (CMake)\nDESCRIPTION: This snippet finds the Threads package and links the Threads library to the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(Threads REQUIRED)\ntarget_link_libraries(${TARGET_NAME} PRIVATE Threads::Threads)\n```\n\n----------------------------------------\n\nTITLE: Exporting Targets for Developer Package in CMake\nDESCRIPTION: Exports the specified targets to be included in the OpenVINO developer package. This makes the targets (format_reader and ie_samples_utils) available for use by other projects that depend on the OpenVINO developer package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nov_developer_package_export_targets(TARGET format_reader)\nov_developer_package_export_targets(TARGET ie_samples_utils)\n```\n\n----------------------------------------\n\nTITLE: LogicalAnd XML Example - NumPy Broadcast\nDESCRIPTION: This XML snippet demonstrates the LogicalAnd operation with NumPy broadcasting. The input tensors have different shapes, and the operation automatically broadcasts them to a common shape before performing the element-wise AND. The dimensions of input and output ports are specified, showing how the shapes change due to broadcasting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/logical/logical-and-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"LogicalAnd\">\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Determine pip Version - CMake\nDESCRIPTION: This CMake code executes a Python command to determine the installed pip version. The extracted version is then stored in the `pip_version` variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nexecute_process(COMMAND ${Python3_EXECUTABLE} -m pip --version\n                OUTPUT_VARIABLE pip_version OUTPUT_STRIP_TRAILING_WHITESPACE)\n\nstring(REGEX MATCH \"pip[ ]+([\\.0-9]*)\" pip_version \"${pip_version}\")\nset(pip_version ${CMAKE_MATCH_1})\n```\n\n----------------------------------------\n\nTITLE: Example: Dump Blobs for Fused Add Nodes\nDESCRIPTION: Sets the environment variable to dump blobs only for nodes with names matching the regex '.+Fused_Add.+' during OpenVINO CPU execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_NODE_NAME=\".+Fused_Add.+\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Creating Library Alias in CMake\nDESCRIPTION: This snippet creates an alias named `openvino::util` for the `TARGET_NAME` library. This allows you to use the more descriptive name `openvino::util` when linking against the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(openvino::util ALIAS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Install Testing Requirements Python\nDESCRIPTION: Installs the required Python packages for running OpenVINO™ Python API tests. This command uses pip to install packages listed in the `requirements_test.txt` file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython -m pip install -r openvino/src/bindings/python/requirements_test.txt\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenVINO repository and initializing submodules\nDESCRIPTION: This script clones the OpenVINO repository from GitHub and initializes all its submodules recursively. This ensures that all necessary components and dependencies are available for the build process. The script navigates into the cloned directory after the clone operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_linux.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/openvinotoolkit/openvino.git\ncd openvino\ngit submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: ReduceSum XML Example (keep_dims=false)\nDESCRIPTION: Demonstrates the usage of ReduceSum in an OpenVINO model using XML configuration when `keep_dims` is set to `false`. The input tensor has dimensions 6x12x10x24, the reduction axes are 2 and 3, and the output tensor has dimensions 6x12, with the reduced dimensions removed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-sum-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceSum\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Compiling Model on Specific GPU Python\nDESCRIPTION: This Python snippet demonstrates how to compile a model on a specific GPU device (identified by ID) using `ov::Core::compile_model()`.  It utilizes the `compile_model_gpu_with_id` fragment from the specified Python file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# [compile_model_gpu_with_id]\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model=model, device_name=\"GPU.1\")\n# [compile_model_gpu_with_id]\n```\n\n----------------------------------------\n\nTITLE: Linear Interpolation 5D Tensor\nDESCRIPTION: Performs linear interpolation on a 5D tensor. It calculates the coordinates in the input tensor corresponding to each output pixel and uses these to interpolate the pixel value. The function handles edge cases by clamping coordinates to the input tensor bounds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n           dz2[z] = 0.5\n\n           for y in range(0, output_height):\n               in_y = self.get_original_coordinate(y, height_scale, output_height, input_height)\n               y_original[y] = in_y\n               in_y = max(0, min(in_y, input_height - 1))\n               in_y1[y] = max(0, min(int(in_y), input_height - 1))\n               in_y2[y] = min(in_y1[y] + 1, input_height - 1)\n               dy1[y] = abs(in_y - in_y1[y])\n               dy2[y] = abs(in_y - in_y2[y])\n\n               if in_y1[y] == in_y2[y]:\n                   dy1[y] = 0.5\n                   dy2[y] = 0.5\n\n           for x in range(0, output_width):\n               in_x = self.get_original_coordinate(x, width_scale, output_width, input_width);\n               x_original[x] = in_x\n               in_x = max(0.0, min(in_x, input_width - 1));\n\n               in_x1[x] = min(in_x, input_width - 1);\n               in_x2[x] = min(in_x1[x] + 1, input_width - 1);\n\n               dx1[x] = abs(in_x - in_x1[x]);\n               dx2[x] = abs(in_x - in_x2[x]);\n               if in_x1[x] == in_x2[x]:\n                   dx1[x] = 0.5\n                   dx2[x] = 0.5\n           for n in range(0, batch_size):\n               for c in range(0, num_channels):\n                   for z in range(0, output_depth):\n                       for y in range(0, output_height):\n                           for x in range(0, output_width):\n                               x111 = reshaped_data[n, c, in_z1[z], in_y1[y], in_x1[x]]\n                               x211 = reshaped_data[n, c, in_z1[z], in_y1[y], in_x2[x]]\n                               x121 = reshaped_data[n, c, in_z1[z], in_y2[y], in_x1[x]]\n                               x221 = reshaped_data[n, c, in_z1[z], in_y2[y], in_x2[x]]\n                               x112 = reshaped_data[n, c, in_z2[z], in_y1[y], in_x1[x]]\n                               x212 = reshaped_data[n, c, in_z2[z], in_y1[y], in_x2[x]]\n                               x122 = reshaped_data[n, c, in_z2[z], in_y2[y], in_x1[x]]\n                               x222 = reshaped_data[n, c, in_z2[z], in_y2[y], in_x2[x]]\n\n                               temp = dx2[x] * dy2[y] * dz2[z] * x111 + dx1[x] * dy2[y] * dz2[z] * x211\n                               temp += dx2[x] * dy1[y] * dz2[z] * x121 + dx1[x] * dy1[y] * dz2[z] * x221\n                               temp += dx2[x] * dy2[y] * dz1[z] * x112 + dx1[x] * dy2[y] * dz1[z] * x212\n                               temp += dx2[x] * dy1[y] * dz1[z] * x122 + dx1[x] * dy1[y] * dz1[z] * x222\n\n                               result[n, c, z, y, x] = temp\n\n           return np.reshape(result, self.output_shape)\n```\n\n----------------------------------------\n\nTITLE: Run End-to-End Tests with Specific Modules (pytest)\nDESCRIPTION: Executes end-to-end tests for a specific set of modules using the `pytest` testing framework. The `-s` flag enables step-by-step logging for debugging. The `--modules` argument specifies the paths to the test modules.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/e2e_tests/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest test_base.py -s --modules=pipelines/production/tf_hub\n```\n\n----------------------------------------\n\nTITLE: SliceScatter-15 NumPy Implementation\nDESCRIPTION: This Python (NumPy) code provides an equivalent implementation of the SliceScatter-15 operation.  It takes data, updates, start, stop, step, and optional axes as input. It creates a copy of the input data, constructs a slice list based on the start, stop, and step values for the specified axes, and then updates the data copy with the provided updates.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-scatter-15.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef slice_scatter_15(\n    data: np.ndarray,\n    updates: np.ndarray,\n    start: List[int],\n    stop: List[int],\n    step: List[int],\n    axes: Optional[List[int]] = None,\n):\n    out = np.copy(data)\n    if axes is None:\n        axes = list(range(len(start)))\n    slice_list = [slice(None)] * data.ndim\n    for slice_start, slice_stop, slice_step, slice_axis in zip(start, stop, step, axes):\n        slice_list[slice_axis] = slice(slice_start, slice_stop, slice_step)\n    out[tuple(slice_list)] = updates\n    return out\n```\n\n----------------------------------------\n\nTITLE: Add Intel GPU Plugin\nDESCRIPTION: Adds the Intel GPU plugin using the `ov_add_plugin` macro, specifying the name, device name, source files, default configuration, and version defines. It uses the variables set previously, such as TARGET_NAME and PLUGIN_SOURCES.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_plugin(NAME ${TARGET_NAME}\n              DEVICE_NAME \"GPU\"\n              SOURCES ${PLUGIN_SOURCES}\n              DEFAULT_CONFIG ${PLUGIN_DEFAULT_CONFIG}\n              VERSION_DEFINES_FOR src/plugin/plugin.cpp)\n```\n\n----------------------------------------\n\nTITLE: Copy OpenVINO Tokenizers Library (MacOS x86)\nDESCRIPTION: Specifies the directory to copy the OpenVINO Tokenizers prebuilt library for MacOS x86 systems.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n<openvino_dir>/runtime/lib/intel64/Release\n```\n\n----------------------------------------\n\nTITLE: Subgraph Selection Algorithm Visualization (Mermaid)\nDESCRIPTION: This Mermaid diagram visualizes the subgraph selection process. It shows the connections between nodes and how the algorithm iteratively merges nodes into subgraphs, rejecting nodes that cause self-references or violate affinity rules. It serves as a graphical aid to understand the steps in the subgraph construction process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/README.md#_snippet_0\n\nLANGUAGE: Mermaid\nCODE:\n```\ngraph TD;\n    1-->2;\n    2-->3;\n    2-->4;\n    3-->5;\n    4-->5;\n    5-->6;\n    6-->7;\n```\n\n----------------------------------------\n\nTITLE: OpenVINO TopK Layer Configuration\nDESCRIPTION: Illustrates an example of the TopK layer configuration within an OpenVINO model, demonstrating how to specify attributes like axis, mode, sort, and stable, as well as input and output port dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/top-k-11.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"TopK\" ... >\n    <data axis=\"3\" mode=\"max\" sort=\"value\" stable=\"true\" index_element_type=\"i64\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n        </port>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>10</dim>\n        </port>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>10</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: BatchNormInference Example (2D input) XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a BatchNormInference layer in OpenVINO for a 2D input tensor. It defines the input and output port dimensions, along with the epsilon attribute for numerical stability. The input tensor has dimensions 10x128, and the gamma, beta, mean, and variance tensors have a dimension of 128.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/batch-norm-inference-5.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"BatchNormInference\" ...>\n    <data epsilon=\"9.99e-06\" />\n    <input>\n        <port id=\"0\">  <!-- input -->\n            <dim>10</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"1\">  <!-- gamma -->\n            <dim>128</dim>\n        </port>\n        <port id=\"2\">  <!-- beta -->\n            <dim>128</dim>\n        </port>\n        <port id=\"3\">  <!-- mean -->\n            <dim>128</dim>\n        </port>\n        <port id=\"4\">  <!-- variance -->\n            <dim>128</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"5\">\n            <dim>10</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Using Optional as the root node in a pattern\nDESCRIPTION: This Python snippet demonstrates how to use Optional as the root node, allowing the pattern to optionally end with the specified node.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.tools.ovc.composite.pattern import WrapType, Optional\n\ndef pattern_optional_root():\n    # Creating nodes\n    relu_node = WrapType(\"opset13.Relu\")\n    optional_node = Optional(relu_node)\n\n    return optional_node\n```\n\n----------------------------------------\n\nTITLE: Specify Blob Dump Format\nDESCRIPTION: Sets the environment variable to specify the format in which the blobs will be dumped, either BIN or TEXT for OpenVINO CPU executions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_FORMAT=<format> binary ...\n```\n\n----------------------------------------\n\nTITLE: Install dependencies on Red Hat/CentOS/Amazon Linux/Fedora\nDESCRIPTION: This code snippet shows how to update, upgrade, and install necessary packages, including python36-devel and mesa-libGL, on Red Hat, CentOS, Amazon Linux 2, or Fedora systems.  These packages are essential for running OpenVINO notebooks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nsudo yum update\nsudo yum upgrade\nsudo yum install python36-devel mesa-libGL\n```\n\n----------------------------------------\n\nTITLE: Configuring Blob Dumping\nDESCRIPTION: These environment variables enable and configure blob dumping, allowing the capture of intermediate data blobs from specified nodes during execution. The `OV_CPU_BLOB_DUMP_NODE_NAME` specifies the node names to dump blobs from (using '*' dumps all nodes), and `OV_CPU_BLOB_DUMP_DIR` specifies the directory to store the dumped blobs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nOV_CPU_BLOB_DUMP_NODE_NAME=\"*\" OV_CPU_BLOB_DUMP_DIR=blob_dump\n```\n\n----------------------------------------\n\nTITLE: LSTMSequence Layer Configuration in OpenVINO (C++)\nDESCRIPTION: This code snippet demonstrates the configuration of an LSTMSequence layer within the OpenVINO framework using C++. It defines the layer's attributes, input ports with their dimensions, and output ports with their corresponding dimensions. The example showcases how to define the input and output tensor shapes, the hidden size and directions of the LSTM sequence layer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/lstm-sequence-5.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"LSTMSequence\" ...>\n       <data hidden_size=\"128\"/>\n       <input>\n           <port id=\"0\">\n               <dim>1</dim>\n               <dim>4</dim>\n               <dim>16</dim>\n           </port>\n           <port id=\"1\">\n               <dim>1</dim>\n               <dim>1</dim>\n               <dim>128</dim>\n           </port>\n           <port id=\"2\">\n               <dim>1</dim>\n               <dim>1</dim>\n               <dim>128</dim>\n           </port>\n           <port id=\"3\">\n               <dim>1</dim>\n           </port>\n            <port id=\"4\">\n               <dim>1</dim>\n               <dim>512</dim>\n               <dim>16</dim>\n           </port>\n            <port id=\"5\">\n               <dim>1</dim>\n               <dim>512</dim>\n               <dim>128</dim>\n           </port>\n            <port id=\"6\">\n               <dim>1</dim>\n               <dim>512</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"7\">\n               <dim>1</dim>\n               <dim>1</dim>\n               <dim>4</dim>\n               <dim>128</dim>\n           </port>\n           <port id=\"8\">\n               <dim>1</dim>\n               <dim>1</dim>\n               <dim>128</dim>\n           </port>\n           <port id=\"9\">\n               <dim>1</dim>\n               <dim>1</dim>\n               <dim>128</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Create RemoteTensor from VASurfaceID (C++)\nDESCRIPTION: This C++ code creates OpenVINO RemoteTensors from a VASurfaceID, enabling integration with video acceleration APIs.  It uses the VAContext to wrap the VASurfaceID into RemoteTensors representing the Y and UV planes of an NV12 surface. Preprocessing is applied to the model. Requires OpenVINO, DirectX 11, VA-API, and OpenCL libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_29\n\nLANGUAGE: cpp\nCODE:\n```\nusing namespace ov::preprocess;\nauto p = PrePostProcessor(model);\np.input().tensor().set_element_type(ov::element::u8)\n                  .set_color_format(ov::preprocess::ColorFormat::NV12_TWO_PLANES, {\"y\", \"uv\"})\n                  .set_memory_type(ov::intel_gpu::memory_type::surface);\np.input().preprocess().convert_color(ov::preprocess::ColorFormat::BGR);\np.input().model().set_layout(\"NCHW\");\nmodel = p.build();\n\nCComPtr<ID3D11Device> device_ptr = get_d3d_device_ptr()\n// create the shared context object\nauto shared_va_context = ov::intel_gpu::ocl::VAContext(core, device_ptr);\n// compile model within a shared context\nauto compiled_model = core.compile_model(model, shared_va_context);\n\nauto param_input_y = model->get_parameters().at(0);\nauto param_input_uv = model->get_parameters().at(1);\n\nauto shape = param_input_y->get_shape();\nauto width = shape[1];\nauto height = shape[2];\n\nVASurfaceID va_surface = decode_va_surface();\n//     ...\n//wrap decoder output into RemoteBlobs and set it as inference input\nauto nv12_blob = shared_va_context.create_tensor_nv12(height, width, va_surface);\n\nauto infer_request = compiled_model.create_infer_request();\ninfer_request.set_tensor(param_input_y->get_friendly_name(), nv12_blob.first);\ninfer_request.set_tensor(param_input_uv->get_friendly_name(), nv12_blob.second);\ninfer_request.start_async();\ninfer_request.wait();\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO with specific configuration using vcpkg triplets\nDESCRIPTION: This command installs OpenVINO with a specific configuration using vcpkg triplets.  For example, installing OpenVINO statically on Windows.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-vcpkg.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nvcpkg install 'openvino:x64-windows-static'\n```\n\n----------------------------------------\n\nTITLE: Benchmark Application Output in C++ (sh)\nDESCRIPTION: This snippet represents the output log of the benchmark_app run from a C++ context. It displays information about tensor configuration and performance metrics such as iterations, duration, latency, and throughput for different input data shapes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_24\n\nLANGUAGE: sh\nCODE:\n```\n[Step 9/11] Creating infer requests and preparing input tensors\n[ INFO ] Test Config 0\n[ INFO ] input  ([N,C,D,H,W], f32, {1, 3, 16, 224, 224}, dyn:{?,3,16,224,224}): random (binary data is expected)\n[ INFO ] Test Config 1\n[ INFO ] input  ([N,C,D,H,W], f32, {2, 3, 16, 224, 224}, dyn:{?,3,16,224,224}): random (binary data is expected)\n[ INFO ] Test Config 2\n[ INFO ] input  ([N,C,D,H,W], f32, {4, 3, 16, 224, 224}, dyn:{?,3,16,224,224}): random (binary data is expected)\n[Step 10/11] Measuring performance (Start inference asynchronously, 11 inference requests, limits: 60000 ms duration)\n[ INFO ] Benchmarking in full mode (inputs filling are included in measurement loop).\n[ INFO ] First inference took 204.40 ms\n[Step 11/11] Dumping statistics report\n[ INFO ] Count:        2783 iterations\n[ INFO ] Duration:     60326.29 ms\n[ INFO ] Latency:\n[ INFO ]    Median:     208.20 ms\n[ INFO ]    Average:    237.47 ms\n[ INFO ]    Min:        85.06 ms\n[ INFO ]    Max:        743.46 ms\n[ INFO ] Latency for each data shape group:\n[ INFO ] 1. input: {1, 3, 16, 224, 224}\n[ INFO ]    Median:     120.36 ms\n[ INFO ]    Average:    117.19 ms\n[ INFO ]    Min:        85.06 ms\n[ INFO ]    Max:        348.66 ms\n[ INFO ] 2. input: {2, 3, 16, 224, 224}\n[ INFO ]    Median:     207.81 ms\n[ INFO ]    Average:    206.39 ms\n[ INFO ]    Min:        167.19 ms\n[ INFO ]    Max:        578.33 ms\n[ INFO ] 3. input: {4, 3, 16, 224, 224}\n[ INFO ]    Median:     387.40 ms\n[ INFO ]    Average:    388.99 ms\n[ INFO ]    Min:        327.50 ms\n[ INFO ]    Max:        743.46 ms\n[ INFO ] Throughput:   107.61 FPS\n```\n\n----------------------------------------\n\nTITLE: Apply LowLatency2 Transformation with Parameters in Python\nDESCRIPTION: This snippet demonstrates how to apply the LowLatency2 transformation with optional parameters in Python, specifically disabling the constant initializer. It uses `ov.pass.LowLatency2(use_const_initializer=False)` to prevent the insertion of a constant subgraph.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nov.pass.LowLatency2(use_const_initializer=False).run_on_model(model)\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Tokenizers with Transformers Support\nDESCRIPTION: Installs the openvino-tokenizers package with the 'transformers' extra. This includes dependencies required for converting and using Hugging Face tokenizers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npip install openvino-tokenizers[transformers]\n```\n\n----------------------------------------\n\nTITLE: Tile Example 2: Repeats larger than data shape in XML\nDESCRIPTION: Example demonstrating the Tile operation where the number of elements in the 'repeats' input is greater than the shape of the 'data' input. The 'data' input shape is promoted before tiling. The XML configuration shows the input and output tensor dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/tile-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Tile\">\n    <input>\n        <port id=\"0\">  <!-- will be promoted to shape (1, 2, 3, 4) -->\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>  <!-- [5, 1, 2, 3] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>5/dim>\n            <dim>2</dim>\n            <dim>6</dim>\n            <dim>12</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Cubic Interpolation Implementation\nDESCRIPTION: Implements cubic interpolation on the input data.  It calculates the interpolated value for each output coordinate by summing the contributions from neighboring input pixels, weighted by cubic coefficients. It uses the `get_original_coordinate` function to map output coordinates to input coordinates and `clip_coord` to avoid out-of-bounds accesses. Requires the `get_cubic_coeff` function to be defined elsewhere.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef cubic_interpolation(self, input_data):\n    rank = len(self.input_shape)\n    result = np.zeros(self.output_shape)\n    num_of_axes = len(self.axes)\n    indices = [ind for ind in np.ndindex(tuple(4 for _ in range(num_of_axes)))]\n    for coordinates in np.ndindex(tuple(self.output_shape)):\n        input_coords = np.array(coordinates, dtype=np.int64)\n        cubic_coeffs = np.zeros((rank, 4))\n        for i, axis in enumerate(self.axes):\n            in_coord = self.get_original_coordinate(coordinates[axis], self.scales[i], self.output_shape[axis], self.input_shape[axis])\n            in_coord_int = math.floor(in_coord)\n            input_coords[axis] = in_coord_int\n            cubic_coeffs[axis] = get_cubic_coeff(in_coord - in_coord_int, self.cube_coeff)\n        summa = 0.0\n        for index in indices:\n            coords_for_sum = input_coords.copy()\n            coeffs_prod = 1.0\n            for i, axis in enumerate(self.axes):\n                coords_for_sum[axis] = self.clip_coord(input_coords[axis] + index[i] - 1, axis)\n            for i, axis in enumerate(self.axes):\n                coeffs_prod = coeffs_prod * cubic_coeffs[axis][index[i]]\n            summa += coeffs_prod * input_data[tuple(coords_for_sum)]\n        result[coordinates] = summa\n    return result\n```\n\n----------------------------------------\n\nTITLE: DFT Layer XML Definition (signal_size, 5D input, -1)\nDESCRIPTION: Defines a DFT layer with a 5-dimensional input tensor, including a signal_size input with '-1' to indicate full size on a specific axis and unsorted axes. This example demonstrates a complex DFT scenario.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/dft-7.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"DFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>580</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim> <!-- axes input contains  [3, 1, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim> <!-- signal_size input contains [170, -1, 1024] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>1024</dim>\n            <dim>170</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: TensorIterator Structure Example (XML)\nDESCRIPTION: This XML example illustrates a typical structure of a TensorIterator layer. It includes the input and output ports, port mappings for connecting external ports to internal layers, back edges for recurrent connections, and the definition of the body network with its layers and edges. This example showcases the essential components of a TensorIterator in OpenVINO's Intermediate Representation (IR).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/tensor-iterator-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"TensorIterator\" ... >\n    <input> ... </input>\n    <output> ... </output>\n    <port_map>\n        <input external_port_id=\"0\" internal_layer_id=\"0\" axis=\"1\" start=\"-1\" end=\"0\" stride=\"-1\"/>\n        <input external_port_id=\"1\" internal_layer_id=\"1\"/>\n        ...\n        <output external_port_id=\"3\" internal_layer_id=\"2\" axis=\"1\" start=\"-1\" end=\"0\" stride=\"-1\"/>\n        ...\n    </port_map>\n    <back_edges>\n        <edge from-layer=\"1\" to-layer=\"1\"/>\n        ...\n    </back_edges>\n    <body>\n        <layers> ... </layers>\n        <edges> ... </edges>\n    </body>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Retrieve Specific Input/Output Port by Name (Python)\nDESCRIPTION: This Python snippet shows how to retrieve a specific input or output port of an OpenVINO model using its tensor name. It relies on the availability and correctness of the tensor names within the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nov_model_input = model.input(original_fw_in_tensor_name)\nov_model_output = model.output(original_fw_out_tensor_name)\n```\n\n----------------------------------------\n\nTITLE: Install Intel OpenCL Drivers on Ubuntu\nDESCRIPTION: This code snippet provides the command to install Intel OpenCL drivers on Ubuntu 20.04. This is necessary to enable inference on Intel Integrated Graphics Cards. It is recommended to execute this command only if OpenCL drivers have not been installed yet.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nsudo apt-get install intel-opencl-icd\n```\n\n----------------------------------------\n\nTITLE: Running OCR Sample with Node.js\nDESCRIPTION: This command executes the optical-character-recognition.js script using Node.js. It requires four command-line arguments: the path to the detection model XML file, the path to the recognition model XML file, the path to the input image, and the device to run the inference on (e.g., AUTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/js/node/optical_character_recognition/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnode optical-character-recognition.js ../../assets/models/horizontal-text-detection-0001.xml ../../assets/models/text-recognition-resnet-fc.xml ../../assets/images/intel_rnb.jpg AUTO\n```\n\n----------------------------------------\n\nTITLE: TorchFX: Run Inference\nDESCRIPTION: Runs inference on a quantized TorchFX model. The provided code shows how to load the quantized model, prepare the input, and obtain the inference results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmodel.eval()\nresults = model(input_tensor)\nprint(results)\n\n```\n\n----------------------------------------\n\nTITLE: NpyArray Structure Definition in C++\nDESCRIPTION: This code snippet defines the NpyArray struct, which is used to store data loaded from .npy and .npz files. It contains the shape of the array (a vector of size_t), the size of each element in bytes (word_size), and a template method `data<T>()` that returns a pointer to the underlying data, cast to the specified type `T`. The caller must ensure that `T` matches the actual data type stored in the file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/cnpy/README.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nstruct NpyArray {\n    std::vector<size_t> shape;\n    size_t word_size;\n    template<typename T> T* data();\n};\n```\n\n----------------------------------------\n\nTITLE: Running a Specific Test with Pytest\nDESCRIPTION: This snippet illustrates how to execute a single test file using `pytest`. It specifies the test file `test_classification_sample_async.py` directly, allowing for targeted testing of a specific sample or functionality. This can be useful for debugging or focused validation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/samples_tests/smoke_tests/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest test_classification_sample_async.py\n```\n\n----------------------------------------\n\nTITLE: Create Python Virtual Environment (Linux/macOS)\nDESCRIPTION: This command creates a Python 3 virtual environment named 'openvino_env' on Linux or macOS.  Using a virtual environment isolates project dependencies and avoids conflicts with other Python projects.  It requires Python 3 to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/pypi_publish/pypi-openvino-rt.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython3 -m venv openvino_env\n```\n\n----------------------------------------\n\nTITLE: LRN Layer Example (XML)\nDESCRIPTION: Illustrates the structure of an LRN layer definition in XML format.  It defines the layer's attributes (alpha, beta, size, bias), input ports with their dimensions, and the output port with its dimensions, providing a concrete example of how to configure an LRN operation within the OpenVINO framework.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/lrn-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"LRN\" ...>\n    <data alpha=\"1.0e-04\" beta=\"0.75\" size=\"5\" bias=\"1\"/>\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim> <!-- value is [1] that means independent normalization for each pixel along channels -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Indentation\nDESCRIPTION: This rule enforces the use of two spaces for indentation in JavaScript and TypeScript code. Consistent indentation improves code readability and is enforced by ESLint with the configuration `indent: ['error', 2]`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nindent: ['error', 2]\n```\n\n----------------------------------------\n\nTITLE: Defining Package Requirements\nDESCRIPTION: This snippet defines the required packages for the OpenVINO Toolkit project. It specifies datasets with a version greater than or equal to 2.13, and transformers with ONNX support for Python versions less than 3.11.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/python/benchmark/bert_benchmark/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\ndatasets>=2.13\ntransformers[onnx]; python_version < \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: OpenVINO: Run Inference\nDESCRIPTION: Performs inference using the quantized OpenVINO model.  The code compiles the model and runs it on a sample input to demonstrate the end-to-end quantization and inference process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncompiled_model = ie_core.compile_model(quantized_model, device_name='CPU')\nresults = compiled_model(input_tensor)\n\n```\n\n----------------------------------------\n\nTITLE: Setting environment variables on Windows (cmd.exe)\nDESCRIPTION: This snippet demonstrates setting environment variables using the `cmd.exe` shell on Windows before running `benchmark_app`. This method is an alternative to PowerShell for enabling debug options.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_debug_utils.md#_snippet_2\n\nLANGUAGE: cmd\nCODE:\n```\nset \"OV_VERBOSE=1\"\nbenchmark_app.exe ...      # Run benchmark_app with OV_VERBOSE option\n```\n\n----------------------------------------\n\nTITLE: Set Compile Definitions for OpenVINO Proxy Plugin\nDESCRIPTION: This snippet sets compile definitions for the OpenVINO proxy plugin.  It defines `IMPLEMENT_OPENVINO_RUNTIME_API` to indicate that this library implements the OpenVINO Runtime API, and `CI_BUILD_NUMBER` to embed the build number into the library. The `CI_BUILD_NUMBER` is escaped to handle potential special characters within the value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME} PRIVATE\n        IMPLEMENT_OPENVINO_RUNTIME_API\n        CI_BUILD_NUMBER=\\\"${CI_BUILD_NUMBER}\\\")\n```\n\n----------------------------------------\n\nTITLE: Standalone Build: Verify Single Image Test Tool Installation\nDESCRIPTION: This snippet demonstrates how to verify the installation of the Single Image Test Tool. It requires sourcing the OpenVINO and OpenCV setup scripts and then executing the `single-image-test` executable with the `-help` flag.  Successful execution will display the CLI options for the tool. The variables `<openvino_install_dir>`, `<opencv_install_dir>`, and `<sit_install_dir>` need to be replaced with the actual paths.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsource <openvino_install_dir>/setupvars.sh\nsource <opencv_install_dir>setup_vars_opencv4.sh\n<sit_install_dir>/tools/single-image-test/single-image-test -help\n```\n\n----------------------------------------\n\nTITLE: OVSA Inference Execution\nDESCRIPTION: Runs the detection.py script to perform inference with the controlled-access model using the OpenVINO Model Server. It specifies various parameters such as the gRPC port, image dimensions, input and output directories, TLS settings, and the model name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_59\n\nLANGUAGE: sh\nCODE:\n```\npython3 detection.py --grpc_port 3335 --batch_size 1 --width 300 --height 300 --input_images_dir images --output_dir results --tls --server_cert /var/OVSA/Modelserver/server.pem --client_cert /var/OVSA/Modelserver/client.pem --client_key /var/OVSA/Modelserver/client.key --model_name controlled-access-model\n```\n\n----------------------------------------\n\nTITLE: Define ov_profiling_info_list struct in C\nDESCRIPTION: This struct represents a list of profiling information entries. It contains an array of `ov_profiling_info_t` structs and the size of the array.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_8\n\nLANGUAGE: C\nCODE:\n```\ntypedef struct {\n\n    ov_profiling_info_t* profiling_infos;\n\n    size_t size;\n\n} ov_profiling_info_list_t;\n```\n\n----------------------------------------\n\nTITLE: Clone with New Inputs in C++\nDESCRIPTION: This snippet demonstrates how to override the `clone_with_new_inputs` method in C++ for a custom OpenVINO operation. The method creates a copy of the operation with new inputs, enabling graph manipulation routines to connect the operation to different nodes during optimization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-openvino-operations.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n    std::shared_ptr<ov::Node> clone_with_new_inputs(const ov::OutputVector& new_args) const override {\n        OPENVINO_ASSERT(new_args.size() == 1, \"Expected 1 element in new_args, but got \", new_args.size());\n        return std::make_shared<Identity>(new_args.at(0));\n    };\n```\n\n----------------------------------------\n\nTITLE: Define ov_profiling_info struct in C\nDESCRIPTION: This struct contains profiling information for a specific node in the model graph. It includes the status of the node (e.g., executed, optimized out), execution times, and node identification details.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_7\n\nLANGUAGE: C\nCODE:\n```\ntypedef struct {\n\n    enum Status {\n\n        NOT_RUN,\n\n        OPTIMIZED_OUT,\n\n        EXECUTED\n\n    } status;\n\n    int64_t real_time;\n\n    int64_t cpu_time;\n\n    const char* node_name;\n\n    const char* exec_type;\n\n    const char* node_type;\n\n} ov_profiling_info_t;\n```\n\n----------------------------------------\n\nTITLE: Get Element Type in OpenVINO\nDESCRIPTION: This snippet shows how to get the element type of an input in OpenVINO using both Python and C++. The element type represents the data type of the input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nov_input.get_element_type()\n```\n\n----------------------------------------\n\nTITLE: Running a Python Sample (macOS)\nDESCRIPTION: This command executes a Python sample. It takes the path to the Python file, the model, the input media, and the target device as arguments. This is for macOS.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_25\n\nLANGUAGE: sh\nCODE:\n```\npython <sample.py file> -m <path_to_model> -i <path_to_media> -d <target_device>\n```\n\n----------------------------------------\n\nTITLE: Create Empty String Tensor (C++)\nDESCRIPTION: Creates an OpenVINO tensor of a specified shape with empty strings as elements. Requires the `openvino/openvino.hpp` header.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/string-tensors.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nov::Tensor tensor(ov::element::string, ov::Shape{3});\n```\n\n----------------------------------------\n\nTITLE: MaxPool with 4D input, 2D kernel, explicit padding (sh)\nDESCRIPTION: Demonstrates MaxPool operation on a 4D input tensor using a 2D kernel with explicit padding. It showcases the input tensor, strides, padding, kernel size, rounding type, auto padding, and the resulting output tensor and indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-8.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[-1, 2, 3],\n                 [4, 5, -6],\n                 [-7, 8, 9]]]]\nstrides = [1, 1]\npads_begin = [1, 1]\npads_end = [1, 1]\nkernel = [2, 2]\nrounding_type = \"floor\"\nauto_pad = \"explicit\"\noutput0 = [[[[-1, 2, 3, 3],\n                   [4, 5, 5, -6],\n                   [4, 8, 9, 9],\n                   [-7, 8, 9, 9]]]]\noutput1 = [[[[0, 1, 2, 2],\n                   [3, 4, 4, 5],\n                   [3, 7, 8, 8],\n                   [6, 7, 8, 8]]]]\n```\n\n----------------------------------------\n\nTITLE: Using Optional as the root node in a pattern\nDESCRIPTION: This C++ snippet demonstrates how to use Optional as the root node, allowing the pattern to optionally end with the specified node.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\nshared_ptr<ov::Node> pattern_optional_root() {\n    // Creating nodes\n    auto relu_node = make_shared<WrapType<opset13::Relu>>();\n    auto optional_node = make_shared<Optional>(relu_node);\n\n    return optional_node;\n}\n```\n\n----------------------------------------\n\nTITLE: Add Subdirectories in CMake\nDESCRIPTION: This snippet uses the `add_subdirectory` command to include specified subdirectories in the build process. This allows for modular organization of the project and inclusion of separate components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(utils)\nadd_subdirectory(al)\n```\n\n----------------------------------------\n\nTITLE: Dump IR at specified stages OpenVINO (sh)\nDESCRIPTION: This snippet illustrates how to dump the IR at specific stages using the OV_CPU_DUMP_IR environment variable. It showcases different options for specifying the transformation stages, output directory, and IR formats. The option names are case-insensitive and processed from left to right.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/graph_serialization.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_DUMP_IR=<space_separated_options> binary ...\n```\n\n----------------------------------------\n\nTITLE: Evaluate in Python\nDESCRIPTION: This snippet demonstrates how to override the `evaluate` method in Python for a custom OpenVINO operation. The `evaluate` method with the code that will run when this operation is encountered in the model graph during the model inference. It works only for CPU device and enables OpenVINO runtime to run your arbitrary Python code as a part of model inference. Also it demonstrates how to override the ``has_evaluate`` method which returns `True` to indicate the availability of ``evaluate`` method for the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-openvino-operations.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n    def evaluate(self, arg1):\n        \"\"\"Evaluate the node.\n\n        It enables OpenVINO runtime to execute this operation as a part of model inference.\n        Works only for CPU device.\n        \"\"\"\n        # The inputs are passed as numpy arrays.\n        # The output must be returned as a numpy array.\n        return arg1 + self._val\n\n    def has_evaluate(self):\n        \"\"\"Indicate that evaluate() method is available.\n\n        It enables the OpenVINO runtime to query whether evaluate() method is implemented for this operation.\n        \"\"\"\n        return True\n```\n\n----------------------------------------\n\nTITLE: Python Image Conversion Example\nDESCRIPTION: This Python code snippet demonstrates how to convert a model to the OpenVINO format using the `openvino` library. It shows how to convert a model from a file path or a Python model object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/image-classification-async.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino as ov\n\nov_model = ov.convert_model('./models/alexnet')\n# or, when model is a Python model object\nov_model = ov.convert_model(alexnet)\n```\n\n----------------------------------------\n\nTITLE: Running Fuzzing (Bash)\nDESCRIPTION: This command runs the fuzzing test executable, specifying a maximum total time and the directory containing the fuzzing corpus. It also uses `LD_PRELOAD` to ensure the ASAN library is pre-loaded when OpenVINO is built as a shared library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# LD_PRELOAD is required when OpenVINO build as shared library, the ASAN library has to be pre-loaded.\n[LD_PRELOAD=path-to-asan-lib] ./read_network-fuzzer -max_total_time=600 ./read_network-corpus\n```\n\n----------------------------------------\n\nTITLE: Benchmark Application Output (sh)\nDESCRIPTION: This snippet represents the output log of the benchmark_app. It shows the steps involved in creating infer requests, preparing input tensors, measuring performance, and dumping statistics. The log provides information about the number of iterations, duration, latency, and throughput for different input shapes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_22\n\nLANGUAGE: sh\nCODE:\n```\n[Step 9/11] Creating infer requests and preparing input tensors\n[ WARNING ] No input files were given for input 'input'!. This input will be filled with random values!\n[ INFO ] Fill input 'input' with random values\n[ INFO ] Defined 3 tensor groups:\n[ INFO ]         input: {1, 3, 16, 224, 224}\n[ INFO ]         input: {2, 3, 16, 224, 224}\n[ INFO ]         input: {4, 3, 16, 224, 224}\n[Step 10/11] Measuring performance (Start inference asynchronously, 11 inference requests, limits: 60000 ms duration)\n[ INFO ] Benchmarking in full mode (inputs filling are included in measurement loop).\n[ INFO ] First inference took 201.15 ms\n[Step 11/11] Dumping statistics report\n[ INFO ] Count:        2811 iterations\n[ INFO ] Duration:     60271.71 ms\n[ INFO ] Latency:\n[ INFO ]    Median:     207.70 ms\n[ INFO ]    Average:    234.56 ms\n[ INFO ]    Min:        85.73 ms\n[ INFO ]    Max:        773.55 ms\n[ INFO ] Latency for each data shape group:\n[ INFO ] 1. input: {1, 3, 16, 224, 224}\n[ INFO ]    Median:     118.08 ms\n[ INFO ]    Average:    115.05 ms\n[ INFO ]    Min:        85.73 ms\n[ INFO ]    Max:        339.25 ms\n[ INFO ] 2. input: {2, 3, 16, 224, 224}\n[ INFO ]    Median:     207.25 ms\n[ INFO ]    Average:    205.16 ms\n[ INFO ]    Min:        166.98 ms\n[ INFO ]    Max:        545.55 ms\n[ INFO ] 3. input: {4, 3, 16, 224, 224}\n[ INFO ]    Median:     384.16 ms\n[ INFO ]    Average:    383.48 ms\n[ INFO ]    Min:        305.51 ms\n[ INFO ]    Max:        773.55 ms\n[ INFO ] Throughput:   108.82 FPS\n```\n\n----------------------------------------\n\nTITLE: Running Model Creation Sample (Python)\nDESCRIPTION: This console command demonstrates how to run the `model_creation_sample.py` script. It requires the path to the LeNet weights file (`lenet.bin`) and the device name (e.g., `GPU`).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/model-creation.rst#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npython model_creation_sample.py <path_to_weights_file> <device_name>\n```\n\n----------------------------------------\n\nTITLE: Run with Multiple Model Specifications (C++)\nDESCRIPTION: This command demonstrates that when multiple model files are given, the last one specified will be used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n./benchmark_app -m model.xml -m model2.xml\n```\n\n----------------------------------------\n\nTITLE: Visualize Model Graph (Python)\nDESCRIPTION: This snippet demonstrates how to visualize an OpenVINO model graph by converting it to an image using Python.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# [ov:visualize]\n```\n\n----------------------------------------\n\nTITLE: Kernel parameter access in SnippetS CPU implementation\nDESCRIPTION: This code snippet demonstrates how the first few kernel parameters are accessed in the SnippetS CPU implementation. The first 8 parameters are passed through a structure and unpacked into registers, while the rest are passed through the stack. The loop trip count and work amount should be placed in GP registers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/docs/snippets_cpu_target.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nauto param0 = abi_params[0];\nauto param1 = abi_params[1];\nauto result = abi_params[2];\n\nauto work_amount = abi_params[3];\n```\n\n----------------------------------------\n\nTITLE: Inverse Layer Definition (2D input)\nDESCRIPTION: This XML code defines an Inverse layer with a 2D input matrix (3x3). The input and output ports are specified with precision FP32 and dimensions representing the rows and columns of the square matrix.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/inverse-14.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... name=\"Inverse\" type=\"Inverse\">\n    <data/>\n    <input>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>3</dim> <!-- 3 rows of square matrix -->\n            <dim>3</dim> <!-- 3 columns of square matrix -->\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"FP32\" names=\"Inverse:0\">\n            <dim>3</dim> <!-- 3 rows of square matrix -->\n            <dim>3</dim> <!-- 3 columns of square matrix -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Build OpenVINO C++ samples\nDESCRIPTION: Executes the `build_samples.sh` script to build the OpenVINO C++ sample applications. Requires the OpenVINO Runtime and build tools to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\n/usr/share/openvino/samples/cpp/build_samples.sh\n```\n\n----------------------------------------\n\nTITLE: Checking PIP Packages CMake\nDESCRIPTION: Checks if required Python packages are installed using `ov_check_pip_packages`.  If the necessary packages are missing, it issues a warning message indicating that TensorFlow testing models will not be generated. The `tensorflow_FOUND` variable determines if TensorFlow is properly set up.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nov_check_pip_packages(REQUIREMENTS_FILE \"${CMAKE_CURRENT_SOURCE_DIR}/requirements.txt\"\n                      MESSAGE_MODE WARNING\n                      WARNING_MESSAGE \"TensorFlow testing models weren't generated, some tests will fail due models not found\"\n                      RESULT_VAR tensorflow_FOUND)\n\nif(tensorflow_FOUND)\n    set(ctest_labels OV UNIT)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Create RemoteTensor from ID3D11Buffer (C++)\nDESCRIPTION: This C++ code snippet demonstrates how to create an OpenVINO RemoteTensor from an ID3D11Buffer using the D3DContext. The D3DContext is obtained from a compiled OpenVINO model. The ID3D11Buffer is then wrapped into a RemoteTensor and set as input for the inference request. Requires OpenVINO, DirectX 11, and OpenCL libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_27\n\nLANGUAGE: cpp\nCODE:\n```\n// ...\n\n// initialize the core and load the network\nov::Core core;\nauto model = core.read_model(\"model.xml\");\nauto compiled_model = core.compile_model(model, \"GPU\");\nauto infer_request = compiled_model.create_infer_request();\n\n\n// obtain the RemoteContext from the compiled model object and cast it to D3DContext\nauto gpu_context = compiled_model.get_context().as<ov::intel_gpu::ocl::D3DContext>();\n\nauto input = model->get_parameters().at(0);\nID3D11Buffer* d3d_handle = get_d3d_buffer();\nauto tensor = gpu_context.create_tensor(input->get_element_type(), input->get_shape(), d3d_handle);\ninfer_request.set_tensor(input, tensor);\n```\n\n----------------------------------------\n\nTITLE: Gather-7 Layer Configuration in CPP\nDESCRIPTION: Illustrates the layer configuration for Gather-7 in a C++ style XML format, specifying input and output port dimensions, as well as the batch_dims attribute.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-7.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"Gather\" version=\"opset7\">\n    <data batch_dims=\"1\" />\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>64</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>\n            <dim>32</dim>\n            <dim>21</dim>\n        </port>\n        <port id=\"2\"/>   <!--  axis = 1  -->\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>2</dim>\n            <dim>32</dim>\n            <dim>21</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: GatherTree Algorithm Pseudocode in Python\nDESCRIPTION: This pseudocode demonstrates the algorithm of the GatherTree operation. It initializes the final_ids tensor with the end_token, then iterates through batches and beams to reorder token IDs based on parent IDs, filling past end tokens with end_token values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-tree-1.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nfinal_ids[ :, :, :] = end_token\nfor batch in range(BATCH_SIZE):\n    for beam in range(BEAM_WIDTH):\n        max_sequence_in_beam = min(MAX_TIME, max_seq_len[batch])\n\n        parent = parent_ids[max_sequence_in_beam - 1, batch, beam]\n\n        final_ids[max_sequence_in_beam - 1, batch, beam] = step_ids[max_sequence_in_beam - 1, batch, beam]\n\n        for level in reversed(range(max_sequence_in_beam - 1)):\n            final_ids[level, batch, beam] = step_ids[level, batch, parent]\n\n            parent = parent_ids[level, batch, parent]\n\n        # For a given beam, past the time step containing the first decoded end_token\n        # all values are filled in with end_token.\n        finished = False\n        for time in range(max_sequence_in_beam):\n            if(finished):\n                final_ids[time, batch, beam] = end_token\n            elif(final_ids[time, batch, beam] == end_token):\n                finished = True\n```\n\n----------------------------------------\n\nTITLE: Free Const Output Port in OpenVINO (C)\nDESCRIPTION: This function releases the memory associated with a const output port object. It requires a pointer to an `ov_output_const_port_t` as input. The function does not return any value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_41\n\nLANGUAGE: C\nCODE:\n```\nvoid ov_output_const_port_free(ov_output_const_port_t* port)\n```\n\n----------------------------------------\n\nTITLE: Benchmark Application Usage Message\nDESCRIPTION: This snippet displays the usage message for the benchmark application, outlining available options and their syntax. It covers essential parameters like the model path, input data, target device, and performance hints.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_11\n\nLANGUAGE: Text\nCODE:\n```\nusage: benchmark_app [OPTION]\n\nOptions:\n    -h, --help                    Print the usage message\n    -m  <path>                    Required. Path to an .xml/.onnx file with a trained model or to a .blob files with a trained compiled model.\n    -i  <path>                    Optional. Path to a folder with images and/or binaries or to specific image or binary file.\n                                  In case of dynamic shapes models with several inputs provide the same number of files for each input (except cases with single file for any input)   :\"input1:1.jpg input2:1.bin\", \"input1:1.bin,2.bin input2:3.bin input3:4.bin,5.bin \". Also you can pass specific keys for inputs: \"random\" - for    fillling input with random data, \"image_info\" - for filling input with image size.\n                                  You should specify either one files set to be used for all inputs (without providing input names) or separate files sets for every input of model    (providing inputs names).\n                                  Currently supported data types: bmp, bin, npy.\n                                  If OPENCV is enabled, this functionality is extended with the following data types:\n                                  dib, jpeg, jpg, jpe, jp2, png, pbm, pgm, ppm, sr, ras, tiff, tif.\n    -d  <device>                  Optional. Specify a target device to infer on (the list of available devices is shown below). Default value is CPU. Use \"-d    HETERO:<comma-separated_devices_list>\" format to specify HETERO plugin. Use \"-d MULTI:<comma-separated_devices_list>\" format to specify MULTI plugin. The application looks for    a suitable plugin for the specified device.\n    -hint  <performance hint> (latency or throughput or cumulative_throughput or none)   Optional. Performance hint allows the OpenVINO device to select the right model-specific    settings.\n                                   'throughput' or 'tput': device performance mode will be set to THROUGHPUT.\n```\n\n----------------------------------------\n\nTITLE: Gather Operation Example 4\nDESCRIPTION: Illustrates the Gather operation where the axis value is greater than the batch_dims value. This example showcases the operation with multi-dimensional data and indices tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-8.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 1\naxis = 2\n\nindices = [[1, 2, 4],  <-- this is applied to the first batch\n           [4, 3, 2]]  <-- this is applied to the second batch\nindices_shape = (2, 3)\n\ndata = [[[[ 1,  2,  3,  4], <-- first batch\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12],\n          [13, 14, 15, 16],\n          [17, 18, 19, 20]]],\n\n        [[[21, 22, 23, 24], <-- second batch\n          [25, 26, 27, 28],\n          [29, 30, 31, 32],\n          [33, 34, 35, 36],\n          [37, 38, 39, 40]]]]\ndata_shape = (2, 1, 5, 4)\n\noutput = [[[[ 5,  6,  7,  8],\n            [ 9, 10, 11, 12],\n            [17, 18, 19, 20]]],\n\n          [[[37, 38, 39, 40],\n            [33, 34, 35, 36],\n            [29, 30, 31, 32]]]]\noutput_shape = (2, 1, 3, 4)\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries\nDESCRIPTION: Conditionally appends the `openvino::commonTestUtils` or `common_test_utils` library to the `link_libraries` list, depending on whether the `OpenVINODeveloperPackage_FOUND` variable is set. This ensures the library links against the appropriate test utilities.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(OpenVINODeveloperPackage_FOUND)\n    list(APPEND link_libraries openvino::commonTestUtils)\nelse()\n    list(APPEND link_libraries common_test_utils)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Specifying Source File for CMake Target\nDESCRIPTION: This command associates the source file 'main.cpp' with the 'gtest_main_manifest' target. The source file is located in the current source directory. The INTERFACE keyword indicates that the source file is used by targets that link against this library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/gtest_main_manifest/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_sources(${TARGET_NAME} INTERFACE ${CMAKE_CURRENT_SOURCE_DIR}/main.cpp)\n```\n\n----------------------------------------\n\nTITLE: get_profiling_info() Method C++\nDESCRIPTION: This code snippet shows the implementation of the `get_profiling_info()` method. It returns the profiling information measured during the execution of the pipeline stages, providing insights into the performance characteristics of the inference process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/synch-inference-request.rst#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nstd::vector<ProfilingInfo> InferRequest::get_profiling_info() {\n    // Returns the profiling info which was measured during pipeline stages execution\n    return m_profiling_info;\n}\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate 3D Tensor Update - C++ (axis=1)\nDESCRIPTION: This code snippet shows how the ScatterElementsUpdate operation updates a 3D tensor when axis is set to 1. It demonstrates the element update logic based on the indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i][indices[i][j][k]][k] = reduction(updates[i][j][k], output[i][indices[i][j][k]][k]) if axis = 1\n```\n\n----------------------------------------\n\nTITLE: Dumping Layout to String in Python\nDESCRIPTION: This snippet illustrates how to convert a layout object to a string representation in Python using `str(layout)`. This can be helpful for debugging and serialization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlayout = ov.Layout(\"NCHW\")\nlayout_str = str(layout)\n```\n\n----------------------------------------\n\nTITLE: Setup Environment Variables Command Prompt\nDESCRIPTION: Sets the environment variables for OpenVINO using the setupvars.bat batch file in Command Prompt.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-windows.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n\"C:\\Program Files (x86)\\Intel\\openvino_2025\\setupvars.bat\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Exposed Class Python\nDESCRIPTION: Shows how to access the `MyTensor` class from the `openvino.runtime` module after it has been exposed in the `__init__.py` file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\nov.MyTensor\n>>> <class 'openvino._pyopenvino.mymodule.MyTensor'>\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files in CMake\nDESCRIPTION: These commands use `file(GLOB_RECURSE)` to find all `.cpp` and `.hpp` files in the specified source and include directories, respectively.  The results are stored in `LIBRARY_SRC` and `LIBRARY_HEADERS` variables, which are used when creating the library. This enables including all source and header files recursively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE LIBRARY_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/include/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example with Same Lower Padding (Shell)\nDESCRIPTION: This example showcases the MaxPool operation with a 4D input, using a 2D kernel and 'same_lower' padding. It presents the input tensor, strides, kernel size, rounding type, auto_pad setting, and the output tensor produced by the MaxPool operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-1.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[-1, 2, 3],\n               [4, 5, -6],\n               [-7, 8, 9]]]]\nstrides = [1, 1]\nkernel = [2, 2]\nrounding_type = \"floor\"\nauto_pad = \"same_lower\"\noutput = [[[[-1, 2, 3],\n                  [4, 5, 5]\n                  [4, 8, 9]]]]\n```\n\n----------------------------------------\n\nTITLE: Creating Fuzz Test Targets in CMake (Loop)\nDESCRIPTION: This `foreach` loop iterates over each source file found in the `tests` variable. For each file, it extracts the test name without the extension, adds a fuzz test target using a custom `add_fuzzer` command (not defined in the provided text), links the necessary libraries, adds a dependency to the 'fuzz' target, and installs the target to the `tests` directory. This automates the process of building and installing individual fuzz tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/src/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(test_source ${tests})\n    get_filename_component(test_name ${test_source} NAME_WE)\n    add_fuzzer(${test_name} ${test_source})\n\n    target_link_libraries(${test_name} PRIVATE\n        openvino::runtime openvino::cnpy openvino::zlib)\n\n    add_dependencies(fuzz ${test_name})\n\n    install(TARGETS ${test_name}\n            RUNTIME DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Install KVM and QEMU\nDESCRIPTION: This command installs the Kernel-based Virtual Machine (KVM), QEMU, and related packages required for virtualization. It includes libvirt-bin for managing virtual machines, bridge-utils for creating network bridges, and virt-manager for a graphical interface to manage VMs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt install qemu qemu-kvm libvirt-bin  bridge-utils  virt-manager\n```\n\n----------------------------------------\n\nTITLE: VariadicSplit XML Example 2\nDESCRIPTION: This XML example demonstrates the use of the VariadicSplit operation with a split along axis 0, using a '-1' in the split_lengths tensor. The input tensor with dimensions [6, 12, 10, 24] is split into two output tensors. The first output has dimensions [4, 12, 10, 24], where '4' is calculated as the remaining length (6 - 2) after considering the second split. The second output has dimensions [2, 12, 10, 24], corresponding to the split_lengths input [-1, 2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/variadic-split-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"VariadicSplit\" ...>\n    <input>\n        <port id=\"0\">            <!-- some data -->\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">            <!-- axis: 0 -->\n        </port>\n        <port id=\"2\">\n            <dim>2</dim>         <!-- split_lengths: [-1, 2] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>4</dim>         <!--  4 = 6 - 2  -->\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"4\">\n            <dim>2</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Using Automatic Batching as an Explicit Device (Shell)\nDESCRIPTION: This shows how to invoke the benchmark_app with the BATCH device explicitly. This enables the user to load BATCH device explicitly, providing control over the target device (GPU or CPU) and optional batch size.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n./benchmark_app -m <model> -d \"BATCH:GPU\"\n./benchmark_app -m <model> -d \"BATCH:GPU(16)\"\n./benchmark_app -m <model> -d \"BATCH:CPU(16)\"\n```\n\n----------------------------------------\n\nTITLE: Excluding Snippets AARCH64 Paths\nDESCRIPTION: This snippet excludes snippets AARCH64 specific paths when architecture is not AARCH64.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_22\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT AARCH64)\n    list(APPEND EXCLUDE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/transformations/snippets/aarch64/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/emitters/snippets/aarch64/*)\nendif()\n```\n\n----------------------------------------\n\nTITLE: ShapeOf-1 Layer XML Example\nDESCRIPTION: This XML snippet demonstrates the usage of the ShapeOf-1 layer in OpenVINO. It defines an input tensor with dimensions [2, 3, 224, 224] and specifies that the output will be a 1D tensor [2, 3, 224, 224] representing the input shape with a dimension of 4.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/shape-of-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ShapeOf\">\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">  <!-- output value is: [2,3,224,224]-->\n            <dim>4</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Synchronous Inference C API\nDESCRIPTION: This demonstrates the C API call to perform synchronous inference, using the `ov_infer_request_infer` function.  The inference is executed in a blocking manner.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/c/hello_classification/README.md#_snippet_2\n\nLANGUAGE: C\nCODE:\n```\n``ov_infer_request_infer``\n```\n\n----------------------------------------\n\nTITLE: Configuring the build with CMake\nDESCRIPTION: This command uses CMake to configure the OpenVINO build process. It specifies the Ninja Multi-Config generator and enables system-wide PugiXML, Snappy, and Protobuf libraries. It's important for setting up the build environment and defining the build rules.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\ncmake -G \"Ninja Multi-Config\" -DENABLE_SYSTEM_PUGIXML=ON -DENABLE_SYSTEM_SNAPPY=ON -DENABLE_SYSTEM_PROTOBUF=ON ..\n```\n\n----------------------------------------\n\nTITLE: Define setup.py Dependencies (CMake)\nDESCRIPTION: This macro defines the dependencies required for setup.py to build the OpenVINO Python API. It iterates through a list of targets (pyopenvino, py_ov_frontends, openvino_c, ov_plugins, ov_frontends) and adds them to the ov_setup_py_deps list if they exist. It also includes Python files, setup.py itself, requirements.txt, readme.txt, license files, and documentation files as dependencies. If wheel_pre_release is enabled, it adds a pre-release note to the dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nmacro(ov_define_setup_py_dependencies)\n    foreach(_target\n            # Python API dependencies\n            pyopenvino py_ov_frontends\n            # C API\n            openvino_c\n            # plugins\n            ov_plugins\n            # frontends\n            ov_frontends)\n        if(TARGET ${_target})\n            list(APPEND ov_setup_py_deps ${_target})\n        endif()\n    endforeach()\n\n    file(GLOB_RECURSE openvino_py_files ${OpenVINOPython_SOURCE_DIR}/src/openvino/*.py)\n    file(GLOB_RECURSE openvino_pyi_files ${OpenVINOPython_SOURCE_DIR}/src/openvino/*.pyi)\n    list(APPEND openvino_py_files ${openvino_pyi_files})\n\n    list(APPEND ov_setup_py_deps\n        ${openvino_py_files}\n        \"${CMAKE_CURRENT_SOURCE_DIR}/wheel/setup.py\"\n        \"${OpenVINOPython_SOURCE_DIR}/requirements.txt\"\n        \"${OpenVINOPython_SOURCE_DIR}/wheel/readme.txt\"\n        \"${OpenVINO_SOURCE_DIR}/LICENSE\"\n        \"${OpenVINO_SOURCE_DIR}/licensing/onednn_third-party-programs.txt\"\n        \"${OpenVINO_SOURCE_DIR}/licensing/runtime-third-party-programs.txt\"\n        \"${OpenVINO_SOURCE_DIR}/licensing/onetbb_third-party-programs.txt\"\n        \"${OpenVINO_SOURCE_DIR}/docs/dev/pypi_publish/pypi-openvino-rt.md\")\n\n    if(wheel_pre_release)\n        list(APPEND ov_setup_py_deps\n            \"${OpenVINO_SOURCE_DIR}/docs/dev/pypi_publish/pre-release-note.md\")\n    endif()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies with pip\nDESCRIPTION: Installs the required Python dependencies for the CPU Dump Check Tool using pip. The dependencies are listed in the `requirements.txt` file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/dump_check/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r ./requirements.txt\n```\n\n----------------------------------------\n\nTITLE: GPU Plugin Workflow Diagram\nDESCRIPTION: This diagram illustrates the interaction between various classes and interfaces within the OpenVINO GPU plugin. It covers the public API, plugin API, implementation details, and memory management aspects of the plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/simplified_workflow.md#_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nclassDiagram\n\n%% Public API classes\nclass `ov::CompiledModel`\nclass `ov::InferRequest`\nclass `ov::Core`\n\n%% Plugin API interface %%\nclass `ov::IPlugin`\nclass `ov::ICompiledModel`\nclass `ov::IAsyncInferRequest`\nclass `ov::ISyncInferRequest`\n\n%% Plugin API Impl %%\nclass `intel_gpu::Plugin` { OpenVINO plugin implementation for GPU }\nclass `intel_gpu::CompiledModel`\nclass `intel_gpu::AsyncInferRequest` { Asynchronous version of infer request }\nclass `intel_gpu::SyncInferRequest` { Inference request for specific executable network. Wrapper for input and output memory }\n\n`intel_gpu::Plugin` --|> `ov::IPlugin`\n`intel_gpu::CompiledModel` --|> `ov::ICompiledModel`\n`intel_gpu::SyncInferRequest` --|> `ov::ISyncInferRequest`\n`intel_gpu::AsyncInferRequest` --|> `ov::IAsyncInferRequest`\n`intel_gpu::Plugin` --> `intel_gpu::CompiledModel` : Create\n`intel_gpu::SyncInferRequest` \"1\" --* \"1\" `intel_gpu::AsyncInferRequest`\n\n%% Plugin implementation details %%\nclass `intel_gpu::TransformationPipeline` {Set of ngraph-based transformations configured by GPU plugin }\nclass `intel_gpu::Graph` { Per stream copy of compiled graph with independent memory }\nclass `intel_gpu::ProgramBuilder` { Object for operations semantic translation and graph compilation }\n\n`ov::Core` --> `intel_gpu::CompiledModel` : compile_model()\n`ov::CompiledModel` -->`intel_gpu::AsyncInferRequest` : create_infer_request()\n`ov::InferRequest` --> `intel_gpu::network` : start_async()\n`intel_gpu::CompiledModel` --> `intel_gpu::AsyncInferRequest` : Create\n`intel_gpu::TransformationPipeline` --> `ov::Model`\n`intel_gpu::TransformationPipeline` --> `intel_gpu::CompiledModel`\n`intel_gpu::Graph` \"1..N\" --* `intel_gpu::CompiledModel`\n`intel_gpu::CompiledModel` --> `intel_gpu::ProgramBuilder` : Create\n`intel_gpu::ProgramBuilder` \"1\" --o \"N\" `intel_gpu::Graph`\n\nclass `intel_gpu::convolution` {convolution operation descriptor}\nclass `intel_gpu::data` {Primitive representing constant data in a topology }\nclass `intel_gpu::input_layout` {Represents dynamic input data}\nclass `intel_gpu::primitive_base` {<<Interface>>}\n`intel_gpu::convolution` ..<| `intel_gpu::primitive_base`\n`intel_gpu::data` ..<| `intel_gpu::primitive_base`\n`intel_gpu::input_layout` ..<| `intel_gpu::primitive_base`\n`Any other primitive` ..<| `intel_gpu::primitive_base`\nclass `intel_gpu::topology` { Set of primitives. Each primitive knows operation parameters, it's inputs and outputs }\nclass `intel_gpu::program` { Class that contains compiled topology. All kernels are selected, memory dependencies are resolved, the only missing thing - memory for intermediate buffers }\n`intel_gpu::primitive_base` \"0..N\" --o `intel_gpu::topology`\n`intel_gpu::program` --> `intel_gpu::topology`\n`intel_gpu::ProgramBuilder` --> `intel_gpu::topology` : Create\n`intel_gpu::ProgramBuilder` --> `intel_gpu::program` : Create\nclass `intel_gpu::program_node` { Base class for representation of a single graph node }\nclass `intel_gpu::primitive_impl` { <<interface>> Base class for representation of a single graph node }\nclass `intel_gpu::typed_primitive_onednn_impl` {Implementations that use oneDNN library }\nclass `oneDNN library` { statically linked into GPU plugin }\nclass `intel_gpu::typed_primitive_ocl_impl` { OCL implementations that use kernels from kernel_selector }\nclass `intel_gpu::kernel_selector` { module that stores OCL kernels for primitives and has embed some rules for optimal kernel selection }\n`intel_gpu::program_node` --o `intel_gpu::program`\n`intel_gpu::primitive_impl` --o `intel_gpu::program_node`\n`intel_gpu::typed_primitive_onednn_impl` ..<| `intel_gpu::primitive_impl`\n`intel_gpu::typed_primitive_ocl_impl` ..<| `intel_gpu::primitive_impl`\n`intel_gpu::typed_primitive_ocl_impl` ..> `intel_gpu::kernel_selector`\n`intel_gpu::typed_primitive_onednn_impl` --> `oneDNN bridge` : Use\n`intel_gpu::typed_primitive_onednn_impl` ..> `oneDNN library`\nclass `intel_gpu::build_options` { Set of options for graph compilations }\nclass `intel_gpu::pass_manager` { Helper to run graph transformations }\nclass `intel_gpu::base_pass` { <<Interface>> Base class for graph transformations}\n`intel_gpu::program` --> `intel_gpu::build_options`\n`intel_gpu::program` --> `intel_gpu::pass_manager` : Use\n`intel_gpu::program` --> `intel_gpu::base_pass` : Use\n`intel_gpu::pass_manager` --> `intel_gpu::base_pass` : Run\nclass `intel_gpu::prepare_primitive_fusing` { Pass that fuses multiple operations into single node }\nclass `intel_gpu::prepare_quantization` { Pass that prepares models for low precision execution }\nclass `intel_gpu::reorder_inputs` { Pass that is responsible for layout/impl selection }\nclass `intel_gpu::compile_graph` { Pass that selects and creates best implementation for each primitive }\nclass `intel_gpu::remove_redundant_reorders` { Pass that optimizes reorders in the graph }\n`intel_gpu::prepare_primitive_fusing`--|> `intel_gpu::base_pass`\n`intel_gpu::prepare_quantization`--|> `intel_gpu::base_pass`\n`intel_gpu::reorder_inputs`--|> `intel_gpu::base_pass`\n`intel_gpu::compile_graph`--|> `intel_gpu::base_pass`\n`intel_gpu::layout_optimizer`--|> `intel_gpu::base_pass`\n`intel_gpu::remove_redundant_reorders`--|> `intel_gpu::base_pass`\n`intel_gpu::reorder_inputs`--> `intel_gpu::layout_optimizer` : Use\nclass `intel_gpu::network` { A program with allocated memory.Can be executed on the device }\n`intel_gpu::AsyncInferRequest` --> `intel_gpu::network` : Set input/output memory and run execution\n`intel_gpu::network` --> `intel_gpu::AsyncInferRequest` : Return inference result\nclass `intel_gpu::tensor` { Size of memory buffer }\nclass `intel_gpu::format` { Order of elements in memory }\nclass `intel_gpu::data_type` { elements precision }\nclass `intel_gpu::memory_pool` { Object that tracks memory allocations and tries to reuse memory buffers }\nclass `intel_gpu::layout` { Memory descriptor }\nclass `intel_gpu::memory` { GPU memory object }\nclass `intel_gpu::stream` { Abstraction for queue. Knows how to submit kernels and provide some synchronization   }\nclass `intel_gpu::event` { Synchronization primitive }\nclass `intel_gpu::kernel` { Holds kernel handle }\nclass `intel_gpu::engine` { Engine for specific device, responsible for memory allocations }\nclass `intel_gpu::device` { Holds context/device handles for selected backend }\nclass `intel_gpu::device_info` { Storage for device capabilities and info }\nclass `intel_gpu::engine_configuration` { Options for engine }\nclass `intel_gpu::device_query` { Detects available devices for given backend }\n`intel_gpu::tensor` --o `intel_gpu::layout`\n`intel_gpu::format` --o `intel_gpu::layout`\n`intel_gpu::data_type` --o `intel_gpu::layout`\n`intel_gpu::layout` --o  `intel_gpu::memory`\n`intel_gpu::memory` --o \"0..N\" `intel_gpu::memory_pool`\n`intel_gpu::memory` --o `intel_gpu::data`\n`intel_gpu::memory_pool` --* `intel_gpu::network`\n`intel_gpu::stream` --* `intel_gpu::network`\n`intel_gpu::stream` --> `intel_gpu::event`\n`intel_gpu::stream` --> `intel_gpu::kernel`\n`intel_gpu::engine` --> `intel_gpu::stream` : Create\n`intel_gpu::engine` --> `intel_gpu::memory` : Create\n`intel_gpu::engine` --> `intel_gpu::engine_configuration`\n`intel_gpu::engine` -- `oneDNN library` : Share context/device/queue handles\n`intel_gpu::device` --o `intel_gpu::engine`\n`intel_gpu::device_info` --o `intel_gpu::device`\n`intel_gpu::device_query` --> `intel_gpu::device`\n```\n\n----------------------------------------\n\nTITLE: Disable Constant Folding, Optimize Transformation Function in OpenVINO C++\nDESCRIPTION: This C++ snippet demonstrates disabling dequantization operations constant folding on constant subgraphs on weights, optimizing the transformation function, and converting operations to operation set 1 within OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/advanced-guides/low-precision-transformations.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nauto& transformation = *transformation;\n\n```\n\n----------------------------------------\n\nTITLE: Get Tensor Name of Port in OpenVINO (C)\nDESCRIPTION: This function retrieves the tensor name of a port object in OpenVINO. It takes a pointer to `ov_output_const_port_t` and returns the tensor name via `char** tensor_name`. The function returns a status code to indicate success or failure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_37\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_port_get_any_name(const ov_output_const_port_t* port, char** tensor_name)\n```\n\n----------------------------------------\n\nTITLE: Running OpenVINO Benchmark Application in Python\nDESCRIPTION: This command runs the Python version of the OpenVINO benchmark application. It estimates deep learning inference performance on supported devices for synchronous and asynchronous modes. The command takes arguments for the model path, input path, and target device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples.rst#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nbenchmark_app -m <model> -i <input> -d <device>\n```\n\n----------------------------------------\n\nTITLE: OpenCL Call Logging with cliloader (Bash)\nDESCRIPTION: This snippet shows how to use `cliloader` from the opencl-intercept-layer to log OpenCL calls during the execution of an OpenVINO application. The `CLI_CallLogging` environment variable is set to 1 to enable call logging.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_debug_utils.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# See OpenCL call log\n$ CLI_CallLogging=1 /path/to/cliloader /path/to/benchmark_app ...\n```\n\n----------------------------------------\n\nTITLE: Generating HTML and CSV Reports using summarize.py for OP conformance (Python)\nDESCRIPTION: This snippet demonstrates how to use the `summarize.py` script to generate HTML and CSV reports from an XML report file for OP conformance.  It requires specifying the XML report file path, the output directory, and the type of report (-t OP).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/README.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\npython3 summarize.py --xml /opt/repo/infrastructure-master/thirdparty/gtest-parallel/report_opset.xml --out /opt/repo/infrastructure-master/thirdparty/gtest-parallel/ -t OP\n```\n\n----------------------------------------\n\nTITLE: Filter Blob Dumps by Node Type\nDESCRIPTION: Filters the blobs to be dumped based on the type of the node for OpenVINO CPU executions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_NODE_TYPE=<space_separated_list_of_types> binary ...\n```\n\n----------------------------------------\n\nTITLE: Selu Layer Definition in OpenVINO XML\nDESCRIPTION: This XML snippet demonstrates how to define a Selu layer within an OpenVINO model.  It showcases the input and output port configurations, specifying the dimensions of the data tensor, alpha value, and lambda value. The layer processes the input data using the Selu activation function and produces an output tensor with the same shape as the input data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/selu-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Selu\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Automatic Device Selection Python\nDESCRIPTION: This Python snippet demonstrates how to compile a model for automatic device selection across multiple GPUs using `ov::Core::compile_model()`.  It utilizes the `compile_model_auto` fragment from the specified Python file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# [compile_model_auto]\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model=model, device_name=\"AUTO:GPU.1,GPU.0\", config={\"CUMULATIVE_THROUGHPUT\": \"YES\"})\n# [compile_model_auto]\n```\n\n----------------------------------------\n\nTITLE: Activate Virtual Environment (Linux/macOS)\nDESCRIPTION: This command activates the virtual environment 'openvino_env' on Linux or macOS using the `source` command.  This ensures subsequent pip installs packages into the isolated environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nsource openvino_env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Running MatcherPass with GraphRewrite (C++)\nDESCRIPTION: This code snippet demonstrates how to run a MatcherPass on an `ov::Model` using GraphRewrite.  This approach enables the execution of the MatcherPass on the entire model and allows for the registration of multiple MatcherPass transformations to be executed in a single graph traversal, improving efficiency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/matcher-pass.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n// [matcher_pass:graph_rewrite]\n#include <openvino/core/rt_info.hpp>\n#include <openvino/opsets/opset10.hpp>\n#include <openvino/pass/matcher_pass.hpp>\n\n#include \"openvino/pass/graph_rewrite.hpp\"\n\nusing namespace std;\nusing namespace ov::opset10;\n\n{\n    auto pass = make_shared<TemplateTransformation>();\n    ov::pass::GraphRewrite graph_rewrite_pass;\n    graph_rewrite_pass.add_matcher_pass(pass);\n    // Create model\n    auto input0 = make_shared<Parameter>(element::f32, Shape{1, 3, 64, 64});\n    auto input1 = make_shared<Parameter>(element::f32, Shape{1, 3, 64, 64});\n    auto add = make_shared<Add>(input0, input1);\n    auto model = make_shared<ov::Model>(add, ov::ParameterVector{input0, input1});\n\n    graph_rewrite_pass.run_on_model(model);\n}\n// [matcher_pass:graph_rewrite]\n```\n\n----------------------------------------\n\nTITLE: ScatterNDUpdate Example 2 in C++\nDESCRIPTION: This C++ code snippet demonstrates updating two slices of a 4x4 shape in the 'data' tensor using ScatterNDUpdate.  The 'indices' tensor specifies which slices to update, and the 'updates' tensor provides the new slice values. The 'output' tensor is the result of the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-nd-update-3.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ndata    = [[[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n          [[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n          [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]],\n          [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]]]\nindices = [[0], [2]]\nupdates = [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n          [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]]]\noutput  = [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n          [[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n          [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],\n          [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]]]\n```\n\n----------------------------------------\n\nTITLE: Linear Interpolation Implementation\nDESCRIPTION: Implements linear interpolation on the input data. It performs antialiasing if downsampling is detected and the antialias flag is set. It uses triangle coefficients to weight the contributions from neighboring input pixels. It requires the `triangle_coeffs` function to be defined elsewhere.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef linear_interpolation(self, input_data):\n    result = np.zeros(self.output_shape)\n    num_of_axes = len(self.axes)\n    is_downsample = False\n\n    for scale in self.scales:\n        is_downsample = is_downsample or (scale < 1)\n\n    antialias = is_downsample and self.antialias\n\n    a = np.zeros(num_of_axes)\n    for i, _ in enumerate(self.axes):\n        a[i] = self.scales[i] if antialias else 1.0\n\n    prod_of_a = np.prod(a)\n    r = np.zeros(num_of_axes).astype(np.int64)\n    for i, _ in enumerate(self.axes):\n        r[i] = 2 if self.scales[i] > 1.0 else int(math.ceil(2.0/a[i]))\n\n    indices = [tuple(np.array(ind).astype(np.int64) - r) for ind in np.ndindex(tuple(2 * r + 1))]\n\n    for coordinates in np.ndindex(tuple(self.output_shape)):\n        icoords = np.array(coordinates).astype(np.float64)\n        icoords_r = np.array(coordinates).astype(np.float64)\n        for i, axis in enumerate(self.axes):\n            in_coord = self.get_original_coordinate(coordinates[axis], self.scales[i], self.output_shape[axis], self.input_shape[axis])\n            icoords[axis] = in_coord\n            icoords_r[axis] = round(in_coord)\n\n        summa = 0.0\n        wsum = 0.0\n\n        for index in indices:\n            inner_coords = np.array(coordinates)\n            for i, axis in enumerate(self.axes):\n                inner_coords[axis] = index[i] + icoords_r[axis]\n\n            conditions = [inner_coords[axis] >= 0 and inner_coords[axis] < self.input_shape[axis] for axis in self.axes]\n            if not all(conditions):\n                continue\n\n            dz = np.zeros(num_of_axes)\n            for i, axis in enumerate(self.axes):\n                dz[i] = icoords[axis] - inner_coords[axis]\n\n            w = prod_of_a * np.prod(triangle_coeffs(a * dz))\n            wsum += w\n            summa += w * input_data[tuple(inner_coords)]\n\n        if wsum == 0:\n            result[coordinates] = 0.0\n        else:\n            result[coordinates] = summa / wsum\n\n    return result\n```\n\n----------------------------------------\n\nTITLE: Linking Dependencies with CMake\nDESCRIPTION: This code links the `sea_itt_lib` target with the `ittapi::ittnotify` library. Platform-specific dependencies are also linked based on the operating system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/itt_collector/sea_itt_lib/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE ittapi::ittnotify)\n\nif(UNIX)\n    target_link_libraries(${TARGET_NAME} PRIVATE ${CMAKE_DL_LIBS})\nelif(WIN32)\n    target_link_libraries(${TARGET_NAME} PRIVATE Dbghelp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: Sets the include directory for the library target, allowing the library to find its own header files during compilation and also allowing dependent projects to find the library's public headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}_obj PRIVATE $<BUILD_INTERFACE:${PUBLIC_HEADERS_DIR}>)\n```\n\n----------------------------------------\n\nTITLE: Generating Makefiles with CMake for OpenVINO Samples (Linux)\nDESCRIPTION: This command uses CMake to generate Makefiles for building the OpenVINO C++ samples in release configuration. It specifies the build type and the path to the samples directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_BUILD_TYPE=Release <INSTALL_DIR>/samples/cpp\n```\n\n----------------------------------------\n\nTITLE: ROIAlign Layer Configuration in XML\nDESCRIPTION: This XML snippet demonstrates the configuration of an ROIAlign layer with specific attributes such as pooled height, pooled width, spatial scale, sampling ratio, and mode. It also defines the input and output ports with their dimensions and precision.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/roi-align-3.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ROIAlign\" ... >\n    <data pooled_h=\"6\" pooled_w=\"6\" spatial_scale=\"16.0\" sampling_ratio=\"2\" mode=\"avg\"/>\n    <input>\n        <port id=\"0\">\n            <dim>7</dim>\n            <dim>256</dim>\n            <dim>200</dim>\n            <dim>200</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1000</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1000</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP32\">\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>6</dim>\n            <dim>6</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Configuring and Adding oneDNN Subdirectory\nDESCRIPTION: This CMake function, `ov_add_onednn`, configures the oneDNN library build.  It sets various options like disabling Python, enabling JIT profiling, controlling ITT tasks, enabling concurrent execution and primitive cache, setting the CPU runtime, disabling GPU runtime and examples, choosing the threading library, selecting ISA kernels, and defining the primitives/workloads to enable. It also configures compiler flags based on the compiler and platform.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/thirdparty/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ov_add_onednn)\n    set(CMAKE_COMPILE_WARNING_AS_ERROR OFF)\n    set(CMAKE_DISABLE_FIND_PACKAGE_PythonInterp ON)\n    set(DNNL_ENABLE_JIT_PROFILING ${BUILD_SHARED_LIBS} CACHE BOOL \"\" FORCE)\n    if(BUILD_SHARED_LIBS AND ENABLE_PROFILING_ITT)\n        set(DNNL_ENABLE_ITT_TASKS ON CACHE BOOL \"\" FORCE)\n    else()\n        set(DNNL_ENABLE_ITT_TASKS OFF CACHE BOOL \"\" FORCE)\n    endif()\n    set(DNNL_ENABLE_CONCURRENT_EXEC ON CACHE BOOL \"\" FORCE)\n    set(DNNL_ENABLE_PRIMITIVE_CACHE ON CACHE BOOL \"\" FORCE) # Enable primitive cache for global sharing\n    set(DNNL_ENABLE_MAX_CPU_ISA ON CACHE BOOL \"\" FORCE)\n    set(DNNL_LIBRARY_TYPE \"STATIC\" CACHE STRING \"\" FORCE)\n    set(DNNL_BUILD_EXAMPLES OFF CACHE BOOL \"\" FORCE)\n    set(DNNL_BUILD_TESTS OFF CACHE BOOL \"\" FORCE)\n    set(DNNL_CPU_RUNTIME \"${THREADING}\" CACHE STRING \"\" FORCE)\n    set(DNNL_GPU_RUNTIME \"NONE\" CACHE STRING \"\" FORCE)\n    set(DNNL_BLAS_VENDOR \"NONE\" CACHE STRING \"\" FORCE)\n    set(ONEDNN_ENABLE_GEMM_KERNELS_ISA \"SSE41\" CACHE STRING \"\" FORCE)\n    # plugin does not use onednn graph\n    set(ONEDNN_BUILD_GRAPH OFF CACHE BOOL \"\" FORCE)\n    # select needed primitives\n    set(DNNL_ENABLE_PRIMITIVE \"CONVOLUTION;DECONVOLUTION;CONCAT;LRN;INNER_PRODUCT;MATMUL;POOLING;REDUCTION;REORDER;RNN;SOFTMAX\" CACHE STRING \"\" FORCE)\n    set(DNNL_ENABLE_WORKLOAD \"INFERENCE\" CACHE STRING \"\" FORCE)\n    set(DNNL_LIBRARY_NAME \"openvino_onednn_cpu\" CACHE STRING \"\" FORCE)\n\n    # Allow to enable oneDNN verbose with CPU_DEBUG_CAPS and rely on oneDNN default configuration otherwise\n    if(ENABLE_CPU_DEBUG_CAPS)\n        set(DNNL_VERBOSE ON CACHE STRING \"\" FORCE)\n    endif()\n\n    if(X86_64)\n        set(DNNL_TARGET_ARCH \"X64\" CACHE STRING \"\" FORCE)\n    elseif(X86)\n        set(DNNL_TARGET_ARCH \"X86\" CACHE STRING \"\" FORCE)\n    elseif(RISCV64)\n        set(DNNL_TARGET_ARCH \"RV64\" CACHE STRING \"\" FORCE)\n    elseif(ARM)\n        set(DNNL_TARGET_ARCH \"ARM\" CACHE STRING \"\" FORCE)\n    elseif(AARCH64)\n        set(DNNL_TARGET_ARCH \"AARCH64\" CACHE STRING \"\" FORCE)\n    else()\n        message(FATAL_ERROR \"Unsupported system processor ${CMAKE_SYSTEM_PROCESSOR}\")\n    endif()\n\n    if(AARCH64 OR ARM)\n        set(DNNL_USE_ACL ON CACHE BOOL \"Use ARM Compute Library kernels in oneDNN\" FORCE)\n    endif()\n\n    set(SDL_cmake_included ON)  ## to skip internal SDL flags. SDL flags are already set on OV level\n    if (ANDROID OR ((CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\" OR OV_COMPILER_IS_CLANG) AND NOT (THREADING STREQUAL \"OMP\")))\n        set(OpenMP_cmake_included ON) ## to skip \"omp simd\" inside a code. Lead to some crashes inside NDK LLVM..\n    endif()\n\n    # WA for old TBBConfig.cmake like tbb2019_20180718oss\n    # they don't check that imported target is already created\n    if(TBB_FOUND)\n        set(TBB_cmake_included ON)\n        set(DNNL_CPU_THREADING_RUNTIME \"${THREADING}\")\n        function(find_package_tbb)\n            # dummy\n        endfunction()\n        link_libraries(TBB::tbb)\n    endif()\n\n    if(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG OR (OV_COMPILER_IS_INTEL_LLVM AND UNIX))\n        ov_add_compiler_flags(-Wno-error)\n        ov_add_compiler_flags(-Wno-undef)\n        ov_add_compiler_flags(-Wno-missing-declarations)\n        if(NOT CMAKE_COMPILER_IS_GNUCXX AND (ARM OR AARCH64))\n            ov_add_compiler_flags(-Wno-macro-redefined)\n        endif()\n        if(CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 11 AND CMAKE_COMPILER_IS_GNUCXX)\n            ov_add_compiler_flags(-Wno-array-bounds)\n            ov_add_compiler_flags(-Wno-stringop-overflow)\n            set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-overloaded-virtual\")\n            if(CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 12)\n                ov_add_compiler_flags(-Wno-restrict)\n            endif()\n        endif()\n        if(OV_COMPILER_IS_INTEL_LLVM)\n            ov_add_compiler_flags(-Wno-deprecated-this-capture)\n            ov_add_compiler_flags(-Wno-deprecated-enum-enum-conversion)\n        endif()\n    elseif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n        # C4849 OpenMP 'reduction' clause ignored in 'simd' directive\n        ov_add_compiler_flags(/wd4849)\n        # C4661 no suitable definition provided for explicit template instantiation request\n        ov_add_compiler_flags(/wd4661)\n        # C4267, 4244 conversion from 'XXX' to 'YYY', possible loss of data\n        ov_add_compiler_flags(/wd4267)\n        ov_add_compiler_flags(/wd4244)\n        # C4334 '<<': result of 32-bit shift implicitly converted to 64 bits\n        ov_add_compiler_flags(/wd4334)\n    endif()\n\n    if(SUGGEST_OVERRIDE_SUPPORTED)\n        # xbyak compilation fails\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-suggest-override\")\n    endif()\n\n    # to find our FindACL.cmake\n    if(DNNL_USE_ACL)\n        list(APPEND CMAKE_MODULE_PATH \"${intel_cpu_thirdparty_SOURCE_DIR}\")\n        # oneDNN needs arm_compute_version.embed file to detect ACL version\n        # since the file has not been generated yet, it is created manually\n        ov_create_acl_version_file()\n    endif()\n\n    add_subdirectory(onednn EXCLUDE_FROM_ALL)\n\n    # install static libraries\n    ov_install_static_lib(dnnl ${OV_CPACK_COMP_CORE})\n\n    if(DNNL_USE_ACL AND NOT BUILD_SHARED_LIBS)\n        # use ACLConfig.cmake in OpenVINOConfig.cmake in case of static build\n        # we cannot use 'ov_install_static_lib' for imported targets,\n        # but for this we need to install library files\n        install(FILES $<TARGET_PROPERTY:arm_compute::arm_compute,IMPORTED_LOCATION>\n                DESTINATION ${OV_CPACK_ARCHIVEDIR}\n                COMPONENT ${OV_CPACK_COMP_CORE}\n                ${OV_CPACK_COMP_CORE_EXCLUDE_ALL})\n        install(FILES \"${intel_cpu_thirdparty_SOURCE_DIR}/ACLConfig.cmake\"\n                DESTINATION ${OV_CPACK_OPENVINO_CMAKEDIR}\n                COMPONENT ${OV_CPACK_COMP_CORE_DEV}\n                ${OV_CPACK_COMP_CORE_DEV_EXCLUDE_ALL})\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Add OpenVINO APT repository (Ubuntu 20)\nDESCRIPTION: Adds the OpenVINO APT repository for Ubuntu 20 to the system's software sources. This allows the system to find and install OpenVINO packages using the APT package manager.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\necho \"deb https://apt.repos.intel.com/openvino ubuntu20 main\" | sudo tee /etc/apt/sources.list.d/intel-openvino.list\n```\n\n----------------------------------------\n\nTITLE: Printing Dynamic Output Shapes - C\nDESCRIPTION: This C code shows how to determine and print the dynamic shape of an output layer in an OpenVINO model, after the model has been compiled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_15\n\nLANGUAGE: C\nCODE:\n```\n#include <openvino/c/openvino.h>\n#include <stdio.h>\n\n#define OV_CHECK(status, message)                                  \\\n    if (status != 0) {                                            \\\n        printf(\"%s\\n\", message);                               \\\n        return 1;                                                  \\\n    }\n\nint main() {\n    // ! [ov_dynamic_shapes:print_dynamic]\n    ov_core_t* core = NULL;\n    OV_CHECK(ov_core_create(&core), \"Failed to create Core\");\n\n    ov_model_t* model = NULL;\n    OV_CHECK(ov_core_read_model(core, \"model.xml\", &model), \"Failed to read model\");\n\n    ov_compiled_model_t* compiled_model = NULL;\n    OV_CHECK(ov_core_compile_model(core, model, \"CPU\", &compiled_model), \"Failed to compile model\");\n\n    ov_output_port output_port = {0};\n    OV_CHECK(ov_compiled_model_output(compiled_model, 0, &output_port), \"Failed to get output port\");\n\n    ov_partial_shape partial_shape;\n    OV_CHECK(ov_output_port_get_partial_shape(output_port, &partial_shape), \"Failed to get partial shape\");\n\n    printf(\"Output Partial Shape:\\n\");\n    printf(\"Rank: %ld\\n\", partial_shape.rank);\n\n    for (size_t i = 0; i < partial_shape.rank; ++i) {\n        if (partial_shape.dims[i] == -1) {\n            printf(\"Dimension %zu: Dynamic\\n\", i);\n        } else {\n            printf(\"Dimension %zu: %ld\\n\", i, partial_shape.dims[i]);\n        }\n    }\n    \n    ov_output_port_free(&output_port);\n    ov_compiled_model_free(compiled_model);\n    ov_model_free(model);\n    ov_core_free(core);\n    // ! [ov_dynamic_shapes:print_dynamic]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example with Valid Padding and Ceil Rounding (Shell)\nDESCRIPTION: This example demonstrates the MaxPool operation with a 4D input, using a 2D kernel, valid padding, and 'ceil' rounding type. It includes the input tensor, strides, kernel size, rounding type, auto_pad setting, and the resulting output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-1.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[-1, 2, 3],\n                 [4, 5, -6],\n                 [-7, 8, 9]]]]\nstrides = [2, 2]\nkernel = [2, 2]\nrounding_type = \"ceil\"\nauto_pad = \"valid\"\noutput = [[[[5, 3],\n                  [8, 9]]]]\n```\n\n----------------------------------------\n\nTITLE: BitwiseAnd example with no broadcast in XML\nDESCRIPTION: Illustrates a BitwiseAnd layer definition in XML with no broadcasting. The input ports '0' and '1' have matching dimensions (256x56), and the output port '2' also has the same dimensions. This example demonstrates the simplest use case where input shapes match.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-and-13.rst#_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"BitwiseAnd\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Einsum with Ellipsis Example 1 C++\nDESCRIPTION: This example shows how Einsum operates on a single input with an equation containing an ellipsis.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/einsum-7.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nA = [[1.0, 2.0, 3.0],\n     [4.0, 5.0, 6.0],\n     [7.0, 8.0, 9.0]]\nequation = \"a...->...\"\nalternative_equation = \"...a->a\"\noutput = [12.0, 15.0, 18.0]\n```\n\n----------------------------------------\n\nTITLE: Implementing C shape creation\nDESCRIPTION: This C code snippet illustrates the creation of C `shape` structures.  It does not directly call C++ interfaces, but creates a C structure.  This structure is used to provide shape information for subsequent C++ calls.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/how_to_wrap_openvino_interfaces_with_c.md#_snippet_3\n\nLANGUAGE: C\nCODE:\n```\nhttps://github.com/openvinotoolkit/openvino/blob/d96c25844d6cfd5ad131539c8a0928266127b05a/src/bindings/c/src/ov_shape.cpp#L17-L33\n```\n\n----------------------------------------\n\nTITLE: Setting Target Compile Definitions\nDESCRIPTION: This snippet sets private compile definitions for the test target `${TARGET_NAME}`. These definitions are used during the compilation of the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/functional/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME} PRIVATE ${COMPILE_DEFINITIONS})\n```\n\n----------------------------------------\n\nTITLE: Disabling FindPkgConfig for Android\nDESCRIPTION: This snippet disables the use of FindPkgConfig for Android builds. This is done because the Android toolchain does not provide pkg-config files, and using the build system's pkg-config can lead to incorrect linking.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(ANDROID)\n    # Android toolchain does not provide pkg-config file. So, cmake mistakenly uses\n    # build system pkg-config executable, which finds packages on build system. Such\n    # libraries cannot be linked into Android binaries.\n    set(CMAKE_DISABLE_FIND_PACKAGE_PkgConfig ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for Final Build (Windows)\nDESCRIPTION: This snippet configures the final OpenVINO build on Windows. It sets various options, including disabling ITT profiling, enabling selective build, specifying the path to the statistics CSV files, disabling GPU and other execution modes, disabling various frontends and enabling LTO. The generator is Visual Studio 16 2019.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\ncmake -G \"Visual Studio 16 2019\" -A x64 -DENABLE_CPPLINT=OFF -DCMAKE_VERBOSE_MAKEFILE=ON -DCMAKE_COMPILE_WARNING_AS_ERROR=OFF -DCMAKE_BUILD_TYPE=Release -DENABLE_FASTER_BUILD=ON -DENABLE_PROFILING_ITT=OFF -DSELECTIVE_BUILD=ON -DENABLE_INTEL_GPU=OFF -DENABLE_MULTI=OFF -DENABLE_AUTO=OFF -DENABLE_AUTO_BATCH=OFF -DENABLE_HETERO=OFF -DENABLE_TEMPLATE=OFF -DENABLE_OV_ONNX_FRONTEND=OFF -DENABLE_OV_PADDLE_FRONTEND=OFF -DENABLE_OV_PYTORCH_FRONTEND=OFF -DENABLE_OV_JAX_FRONTEND=OFF -DENABLE_OV_TF_FRONTEND=OFF -DSELECTIVE_BUILD_STAT=%OPENVINO_HOME%\\cc_data\\*.csv -DBUILD_SHARED_LIBS=OFF -DENABLE_LTO=ON -DENABLE_ONEDNN_FOR_GPU=OFF -DENABLE_OV_TF_LITE_FRONTEND=OFF -DENABLE_PROFILING_FIRST_INFERENCE=OFF ..\n```\n\n----------------------------------------\n\nTITLE: Getting Tensor Element Type in OpenVINO (C)\nDESCRIPTION: This function retrieves the element type of an OpenVINO tensor. It takes a pointer to the tensor and a pointer to an element type enum as input. The function returns a status code indicating success or failure in retrieving the tensor element type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_46\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_tensor_get_element_type(const ov_tensor_t* tensor, ov_element_type_e* type)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Include Directories in CMake\nDESCRIPTION: This snippet sets the include directories for the `TARGET_NAME` library. It adds the `UTIL_INCLUDE_DIR` as a public include directory, allowing other projects that link against this library to find its header files.  The $<BUILD_INTERFACE:...> generator expression ensures the include directory is only used when building the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC $<BUILD_INTERFACE:${UTIL_INCLUDE_DIR}>)\n```\n\n----------------------------------------\n\nTITLE: Including CMake Module for Frontend\nDESCRIPTION: This snippet includes the `frontend_module.cmake` file from the `pyopenvino_SOURCE_DIR`, which likely contains definitions and configurations needed for managing frontends within the OpenVINO project. This is a prerequisite for defining the TensorFlow frontend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/frontend/tensorflow/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(${pyopenvino_SOURCE_DIR}/frontend/frontend_module.cmake)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: This snippet shows how to install the necessary Python dependencies for running the smoke tests. It uses `pip3` to install the packages listed in the `requirements.txt` file located within the smoke tests directory of the OpenVINO installation. The `<INSTALL_DIR>` placeholder should be replaced with the actual OpenVINO installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/samples_tests/smoke_tests/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r  <INSTALL_DIR>\\tests\\smoke_tests\\requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Example Disassembled JIT Code\nDESCRIPTION: Shows an example output of the JIT disassembly tool, displaying the disassembled code with annotations indicating the corresponding source code file and line number. This output format allows users to trace JIT-generated instructions back to their source.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/dump_jit_disassm/README.md#_snippet_3\n\nLANGUAGE: assembly\nCODE:\n```\n0000000000000000 <.data>:\n       0:       53                      push   rbx      jit_avx512_core_amx_1x1_conv_kernel.cpp:834\n       1:       55                      push   rbp      jit_avx512_core_amx_1x1_conv_kernel.cpp:834\n       2:       41 54                   push   r12      jit_avx512_core_amx_1x1_conv_kernel.cpp:834\n       4:       41 55                   push   r13      jit_avx512_core_amx_1x1_conv_kernel.cpp:834\n       6:       41 56                   push   r14      jit_avx512_core_amx_1x1_conv_kernel.cpp:834\n       8:       41 57                   push   r15      jit_avx512_core_amx_1x1_conv_kernel.cpp:834\n       a:       bd 00 04 00 00          mov    ebp,0x400\n       f:       48 83 ec 08             sub    rsp,0x8\n      13:       4c 8b bf 88 00 00 00    mov    r15,QWORD PTR [rdi+0x88] jit_uni_postops_injector.cpp:387 / jit_avx512_core_amx_1x1_conv_kernel.cpp:837\n      1a:       4d 8b 37                mov    r14,QWORD PTR [r15]      jit_uni_postops_injector.cpp:389 / jit_avx512_core_amx_1x1_conv_kernel.cpp:837\n      1d:       4c 89 34 24             mov    QWORD PTR [rsp],r14      jit_uni_postops_injector.cpp:387 / jit_avx512_core_amx_1x1_conv_kernel.cpp:837\n```\n\n----------------------------------------\n\nTITLE: Adding OpenVINO Debian Packages as Dependencies\nDESCRIPTION: This snippet shows how to add OpenVINO Debian packages to the build-packages and stage-packages dependencies in the application's snapcraft.yaml. This ensures that the required OpenVINO libraries are available during the build and runtime of the snap.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/integrate-openvino-with-ubuntu-snap.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nparts:\n  app-build:\n    build-packages:\n      - openvino-libraries-dev\n    stage-packages:\n      - openvino-libraries-2024.1.0\n```\n\n----------------------------------------\n\nTITLE: Get Shape of Const Port in OpenVINO (C)\nDESCRIPTION: This function retrieves the shape of a const port object in OpenVINO. It takes a pointer to `ov_output_const_port_t` as input and returns the tensor shape via `ov_shape_t`. The function returns a status code to indicate success or failure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_35\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_const_port_get_shape(const ov_output_const_port_t* port, ov_shape_t* tensor_shape)\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories and Target Properties in CMake\nDESCRIPTION: This snippet configures the include directories and output name for the target library. It adds the current source directory as a private include directory and sets the output name to `IntelSEAPI`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/itt_collector/sea_itt_lib/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})\nset_target_properties(${TARGET_NAME} PROPERTIES OUTPUT_NAME IntelSEAPI)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties\nDESCRIPTION: Sets the INTERPROCEDURAL_OPTIMIZATION_RELEASE property for the target, enabling link-time optimization (LTO) if ENABLE_LTO is set. This improves the performance of the resulting library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/backend/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Get Const Output Port (Single Output Model) in OpenVINO (C)\nDESCRIPTION: This function is used to get a single const output port of an OpenVINO model that supports only a single output. It takes a pointer to `ov_model_t` and returns a pointer to `ov_output_const_port_t`. It returns a status code indicating success or failure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_28\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_model_const_output(const ov_model_t* model, ov_output_const_port_t** output_port);\n```\n\n----------------------------------------\n\nTITLE: Set Threading Interface\nDESCRIPTION: Configures the threading interface for the target, setting up how the plugin handles multithreading. This is likely using a custom `ov_set_threading_interface_for` macro/function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Greater-1 XML Example (No Broadcast) - OpenVINO\nDESCRIPTION: This XML snippet demonstrates the Greater-1 operation with no broadcasting enabled. The input tensors must have matching shapes. The 'auto_broadcast' attribute is set to 'none', ensuring no automatic shape adjustments are performed before the comparison.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/greater-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Greater\">\n    <data auto_broadcast=\"none\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: TopK Operation in C++\nDESCRIPTION: This code snippet illustrates the computation performed by the TopK operation in C++. It shows how the top_k function is applied to each slice of the input tensor along the specified axis to determine the top k values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/top-k-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i1, ..., i(axis-1), j, i(axis+1) ..., iN] = top_k(input[i1, ...., i(axis-1), :, i(axis+1), ..., iN]), k, sort, mode)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Environment in Build Part\nDESCRIPTION: This snippet shows how to set the OpenVINO environment variables in the build part of the application's snapcraft.yaml. This includes setting the OpenVINO_DIR and LD_LIBRARY_PATH to point to the OpenVINO installation within the snap.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/integrate-openvino-with-ubuntu-snap.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nparts:\n app-build:\n   build-environment:\n     - OpenVINO_DIR: /snap/openvino-libs/current/usr/local/runtime/cmake\n     - LD_LIBRARY_PATH: $LD_LIBRARY_PATH:/snap/openvino-libs/current/usr/local/runtime/3rdparty/tbb/lib\n```\n\n----------------------------------------\n\nTITLE: Convert SavedModel Format (TF2) using Python\nDESCRIPTION: This snippet demonstrates how to convert a TensorFlow 2 SavedModel format to OpenVINO IR using the `openvino.convert_model` function in Python. The input is the path to the SavedModel directory. The output is an OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nov_model = ov.convert_model('path_to_saved_model_dir')\n```\n\n----------------------------------------\n\nTITLE: AsyncInferRequest Destructor (C++)\nDESCRIPTION: The destructor ensures that the inference pipeline is finished before the object is destroyed. It uses the stop_and_wait method of the base class to wait for all tasks in the pipeline to complete, preventing potential resource leaks or crashes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/asynch-inference-request.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nAsyncInferRequest::~AsyncInferRequest() {\n    ov::IAsyncInferRequest::stop_and_wait();\n}\n```\n\n----------------------------------------\n\nTITLE: Check Codestyle with mypy Python\nDESCRIPTION: Checks the code style of the OpenVINO™ Python API using `mypy`. This command runs `mypy` on the specified source directory (`./src/openvino`) using a custom configuration file (`./setup.cfg`).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npython -m mypy ./src/openvino --config-file ./setup.cfg\n```\n\n----------------------------------------\n\nTITLE: Import Libraries for Model Creation (Python)\nDESCRIPTION: This snippet shows the necessary import statements in Python to create a model using OpenVINO opset15 operations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n# [import]\n```\n\n----------------------------------------\n\nTITLE: Convert ClosedJaxpr to OpenVINO (Python)\nDESCRIPTION: This code demonstrates how to convert a JAX function to a ClosedJaxpr object using `jax.make_jaxpr` and then convert it to an OpenVINO model using `ov.convert_model`. It requires the `jax`, `jax.numpy`, and `openvino` libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-jax.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport openvino as ov\n\n# let us have some JAX function\ndef jax_func(x, y):\n    return jax.lax.tanh(jax.lax.max(x, y))\n\n# 1. Create ClosedJaxpr object\nx = jnp.array([1.0, 2.0])\ny = jnp.array([-1.0, 10.0])\njaxpr = jax.make_jaxpr(jax_func)(x, y)\n# 2. Convert to OpenVINO\nov_model = ov.convert_model(jaxpr)\n```\n\n----------------------------------------\n\nTITLE: Building with CMake (Windows)\nDESCRIPTION: This snippet builds the OpenVINO project using cmake, specifying the Debug configuration. It's used in conjunction with the previous snippet to compile the project after it's been configured.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\ncmake --build . --config Debug\n```\n\n----------------------------------------\n\nTITLE: Make Stateful Model Using Parameter/Result Operations in Python\nDESCRIPTION: This snippet demonstrates how to apply the MakeStateful transformation to an OpenVINO model using Parameter and Result operations in Python.  It uses the 'param_res_names' argument with Parameter and Result operation nodes instead of tensor names.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom openvino.tools.ovc.composite_transformation import CompositeTransform\n\nmodel = CompositeTransform(transformations=[(\"MakeStateful\", {\"param_res_names\": {param_node: result_node}})]).apply(model)\n```\n\n----------------------------------------\n\nTITLE: Gather Layer XML Configuration\nDESCRIPTION: Defines the XML configuration for a Gather layer in OpenVINO. Includes input and output port definitions, along with data attributes such as batch_dims.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-8.rst#_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Gather\" version=\"opset8\">\n    <data batch_dims=\"1\" />\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>64</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>\n            <dim>32</dim>\n            <dim>21</dim>\n        </port>\n        <port id=\"2\"/>   <!--  axis = 1  -->\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>2</dim>\n            <dim>32</dim>\n            <dim>21</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Compile Model with Specific GPUs and CUMULATIVE_THROUGHPUT in Python\nDESCRIPTION: This snippet demonstrates compiling a model with AUTO, specifying specific GPU devices (GPU.1, GPU.0) and the CUMULATIVE_THROUGHPUT performance hint in Python.  This allows explicit control over which GPUs are used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\ncompiled_model = core.compile_model(model, \"AUTO:GPU.1,GPU.0\", {hints.performance_mode: hints.PerformanceMode.CUMULATIVE_THROUGHPUT})\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Options\nDESCRIPTION: This snippet sets compile options for the target library based on the configuration (Release) and compiler ID (MSVC, GNU). It disables ignored attributes for newer versions of GCC and optimizes the code size for MSVC Release builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_options(${TARGET_NAME} PRIVATE\n  $<$<CONFIG:Release>:\n    $<IF:$<CXX_COMPILER_ID:MSVC>,/Os,-Os\n    >\n  >\n  $<$<CXX_COMPILER_ID:GNU>:\n    $<$<VERSION_GREATER:$<CXX_COMPILER_VERSION>,6.0>:\n      -Wno-ignored-attributes\n    >\n  >)\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example 4 (same_upper padding)\nDESCRIPTION: This example shows how MaxPool functions with a 4D input, a 2D kernel, and `auto_pad` set to 'same_upper'. It details the input data, strides, kernel size, and the resultant output shapes and values following the MaxPool operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/pooling_shape_rules.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[-1, 2, 3],\n                 [4, 5, -6],\n                 [-7, 8, 9]],\n                [[2, -1, 5],\n                 [6, -7, 1],\n                 [8, 2, -3]]]]   # shape: (1, 2, 3, 3)\n      strides = [1, 1]\n      kernel = [2, 2]\n      rounding_type = \"floor\"\n      auto_pad = \"same_upper\"\n      output0 = [[[[5, 5, 3],\n                   [8, 9, 9]\n                   [8, 9, 9]],\n                  [[6, 5, 5],\n                   [8, 2, 1],\n                   [8, 2, -3]]]]   # shape: (1, 2, 3, 3)\n      output1 = [[[[4, 4, 2],\n                   [7, 8, 8],\n                   [7, 8, 8]],\n                  [[12, 11, 11],\n                   [15, 16, 14],\n                   [15, 16, 17]]]]   # shape: (1, 2, 3, 3)\n```\n\n----------------------------------------\n\nTITLE: Load Checkpoint - TensorFlow 2\nDESCRIPTION: This code snippet demonstrates how to restore the model from a checkpoint in TensorFlow 2 using NNCF's API. This is useful for resuming training from a saved state.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ncompression_ctrl.load_state(\"checkpoint.pth\")\n```\n\n----------------------------------------\n\nTITLE: Double Conversion in Philox Algorithm (C++)\nDESCRIPTION: This code snippet describes how to set the sign, exponent and mantissa to get a double value using Philox algorithm, starting from two concatenated uint32 values from random integer generator.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nsign = 0\nexponent = 1023 // representation of a zero exponent.\nmantissa = 52 // right bits from two concatenated uint32 values from random integer generator.\n```\n\n----------------------------------------\n\nTITLE: NV12 Preprocessing with Two Planes (Single Batch) in OpenVINO (C)\nDESCRIPTION: Demonstrates how to set input tensors with a vector of shared surfaces for each plane when batching and surface sharing are required simultaneously, using NV12 format with two planes in a single batch scenario using the C API. It utilizes the ov::InferRequest::set_tensors method to provide the shared surfaces to the OpenVINO inference engine. Requires OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_35\n\nLANGUAGE: c\nCODE:\n```\n//! [single_batch]\nsize_t shape[2] = {height + height / 2, width};\nov_tensor_t input_tensor = {0};\nov_core_create_tensor(OV_TYPE_U8, shape, 2, input_data, &input_tensor);\nov_input_port_t input_port = {0};\nov_compiled_model_get_input(compiled_model, &input_port);\nov_infer_request_set_tensor(infer_request, input_port, input_tensor);\n//! [single_batch]\n```\n\n----------------------------------------\n\nTITLE: ScatterUpdate Example 1 in XML\nDESCRIPTION: This example demonstrates a ScatterUpdate layer configuration in XML. It defines the input data, indices, updates, and axis tensors, along with their dimensions, and the resulting output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-update-3.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ScatterUpdate\">\n    <input>\n        <port id=\"0\">  <!-- data -->\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>10</dim>\n            <dim>15</dim>\n        </port>\n        <port id=\"1\">  <!-- indices -->\n            <dim>125</dim>\n            <dim>20</dim>\n        </port>\n        <port id=\"2\">  <!-- updates -->\n            <dim>1000</dim>\n            <dim>125</dim>\n            <dim>20</dim>\n            <dim>10</dim>\n            <dim>15</dim>\n        </port>\n        <port id=\"3\">   <!-- axis -->\n            <dim>1</dim> <!-- value [1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"4\" precision=\"FP32\"> <!-- output -->\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>10</dim>\n            <dim>15</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for DPC++ Build (Windows)\nDESCRIPTION: This CMake configuration snippet sets the C and C++ compilers to icx, enabling DPC++ compilation. It also disables the Intel CPU plugin and system OpenCL to avoid potential conflicts during the build process. The snippet also includes flags to help resolve GCC version issues and enables ccache for faster rebuilds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_with_sycl.md#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\n-DCMAKE_C_COMPILER:FILEPATH=icx -DCMAKE_CXX_COMPILER:FILEPATH=icx\n-DENABLE_INTEL_CPU=OFF\n-DENABLE_SYSTEM_OPENCL=OFF\n-DCMAKE_CXX_COMPILER_LAUNCHER=ccache\n```\n\n----------------------------------------\n\nTITLE: Creating a symbolic link for python headers\nDESCRIPTION: This snippet creates a symbolic link to allow Python to find the `pyconfig.h` header file in `/usr/include/python3.10/`. This header is typically located in `/usr/include/riscv64-linux-gnu/` during cross-compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\nln -s /usr/include/riscv64-linux-gnu/ /usr/include/python3.10/\n```\n\n----------------------------------------\n\nTITLE: Using Or for pattern matching\nDESCRIPTION: This Python snippet demonstrates how to use the Or operator to match either one sequence of nodes or another, effectively creating two distinct branches in the pattern matching process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.tools.ovc.composite.pattern import WrapType, Or\n\ndef pattern_or():\n    # Creating two different branches for the pattern\n    relu_node = WrapType(\"opset13.Relu\")\n    sigmoid_node = WrapType(\"opset13.Sigmoid\")\n\n    # Using Or to create two different branches for matching\n    pattern_or_node = Or(relu_node, sigmoid_node)\n\n    return pattern_or_node\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to Target in CMake\nDESCRIPTION: Links the executable target `ov_capi_test` with the `openvino_c`, `openvino::util`, `common_test_utils`, and `gtest_main` libraries.  These are the dependencies required for the tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino_c openvino::util\n    common_test_utils gtest_main)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO Environment Variables\nDESCRIPTION: These commands navigate to the OpenVINO installation directory and source the `setupvars.sh` script to set the necessary environment variables for compiling and running OpenVINO applications.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-macos.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncd /opt/intel/openvino_2025\nsource /opt/intel/openvino_2025/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: GatherND Element Gathering Example\nDESCRIPTION: This example demonstrates how GatherND gathers elements from the data tensor based on the provided indices. The indices tensor specifies the coordinates within the data tensor from which elements are extracted to form the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-5.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nindices = [[0, 0],\n              [1, 0]]\ndata    = [[1, 2],\n              [3, 4]]\noutput  = [1, 3]\n```\n\n----------------------------------------\n\nTITLE: Gather-7 Example with axis > batch_dims in shell\nDESCRIPTION: Illustrates Gather-7 with batch_dims set to 1 and axis to 2, demonstrating the operation where the axis is greater than batch_dims.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-7.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 1\naxis = 2\n\nindices = [[1, 2, 4],  <-- this is applied to the first batch\n           [4, 3, 2]]  <-- this is applied to the second batch\nindices_shape = (2, 3)\n\ndata = [[[[ 1,  2,  3,  4], <-- first batch\n          [ 5,  6,  7,  8],\n          [ 9, 10, 11, 12],\n          [13, 14, 15, 16],\n          [17, 18, 19, 20]]],\n\n          [[[21, 22, 23, 24], <-- second batch\n          [25, 26, 27, 28],\n          [29, 30, 31, 32],\n          [33, 34, 35, 36],\n          [37, 38, 39, 40]]]]\ndata_shape = (2, 1, 5, 4)\n\noutput = [[[[ 5,  6,  7,  8],\n            [ 9, 10, 11, 12],\n            [17, 18, 19, 20]]],\n\n            [[[37, 38, 39, 40],\n            [33, 34, 35, 36],\n            [29, 30, 31, 32]]]]\noutput_shape = (2, 1, 3, 4)\n```\n\n----------------------------------------\n\nTITLE: Device Specific Remote Tensor Public API Header\nDESCRIPTION: This code snippet demonstrates the header-only public API for device-specific remote tensors. It includes a static type_check() method to verify the remote tensor type and get_data() methods to access the remote data. This interface is designed to be independent of the plugin library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/remote-tensor.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n/*! [remote_tensor:public_header]\n* \\brief Example of a public remote tensor API\n*/\n#pragma once\n\n#include <openvino/runtime/remote_tensor.hpp>\n\n#include <memory>\n#include <vector>\n\nnamespace template_plugin {\n\n/**\n * \\brief Abstract Remote Tensor for Template Plugin Device.\n *        The class represents memory which can be located somewhere (for example on GPU) and this memory can be used for Inference.\n *        The class is device specific that means the class knows on which device memory is located.\n */\nclass RemoteTensor : public ov::RemoteTensor {\npublic:\n    /**\n     * \\brief The method to understand that abstract \\ref ov::RemoteTensor can be casted to this particular RemoteTensor type.\n     * \\param remote_tensor - Abstract Remote Tensor.\n     * \\return Returns true if cast is possible.\n     */\n    static bool type_check(const ov::RemoteTensor& remote_tensor);\n\n    /**\n     * \\brief Returns the pointer to the data.\n     * \\return Read only pointer to the data.\n     */\n    virtual const void* get_data() const = 0;\n\n    /**\n     * \\brief Returns the pointer to the data.\n     * \\return Pointer to the data which can be used to copy output from network.\n     */\n    virtual void* get_data() = 0;\n\n    /**\n     * \\brief Returns shared context associated with a remote tensor.\n     * \\return Context which contains device handle and its context.\n     */\n    virtual std::shared_ptr<ov::RemoteContext> get_context() const = 0;\n};\n\n}  // namespace template_plugin\n/*! [remote_tensor:public_header] */\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Command to Generate Kernel Sources and Headers\nDESCRIPTION: Defines a custom command that uses a Python script (`CODEGEN_SCRIPT`) to generate OpenCL kernel source and header files from `.cl` files. It creates the necessary directory and specifies inputs and outputs for the code generation process, including kernel directories, header directories and target languages. The command depends on the kernel files and the code generation script itself.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/ocl_v2/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(OUTPUT \"${CODEGEN_CACHE_DIR}/${KERNEL_SOURCES}\"\n  COMMAND \"${CMAKE_COMMAND}\" -E make_directory \"${CODEGEN_CACHE_DIR}\"\n  COMMAND \"${Python3_EXECUTABLE}\" \"${CODEGEN_SCRIPT}\" -out_sources \"${CODEGEN_CACHE_DIR}/${KERNEL_SOURCES}\"\n                                                      -out_headers \"${CODEGEN_CACHE_DIR}/${KERNEL_HEADERS}\"\n                                                      -in_kernels_dir \"${CMAKE_CURRENT_SOURCE_DIR}/\"\n                                                      -in_headers_dir \"${MAIN_DIR}/src/kernel_selector/cl_kernels/include\"\n                                                      -lang \"ocl\"\n  DEPENDS ${KERNELS} \"${CODEGEN_SCRIPT}\"\n  COMMENT \"Generating ${CODEGEN_CACHE_DIR}/${KERNEL_SOURCES} ...\"\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing USM Device Memory C++\nDESCRIPTION: Accessing `usm_device` memory requires copying the data into host memory first, as the host cannot directly access it. This is necessary to ensure that the buffer can be read from the host and used for debugging purposes like layer in/out buffer dumps. This ensures data consistency and allows inspection of data residing on the device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/execution_of_inference.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// If it is `usm_device`, it is accessed after copying the data into host memory because the host cannot access `usm_device` directly.\n```\n\n----------------------------------------\n\nTITLE: Linking Target to TensorFlow Lite Frontend in CMake\nDESCRIPTION: This command links the `tensorflow_lite_fe_standalone_build_test` target to the `openvino_tensorflow_lite_frontend` library. The `PUBLIC` keyword ensures that the dependency is propagated to downstream targets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/tests/standalone_build/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino_tensorflow_lite_frontend)\n```\n\n----------------------------------------\n\nTITLE: ScatterNDUpdate Example 1 (C++)\nDESCRIPTION: This C++ snippet demonstrates a simple update case with the 'reduction' attribute set to 'none'.  It shows how elements in the 'data' array are updated at specific indices with corresponding values from the 'updates' array. No dependencies required as it is example data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-nd-update-15.rst#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ndata    = [1, 2, 3, 4, 5, 6, 7, 8]\nindices = [[4], [3], [1], [7], [-2], [-4]]\nupdates = [9, 10, 11, 12, 13, 14]\noutput  = [1, 11, 3, 10, 4, 6, 13, 12]\n```\n\n----------------------------------------\n\nTITLE: Building Target Faster with Unity Builds in CMake\nDESCRIPTION: This snippet calls the custom CMake function `ov_build_target_faster` to enable unity builds for the `op_conformance_utils` target. Unity builds can improve compilation speed by merging multiple source files into a single compilation unit.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/op_conformance_utils/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME} UNITY)\n```\n\n----------------------------------------\n\nTITLE: Enable Dynamic Shapes Support in Kernel\nDESCRIPTION: This code snippet demonstrates how to enable dynamic shapes support for a kernel by setting the EnableDynamicShapesSupport flag in the ParamsKey.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/dynamic_shape/dynamic_impl.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nParamsKey CumSumKernelRef::GetSupportedKey() const {\n    ParamsKey k;\n    k.EnableDynamicShapesSupport();\n    return k;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library (CMake)\nDESCRIPTION: This snippet installs the static library to the specified location. It uses a custom function `ov_install_static_lib` to handle the installation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/shape_inference/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: InferRequest Constructor C++\nDESCRIPTION: This code snippet shows the constructor for the InferRequest class. It initializes helper fields and calls methods that allocate tensors, preparing the request for inference. It utilizes input/output information from the compiled model to understand shape and element type of tensors, settable with ov::InferRequest::set_tensor and retrievable with ov::InferRequest::get_tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/synch-inference-request.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nInferRequest::InferRequest() : ov::ISyncInferRequest() {\n    // Initialize helper fields\n\n    // Allocate tensors\n    allocate_tensors();\n}\n```\n\n----------------------------------------\n\nTITLE: Setting environment variables for Python API\nDESCRIPTION: These commands set the necessary environment variables for using the OpenVINO Runtime Python API. They update `PYTHONPATH`, `LD_LIBRARY_PATH`, and `PATH` to include the paths to the built Python libraries, shared libraries, and OpenVINO Compiler (OVC) tools, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_linux.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nexport PYTHONPATH=<openvino_repo>/bin/intel64/Release/python:<openvino_repo>/tools/ovc:$PYTHONPATH\nexport LD_LIBRARY_PATH=<openvino_repo>/bin/intel64/Release:$LD_LIBRARY_PATH\nexport PATH=<openvino_repo>/tools/ovc/openvino/tools/ovc:$PATH\n```\n\n----------------------------------------\n\nTITLE: DFT Layer XML Definition (No signal_size, 3D input)\nDESCRIPTION: Defines a DFT layer in XML format without the signal_size input for a 3-dimensional input tensor. It includes input and output port dimensions, with the axes parameter indicating the dimensions for DFT calculation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/dft-7.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"DFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- axes input contains [0, 1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Download and extract Python\nDESCRIPTION: Downloads the Python source code archive from the Python releases page using `curl` and extracts it using `tar`. This prepares the source code for building and installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/python_version_upgrade.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -O https://www.python.org/ftp/python/3.8.13/Python-3.8.13.tgz\ntar -xf Python-3.8.13.tgz\n```\n\n----------------------------------------\n\nTITLE: Run TensorFlow 1 Layer Tests\nDESCRIPTION: This command runs the entire suite of TensorFlow 1 layer tests using py.test. It validates the support of TensorFlow 1 operations by the OpenVINO TensorFlow Frontend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npy.test tensorflow_tests\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Minimum Reduction - C++\nDESCRIPTION: This code snippet demonstrates the 'min' reduction mode for the ScatterElementsUpdate operation. The minimum value between the corresponding element from the updates tensor and the element in the data tensor at the specified index is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\noutput[indices[i]] = min(updates[i], output[indices[i]]) axis = 0\n```\n\n----------------------------------------\n\nTITLE: Defining oneDNN Symbols for Static Builds\nDESCRIPTION: This snippet parses oneDNN API symbols from header files and defines them with a prefix `ov_cpu_` if `BUILD_SHARED_LIBS` is not enabled. It is designed for static library builds to avoid symbol conflicts.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n    # Symbols are located in both src and include folders\n    file(GLOB_RECURSE onednn_files\n        \"${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/onednn/include/*.cpp\"\n        \"${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/onednn/include/*.hpp\"\n        \"${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/onednn/include/*.h\"\n        \"${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/onednn/include/*.hpp\"\n        \"${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/onednn/src/*.cpp\"\n        \"${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/onednn/src/*.hpp\"\n        \"${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/onednn/src/*.h\"\n        \"${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/onednn/src/*.hpp\")\n\n    # parse API symbols\n    foreach(onednn_file IN LISTS onednn_files)\n        # symbols in form:\n        # dnnl_status_t DNNL_API dnnl_engine_get_kind\n        file(STRINGS \"${onednn_file}\" onednn_symbols_defined_on_single_line\n             REGEX \"DNNL_API[ \\*]*dnnl[a-zA-Z0-9_]*\")\n        # symbols in form (cmake has issue with symbols defined on multiple lines and we have to use new pattern):\n        # dnnl_status_t DNNL_API\n        # dnnl_engine_get_kind\n        file(STRINGS \"${onednn_file}\" onednn_symbols_defined_on_multiple_lines\n             REGEX \"^dnnl[a-zA-Z0-9_]*\\\\(\")\n        # symbols in form:\n        # typedef struct dnnl_graph_graph *dnnl_graph_graph_t;\n        file(STRINGS \"${onednn_file}\" onednn_symbols_typedef\n             REGEX \"^typedef struct dnnl_.*\")\n\n        if(onednn_symbols_defined_on_single_line OR\n           onednn_symbols_defined_on_multiple_lines OR\n           onednn_symbols_typedef)\n            # parse concrete symbols from read line\n            string(REGEX MATCHALL \"dnnl[a-zA-Z0-9_]+\" onednn_parsed_symbols\n                ${onednn_symbols_defined_on_single_line}\n                ${onednn_symbols_defined_on_multiple_lines}\n                ${onednn_symbols_typedef})\n            list(APPEND onednn_symbols ${onednn_parsed_symbols})\n        endif()\n    endforeach()\n\n    # remove all duplicates\n    list(REMOVE_DUPLICATES onednn_symbols)\n\n    # also override namespaces\n    list(APPEND onednn_symbols dnnl oneapi)\n\n    # redefine all collected symbols\n    foreach(onednn_symbol IN LISTS onednn_symbols)\n        if(NOT onednn_symbol MATCHES \"^#.+\")\n            add_compile_definitions(${onednn_symbol}=ov_cpu_${onednn_symbol})\n        endif()\n    endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Static Library (CMake)\nDESCRIPTION: This snippet creates a static library named `${TARGET_NAME}` using the collected source files and generated code.  The `STATIC` keyword specifies that a static library should be created.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${LIBRARY_SRC} ${CODEGEN_CACHE_SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Run Memory Tests with Pytest\nDESCRIPTION: This snippet illustrates how to run memory tests using pytest. It invokes pytest on a test file and passes the path to the test executable as an argument. Another example runs pytest directly on the `run_memorytest.py` script, presumably for testing the parsing statistics functionality of the script.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest ./test_runner/test.py --exe <install_path>/tests/memorytest_infer\n# For parse_stat testing:\npytest ./scripts/run_memorytest.py\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Product Reduction - C++\nDESCRIPTION: This code snippet demonstrates the 'prod' reduction mode for the ScatterElementsUpdate operation. The corresponding elements from the updates tensor are multiplied with the elements in the data tensor at the specified indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\noutput[indices[i]] *= updates[i], axis = 0\n```\n\n----------------------------------------\n\nTITLE: Shape Prediction Function Signature C++\nDESCRIPTION: This is the signature of the `predict_preallocation_shape` function in the ShapePredictor class. It takes the primitive ID, current shape, data type bitwidth, and a boolean indicating if the buffer can be reused, and returns a pair indicating if prediction was successful and the predicted shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/dynamic_shape/memory_preallocation.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstd::pair<bool, ov::Shape> predict_preallocation_shape(const std::string& id,\n                                                       const ov::Shape& current_shape,\n                                                       size_t dt_bitwidth,\n                                                       bool can_reuse_buffer);\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Constant Mode Example (Positive Pads) - C++\nDESCRIPTION: Illustrates padding with the constant mode where new elements are filled with a predefined value (or zero if not provided). pads_begin and pads_end specify the number of elements to add to the beginning and end of each axis, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\npads_begin = [0, 1]\npads_end = [2, 3]\n\nDATA =\n[[1,  2,  3,  4]\n[5,  6,  7,  8]\n[9, 10, 11, 12]]\n\npad_mode = \"constant\"\n\nOUTPUT =\n[[ 0,  1,  2,  3,  4,  0,  0,  0 ]\n[ 0,  5,  6,  7,  8,  0,  0,  0 ]\n[ 0,  9, 10, 11, 12,  0,  0,  0 ]\n[ 0,  0,  0,  0,  0,  0,  0,  0 ]\n[ 0,  0,  0,  0,  0,  0,  0,  0 ]]\n```\n\n----------------------------------------\n\nTITLE: Running Python ONNX Standard Compliance Tests (Installation Layout)\nDESCRIPTION: This command runs the Python tests to confirm operator compliance with the ONNX standard using pytest in the installation layout.  It excludes tests related to CUDA. Replace <OV_INSTALL_DIR> with the OpenVINO installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\npytest <OV_INSTALL_DIR>/tests/pyopenvino/tests/test_onnx/test_backend.py -sv -k 'not cuda'\n```\n\n----------------------------------------\n\nTITLE: RDFT Layer XML Configuration (No signal_size, 3D Input)\nDESCRIPTION: Configures an RDFT layer in XML without a signal_size input, processing a 3D input tensor. The layer takes a 3D tensor as input and transforms it using the specified axes [1, 2], resulting in a 4D output tensor. The output dimensions are adjusted based on the Fourier transform.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/rdft-9.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"RDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>320</dim>\n            <dim>320</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- axes input contains [1, 2] -->\n        </port>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>320</dim>\n            <dim>161</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Set Static Input Shape (Python)\nDESCRIPTION: This Python code snippet shows how to set static input shapes for a model. It compiles the model with a specific input shape, optimizing performance when input shapes don't change between inferences. It uses reshape method on compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\nmodel.reshape({\"input_0\": ov.Shape([1, 3, 224, 224])})\ncompiled_model = core.compile_model(model, \"CPU\")\n```\n\n----------------------------------------\n\nTITLE: Activate Virtual Environment (Windows)\nDESCRIPTION: This command activates the virtual environment 'openvino_env' on Windows.  Activating the environment ensures that subsequent pip commands install packages within the isolated environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nopenvino_env\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Pattern with Arbitrary Number of Inputs (C++)\nDESCRIPTION: This code snippet shows how to construct a pattern that matches an operation with an arbitrary number of inputs. This example demonstrates a pattern for a `Concat` operation where the number of input tensors can vary.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/matcher-pass.rst#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n// [pattern:concat_example]\n#include <openvino/core/node.hpp>\n#include <openvino/opsets/opset10.hpp>\n#include <openvino/pass/pattern/op/any_input.hpp>\n\n#include \"openvino/pass/graph_rewrite.hpp\"\n\nusing namespace std;\nusing namespace ov::opset10;\n\n{\n    OutputVector inputs;\n    for (size_t i = 0; i < 3; ++i) {\n        inputs.push_back(ov::pass::pattern::any_input());\n    }\n    auto concat = make_shared<Concat>(inputs, 1);\n}\n// [pattern:concat_example]\n```\n\n----------------------------------------\n\nTITLE: MVN Kernel Selector Instance in C++\nDESCRIPTION: This code defines a `mvn_kernel_selector` class, derived from `kernel_selector_base`, responsible for managing available kernels for the MVN (Mean Variance Normalization) operation. It uses a singleton pattern to ensure a single instance and includes methods for attaching available kernels and selecting the optimal kernel based on given parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_kernels.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nclass mvn_kernel_selector : public kernel_selector_base {\npublic:\n    static mvn_kernel_selector& Instance() {\n        static mvn_kernel_selector instance_;\n        return instance_;\n    }\n\n    mvn_kernel_selector();\n\n    KernelsData GetBestKernels(const Params& params) const override;\n}\n\n// The list of available kernels is usually specified in kernel_selector c-tor using `Attach` method whith creates instance of each type\n// and append it to implementations list.\n// In this case we have 3 available kernels for MVN operation. Kernels might have different priorities and support only subset of operation parameters\n// E.g. MVNKernel_b_fs_yx_fsv16_imad supports only `fsv16` blocked layouts and INT8/UINT8 input data types\nmvn_kernel_selector::mvn_kernel_selector() {\n    Attach<MVNKernelRef>();\n    Attach<MVNKernelBfyxOpt>();\n    Attach<MVNKernel_b_fs_yx_fsv16_imad>();\n}\n\n// This method is used to get the optimal kernel for given parameters\n// There are 2 base methods to pick optimal kernels: `GetNaiveBestKernel` and `GetAutoTuneBestKernel`\n// If kernel supports auto tuning, then it uses `GetAutoTuneBestKernel`, otherwise, it uses `GetNaiveBestKernel`\n// parameterized with `KernelType` which specifies the operation type which is implemented by the specific kernel selector\nKernelsData mvn_kernel_selector::GetBestKernels(const Params& params) const {\n    return GetNaiveBestKernel(params, options, KernelType::MVN);\n}\n```\n\n----------------------------------------\n\nTITLE: Run benchmark (Python)\nDESCRIPTION: This command executes the throughput_benchmark.py script with a specified model file path. The script will perform the benchmark and report the throughput.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/throughput-benchmark.rst#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\npython throughput_benchmark.py ./models/googlenet-v1.xml\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This snippet sets the target name to 'ov_snippets_models' using the `set` command. This variable is used later to define and configure the target within the CMake build system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/ov_helpers/ov_snippets_models/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_snippets_models)\n```\n\n----------------------------------------\n\nTITLE: Running Code Style Checks with Flake8 and Mypy (Shell)\nDESCRIPTION: This shell script demonstrates how to run code style checks using flake8 and mypy on the OpenVINO™ Python API. It first navigates to the Python API directory and then executes flake8 and mypy with a specified configuration file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/contributing.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ncd .../openvino/src/bindings/python/\n\nflake8 src/openvino/ --config=setup.cfg\nflake8 tests/ --config=setup.cfg\n\nmypy src/openvino/ --config-file ./setup.cfg\n```\n\n----------------------------------------\n\nTITLE: CMake Integration for C++ (PyPI)\nDESCRIPTION: This CMake snippet demonstrates how to integrate OpenVINO with a C++ application when OpenVINO is installed via PyPI.  It finds the OpenVINO package, sets the include directories, and links the required libraries.  This approach is specific to PyPI-based OpenVINO installations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_41\n\nLANGUAGE: cpp\nCODE:\n```\n#! [cmake:integration_example_cpp_py]\ncmake_minimum_required(VERSION 3.13)\nproject(openvino_example)\n\nfind_package(OpenVINO REQUIRED COMPONENTS runtime)\n\nadd_executable(${PROJECT_NAME} main.cpp)\ntarget_include_directories(${PROJECT_NAME} PUBLIC ${OpenVINO_INCLUDE_DIRS})\ntarget_link_libraries(${PROJECT_NAME} ${OpenVINO_LIBRARIES})\n#! [cmake:integration_example_cpp_py]\n```\n\n----------------------------------------\n\nTITLE: Running OpenVINO Setup Script in PowerShell on Windows\nDESCRIPTION: This command runs the `setupvars.sh` script to set up the OpenVINO environment variables in PowerShell on Windows. It is a prerequisite for using OpenVINO Runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n<INSTALL_DIR>/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: RandomUniform Example (f32)\nDESCRIPTION: Example of RandomUniform output with initial_seed = 150, output_type = f32, and alignment = PYTORCH. The input shape is [3, 3].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\ninput_shape  = [ 3, 3 ]\noutput  = [[0.6789123  0.31274895 0.91842768] \\\n           [0.9312087   0.13456984 0.49623574] \\\n           [0.5082716   0.23938411 0.97856429]]\n```\n\n----------------------------------------\n\nTITLE: BatchNormInference Example (4D input) XML\nDESCRIPTION: This XML snippet illustrates the configuration of a BatchNormInference layer in OpenVINO for a 4D input tensor. It specifies the input and output port dimensions, along with the epsilon attribute. The input tensor has dimensions 1x3x224x224, and the gamma, beta, mean, and variance tensors have a dimension of 3.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/batch-norm-inference-5.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"BatchNormInference\" ...>\n    <data epsilon=\"9.99e-06\" />\n    <input>\n        <port id=\"0\">  <!-- input -->\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">  <!-- gamma -->\n            <dim>3</dim>\n        </port>\n        <port id=\"2\">  <!-- beta -->\n            <dim>3</dim>\n        </port>\n        <port id=\"3\">  <!-- mean -->\n            <dim>3</dim>\n        </port>\n        <port id=\"4\">  <!-- variance -->\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"5\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Float32 Value Calculation (C++)\nDESCRIPTION: This code snippet calculates the float32 value from a uint32 random value x, using the pre-set exponent. It performs a bitwise operation and subtraction to get the final float32 value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nval = ((exponent << 23) | x & 0x7fffffu) - 1.0,\n```\n\n----------------------------------------\n\nTITLE: DepthToSpace with depth_first Mode in Python\nDESCRIPTION: This Python code snippet demonstrates the transformation of the input tensor when the 'mode' attribute is set to 'depth_first'. It reshapes and transposes the input data differently to move depth information into spatial dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/depth-to-space-1.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nx' = reshape(data, [N, C / (block_size ^ K), block_size, block_size, ..., block_size, D1, D2, ..., DK])\nx'' = transpose(x', [0,  1,  K + 2, 2, K + 3, 3, K + 4, 4, ..., K + (K + 1), K + 1])\ny = reshape(x'', [N, C / (block_size ^ K), D1 * block_size, D2 * block_size, D3 * block_size, ..., DK * block_size])\n```\n\n----------------------------------------\n\nTITLE: Define Model getOutputShape Method (TypeScript)\nDESCRIPTION: This code defines the `getOutputShape` method of the `Model` interface. It returns the shape of the output at the specified index as an array of numbers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\ngetOutputShape(): number[]\n```\n\n----------------------------------------\n\nTITLE: MatcherPass Example (Python)\nDESCRIPTION: This Python code demonstrates how to create and use a MatcherPass to replace ReLU operations with Sigmoid operations.  It involves defining a pattern for the ReLU operation and a callback function that performs the replacement when the pattern is matched.  The MatcherPass is then applied to a model to perform the transformation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/matcher-pass.rst#_snippet_2\n\nLANGUAGE: py\nCODE:\n```\n# [matcher_pass:ov_matcher_pass_py]\nimport openvino.runtime as ov\nfrom openvino.runtime import Node, Model\nfrom openvino.runtime import opset10 as opset\nfrom openvino.runtime.passes import MatcherPass, GraphRewrite\n\ndef relu_to_sigmoid():\n    \"\"\"\n    This example shows how to create and use MatcherPass to replace\n    ReLU operation to Sigmoid operation.\n    \"\"\"\n\n    class ReluToSigmoid(MatcherPass):\n        \"\"\"\n        Transformation that replaces ReLU operations to Sigmoid operations.\n        \"\"\"\n\n        def __init__(self):\n            \"\"\"\n            Initialize Relu->Sigmoid transformation.\n            \"\"\"\n            # define a pattern\n            pattern = opset.relu(ov.op.Parameter(ov.runtime.element.f32, ov.Shape([1, 3, 64, 64]))) # type: ignore\n\n            # define a callback function that will be applied when the pattern is found\n            def callback(matcher: ov.pass.Matcher) -> bool:\n                \"\"\"\n                Transformation function that replaces the matched ReLU node with a Sigmoid.\n\n                :param matcher: matcher that contains the matched nodes.\n                :return:\n                \"\"\"\n                relu = matcher.get_match()\n                data = relu.input_value(0)\n\n                # create nodes\n                sigmoid = opset.sigmoid(data)\n\n                # replace a sub-graph\n                sigmoid.output(0).replace(relu.output(0))\n                return True\n\n            # initialize MatcherPass\n            super().__init__()\n            # register the pattern and callback function\n            self.register_matcher(ov.pass.Matcher(pattern, \"ReluToSigmoid\"), callback)\n\n    # create a model\n    shape = [1, 3, 64, 64]\n    param = opset.parameter(shape, dtype=ov.runtime.element.f32)\n    relu = opset.relu(param)\n    model = Model([relu], [param], \"model\")\n\n    # apply the transformation\n    pass_manager = GraphRewrite()\n    pass_manager.add_matcher_pass(ReluToSigmoid())\n    pass_manager.run_on_model(model)\n\n    return model\n\n# [matcher_pass:ov_matcher_pass_py]\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library in CMake\nDESCRIPTION: This snippet creates an OBJECT library named `${TARGET_NAME}_obj` from the specified source and header files. It also sets private compile definitions and builds the target faster with unity builds and precompiled headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME}_obj OBJECT ${LIBRARY_SRC} ${PUBLIC_HEADERS})\ntarget_compile_definitions(${TARGET_NAME}_obj PRIVATE IMPLEMENT_OPENVINO_API)\n\nov_build_target_faster(${TARGET_NAME}_obj\n    UNITY\n    PCH PRIVATE \"src/precomp.hpp\"\n)\n```\n\n----------------------------------------\n\nTITLE: ReadValue XML Layer Definition\nDESCRIPTION: This XML snippet defines a ReadValue layer in OpenVINO. It specifies the variable_id, input dimensions, and output dimensions. The ReadValue operation retrieves the value associated with 'lstm_state_1'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/read-value-3.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"ReadValue\" ...>\n    <data variable_id=\"lstm_state_1\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ReduceL2 reducing channel and spatial dims in OpenVINO XML\nDESCRIPTION: This example shows ReduceL2 reducing across channel and spatial dimensions (`axes` = [1]) and `keep_dims` is false, so the channel dimension is removed from the output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-l2-4.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceL2\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [1] that means independent reduction in each channel and spatial dimensions -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Running Python ONNX Operators Tests (Build Layout)\nDESCRIPTION: This command runs the Python ONNX operators tests using pytest in the build layout.  It excludes the 'test_zoo_models.py' and 'test_backend.py' files from the test execution.  Replace <OV_REPO_DIR> with the OpenVINO repository directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\npytest <OV_REPO_DIR>/src/bindings/python/tests/test_onnx \\\n    --ignore=<OV_REPO_DIR>/src/bindings/python/tests/test_onnx/test_zoo_models.py \\\n    --ignore=<OV_REPO_DIR>/src/bindings/python/tests/test_onnx/test_backend.py\n```\n\n----------------------------------------\n\nTITLE: Activating a Virtual Environment with pyenv\nDESCRIPTION: This shell command activates the virtual environment named 'ov-py310' using pyenv. Activating the environment modifies the shell's PATH so that the Python interpreter and installed packages within the environment are used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/build.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npyenv activate ov-py310\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target\nDESCRIPTION: This snippet adds a custom target for running clang-format on the source code of the library. `ov_add_clang_format_target` is a custom function (likely defined elsewhere) that handles the clang-format configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/test_builtin_extensions/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Enable LTO\nDESCRIPTION: Enables Link Time Optimization (LTO) if the `ENABLE_LTO` variable is set. This improves performance by allowing the linker to perform optimizations across multiple compilation units.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/functional/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_LTO)\n  set(CMAKE_INTERPROCEDURAL_OPTIMIZATION_RELEASE ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Retrieve Specific Input/Output Port by Index (C++)\nDESCRIPTION: This C++ snippet demonstrates retrieving a specific input or output port of an OpenVINO model using its index. This method is recommended when tensor names are not available or reliable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nauto ov_model_input = ov_model->input(index);\nauto ov_model_output = ov_model->output(index);\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO samples\nDESCRIPTION: This command builds the OpenVINO C++ samples in Release mode, utilizing multiple cores for parallel compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/use_device_mem.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nopenvino_install\\samples\\cpp> cmake --build build --config Release --parallel\n```\n\n----------------------------------------\n\nTITLE: Adding the Plugin with ov_add_plugin\nDESCRIPTION: This snippet utilizes the `ov_add_plugin` function to define the plugin, specify its device name, source files, and configure version definitions for a particular source file. It relies on variables defined earlier such as `${NPU_PLUGIN_TARGET}`, `${NPU_DEVICE_NAME}`, `${SOURCES}`, `${NPUW_SOURCES}`, and `${NPU_PLUGIN_ENGINE_SOURCE_FILE}`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/plugin/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_plugin(NAME ${NPU_PLUGIN_TARGET}\n    DEVICE_NAME ${NPU_DEVICE_NAME}\n    SOURCES ${SOURCES} ${NPUW_SOURCES}\n    VERSION_DEFINES_FOR ${NPU_PLUGIN_ENGINE_SOURCE_FILE}\n)\n```\n\n----------------------------------------\n\nTITLE: 2D ConvolutionBackpropData with output_shape Input Example\nDESCRIPTION: Configuration of a 2D ConvolutionBackpropData layer using the output_shape input. This shows how to specify the output shape directly as an input tensor.  The parameters for dilations, padding and strides are defined, along with auto_pad set to valid.  The output_shape is indicated by a comment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/convolution-backprop-data-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"5\" name=\"upsampling_node\" type=\"ConvolutionBackpropData\">\n    <data dilations=\"1,1\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"1,1\" output_padding=\"0,0\" auto_pad=\"valid\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>20</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>20</dim>\n            <dim>10</dim>\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n        <port id=\"2\">\n            <dim>2</dim> <!-- output_shape value is: [450, 450]-->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>10</dim>\n            <dim>450</dim>\n            <dim>450</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding OpenVINO Plugin\nDESCRIPTION: This snippet uses the `ov_add_plugin` function to create a shared library for the plugin.  It specifies the name, device name, sources, and other options such as Clang format and version defines.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_plugin(NAME ${TARGET_NAME}\n              DEVICE_NAME \"TEMPLATE\"\n              SOURCES ${SOURCES} ${HEADERS}\n              ${skip_plugin}\n              VERSION_DEFINES_FOR plugin.cpp\n              ADD_CLANG_FORMAT)\n```\n\n----------------------------------------\n\nTITLE: Run Specific Test Case\nDESCRIPTION: Runs a specific test case by providing the full path to the test file and the test function using `pytest`. This allows for focused testing of individual components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npytest tests/test_runtime/test_core.py::test_available_devices\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Semicolon Spacing\nDESCRIPTION: This rule enforces consistent spacing around semicolons in JavaScript and TypeScript code. It improves code aesthetics and is enforced by ESLint with the configuration `semi-spacing: ['error']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\nsemi-spacing: ['error']\n```\n\n----------------------------------------\n\nTITLE: Wrapping ov::Core creation in C\nDESCRIPTION: This C code snippet demonstrates how the C API wraps the C++ `ov::Core` creation. It includes parameter checking and handling the shared pointer to the C++ object. This example focuses on the function with the default `xml_config_file` parameter.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/how_to_wrap_openvino_interfaces_with_c.md#_snippet_1\n\nLANGUAGE: C\nCODE:\n```\nhttps://github.com/openvinotoolkit/openvino/blob/d96c25844d6cfd5ad131539c8a0928266127b05a/src/bindings/c/src/ov_core.cpp#L48-L64\n```\n\n----------------------------------------\n\nTITLE: Interval Dimension C++\nDESCRIPTION: These code snippets demonstrate how to create interval Dimension objects in OpenVINO using C++.\nThey initialize `ov::Dimension` objects representing a dimension with a minimum and maximum value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/docs/shape_propagation.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nov::Dimension{1, 10};\nov::Dimension{0, MAX_INT};\n```\n\n----------------------------------------\n\nTITLE: Run Docker Image (CPU and GPU)\nDESCRIPTION: Runs the Docker image with GPU support enabled (requires Intel GPU). It maps the `/dev/dri` device and adds the user to the render group. Port 8888 is mapped for accessing the notebooks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_28\n\nLANGUAGE: console\nCODE:\n```\ndocker run -it --device=/dev/dri --group-add=$(stat -c \"%g\" /dev/dri/render* | head -n 1) -p 8888:8888 openvino_notebooks\n```\n\n----------------------------------------\n\nTITLE: Adding Unit Test Target\nDESCRIPTION: This CMake function adds a test target named '${TARGET_NAME}' with specified source files, include directories, linked libraries, and labels.  The 'ov_add_test_target' macro likely encapsulates the logic for creating a test executable using CMake's 'add_executable' and 'add_test' commands. The INCLUDES section defines the include paths needed to compile the test source code.  The EXCLUDED_SOURCE_PATHS section prevents certain files from being compiled into the test executable. OBJECT_FILES lists pre-compiled object files to be linked into the test executable. LINK_LIBRARIES lists the libraries required by the test executable. ADD_CPPLINT enables cpplint checks during the build. LABELS assigns labels to the test for categorization and filtering.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        INCLUDES\n            PUBLIC\n                $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/src\n                $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/src/nodes\n                $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/thirdparty/onednn\n                $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/thirdparty/onednn/src\n                $<TARGET_PROPERTY:openvino::conditional_compilation,INTERFACE_INCLUDE_DIRECTORIES>\n            PRIVATE\n                $<TARGET_PROPERTY:openvino::snippets,SOURCE_DIR>/include\n        EXCLUDED_SOURCE_PATHS\n            ${EXCLUDED_SOURCE_PATHS_FOR_UNIT_TEST}\n            ${CMAKE_CURRENT_SOURCE_DIR}/vectorized\n\n        OBJECT_FILES\n            ${OBJ_LIB}\n        LINK_LIBRARIES\n            gtest\n            gtest_main\n            gmock\n            dnnl\n            openvino::shape_inference\n            openvino_runtime_s\n            unit_test_utils\n            ov_snippets_models\n            snippets_test_utils\n            ${MLAS_LIBRARY}\n            ${SHL_LIBRARY}\n            ${KLEIDIAI_LIBRARY}\n        ADD_CPPLINT\n        LABELS\n            OV UNIT CPU\n)\n```\n\n----------------------------------------\n\nTITLE: Copy OpenVINO Tokenizers Library (Linux x86)\nDESCRIPTION: Specifies the directory to copy the OpenVINO Tokenizers prebuilt library for Linux x86 systems.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n<openvino_dir>/runtime/lib/intel64/\n```\n\n----------------------------------------\n\nTITLE: Adding Extension to OpenVINO Core\nDESCRIPTION: Registers extensions to a Core object, allowing users to extend the functionality of the OpenVINO runtime with custom operations or layers. The libraryPath parameter specifies the path to the shared library containing the extensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\naddExtension(libraryPath): void\n```\n\n----------------------------------------\n\nTITLE: Installing Target\nDESCRIPTION: This snippet installs the built target (the shared library) to a specified destination. It also defines the component and excludes it from being included in all installations by default.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/src/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n        LIBRARY DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking asl-recognition model on GPU in throughput mode (Python)\nDESCRIPTION: This snippet demonstrates how to run the OpenVINO Benchmark Tool to measure the throughput of the 'asl-recognition' model on a GPU. It uses the `benchmark_app` command with the `-m` option to specify the model path, `-d` to select the GPU device, and `-hint` to set the performance hint to throughput. The model is expected to be in the OpenVINO Intermediate Representation (IR) format ('.xml' file).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app -m omz_models/intel/asl-recognition-0004/FP16/asl-recognition-0004.xml -d GPU -hint throughput\n```\n\n----------------------------------------\n\nTITLE: Creating pybind11 Module in CMake\nDESCRIPTION: This CMake snippet uses `pybind11_add_module` to create the pyopenvino module. It specifies the project name, marks it as a MODULE without extras, and lists the source files. A regular expression is used to exclude frontend files which are handled by subdirectories. It then sets include directories and links the module to OpenVINO libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES core/*.cpp experimental/*.cpp graph/*.cpp frontend/*.cpp utils/*.cpp pyopenvino.cpp)\nlist(FILTER SOURCES EXCLUDE REGEX \".*(frontend/(onnx|tensorflow|paddle|pytorch|jax))/*\")\n\npybind11_add_module(${PROJECT_NAME} MODULE NO_EXTRAS ${SOURCES})\n\ntarget_include_directories(${PROJECT_NAME} PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}/..\")\n\ntarget_link_libraries(${PROJECT_NAME} PRIVATE openvino::core::dev openvino::runtime openvino::offline_transformations)\n```\n\n----------------------------------------\n\nTITLE: Multinomial Layer Configuration (Single Batch)\nDESCRIPTION: This XML snippet demonstrates the configuration of a Multinomial layer in OpenVINO for a single batch input. It showcases the use of attributes such as `convert_type`, `with_replacement`, `log_probs`, `global_seed`, and `op_seed`. The input provides probabilities for each class, and the output defines the shape and type of the sampled indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/multinomial-13.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... name=\"Multinomial\" type=\"Multinomial\">\n    <data convert_type=\"f32\", with_replacement=\"true\", log_probs=\"false\", global_seed=\"234\", op_seed=\"148\"/>\n    <input>\n        <port id=\"0\" precision=\"FP32\">  <!-- probs value: [[0.1, 0.5, 0.4]] -->\n            <dim>1</dim> <!-- batch size of 2 -->\n            <dim>3</dim>\n        </port>\n        <port id=\"1\" precision=\"I32\"/> <!-- num_samples value: 5 -->\n    </input>\n    <output>\n        <port id=\"3\" precision=\"I32\" names=\"Multinomial:0\">\n            <dim>1</dim> <!--dimension depends on input batch size -->\n            <dim>5</dim> <!--dimension depends on num_samples -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: StridedSlice Basic Example in XML\nDESCRIPTION: Demonstrates a basic StridedSlice layer configuration with different strides.  The example shows slicing on a 6D array with specific begin, end, and stride values for each dimension. It is equivalent to array[0:4, 1:4, 0:4:2, 1:4:2, 3:0:-1, 3:0:-2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/strided-slice-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StridedSlice\" ...>\n    <data/>\n    <input>\n        <port id=\"0\">\n            <dim>4</dim>\n            <dim>4</dim>\n            <dim>4</dim>\n            <dim>4</dim>\n            <dim>4</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>6</dim> <!-- begin: [0, 1, 0, 1, 3, 3] -->\n        </port>\n        <port id=\"2\">\n            <dim>6</dim> <!-- end: [4, 4, 4, 4, 0, 0] -->\n        </port>\n        <port id=\"3\">\n            <dim>6</dim> <!-- stride: [1, 1, 2, 2, -1, -2] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>4</dim> <!-- element ids: [0, 1, 2, 3] -->\n            <dim>3</dim> <!-- element ids: [1, 2, 3] -->\n            <dim>2</dim> <!-- element ids: [0, 2] -->\n            <dim>2</dim> <!-- element ids: [1, 3] -->\n            <dim>4</dim> <!-- element ids: [3, 2, 1, 0] -->\n            <dim>2</dim> <!-- element ids: [3, 1] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Dump All Blobs by Name\nDESCRIPTION: Sets the environment variable using a wildcard regex to dump all blobs during OpenVINO CPU execution by matching all node names.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_NODE_NAME=\"*\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Nightly from PyPI with pip\nDESCRIPTION: This command installs the OpenVINO nightly build from PyPI using pip. This is a simplified command compared to using the extra index URL, but may not always be the recommended approach.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/release-notes-openvino/release-policy.rst#_snippet_2\n\nLANGUAGE: py\nCODE:\n```\npip install openvino-nightly\n```\n\n----------------------------------------\n\nTITLE: Adding the OpenCL GPU Backend Target\nDESCRIPTION: Adds an OpenVINO GPU backend target using the `ov_gpu_add_backend_target` macro. This macro configures the build process for the OpenCL-based GPU backend, including specifying include directories for the generated code, adding dependencies on the code generation target, and enabling Clang format checks. This macro handles the specifics of adding the OpenCL backend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/ocl_v2/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_gpu_add_backend_target(\n    NAME ${TARGET_NAME}\n    INCLUDES $<BUILD_INTERFACE:${CODEGEN_INCDIR}>\n    BYPASS\n        ADDITIONAL_SOURCE_DIRS ${CODEGEN_INCDIR}\n        DEPENDENCIES run_ocl_codegen\n        ADD_CLANG_FORMAT\n)\n```\n\n----------------------------------------\n\nTITLE: Save Checkpoint - PyTorch\nDESCRIPTION: This code snippet shows how to save a model checkpoint during training in PyTorch. This uses NNCF's API for handling model checkpoints when the model is wrapped by NNCF.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncompression_ctrl.save_checkpoint(\"checkpoint.pth\")\n```\n\n----------------------------------------\n\nTITLE: IRDFT Layer Definition (4D Input, No Signal Size) XML\nDESCRIPTION: Defines an IRDFT layer in XML for OpenVINO, processing a 4D input tensor without specifying the optional signal_size. The input tensor has dimensions 1x161x161x2, and the axes tensor specifies dimensions 1 and 2 for the IRDFT operation. The output tensor will have dimensions 1x161x320.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/irdft-9.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IRDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>161</dim>\n            <dim>161</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- [1, 2] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>161</dim>\n            <dim>320</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Pad Output Example - Constant Mode - C++\nDESCRIPTION: Illustrates the output of the Pad operation in 'constant' mode, where the padded values are filled with a constant (in this case, implicitly 0, as pad_value is not provided). It shows how the input tensor is extended based on pads_begin and pads_end attributes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nINPUT =\n    [[ 1  2  3  4 ]\n    [ 5  6  7  8 ]\n    [ 9 10 11 12 ]]\n\npads_begin = [0, 1]\npads_end = [2, 3]\n\nOUTPUT =\n    [[ 0  1  2  3  4  0  0  0 ]\n    [ 0  5  6  7  8  0  0  0 ]\n    [ 0  9 10 11 12  0  0  0 ]\n    [ 0  0  0  0  0  0  0  0 ]\n    [ 0  0  0  0  0  0  0  0 ]]\n```\n\n----------------------------------------\n\nTITLE: Tensor Layout Example: b_fs_yx_fsv16\nDESCRIPTION: Demonstrates the memory layout of a tensor with the blocked format 'b_fs_yx_fsv16', including the feature slice (fs) and feature slice vector (fsv) dimensions. The example shows how elements are reordered in memory compared to the planar 'bfyx' format, and the buffer size increases to account for padding when the original feature dimension is not divisible by the block size (16).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_memory_formats.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n// first batch\ni = 0   => [b=0; f=0;  y=0; x=0] == [b=0; fs=0; y=0; x=0; fsv=0];\ni = 1   => [b=0; f=1;  y=0; x=0] == [b=0; fs=0; y=0; x=0; fsv=1];\ni = 2   => [b=0; f=2;  y=0; x=0] == [b=0; fs=0; y=0; x=0; fsv=2];\n...\ni = 15  => [b=0; f=15; y=0; x=0] == [b=0; fs=0; y=0; x=0; fsv=15];\n\ni = 16  => [b=0; f=0;  y=0; x=1] == [b=0; fs=0; y=0; x=1; fsv=0];\ni = 17  => [b=0; f=1;  y=0; x=1] == [b=0; fs=0; y=0; x=1; fsv=1];\ni = 18  => [b=0; f=2;  y=0; x=1] == [b=0; fs=0; y=0; x=1; fsv=2];\n...\ni = 31  => [b=0; f=15; y=0; x=1] == [b=0; fs=0; y=0; x=1; fsv=15];\n\ni = 32  => [b=0; f=0;  y=1; x=0] == [b=0; fs=0; y=1; x=0; fsv=0];\ni = 33  => [b=0; f=1;  y=1; x=0] == [b=0; fs=0; y=1; x=0; fsv=1];\ni = 34  => [b=0; f=2;  y=1; x=0] == [b=0; fs=0; y=1; x=0; fsv=2];\n...\ni = 47  => [b=0; f=15; y=1; x=0] == [b=0; fs=0; y=1; x=0; fsv=15];\n\ni = 48  => [b=0; f=0;  y=1; x=1] == [b=0; fs=0; y=1; x=1; fsv=0];\ni = 49  => [b=0; f=1;  y=1; x=1] == [b=0; fs=0; y=1; x=1; fsv=1];\ni = 50  => [b=0; f=2;  y=1; x=1] == [b=0; fs=0; y=1; x=1; fsv=2];\n...\ni = 63  => [b=0; f=15; y=1; x=1] == [b=0; fs=0; y=1; x=1; fsv=15];\n\n// second batch\ni = 64  => [b=1; f=0;  y=0; x=0] == [b=1; fs=0; y=0; x=0; fsv=0];\ni = 65  => [b=1; f=1;  y=0; x=0] == [b=1; fs=0; y=0; x=0; fsv=1];\ni = 66  => [b=1; f=2;  y=0; x=0] == [b=1; fs=0; y=0; x=0; fsv=2];\n...\ni = 79  => [b=1; f=15; y=0; x=0] == [b=1; fs=0; y=0; x=0; fsv=15];\n\ni = 80  => [b=1; f=0;  y=0; x=1] == [b=1; fs=0; y=0; x=1; fsv=0];\ni = 81  => [b=1; f=1;  y=0; x=1] == [b=1; fs=0; y=0; x=1; fsv=1];\ni = 82  => [b=1; f=2;  y=0; x=1] == [b=1; fs=0; y=0; x=1; fsv=2];\n...\ni = 95  => [b=1; f=15; y=0; x=1] == [b=1; fs=0; y=0; x=1; fsv=15];\n\ni = 96  => [b=1; f=0;  y=1; x=0] == [b=1; fs=0; y=1; x=0; fsv=0];\ni = 97  => [b=1; f=1;  y=1; x=0] == [b=1; fs=0; y=1; x=0; fsv=1];\ni = 98  => [b=1; f=2;  y=1; x=0] == [b=1; fs=0; y=1; x=0; fsv=2];\n...\ni = 111 => [b=1; f=15; y=1; x=0] == [b=1; fs=0; y=1; x=0; fsv=15];\n\ni = 112 => [b=1; f=0;  y=1; x=1] == [b=1; fs=0; y=1; x=1; fsv=0];\ni = 113 => [b=1; f=1;  y=1; x=1] == [b=1; fs=0; y=1; x=1; fsv=1];\ni = 114 => [b=1; f=2;  y=1; x=1] == [b=1; fs=0; y=1; x=1; fsv=2];\n...\ni = 127 => [b=1; f=15; y=1; x=1] == [b=1; fs=0; y=1; x=1; fsv=15];\n```\n\n----------------------------------------\n\nTITLE: ScatterUpdate Example 2 in XML\nDESCRIPTION: This example showcases another ScatterUpdate layer configuration in XML. It provides specific dimension values for input data, indices, updates, and the axis, illustrating how updates are applied to the data tensor to produce the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-update-3.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ScatterUpdate\">\n    <input>\n        <port id=\"0\">  <!-- data -->\n            <dim>3</dim>    <!-- {{-1.0f, 1.0f, -1.0f, 3.0f, 4.0f},  -->\n            <dim>5</dim>    <!-- {-1.0f, 6.0f, -1.0f, 8.0f, 9.0f},   -->\n        </port>             <!-- {-1.0f, 11.0f, 1.0f, 13.0f, 14.0f}} -->\n        <port id=\"1\">  <!-- indices -->\n            <dim>2</dim> <!-- {0, 2} -->\n        </port>\n        <port id=\"2\">  <!-- updates -->\n            <dim>3</dim> <!-- {1.0f, 1.0f} -->\n            <dim>2</dim> <!-- {1.0f, 1.0f} -->\n        </port>          <!-- {1.0f, 2.0f} -->\n        <port id=\"3\">   <!-- axis -->\n            <dim>1</dim> <!-- {1} -->\n        </port>\n    </input>\n    <output>\n        <port id=\"4\">  <!-- output -->\n            <dim>3</dim>    <!-- {{1.0f, 1.0f, 1.0f, 3.0f, 4.0f},   -->\n            <dim>5</dim>    <!-- {1.0f, 6.0f, 1.0f, 8.0f, 9.0f},    -->\n        </port>             <!-- {1.0f, 11.0f, 2.0f, 13.0f, 14.0f}} -->\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Set PATH (Azure ML)\nDESCRIPTION: This command adds the `openvino_env` Conda environment's `bin` directory to the system's PATH environment variable.  This ensures that executables within the environment are accessible.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_23\n\nLANGUAGE: console\nCODE:\n```\nset PATH=\"/anaconda/envs/openvino_env/bin;%PATH%\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Environment Variables (Windows PowerShell)\nDESCRIPTION: This command executes the `setupvars.ps1` PowerShell script, which sets the necessary environment variables for using OpenVINO on Windows.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n. <path-to-setupvars-folder>/setupvars.ps1\n```\n\n----------------------------------------\n\nTITLE: Add Operation with NumPy Broadcasting in OpenVINO (XML)\nDESCRIPTION: Shows the Add operation in OpenVINO with NumPy broadcasting enabled. Input tensors have different shapes, and the auto_broadcast attribute is set to \"numpy\", allowing for broadcasting according to NumPy rules.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/add-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Add\">\n    <data auto_broadcast=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Create RemoteContext from VADisplay (C++)\nDESCRIPTION: This snippet demonstrates how to create an `ov::RemoteContext` from an existing VA Display (`VADisplay`) using the GPU plugin's C++ API. It initializes the OpenVINO core and retrieves the extension. Requires the OpenVINO runtime and intel_gpu ocl headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n//! [context_from_va_display]\n#include <openvino/runtime/core.hpp>\n#include <openvino/runtime/intel_gpu/ocl/va.hpp>\n\n#if defined(OPENVINO_USE_VAAPI)\n#include <va/va.h>\n#endif\n\nvoid create_context_from_va_display(VADisplay display) {\n    // context from user-defined VA display\n    ov::Core core;\n    auto remote_context = core.get_extension<ov::intel_gpu::ocl::va_context>(display);\n    (void)remote_context;\n}\n//! [context_from_va_display]\n```\n\n----------------------------------------\n\nTITLE: EmbeddingBagPacked with Mean Reduction - XML\nDESCRIPTION: Demonstrates the EmbeddingBagPacked operation with 'mean' reduction. The input embedding table, indices, and the resulting output are defined within the layer's input and output ports. per_sample_weights are not supported in mean mode.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sparse/embedding-bag-packed-15.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"EmbeddingBagPacked\" ... >\n    <data reduction=\"mean\"/>\n    <input>\n        <port id=\"0\">     <!-- emb_table value is: [[-0.2, -0.6], [-0.1, -0.4], [-1.9, -1.8], [-1.,  1.5], [ 0.8, -0.7]] -->\n            <dim>5</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">     <!-- indices value is: [[0, 2], [1, 2], [3, 4]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">     <!-- output value is: [[-1.05, -1.2], [-1., -1.1], [-0.1, 0.4]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Globbing OpenCL Kernel Files\nDESCRIPTION: Uses `file(GLOB_RECURSE)` to recursively find all `.cl` files in the current source directory. These files are the OpenCL kernel source files that will be processed by the code generation script.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/ocl_v2/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE KERNELS \"${CMAKE_CURRENT_SOURCE_DIR}/*.cl\")\n```\n\n----------------------------------------\n\nTITLE: Get Output Tensor from InferRequest TypeScript\nDESCRIPTION: Retrieves the output tensor of the InferRequest. It allows to specify the index of the output tensor. If the model has only one output, the index is optional. Returns a Tensor object representing the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ngetOutputTensor(): Tensor\n```\n\nLANGUAGE: typescript\nCODE:\n```\ngetOutputTensor(idx?: number): Tensor\n```\n\n----------------------------------------\n\nTITLE: ReduceMin Operation Calculation (C++)\nDESCRIPTION: This C++ code snippet demonstrates the calculation performed by the ReduceMin operation. It iterates through the input data and finds the minimum value along the dimensions specified by the axes input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-min-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i0, i1, ..., iN] = min[j0, ..., jN](x[j0, ..., jN]))\n```\n\n----------------------------------------\n\nTITLE: Globbing and Setting Variables in CMake\nDESCRIPTION: This CMake snippet uses `file(GLOB_RECURSE)` to recursively find all `.hpp` files in the current source directory. It then sets CMake variables `TEST_COMMON_SOURCE_FILES`, `TEST_COMMON_SOURCE_DIR`, and `TEST_COMMON_INCLUDE_DIR` with the discovered files and directories, making them available in parent scopes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE TEMP\n    \"${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\"\n  )\n\nset(TEST_COMMON_SOURCE_FILES \"${TEMP}\" PARENT_SCOPE)\nset(TEST_COMMON_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}\" PARENT_SCOPE)\nset(TEST_COMMON_INCLUDE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}\" PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Adding Library Target in CMake\nDESCRIPTION: This command adds a library target named `${TARGET_NAME}` to the project. It specifies that the target is a module (shared library) and lists the source files to be compiled. The module type is essential for OpenVINO extensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/template_extension/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} MODULE ${SRC})\n```\n\n----------------------------------------\n\nTITLE: GatherTree Layer Example in XML\nDESCRIPTION: This XML code demonstrates an example of the GatherTree layer configuration. It shows the input and output port dimensions. The input ports represent step_ids, parent_ids, max_seq_len, and end_token.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-tree-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"GatherTree\" ...>\n    <input>\n        <port id=\"0\">\n            <dim>100</dim>\n            <dim>1</dim>\n            <dim>10</dim>\n        </port>\n        <port id=\"1\">\n            <dim>100</dim>\n            <dim>1</dim>\n            <dim>10</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n        </port>\n        <port id=\"3\">\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>100</dim>\n            <dim>1</dim>\n            <dim>10</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Set TEST_DEVICE Environment Variable\nDESCRIPTION: Sets the `TEST_DEVICE` environment variable to specify the plugin to use for running tests (e.g., GPU). If not set, tests run on the CPU plugin by default.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexport TEST_DEVICE=GPU\n```\n\n----------------------------------------\n\nTITLE: Creating OpenVINO C++ Integration Example\nDESCRIPTION: This snippet defines a CMake configuration for building a C++ OpenVINO integration example. It finds the OpenVINO package, creates an executable named `ov_integration_snippet`, and links it with the `openvino::runtime` library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/snippets/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME \"ov_integration_snippet\")\n# [cmake:integration_example_cpp]\ncmake_minimum_required(VERSION 3.10)\nset(CMAKE_CXX_STANDARD 17)\n\nfind_package(OpenVINO REQUIRED)\n\nadd_executable(${TARGET_NAME} src/main.cpp)\n\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino::runtime)\n\n# [cmake:integration_example_cpp]\n```\n\n----------------------------------------\n\nTITLE: Build C OpenVINO Samples\nDESCRIPTION: This script builds the OpenVINO C sample applications. It is located in the /usr/share/openvino/samples/c/ directory. The script compiles the sample applications, allowing users to explore OpenVINO functionality in C. Requires execution from a shell.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\n/usr/share/openvino/samples/c/build_samples.sh\n```\n\n----------------------------------------\n\nTITLE: Linking OpenVINO Libraries\nDESCRIPTION: This snippet links the plugin target with necessary OpenVINO Runtime libraries, specifically `openvino::interpreter_backend` and `openvino::reference`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/src/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE\n    openvino::interpreter_backend\n    openvino::reference)\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Mean Reduction - C++\nDESCRIPTION: This code snippet demonstrates the 'mean' reduction mode for the ScatterElementsUpdate operation. The mean value of the corresponding elements from the updates tensor and the data tensor at the specified index is calculated and used for the update.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\noutput[indices[i]] = mean(updates[i], output[indices[i]]) axis = 0\n```\n\n----------------------------------------\n\nTITLE: Pattern with Any Input (C++)\nDESCRIPTION: This code snippet demonstrates how to construct a pattern using `ov::pass::pattern::any_input` to represent inputs with undefined types.  It creates a Multiply pattern with an arbitrary first input and a Constant as the second input. Because Multiply is commutative, the order of inputs (any_input/Constant or Constant/any_input) does not matter for matching.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/matcher-pass.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n// [pattern:label_example]\n#include <openvino/core/node.hpp>\n#include <openvino/opsets/opset10.hpp>\n#include <openvino/pass/pattern/op/any_input.hpp>\n\n#include \"openvino/pass/graph_rewrite.hpp\"\n\nusing namespace std;\nusing namespace ov::opset10;\n\n{\n    auto any_input = ov::pass::pattern::any_input();\n    auto constant = make_shared<Constant>(element::f32, Shape{1}, vector<float>{1.0f});\n    auto mul = make_shared<Multiply>(any_input, constant);\n}\n// [pattern:label_example]\n```\n\n----------------------------------------\n\nTITLE: Find OpenVINO package in CMake\nDESCRIPTION: This CMake command finds the OpenVINO package, making it available for use in the project. The `REQUIRED` keyword ensures that the build process will fail if OpenVINO cannot be found.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-vcpkg.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nfind_package(OpenVINO REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Setting Install RPATH\nDESCRIPTION: This snippet sets the install RPATH for the target `${PROJECT_NAME}` using the `ov_set_install_rpath` macro. It specifies the runtime directory `${OV_CPACK_RUNTIMEDIR}`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_install_rpath(${PROJECT_NAME}\n    ${OV_CPACK_RUNTIMEDIR} ${OV_CPACK_RUNTIMEDIR})\n```\n\n----------------------------------------\n\nTITLE: Create Python Virtual Environment\nDESCRIPTION: Creates and activates a Python virtual environment.  This isolates project dependencies and prevents conflicts with other Python projects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython3 -m venv venv\n```\n\n----------------------------------------\n\nTITLE: Glob Plugin Sources\nDESCRIPTION: Uses GLOB_RECURSE to find all C++ source files in the `src/plugin` directory and HPP header files in the `include/intel_gpu/plugin` directory. The found files are stored in the `PLUGIN_SOURCES` variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE PLUGIN_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/src/plugin/*.cpp ${CMAKE_CURRENT_SOURCE_DIR}/include/intel_gpu/plugin/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Reproduce Opset Conformance Results with Custom Config\nDESCRIPTION: This command reproduces opset conformance results for OMZ on TEMPLATE using a custom configuration file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython3 run_conformance.py -d GPU -c /path/to/ov/config.lst\n```\n\n----------------------------------------\n\nTITLE: Conditional Dependency Addition based on Feature Flags in CMake\nDESCRIPTION: This snippet demonstrates how to conditionally add dependencies and compile definitions based on CMake flags. It checks if specific features like IR frontend, HETERO, AUTO and AUTO_BATCH plugins are enabled and then appends the corresponding dependencies and definitions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/tests/functional/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(COMPILE_DEFINITIONS \"\")\n\nif(ENABLE_OV_IR_FRONTEND)\n    list(APPEND DEPENDENCIES openvino_ir_frontend)\n    list(APPEND COMPILE_DEFINITIONS ENABLE_OV_IR_FRONTEND)\nendif()\n\nif(ENABLE_HETERO)\n    list(APPEND DEPENDENCIES openvino_hetero_plugin)\n    list(APPEND COMPILE_DEFINITIONS ENABLE_HETERO)\nendif()\n\nif(ENABLE_AUTO AND ENABLE_MULTI)\n    list(APPEND DEPENDENCIES openvino_auto_plugin)\n    list(APPEND COMPILE_DEFINITIONS ENABLE_AUTO)\nendif()\n\nif(ENABLE_AUTO_BATCH)\n    list(APPEND DEPENDENCIES openvino_auto_batch_plugin)\n    list(APPEND COMPILE_DEFINITIONS ENABLE_AUTO_BATCH)\nendif()\n\nif(ENABLE_PROXY)\n    list(APPEND COMPILE_DEFINITIONS PROXY_PLUGIN_ENABLED)\nendif()\n```\n\n----------------------------------------\n\nTITLE: benchmark_app Latency results output example (sh)\nDESCRIPTION: This code snippet shows the latency section of the output from `benchmark_app`. It indicates that this 'Max' value is not representative of the auto-batched execution and recommends focusing on 'Median' or 'Average' latency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\n[ INFO ] Latency:\n[ INFO ]  Max:      1000.18 ms\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name\nDESCRIPTION: Defines the target name for the Intel GPU plugin.  This variable is used throughout the CMake script to refer to the plugin being built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset (TARGET_NAME \"openvino_intel_gpu_plugin\")\n```\n\n----------------------------------------\n\nTITLE: Run Tests (Bash)\nDESCRIPTION: This command executes the test suite for the `openvino-node` package. Running the tests verifies that the package has been built and installed correctly and that all its functionalities are working as expected.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nnpm run test\n```\n\n----------------------------------------\n\nTITLE: CCache Installation (Linux)\nDESCRIPTION: Installs CCache on a Linux system, configures symbolic links, and adds it to the PATH environment variable. This optimizes build times by caching compiled objects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/commit_slider/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y ccache\nsudo /usr/sbin/update-ccache-symlinks\necho 'export PATH=\"/usr/lib/ccache:$PATH\"' | tee -a ~/.bashrc\nsource ~/.bashrc && echo $PATH\n```\n\n----------------------------------------\n\nTITLE: Cache Value Builder C++\nDESCRIPTION: This snippet presents the builder interface which is a callable object (e.g., a lambda or function object) used to construct the cached value (`ValueType`) when a cache miss occurs. The builder takes the `KeyType` as input, which uniquely identifies the cached value, and returns a fully constructed `ValueType`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/runtime_parameters_cache.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nValueType build(const KeyType& key);\n```\n\n----------------------------------------\n\nTITLE: Getting Tensor Data Pointer in OpenVINO (C)\nDESCRIPTION: This function provides access to the underlying host memory of an OpenVINO tensor. It takes a pointer to the tensor and a pointer to a void pointer as input. The function returns a status code indicating success or failure in accessing the tensor data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_48\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_tensor_data(const ov_tensor_t* tensor, void** data)\n```\n\n----------------------------------------\n\nTITLE: Registering an OpExtension in C++\nDESCRIPTION: This snippet shows how to register a custom operation using `OpExtension` in C++. It directly maps the custom operation to an existing OpenVINO operation, in this case, `ov::opset9::Add`. This is suitable when the OpenVINO operation provides the desired functionality directly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/how_to_add_op.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\ncore.add_extension(ov::frontend::onnx::OpExtension<ov::opset9::Add>(\"org.openvinotoolkit\", \"CustomAdd\"));\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO Dependencies\nDESCRIPTION: This script installs the required system dependencies for OpenVINO. The script is located in the extracted installation directory. The `sudo -E` command executes the script with elevated privileges and preserves the user's environment variables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ncd /opt/intel/openvino_2025.1.0\nsudo -E ./install_dependencies/install_openvino_dependencies.sh\n```\n\n----------------------------------------\n\nTITLE: Setting FlatBuffers Compiler Variables\nDESCRIPTION: This snippet sets the `flatbuffers_COMPILER` and `flatbuffers_DEPENDENCY` variables based on whether `flatc` is built directly or cross-compiled. If built directly, it uses the target file of `flatc`. If cross-compiled, it points to the installed `flatc` executable in the native install directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/flatbuffers/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(FLATBUFFERS_BUILD_FLATC)\n    if(CMAKE_COMPILER_IS_GNUCXX)\n        set_target_properties(flatc PROPERTIES COMPILE_OPTIONS \"-Wno-shadow\")\n    endif()\n\n    set(flatbuffers_COMPILER $<TARGET_FILE:flatc> PARENT_SCOPE)\n    set(flatbuffers_DEPENDENCY flatc PARENT_SCOPE)\nelse()\n    set(HOST_FLATC_INSTALL_DIR \"${CMAKE_CURRENT_BINARY_DIR}/install\")\n\n    ov_native_compile_external_project(\n        TARGET_NAME host_flatc\n        NATIVE_INSTALL_DIR \"${HOST_FLATC_INSTALL_DIR}\"\n        CMAKE_ARGS \"-DFLATBUFFERS_BUILD_TESTS=${FLATBUFFERS_BUILD_TESTS}\"\n                   \"-DFLATBUFFERS_BUILD_FLATLIB=OFF\"\n                   \"-DFLATBUFFERS_BUILD_FLATC=ON\"\n                   \"-DFLATBUFFERS_BUILD_SHAREDLIB=OFF\"\n                   \"-DFLATBUFFERS_CPP_STD=${FLATBUFFERS_CPP_STD}\"\n        NATIVE_SOURCE_SUBDIR \"flatbuffers\"\n        NATIVE_TARGETS flatc)\n\n    set(flatbuffers_COMPILER \"${HOST_FLATC_INSTALL_DIR}/bin/flatc\")\n    add_executable(flatbuffers::flatc IMPORTED GLOBAL)\n    set_property(TARGET flatbuffers::flatc APPEND PROPERTY IMPORTED_CONFIGURATIONS RELEASE)\n    set_target_properties(flatbuffers::flatc PROPERTIES\n        IMPORTED_LOCATION_RELEASE \"${flatbuffers_COMPILER}\")\n    set_target_properties(flatbuffers::flatc PROPERTIES\n        MAP_IMPORTED_CONFIG_DEBUG Release\n        MAP_IMPORTED_CONFIG_MINSIZEREL Release\n        MAP_IMPORTED_CONFIG_RELWITHDEBINFO Release)\n    add_dependencies(flatbuffers::flatc host_flatc)\n\n    set(flatbuffers_DEPENDENCY host_flatc PARENT_SCOPE)\n    set(flatbuffers_COMPILER \"${flatbuffers_COMPILER}\" PARENT_SCOPE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Test Target with CMake Function\nDESCRIPTION: This CMake snippet utilizes the `ov_add_test_target` function to configure the NPU unit test target. It specifies the target name, root directory, additional source directories, dependencies, include directories, object files, link libraries, and labels for the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/unit/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        ADDITIONAL_SOURCE_DIRS\n            ${OpenVINO_SOURCE_DIR}/src/plugins/intel_npu/src/plugin/npuw/\n        DEPENDENCIES\n            openvino::runtime\n        INCLUDES\n            ${CMAKE_CURRENT_SOURCE_DIR}\n            ${CMAKE_CURRENT_SOURCE_DIR}/npuw\n            ${CMAKE_CURRENT_SOURCE_DIR}/npuw/model_generator\n            ${OpenVINO_SOURCE_DIR}/src/plugins/intel_npu/src/plugin/npuw\n            ${OpenVINO_SOURCE_DIR}/src/plugins/intel_npu/src/utils/include\n            ${OpenVINO_SOURCE_DIR}/src/plugins/intel_npu/src/plugin/include\n            ${OpenVINO_SOURCE_DIR}/src/plugins/intel_npu/src/al/include\n        OBJECT_FILES\n            ${OpenVINO_SOURCE_DIR}/src/plugins/intel_npu/src/plugin/src/metadata.cpp\n            ${OpenVINO_SOURCE_DIR}/src/plugins/intel_npu/src/plugin/npuw/llm_compiled_model.cpp            \n        LINK_LIBRARIES\n            ${MANDATORY_UNIT_TESTS_LIBS}\n        LABELS\n            NPUW\n)\n```\n\n----------------------------------------\n\nTITLE: Getting Layout from Model Input/Output in C++\nDESCRIPTION: This snippet demonstrates how to retrieve the layout from a model's input or output in C++ using OpenVINO's helper functions. This simplifies accessing layout information from a model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nov::Layout input_layout = ov::layout::get_layout(model->input());\nov::Layout output_layout = ov::layout::get_layout(model->output());\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (TensorFlow) with OpenVINO in C\nDESCRIPTION: This code compiles a model in TensorFlow format using the `ov_core_compile_model` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests. Memory management must be handled manually in C.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\nov_compiled_model_t* compiled_model_tf = NULL;\nov_core_compile_model(core, \"path_to_model.pb\", \"AUTO\", &compiled_model_tf);\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Package\nDESCRIPTION: This command installs the base OpenVINO package from PyPI. The `openvino` package provides the necessary components for running inference with OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-pip.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython -m pip install openvino\n```\n\n----------------------------------------\n\nTITLE: Grouping Source Files in Visual Studio Project with CMake\nDESCRIPTION: This snippet creates named folders for the source files within a Visual Studio project (.vcproj). It organizes the files into \"src\" and \"include\" groups, which improves project organization within Visual Studio.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${LIBRARY_HEADERS} ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: ConvolutionBackpropData Padding Calculation Formula\nDESCRIPTION: This formula calculates the output size (Y_i) based on stride, input size (X_i), kernel size (K_i), dilations, padding, and output padding.  It is used when the auto_pads attribute is not specified, allowing for explicit control over padding.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/convolution-backprop-data-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\nif auto_pads != None:\n    pads_begin[i] = 0\n    pads_end[i] = 0\n\nY_i = stride[i] * (X_i - 1) + ((K_i - 1) * dilations[i] + 1) - pads_begin[i] - pads_end[i] + output_padding[i]\n```\n\n----------------------------------------\n\nTITLE: Exporting GPG Key to ASC Format\nDESCRIPTION: This snippet demonstrates how to export the GPG key to an ASCII armored (ASC) file using the `gpg --armor --export` command. This is needed for including the key in the snap package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/integrate-openvino-with-ubuntu-snap.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ngpg --armor --export E9BF0AFC46D6E8B7DA5882F1BAC6F0C353D04109./GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB > 53D04109.acs\n```\n\n----------------------------------------\n\nTITLE: InterpolateCalculation Call Method\nDESCRIPTION: The `__call__` method orchestrates the interpolation process. It corrects the padding, infers the output shape, pads the input data, calculates scales if necessary, and then calls the appropriate interpolation function based on the selected mode.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef __call__(self, input_data, sizes, scales, axes):\n    rank = input_data.ndim\n    self.pads_begin = InterpolateCalculation.correct_pad(self.pads_begin, rank)\n    self.pads_end = InterpolateCalculation.correct_pad(self.pads_end, rank)\n    self.pads = list(zip(self.pads_begin, self.pads_end))\n    self.axes = np.array(axes).astype(np.int64)\n\n    self.output_shape = self.shape_infer(input_data, sizes, scales)\n    padded_data = np.pad(input_data, self.pads, 'constant')\n\n    if self.shape_calculation_mode == ShapeCalculationMode.SIZES:\n        num_of_axes = len(self.axes)\n        self.scales = np.zeros(num_of_axes)\n        for i, axis in enumerate(axes):\n            self.scales[i] = self.output_shape[axis] / padded_data.shape[axis]\n    else:\n        self.scales = scales\n\n    if self.mode == 'nearest':\n        self.all_scales = np.ones(rank).astype(np.float)\n        for i, axis in enumerate(self.axes):\n            self.all_scales[axis] = self.scales[i]\n\n    self.input_shape = padded_data.shape\n    return self.func(padded_data)\n```\n\n----------------------------------------\n\nTITLE: Building C++ Samples on Windows (PowerShell)\nDESCRIPTION: This command executes the `build_samples.ps1` PowerShell script to build the C++ samples on Windows.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n& <path-to-build-samples-folder>/build_samples.ps1\n```\n\n----------------------------------------\n\nTITLE: Conditional KLEIDIAI Library Setting\nDESCRIPTION: If 'ENABLE_KLEIDIAI_FOR_CPU' is enabled, this sets the 'KLEIDIAI_LIBRARY' variable to 'kleidiai'. The library is later linked to the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_KLEIDIAI_FOR_CPU)\n    set(KLEIDIAI_LIBRARY \"kleidiai\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Threading Interface CMake\nDESCRIPTION: Sets the threading interface for the specified test target. This likely configures how the tests interact with threading libraries or frameworks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/tests/functional/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Protopipe Execution Command\nDESCRIPTION: This command line instruction demonstrates how to execute Protopipe with a configuration file and the `--drop_frames` option. It instructs Protopipe to load the configuration from 'config.yaml' and drop frames if they are completed before the next stream iteration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n./protopipe -cfg config.yaml --drop_frames\n```\n\n----------------------------------------\n\nTITLE: Setting Tensor for Inference Request in C++\nDESCRIPTION: This snippet demonstrates how to set a tensor for an inference request with a user-provided `ov::Tensor` object. The `set_tensor` method configures the inference to use the specified tensor for input or output.  Note that this involves memory copies to/from the original buffers allocated by the NPU plugin, which may impact performance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/README.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\ninferRequest.set_tensor(tensor_name, tensor);\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Inclusion in CMake\nDESCRIPTION: This snippet conditionally includes several subdirectories if the `ENABLE_NPU_PLUGIN_ENGINE` variable is true. This allows for building components related to the NPU plugin engine only when it's enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (ENABLE_NPU_PLUGIN_ENGINE)\n    add_subdirectory(common)\n    add_subdirectory(compiler_adapter)\n    add_subdirectory(backend)\n    add_subdirectory(plugin)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Protopipe Execute Filter Command (Named)\nDESCRIPTION: This command shows how to filter and execute scenarios based on custom names defined in the configuration file, rather than the default numbered names.  The `-exec_filter` option uses a regular expression to match scenarios whose names contain \"-A\" or \"-B\".\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n./protopipe --cfg scenarios.yaml --niter 100 --exec_filter \".*-[AB].*\"\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library\nDESCRIPTION: This snippet installs the static library using the `ov_install_static_lib` function and assigns it to the component `OV_CPACK_COMP_CORE`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: Defining Compound Operation with YAML\nDESCRIPTION: This code snippet demonstrates how to define a compound operation in Protopipe using YAML.  A compound operation encapsulates a subgraph consisting of `Infer` and `CPU` node types, allowing for the creation of more complex and modular pipelines.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nop_desc:\n  - { tag: A, path: Model-A.xml }\n  - tag: B,\n    type: Compound,\n    repeat_count: 10,\n    op_desc:\n      - { tag: D, path: Model-D.xml }\n      - { tag: E, path: Model-E.xml }\n      - { tag: F, path: Model-F.xml }\n    connections:\n      - [D, E]\n      - [D, F]\n  - { tag: C, path: Model-C.xml }\nconnections:\n  - [A, B, C]\n```\n\n----------------------------------------\n\nTITLE: Python Inference Execution\nDESCRIPTION: This command line instruction shows how to run the hello_reshape_ssd.py sample. It requires the path to the model (.xml), the path to the image, and the device name (e.g., GPU) as command-line arguments to perform inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-reshape-ssd.rst#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npython hello_reshape_ssd.py ./test_data/models/mobilenet-ssd.xml banana.jpg GPU\n```\n\n----------------------------------------\n\nTITLE: Adding Test Target CMake\nDESCRIPTION: This snippet creates a test target named `${TARGET_NAME}` (which is `ov_auto_func_tests`). It specifies the source directory, link libraries (OpenVINO runtime, gtest, and shared tests), include directories, and test labels (Multi, Auto). It also adds clang-format checks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/functional/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        LINK_LIBRARIES\n            openvino::runtime::dev\n            gtest\n            gtest_main\n            funcSharedTests\n        INCLUDES\n            ${CMAKE_CURRENT_SOURCE_DIR}\n            ${TEST_COMMON_INCLUDE_DIR}\n        ADD_CLANG_FORMAT\n        LABELS\n            Multi\n            Auto\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Test Target CMake\nDESCRIPTION: Adds a test target named `${TARGET_NAME}` to the build system. It specifies the root directory, include directories (current source directory and shared headers directory), link libraries (funcSharedTests), dependencies (openvino_auto_batch_plugin), and labels (OV AUTO_BATCH).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/tests/functional/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_test_target(\n        NAME\n            ${TARGET_NAME}\n        ROOT\n            ${CMAKE_CURRENT_SOURCE_DIR}\n        INCLUDES\n            ${CMAKE_CURRENT_SOURCE_DIR}\n            ${SHARED_HEADERS_DIR}\n        LINK_LIBRARIES\n            funcSharedTests\n        DEPENDENCIES\n            openvino_auto_batch_plugin\n        ADD_CPPLINT\n        LABELS\n            OV AUTO_BATCH\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO with CMake for riscv-gnu-toolchain\nDESCRIPTION: This snippet configures the OpenVINO build using CMake for a RISC-V target using the standard riscv-gnu-toolchain.  It specifies the build type, installation prefix, toolchain file, and the root directory of the toolchain.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ncmake .. \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DCMAKE_INSTALL_PREFIX=<openvino_install_path> \\\n  -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/riscv64-gnu.toolchain.cmake \\\n  -DRISCV_TOOLCHAIN_ROOT=/opt/riscv\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties in CMake\nDESCRIPTION: Sets the `FOLDER` property for the target, which helps organize the target in IDEs like Visual Studio. In this case, the target is placed in the 'cpp_samples' folder.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/format_reader/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES FOLDER cpp_samples)\n```\n\n----------------------------------------\n\nTITLE: Getting Graph from TorchScript with TorchScriptPythonDecoder (Python)\nDESCRIPTION: This snippet demonstrates how to access the graph element directly from the TorchScriptPythonDecoder after converting a model. It decodes a PyTorch model using TorchScriptPythonDecoder and prints the graph element which represents the model's structure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/pytorch/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndecoder = TorchScriptPythonDecoder(model, example_input=example)\nprint(decoder.graph_element)\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for CPU Plugin Target\nDESCRIPTION: Sets the include directories for the specified target privately to the `src` directory within the current source directory. This ensures that the target can find the necessary header files during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_31\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/src)\n```\n\n----------------------------------------\n\nTITLE: Running C++ ONNX Frontend Tests with GTest\nDESCRIPTION: This command runs the C++ ONNX frontend tests using the gtest framework. It filters the tests based on a specified pattern (e.g., '*add*'). Replace <OV_REPO_DIR> with the OpenVINO repository directory and <OV_BUILD_TYPE> with either Debug or Release, depending on the build configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n<OV_REPO_DIR>/bin/intel64/<OV_BUILD_TYPE>/ov_onnx_frontend_tests --gtest_filter=*add*\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorFlow Lite Frontend with CMake\nDESCRIPTION: This CMake code snippet uses the `ov_add_frontend` macro to define and configure the TensorFlow Lite frontend for use with OpenVINO. It specifies the frontend's name, indicates that it's a linkable component, provides a file description, and lists the necessary link libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_frontend(NAME tensorflow_lite\n                LINKABLE_FRONTEND\n                FILEDESCRIPTION \"FrontEnd to load and convert TensorFlow Lite file format\"\n                LINK_LIBRARIES openvino::core::dev openvino::frontend::tensorflow_common)\n```\n\n----------------------------------------\n\nTITLE: get_property() Implementation C++\nDESCRIPTION: This snippet shows how to retrieve a property of the compiled model by name.  It accesses configuration values that were used during compilation.  This allows developers to inspect the configuration of a compiled model, especially when the model was imported and compiled by other tools.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/compiled-model.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nov::Any CompiledModel::get_property(const std::string& name) const {\n    if (name == ov::supported_properties) {\n        return std::vector<ov::PropertyName>{\n            ov::PropertyName{ov::supported_properties.name(), ov::PropertyMutability::RO},\n            ov::PropertyName{ov::model_name.name(), ov::PropertyMutability::RO},\n            ov::PropertyName{ov::optimal_number_of_infer_requests.name(), ov::PropertyMutability::RO},\n            ov::PropertyName{ov::hint::inference_precision.name(), ov::PropertyMutability::RO},\n            ov::PropertyName{ov::device::id.name(), ov::PropertyMutability::RO},\n            ov::PropertyName{ov::device::full_name.name(), ov::PropertyMutability::RO},\n            ov::PropertyName{ov::loaded_from_cache.name(), ov::PropertyMutability::RO},\n            ov::PropertyName{ov::execution_devices.name(), ov::PropertyMutability::RO}\n        };\n    } else if (name == ov::model_name) {\n        return m_model->get_friendly_name();\n    } else if (name == ov::optimal_number_of_infer_requests) {\n        return static_cast<unsigned int>(1); // simplest impl\n    } else if (name == ov::execution_devices) {\n        return std::vector<std::string>{{\"}TEMPLATE\"}};\n    } else if (name == ov::loaded_from_cache) {\n        return m_loaded_from_cache;\n    } else if (name == ov::hint::inference_precision) {\n        if (m_cfg.count(ov::hint::inference_precision.name()))\n            return m_cfg.at(ov::hint::inference_precision.name());\n        else\n            return ov::Any{};\n    } else if (name == ov::device::full_name) {\n        return std::string{\"TemplatePlugin\"};\n    } else if (name == ov::device::id) {\n        return std::string{\"0\"};\n    } else {\n        OPENVINO_THROW(\"Unsupported property \" + name);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directory in CMake\nDESCRIPTION: This snippet sets the include directory for the utility library. It specifies where the header files for the library are located. This directory is then used later to include these headers during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(UTIL_INCLUDE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/include/)\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for Classification Sample\nDESCRIPTION: This CMake snippet configures the build process for the 'classification_sample_async' application. It defines the application name, lists the source and header files, and specifies the dependencies required for compilation and linking.  It relies on the 'ov_add_sample' macro.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/classification_sample_async/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_sample(NAME classification_sample_async\n              SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/main.cpp\"\n              HEADERS \"${CMAKE_CURRENT_SOURCE_DIR}/classification_sample_async.h\"\n              DEPENDENCIES ${GFLAGS_TARGET} format_reader ie_samples_utils)\n```\n\n----------------------------------------\n\nTITLE: Starting Guest VM with vTPM (sh)\nDESCRIPTION: Starts the Guest VM using QEMU with KVM acceleration, specifies memory and CPU configurations, defines network interfaces, and connects to the virtual TPM device. It uses a disk image, configures networking via tap devices with associated scripts, and sets up a VNC server for remote access.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\nsudo qemu-system-x86_64 \\\n -cpu host \\\n -enable-kvm \\\n -m 8192 \\\n -smp 8,sockets=1,cores=8,threads=1 \\\n -device e1000,netdev=hostnet0,mac=52:54:00:d1:66:6f \\\n -netdev tap,id=hostnet0,script=<path-to-scripts>/br0-qemu-ifup,downscript=<path-to-scripts>/br0-qemu-ifdown \\\n -device e1000,netdev=hostnet1,mac=52:54:00:d1:66:5f \\\n -netdev tap,id=hostnet1,script=<path-to-scripts>/virbr0-qemu-ifup,downscript=<path-to-scripts>/virbr0-qemu-ifdown \\\n -drive if=virtio,file=<path-to-disk-image>/ovsa_isv_dev_vm_disk.qcow2,cache=none \\\n -chardev socket,id=chrtpm,path=/var/OVSA/vtpm/vtpm_isv_dev/swtpm-sock \\\n -tpmdev emulator,id=tpm0,chardev=chrtpm \\\n -device tpm-tis,tpmdev=tpm0 \\\n -vnc :1\n```\n\n----------------------------------------\n\nTITLE: Fine-tune Pruned Model - TensorFlow 2\nDESCRIPTION: This code snippet shows how to fine-tune the pruned model in TensorFlow 2, ensuring that the training schedule and learning rate are similar to those used for the original model. This is crucial for maintaining accuracy after pruning.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwith tf.GradientTape() as tape:\n    output = model(input_batch)\n    loss = loss_fn(output, target_batch)\n\ngrads = tape.gradient(loss, model.trainable_variables)\noptimizer.apply_gradients(zip(grads, model.trainable_variables))\n```\n\n----------------------------------------\n\nTITLE: Cross-compilation CMake Configuration\nDESCRIPTION: This snippet configures CMake for cross-compilation on macOS Apple Silicon. It adds `-DCMAKE_OSX_ARCHITECTURES=x86_64` to the cmake configuration step and disables system library usage explicitly, ensuring that the build targets x86_64 architecture.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -DCMAKE_OSX_ARCHITECTURES=x86_64 ..\n```\n\n----------------------------------------\n\nTITLE: Model Operations C++\nDESCRIPTION: Retrieves the inputs and outputs of an OpenVINO model. This is necessary for understanding the model's expected input format and interpreting its output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_classification/README.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n``ov::Model::inputs``\n```\n\nLANGUAGE: C++\nCODE:\n```\n``ov::Model::outputs``\n```\n\n----------------------------------------\n\nTITLE: Adding CPack Component in CMake\nDESCRIPTION: This CMake snippet adds a CPack component for the pyopenvino module, versioned by the Python version. The component is hidden, indicating it's likely an internal component. The `ov_cpack_add_component` function is a custom function used within the OpenVINO build system to handle packaging.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nov_cpack_add_component(${OV_CPACK_COMP_PYTHON_OPENVINO}_${pyversion}\n                        HIDDEN)\n```\n\n----------------------------------------\n\nTITLE: Adding Version Defines in CMake\nDESCRIPTION: This CMake code snippet adds version defines to the pyopenvino module by calling the custom function `ov_add_version_defines`. This likely adds preprocessor definitions that contain version information, which can be accessed in the C++ code to expose version information to the Python module.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_version_defines(pyopenvino.cpp ${PROJECT_NAME})\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Operation in C++\nDESCRIPTION: This snippet shows how to register the `custom_add` operation with the ONNX Frontend using the `REGISTER_OPERATOR_WITH_DOMAIN` macro. It associates the operation name \"CustomAdd\", version 1, and the implementation function `custom_add` with the `OPENVINO_ONNX_DOMAIN` domain.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/how_to_add_op.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"op/org.openvinotoolkit/custom_add.hpp\"\n...\nREGISTER_OPERATOR_WITH_DOMAIN(OPENVINO_ONNX_DOMAIN, \"CustomAdd\", 1, custom_add);\n```\n\n----------------------------------------\n\nTITLE: Einsum Layer with implicit equation XML Configuration\nDESCRIPTION: This XML snippet demonstrates the configuration of an Einsum layer with the equation `ab...,ac...,ade->...bc`. The layer takes three inputs with shapes 2x3x4, 2x7x1, and 2x4x7 respectively and outputs a tensor with shape 4x3x7. This showcases an implicit equation using ellipsis to represent multiple dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/einsum-7.rst#_snippet_10\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Einsum\" version=\"opset7\">\n    <data equation=\"ab...,ac...,ade->...bc\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>\n            <dim>7</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"3\">\n            <dim>2</dim>\n            <dim>4</dim>\n            <dim>7</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>4</dim>\n            <dim>3</dim>\n            <dim>7</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Squeeze 2D Tensor to Static Shape using OpenVINO XML\nDESCRIPTION: This example shows how to squeeze a 2D tensor with dynamic and static shape elements to a static shape output, following opset1 rules. The 'allow_axis_skip' attribute is set to false.  Axis 1 is specified for squeezing, and it's assumed the actual value of the dynamic dimension is squeezable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/squeeze-15.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Squeeze\" version=\"opset15\">\n    <data allow_axis_skip=\"false\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>-1</dim>\n        </port>\n    </input>\n    <input>\n        <port id=\"1\">\n            <dim>1</dim>  <!-- value is [1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>2</dim>  <!-- assumes: actual value of <dim>-1</dim> is squeezable -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Run Conformance Tests on GPU\nDESCRIPTION: This command runs the opset conformance tests for OMZ models on the GPU using the default method.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython3 run_conformance.py -d GPU\n```\n\n----------------------------------------\n\nTITLE: Create an Advanced Model (Python)\nDESCRIPTION: This snippet shows how to create a more advanced model with multiple outputs in OpenVINO using Python.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n# [ov:create_advanced_model]\n```\n\n----------------------------------------\n\nTITLE: Running benchmark_app with Cumulative Throughput Hint on AUTO plugin\nDESCRIPTION: This snippet demonstrates how to run the `benchmark_app` with the `-hint cumulative_throughput` option, targeting the AUTO plugin. It uses a sample model file (`add_abc.xml`) and specifies the target device as `AUTO`. This configuration aims to maximize the overall throughput by utilizing available devices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/tests.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nopenvino/bin/intel64/Release$ ./benchark_app -m openvino/src/core/tests/models/ir/add_abc.xml -d AUTO -hint cumulative_throughput\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO Static Libraries with CMake (sh)\nDESCRIPTION: This snippet builds OpenVINO Runtime in static mode by setting the `BUILD_SHARED_LIBS` CMake option to `OFF`. It also specifies the location of the OpenVINO source code. Then the usual CMake build command is executed, followed by the install step.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/static_libaries.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DBUILD_SHARED_LIBS=OFF <all other CMake options> <openvino_sources root>\n```\n\nLANGUAGE: sh\nCODE:\n```\ncmake --build . --target openvino --config Release -j12\n```\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_INSTALL_PREFIX=<install_root> -P cmake_install.cmake\n```\n\n----------------------------------------\n\nTITLE: BitwiseRightShift Layer Numpy Broadcast XML Configuration\nDESCRIPTION: This XML configuration demonstrates a BitwiseRightShift layer where numpy broadcasting is performed. Input tensors have different dimensions, and the output tensor has the shape resulting from the numpy broadcasting rules. The auto_broadcast attribute is implicitly set to \"numpy\" by default, so the shapes are broadcasted to (8, 7, 6, 5).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-right-shift-15.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"BitwiseRightShift\">\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark App with Input Image (C++)\nDESCRIPTION: This command runs the OpenVINO benchmark application using a C++ executable, specifying the path to the model and an input image. It assumes that the `benchmark_app` executable is in the current directory. It requires the OpenVINO C++ runtime to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\n./benchmark_app -m model.xml -i test1.jpg\n```\n\n----------------------------------------\n\nTITLE: Float32 Conversion in Mersenne-Twister (C++)\nDESCRIPTION: This code snippet demonstrates how to convert a 32-bit unsigned integer to a float32 value within the Mersenne-Twister algorithm. It uses a mask and divisor to scale the integer value to a floating-point number between 0 and 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\nmantissa_digits = 24 //(mantissa / significand bits count of float + 1, equal to std::numeric_limits<float>::digits == FLT_MANT_DIG == 24)\nmask = uint32(uint64(1) << mantissa_digits - 1)\ndivisor = float(1) / (uint64(1) << mantissa_digits)\noutput = float((x & mask) * divisor)\n```\n\n----------------------------------------\n\nTITLE: Sequence Diagram: JIT Emitter Workflow\nDESCRIPTION: This sequence diagram depicts the interaction between jit_kernel, jit_operation_emitter, and jit_emitter during various stages of the JIT compilation process. It includes steps for preparation, code emission, and data emission.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/src/emitters/README.md#_snippet_1\n\nLANGUAGE: Mermaid\nCODE:\n```\nsequenceDiagram\n    participant jit_kernel\n    participant jit_operation_emitter\n    participant jit_emitter\n    \n    jit_kernel->>jit_kernel: preamble\n\n    Note left of jit_operation_emitter: preparation\n    jit_kernel->>jit_operation_emitter: get_supported_precisions\n    jit_operation_emitter->>jit_kernel: \n\n    jit_kernel->>jit_operation_emitter: constructor\n    jit_operation_emitter->>jit_emitter: prepare_table\n    jit_emitter->>jit_operation_emitter: \n    jit_operation_emitter->>jit_operation_emitter: register_table_entries\n    jit_operation_emitter->>jit_kernel: \n    jit_operation_emitter->>jit_operation_emitter: create dependent emitter instance\n    jit_operation_emitter->>jit_kernel: \n\n    jit_kernel->>jit_operation_emitter: get_inputs_count\n    jit_operation_emitter->>jit_kernel: \n\n    jit_kernel->>jit_operation_emitter: get_aux_vecs_count\n    jit_operation_emitter->>jit_kernel: \n\n    jit_kernel->>jit_operation_emitter: get_aux_gprs_count\n    jit_operation_emitter->>jit_kernel: \n\n    Note left of jit_operation_emitter: code emission\n    jit_kernel->>jit_emitter: emit_code\n    jit_emitter->>jit_emitter: emitter_preamble\n    jit_emitter->>jit_operation_emitter: emit_impl\n    jit_operation_emitter->>jit_operation_emitter: emit_isa\n    jit_operation_emitter->>jit_emitter: \n    jit_emitter->>jit_emitter: emitter_postamble\n    jit_emitter->>jit_kernel: \n\n    Note left of jit_operation_emitter: data emission\n    jit_kernel->>jit_operation_emitter: emit_data\n    jit_operation_emitter->>jit_emitter: emit_data\n    jit_emitter->>jit_operation_emitter: \n    jit_operation_emitter->>jit_operation_emitter: dependent emitter: emit_data \n    jit_operation_emitter->>jit_kernel: \n    \n    jit_kernel->>jit_kernel: postamble\n```\n\n----------------------------------------\n\nTITLE: Adding Smart CI as a Prerequisite Job (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to add Smart_CI as a 'needs' dependency for a job that validates a component. This ensures the validation job has access to the Smart CI outputs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_9\n\nLANGUAGE: YAML\nCODE:\n```\njob_that_validates_your_component:\n  needs: Smart_CI  # if other job is already specified here, add Smart_CI to the list like that: [Other_Job_ID, Smart_CI]\n  ...\n```\n\n----------------------------------------\n\nTITLE: Generating Visual Studio Solution with CMake\nDESCRIPTION: This snippet uses CMake to generate a Visual Studio solution file for building OpenVINO. It requires specifying the path to the OpenVINO source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_windows.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncmake -G \"Visual Studio 17 2022\" <path/to/openvino>\n```\n\n----------------------------------------\n\nTITLE: Adding Unit Test Target (CMake)\nDESCRIPTION: Defines the unit test target using the `ov_add_test_target` macro. This includes specifying the target name, root directory, additional source directories, include directories, linked libraries, dependencies, and labels.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/tests/unit/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n        NAME\n            ${TARGET_NAME}\n        ROOT\n            ${CMAKE_CURRENT_SOURCE_DIR}\n        ADDITIONAL_SOURCE_DIRS\n            ${OpenVINO_SOURCE_DIR}/src/plugins/auto_batch/src\n        INCLUDES\n            ${CMAKE_CURRENT_SOURCE_DIR}\n            ${OpenVINO_SOURCE_DIR}/src/plugins/auto_batch/src\n            ${SHARED_HEADERS_DIR}\n        LINK_LIBRARIES\n            unit_test_utils\n        DEPENDENCIES\n            mock_engine\n        ADD_CPPLINT\n        LABELS\n            OV UNIT AUTO_BATCH\n)\n```\n\n----------------------------------------\n\nTITLE: Collecting Statistics Data (Windows)\nDESCRIPTION: This snippet builds the `sea_itt_lib` target, sets the path to include TBB binaries, creates a directory for collected data, and then uses `sea_runtool.py` to collect profiling data by running the `benchmark_app.exe`. The collected data is stored in the specified output directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\ncd %OPENVINO_HOME%\\build_cc\ncmake --build . --config Debug --target sea_itt_lib\ncd %OPENVINO_HOME%\nset PATH=%PATH%;%OPENVINO_HOME%\\\\temp\\tbb\\bin\nmkdir cc_data\ncd %OPENVINO_HOME%\\cc_data\npython3 ..\\thirdparty\\itt_collector\\runtool\\sea_runtool.py --bindir ..\\bin\\intel64\\Debug -o %OPENVINO_HOME%\\cc_data\\data ! ..\\bin\\intel64\\Debug\\benchmark_app.exe -niter 1 -nireq 1 -m <your_model.xml>\n```\n\n----------------------------------------\n\nTITLE: ReduceProd Layer with keep_dims=false (XML) - Example 1\nDESCRIPTION: This XML snippet defines a ReduceProd layer with 'keep_dims' set to 'false'. The input tensor is 6x12x10x24, and reduction happens along axes 2 and 3. The output tensor becomes 6x12 as the reduced dimensions are dropped.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-prod-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceProd\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ReduceSum Operation C++\nDESCRIPTION: Illustrates the general formula for the ReduceSum operation in C++.  It computes the sum of elements along specified axes within an input tensor. Indices i0, ..., iN run through all valid indices, and summation has jk = ik for dimensions not specified by the axes input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-sum-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i0, i1, ..., iN] = sum[j0, ..., jN](x[j0, ..., jN]))\n```\n\n----------------------------------------\n\nTITLE: Reading Model Synchronously in OpenVINO (TypeScript)\nDESCRIPTION: This snippet shows the `readModelSync` function, a synchronous version of `readModel`, for reading models. It supports various formats like IR, ONNX, PDPD, TF, and TFLite, taking the model path and an optional weights path as arguments and returning a Model object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nreadModelSync(modelPath, weightsPath?): Model\n```\n\nLANGUAGE: typescript\nCODE:\n```\nweightsPath: string\n```\n\nLANGUAGE: typescript\nCODE:\n```\nreadModelSync(modelPath, weights): Model\n```\n\nLANGUAGE: typescript\nCODE:\n```\nreadModelSync(modelBuffer, weightsBuffer?): Model\n```\n\nLANGUAGE: typescript\nCODE:\n```\nweightsBuffer: Uint8Array\n```\n\n----------------------------------------\n\nTITLE: Roll Layer Configuration Example 2\nDESCRIPTION: This XML configuration shows a Roll layer where the 'shift' value is a scalar, and multiple axes are specified. The configuration defines input and output tensor dimensions for the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/roll-7.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Roll\">\n        <input>\n            <port id=\"0\">\n                <dim>3</dim>\n                <dim>10</dim>\n                <dim>100</dim>\n                <dim>200</dim>\n            </port>\n            <port id=\"1\">\n                <dim>1</dim>\n            </port>\n            <port id=\"2\">\n                <dim>2</dim> <!-- shifting along specified axes with the same shift value -->\n            </port>\n        </input>\n        <output>\n            <port id=\"0\">\n                <dim>3</dim>\n                <dim>10</dim>\n                <dim>100</dim>\n                <dim>200</dim>\n            </port>\n        </output>\n    </layer>\n```\n\n----------------------------------------\n\nTITLE: Building Executables from Source Files (CMake)\nDESCRIPTION: This code snippet iterates through all .cpp files in the directory, creating an executable for each. It extracts the file name (without extension) as the target name and links the `tests_shared_lib` and `timetests_helper` libraries to each executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/src/timetests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nFILE(GLOB tests \"*.cpp\")\n\nforeach(test_source ${tests})\n    get_filename_component(test_name ${test_source} NAME_WE)\n    add_executable(${test_name} ${test_source})\n\n    target_link_libraries(${test_name} PRIVATE tests_shared_lib timetests_helper)\n\n    add_dependencies(time_tests ${test_name})\n\n    install(TARGETS ${test_name}\n            RUNTIME DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Network Structure Definition in C++\nDESCRIPTION: Defines the structure of the `network` class, which represents the entire neural network. It contains the program, stream, memory pool, and collections of primitive instances (inputs, outputs, execution order). The `execute` method triggers the execution of the network.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/basic_data_structures.md#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nstruct network {\n...\n    program::ptr _program;\n    stream::ptr _stream;\n    std::unique_ptr<memory_pool> _memory_pool;\n    std::map<primitive_id, std::shared_ptr<primitive_inst>> _primitives;\n    std::vector<std::shared_ptr<primitive_inst>> _inputs;\n    std::vector<std::shared_ptr<primitive_inst>> _outputs;\n    std::list<std::shared_ptr<primitive_inst>> _exec_order;\n    std::list<std::shared_ptr<primitive_inst>> _data_outputs;\n    std::unordered_map<primitive_id, event::ptr> _events;\n    output_chains_map _output_chains;\n...\n    std::map<primitive_id, network_output> execute(const std::vector<event::ptr>& dependencies = {});\n    void set_arguments();\n    void allocate_primitives();\n};\n```\n\n----------------------------------------\n\nTITLE: Preprocess Property Declaration\nDESCRIPTION: Defines the preprocess property within the NodeAddon, which contains properties related to pre- and post-processing steps. Specifically, it includes resizeAlgorithm for selecting resizing algorithms and PrePostProcessor for creating pre/post-processing graphs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/addon.rst#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\npreprocess: {\n          resizeAlgorithm: typeof resizeAlgorithm;\n          PrePostProcessor: PrePostProcessorConstructor;\n      }\n```\n\n----------------------------------------\n\nTITLE: Enable Version and Language Selectors in conf.py\nDESCRIPTION: This snippet demonstrates how to enable version and language selectors by adding configuration to the `html_context` variable in your `conf.py` file.  It defines `current_version`, `current_language`, `languages`, and `versions`. You need to define both `current_version` and `versions` to enable the version selector, and both `current_language` and `languages` to enable the language selector.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/openvino_sphinx_theme/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nhtml_context = {\n    'current_version': 'latest',\n    'current_language': 'en',\n    'languages': (('English', '/en/latest/'), ('Chinese', '/cn/latest/')),\n    'versions': (('latest', '/en/latest/'), ('2022.1', '/en/2022.1'))\n}\n```\n\n----------------------------------------\n\nTITLE: Ignoring Linker Warnings on MSVC for OpenVINO Core\nDESCRIPTION: This snippet ignores specific linker warnings (4217, 4286) when using the MSVC compiler. These warnings are related to dllimport attributes and can be safely ignored in this context.  The code conditionally sets the link type to PRIVATE or PUBLIC based on whether a shared library is being built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    # openvino_core is linked against openvino::reference, openvino::shape_inference static libraries\n    # which include openvino_core headers with dllimport attribute. Linker complains about it\n    # but no way to fix this: linking with no attribute defaults to dllexport and we have\n    # multiple defitions for openvino_core symbols.\n    #\n    # The possible way is to use object libraries for openvino::reference\n    # but it's not convinient since these libraries are exported from build tree\n    # and it's better to use them as static libraries in 3rd party projects\n    if(BUILD_SHARED_LIBS)\n        set(link_type PRIVATE)\n    else()\n        set(link_type PUBLIC)\n    endif()\n\n    target_link_options(openvino_core_obj ${link_type} \"/IGNORE:4217,4286\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Example: Dump Blobs with Specific Execution IDs\nDESCRIPTION: Sets the environment variable to dump blobs only for nodes with the specified execution IDs during OpenVINO CPU execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_NODE_EXEC_ID='1 12 45' binary ...\n```\n\n----------------------------------------\n\nTITLE: Example Output\nDESCRIPTION: Example output for the inference results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_35\n\nLANGUAGE: sh\nCODE:\n```\nTop 10 results:\n\nImage dog.bmp\n\n   classid probability label\n   ------- ----------- -----\n   156     0.6875963   Blenheim spaniel\n   215     0.0868125   Brittany spaniel\n   218     0.0784114   Welsh springer spaniel\n   212     0.0597296   English setter\n   217     0.0212105   English springer, English springer spaniel\n   219     0.0194193   cocker spaniel, English cocker spaniel, cocker\n   247     0.0086272   Saint Bernard, St Bernard\n   157     0.0058511   papillon\n   216     0.0057589   clumber, clumber spaniel\n   154     0.0052615   Pekinese, Pekingese, Peke\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO Wheel package using pip\nDESCRIPTION: This command installs the OpenVINO wheel package (.whl) using pip. It installs the wheel from the specified path.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_linux.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\npip install <openvino_repo>/build/wheel/openvino-2022.2.0-000-cp37-cp37-manylinux_2_35_x86_64.whl\n```\n\n----------------------------------------\n\nTITLE: Setting OneAPI Environment Variables\nDESCRIPTION: This command sets the environment variables required to use the Intel OneAPI toolkit. It is crucial for the build process as it configures the necessary paths and tools.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_with_sycl.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsource /opt/intel/oneapi/setvars.sh\n```\n\n----------------------------------------\n\nTITLE: Conv Eltwise Fusion Test Case C++\nDESCRIPTION: This C++ code defines a test case for convolution fusion with multiple elementwise operations and clamp activation. It creates fused and non-fused networks using `create_topologies`, sets the `conv_prim` implementation to `format::b_fs_yx_fsv16` using `build_option::force_implementations`, and compares the outputs using the `execute` method. The test skips execution on devices that support immad instructions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_unit_test.md#_snippet_6\n\nLANGUAGE: c++\nCODE:\n```\nclass conv_fp32_multi_eltwise_4_clamp : public ConvFusingTest {};\nTEST_P(conv_fp32_multi_eltwise_4_clamp, basic) {\n    if (engine.get_device_info().supports_immad) {\n        return;\n    }\n    auto p = GetParam();\n    create_topologies(\n        input_layout(\"input\", get_input_layout(p)),\n        data(\"eltwise1_data\", get_mem(get_output_layout(p))),\n        data(\"eltwise2_data\", get_mem(get_output_layout(p))),\n        data(\"eltwise4_data\", get_mem(get_output_layout(p))),\n        data(\"bias\", get_mem(get_bias_layout(p))),\n        data(\"weights\", get_mem(get_weights_layout(p))),\n        convolution(\"conv_prim\", \"input\", { \"weights\" }, { \"bias\" }, p.groups, p.stride, p.pad, p.dilation),\n        eltwise(\"eltwise1_add\", \"conv_prim\", \"eltwise1_data\", eltwise_mode::sum),\n        activation(\"activation\", \"eltwise1_add\", activation_func::clamp, { 0.5f, 2.5f }),\n        eltwise(\"eltwise2_mul\", \"activation\", \"conv_prim\", eltwise_mode::prod),\n        eltwise(\"eltwise3_div\", \"eltwise2_mul\", \"eltwise2_data\", eltwise_mode::prod),\n        eltwise(\"eltwise4_add\", \"eltwise3_div\", \"eltwise4_data\", eltwise_mode::sum),\n        reorder(\"reorder_bfyx\", \"eltwise4_add\", p.default_format, data_types::f32)\n    );\n    implementation_desc conv_impl = { format::b_fs_yx_fsv16, \"\" };\n    bo_fused.set_option(build_option::force_implementations({ { \"conv_prim\", conv_impl } }));\n    tolerance = 1e-5f;\n    execute(p);\n}\n\n```\n\n----------------------------------------\n\nTITLE: ReduceLogicalOr Example 3 (keep_dims=false, axis=1)\nDESCRIPTION: This XML example shows the ReduceLogicalOr operation with `keep_dims` set to `false` and reduction along axis 1. The input tensor is 6x12x10x24, and the output is a 6x10x24 tensor after reducing the second dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-logical-or-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceLogicalOr\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [1] that means independent reduction in each channel and spatial dimensions -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for DPC++ Build (Linux)\nDESCRIPTION: This CMake configuration snippet sets the C and C++ compilers to icx and icpx, respectively, enabling DPC++ compilation. It also disables the Intel CPU plugin and system OpenCL to avoid potential conflicts during the build process. The snippet also includes flags to help resolve GCC version issues and enables ccache for faster rebuilds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_with_sycl.md#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n-DCMAKE_C_COMPILER:FILEPATH=icx -DCMAKE_CXX_COMPILER:FILEPATH=icpx\n-DENABLE_INTEL_CPU=OFF\n-DCMAKE_CXX_FLAGS:STRING=--gcc-install-dir=/lib/gcc/x86_64-linux-gnu/12/ -DCMAKE_C_FLAGS:STRING=--gcc-install-dir=/lib/gcc/x86_64-linux-gnu/12/\n-DENABLE_SYSTEM_OPENCL=OFF\n-DCMAKE_CXX_COMPILER_LAUNCHER=ccache\n```\n\n----------------------------------------\n\nTITLE: Running a C++ Sample (macOS)\nDESCRIPTION: This command executes a C++ sample. It takes the path to the executable, the input media, the model, and the target device as arguments. This is for macOS.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_28\n\nLANGUAGE: sh\nCODE:\n```\n<sample.exe file> -i <path_to_media> -m <path_to_model> -d <target_device>\n```\n\n----------------------------------------\n\nTITLE: Conditional Auto Batch Plugin Dependency\nDESCRIPTION: This snippet conditionally adds the `openvino_auto_batch_plugin` dependency and the `ENABLE_AUTO_BATCH` compile definition if `ENABLE_AUTO_BATCH` is set. This enables auto-batching functionality for the tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/functional/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_AUTO_BATCH)\n    list(APPEND DEPENDENCIES openvino_auto_batch_plugin)\n    list(APPEND COMPILE_DEFINITIONS ENABLE_AUTO_BATCH)\nendif()\n```\n\n----------------------------------------\n\nTITLE: ExperimentalDetectronGenerateProposalsSingleImage XML Configuration\nDESCRIPTION: This XML snippet demonstrates the configuration of the ExperimentalDetectronGenerateProposalsSingleImage operation with specific attributes such as min_size, nms_threshold, post_nms_count, and pre_nms_count. It also defines the input and output ports with their respective dimensions and precisions. The input ports define the dimensions of the image size, anchors, deltas, and proposal scores tensors. The output ports define the dimensions and precision of the ROIs and ROIs scores tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/experimental-detectron-generate-proposals-single-image-6.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ExperimentalDetectronGenerateProposalsSingleImage\" version=\"opset6\">\n    <data min_size=\"0.0\" nms_threshold=\"0.699999988079071\" post_nms_count=\"1000\" pre_nms_count=\"1000\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n        </port>\n        <port id=\"1\">\n            <dim>12600</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"2\">\n            <dim>12</dim>\n            <dim>50</dim>\n            <dim>84</dim>\n        </port>\n        <port id=\"3\">\n            <dim>3</dim>\n            <dim>50</dim>\n            <dim>84</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"4\" precision=\"FP32\">\n            <dim>1000</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"5\" precision=\"FP32\">\n            <dim>1000</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Disable Suggest Override Warning\nDESCRIPTION: This snippet disables the '-Wno-suggest-override' compiler flag if the SUGGEST_OVERRIDE_SUPPORTED variable is set. This is done to prevent compilation failures with xbyak.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/functional/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(SUGGEST_OVERRIDE_SUPPORTED)\n    # xbyak compilation fails\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-suggest-override\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: ScatterNDUpdate Example 1 in C++\nDESCRIPTION: This C++ code snippet demonstrates updating four single elements in the 'data' tensor using ScatterNDUpdate. The 'indices' tensor specifies the positions to update, and the 'updates' tensor provides the new values. The 'output' tensor is the result of the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-nd-update-3.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ndata    = [1, 2, 3, 4, 5, 6, 7, 8]\nindices = [[4], [3], [1], [7]]\nupdates = [9, 10, 11, 12]\noutput  = [1, 11, 3, 10, 9, 6, 7, 12]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO PyTorch Frontend with CMake\nDESCRIPTION: This CMake snippet configures the OpenVINO build system to add the PyTorch frontend. It specifies the frontend's name, linkable status, protobuf shutdown behavior, a brief description, and required libraries for linking against the OpenVINO core.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/pytorch/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_frontend(NAME pytorch\n                LINKABLE_FRONTEND\n                SHUTDOWN_PROTOBUF\n                FILEDESCRIPTION \"FrontEnd to load and convert TorchScript models from PyTorch\"\n                LINK_LIBRARIES openvino::util openvino::core::dev)\n```\n\n----------------------------------------\n\nTITLE: Exporting Targets for Developer Package\nDESCRIPTION: Exports the target for use in developer packages, allowing other projects to link against it. The `ov_developer_package_export_targets` macro creates the necessary files for exporting the target, including information about include directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/common/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET openvino::npu_common\n                                    INSTALL_INCLUDE_DIRECTORIES\n                                        ${CMAKE_CURRENT_SOURCE_DIR}/include)\n```\n\n----------------------------------------\n\nTITLE: Installing Test Executables (CMake)\nDESCRIPTION: This snippet installs the created test executables into the `tests` directory. The `RUNTIME` destination specifies where the executables are placed after installation. The `COMPONENT tests` option is used for grouping related install targets, and `EXCLUDE_FROM_ALL` prevents them from being installed with the default `install` target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/src/timetests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS ${test_name}\n            RUNTIME DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Triggering Inference Execution C++\nDESCRIPTION: The `inferRequest->infer()` or `inferRequest->start_async()` methods are used to initiate network execution within OpenVINO. The `infer()` method executes the inference synchronously, while `start_async()` starts it asynchronously. These methods trigger the underlying kernel execution within the GPU plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/execution_of_inference.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ninferRequest->infer()\n```\n\nLANGUAGE: cpp\nCODE:\n```\ninferRequest->start_async()\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This CMake snippet adds the 'onnx_common' and 'frontend' directories as subprojects to the main OpenVINO project.  It also conditionally includes the 'tests' subdirectory if the `ENABLE_TESTS` variable is set to true.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(onnx_common)\nadd_subdirectory(frontend)\n\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Selecting nodes with specific type - Bash\nDESCRIPTION: This example shows how to set `OV_CPU_INFER_PRC_POS_PATTERN` to select all nodes with a specific type, such as `FullyConnected`, rather than filtering by original layer name. The pattern matches the format `NodeType@orginialLayers`. It's useful for debugging accuracy issues related to specific operation types.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/infer_prc.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n\"^FullyConnected@\"\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Generation with OpenVINO GenAI (C++)\nDESCRIPTION: This C++ snippet demonstrates how to use the `Text2ImagePipeline` from the `openvino_genai` library to generate an image from a text prompt. It takes a model directory and a prompt as command-line arguments, initializes the pipeline, generates the image, and saves it as a BMP file using the `imwrite` helper function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n#include \"openvino/genai/image_generation/text2image_pipeline.hpp\"\n\n#include \"imwrite.hpp\"\n\nint32_t main(int32_t argc, char* argv[]) try {\n    OPENVINO_ASSERT(argc == 3, \"Usage: \", argv[0], \" <MODEL_DIR> '<PROMPT>' \");\n\n    const std::string models_path = argv[1], prompt = argv[2];\n    const std::string device = \"CPU\";  // GPU can be used as well\n\n    ov::genai::Text2ImagePipeline pipe(models_path, device);\n    ov::Tensor image = pipe.generate(prompt,\n        ov::genai::width(512),\n        ov::genai::height(512),\n        ov::genai::num_inference_steps(20),\n        ov::genai::num_images_per_prompt(1));\n\n    // writes `num_images_per_prompt` images by pattern name\n    imwrite(\"image_%d.bmp\", image, true);\n\n    return EXIT_SUCCESS;\n} catch (const std::exception& error) {\n    try {\n        std::cerr << error.what() << '\\n';\n    } catch (const std::ios_base::failure&) {}\n    return EXIT_FAILURE;\n} catch (...) {\n    try {\n        std::cerr << \"Non-exception object thrown\\n\";\n    } catch (const std::ios_base::failure&) {}\n    return EXIT_FAILURE;\n}\n```\n\n----------------------------------------\n\nTITLE: Pad Layer Definition - Constant Mode - XML\nDESCRIPTION: Defines a Pad layer in OpenVINO's XML format, configured with 'constant' pad mode. It specifies input and output port dimensions, pads_begin, pads_end, and pad_value. The example demonstrates padding a 1x3x32x40 tensor with specific padding values, resulting in a 2x8x37x48 output tensor filled with 15.0.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Pad\" ...>\n    <data pad_mode=\"constant\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>40</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>     <!-- pads_begin = [0, 5, 2, 1]  -->\n        </port>\n        <port id=\"2\">\n            <dim>4</dim>     <!-- pads_end = [1, 0, 3, 7] -->\n        </port>\n        <port id=\"3\">\n                            <!-- pad_value = 15.0 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>2</dim>     <!-- 2 = 0 + 1 + 1 = pads_begin[0] + input.shape[0] + pads_end[0] -->\n            <dim>8</dim>     <!-- 8 = 5 + 3 + 0 = pads_begin[1] + input.shape[1] + pads_end[1] -->\n            <dim>37</dim>    <!-- 37 = 2 + 32 + 3 = pads_begin[2] + input.shape[2] + pads_end[2] -->\n            <dim>48</dim>    <!-- 48 = 1 + 40 + 7 = pads_begin[3] + input.shape[3] + pads_end[3] -->\n                            <!-- all new elements are filled with 15.0 value -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Cloning riscv-gnu-toolchain and building\nDESCRIPTION: This snippet clones the riscv-gnu-toolchain repository, configures it with a specified installation path (/opt/riscv), and then builds the toolchain along with the QEMU emulator. This toolchain provides support for RVV 0.7.1 and ratified RVV 1.0.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/riscv-collab/riscv-gnu-toolchain.git\ncd riscv-gnu-toolchain\n./configure --prefix=/opt/riscv\nmake linux build-qemu -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: CC Support and Include Directories\nDESCRIPTION: This snippet enables support for a C compiler for the target and specifies include directories. It includes the current source directory and the include directory of the OpenVINO Template Plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_mark_target_as_cc(${TARGET_NAME})\n\ntarget_include_directories(${TARGET_NAME} PRIVATE\n    \"${CMAKE_CURRENT_SOURCE_DIR}\"\n    \"${OpenVINOTemplatePlugin_SOURCE_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files in CMake\nDESCRIPTION: Uses the file(GLOB_RECURSE) command to find all source files (*.cpp) and header files (*.hpp) in the specified directories, storing the results in the LIBRARY_SRC and PUBLIC_HEADERS variables respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE PUBLIC_HEADERS ${PUBLIC_HEADERS_DIR}/low_precision/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Get Inference Number of Threads (Python)\nDESCRIPTION: This code demonstrates how to query the number of threads used for inference on the CPU device in Python using `ov::CompiledModel::get_property`.  It retrieves the number of threads and prints the value.  Requires a compiled OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ncore = ov.Core()\nmodel_path = \"path_to_model.xml\"\ndevice_name = \"CPU\"\nmodel = core.read_model(model_path)\ncompiled_model = core.compile_model(model, device_name)\ninference_num_threads = compiled_model.get_property(\"num_threads\")\nprint(f\"Inference num threads: {inference_num_threads}\")\n```\n\n----------------------------------------\n\nTITLE: Erf Layer XML Configuration in OpenVINO\nDESCRIPTION: This XML snippet shows how to configure an Erf layer in OpenVINO. It defines the input and output ports with their dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/erf-1.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"Erf\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating Build Directory for OpenVINO\nDESCRIPTION: This snippet creates a build directory and navigates into it.  This is where the CMake configuration and build process will take place.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_windows.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nmkdir build && cd build\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example with Explicit Padding (Shell)\nDESCRIPTION: This example demonstrates the MaxPool operation with a 4D input, using a 2D kernel and explicit padding. It shows the input tensor, strides, padding, kernel size, rounding type, auto_pad setting, and the resulting output tensor after the MaxPool operation is applied.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-1.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[-1, 2, 3],\n                 [4, 5, -6],\n                 [-7, 8, 9]]]]\nstrides = [1, 1]\npads_begin = [1, 1]\npads_end = [1, 1]\nkernel = [2, 2]\nrounding_type = \"floor\"\nauto_pad = \"explicit\"\noutput = [[[[-1, 2, 3, 3],\n                 [4, 5, 5, -6],\n                 [4, 8, 9, 9],\n                 [-7, 8, 9, 9]]]]\n```\n\n----------------------------------------\n\nTITLE: Loop Condition Examples in Shell\nDESCRIPTION: Demonstrates different loop condition scenarios using pseudo-code, outlining the behavior of the Loop operation with various combinations of trip count and execution condition inputs. It illustrates infinite loops, while loops, do-while loops, for loops, and for loops with conditions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/loop-5.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ninput (-1, true) // infinite loop\n    bool cond = true;\n    for (int i = 0; cond; ++i)\n    {\n        cond = true; // sub-graph calculating condition must always return \"true\"!\n    }\n\ninput (-1, cond) // while loop\n    bool cond = ...;\n    for (int i = 0; cond; ++i)\n    {\n        cond = ...;\n    }\n\ninput (-1, true) // do-while loop\n    bool cond = true;\n    for (int i = 0; cond; ++i)\n    {\n        cond = ...;\n    }\n\ninput (trip_count, true) // for loop\n    int trip_count = ...;\n    bool cond = true;\n    for (int i = 0; i < trip_count; ++i)\n    {\n        cond = true; // sub-graph calculating condition must always return \"true\"!\n    }\n\ninput (trip_count, cond) // for with condition\n    int trip_count = ...;\n    bool cond = ...;\n    for (int i = 0; i < trip_count && cond; ++i)\n    {\n        cond = ...;\n    }\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example 8 (non-default axis)\nDESCRIPTION: This example shows how MaxPool operates on a 4D input with a 2D kernel, with a non-default `axis` value. It describes the input shape, output shape, and demonstrates the use of the axis parameter.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/pooling_shape_rules.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]],\n                [[10, 11, 12],\n                 [13, 14, 15],\n                 [16, 17, 18]]]]   # shape: (1, 2, 3, 3)\n      strides = [1, 1]\n      kernel = [2, 2]\n      dilations = [1, 1]\n      rounding_type = \"floor\"\n      auto_pad = \"explicit\"\n      pads_begin = [0, 0]\n      pads_end = [0, 0]\n      axis = 2\n      output0 = [[[[5, 6],\n                   [8, 9]],\n                  [[14, 15],\n                   [17, 18]]]]   # shape: (1, 2, 2, 2)\n      output1 = [[[[4, 5],\n                   [7, 8]],\n                  [[4, 5],\n                   [7, 8]]]]   # shape: (1, 2, 2, 2)\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to OpenVINO Core Object Library\nDESCRIPTION: This snippet links various libraries to the `openvino_core_obj` target. These libraries provide essential functionalities to the OpenVINO core, such as reference implementations, utilities, shape inference, and the developer API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(openvino_core_obj PRIVATE openvino::reference openvino::util\n                                         openvino::pugixml openvino::shape_inference openvino::core::dev)\n```\n\n----------------------------------------\n\nTITLE: Docker Build Command for Local Reproduction\nDESCRIPTION: This command demonstrates how to build a custom Docker image locally, mirroring the CI environment for local reproducibility. It uses the Dockerfile located at `.github/dockerfiles/ov_test/ubuntu_22_04_x64/Dockerfile` and tags the image as `my_local_ov_test_build`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/docker_images.md#_snippet_4\n\nLANGUAGE: docker\nCODE:\n```\ncd _git/openvino\ndocker build -f .github/dockerfiles/ov_test/ubuntu_22_04_x64/Dockerfile -t my_local_ov_test_build .\n```\n\n----------------------------------------\n\nTITLE: MatrixNonMaxSuppression Layer Definition in C++\nDESCRIPTION: This code snippet demonstrates how to define a MatrixNonMaxSuppression layer in C++ using the OpenVINO Inference Engine. It shows the layer's attributes, input ports with their dimensions, and output ports with their precision and dimensions. The dimensions may be static or dynamic (indicated by -1).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/matrix-non-max-suppression-8.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"MatrixNonMaxSuppression\" ... >\n    <data decay_function=\"gaussian\" sort_result=\"score\" output_type=\"i64\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>100</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>\n            <dim>5</dim>\n            <dim>100</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"5\" precision=\"FP32\">\n            <dim>-1</dim> <!-- \"-1\" means a undefined dimension calculated during the model inference -->\n            <dim>6</dim>\n        </port>\n        <port id=\"6\" precision=\"I64\">\n            <dim>-1</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"7\" precision=\"I64\">\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Environment Variables in PowerShell\nDESCRIPTION: This command sets up the OpenVINO environment variables in PowerShell.  It sources the `setupvars.ps1` script.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\n. <path-to-setupvars-folder>/setupvars.ps1\n```\n\n----------------------------------------\n\nTITLE: Using WrapType with a list of node types\nDESCRIPTION: This Python snippet demonstrates using WrapType to match either a Relu or a Sigmoid node in a pattern. The pattern_sig variable can represent either type, allowing for more versatile matching.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.tools.ovc.composite.pattern import WrapType\n\ndef wrap_type_list():\n    pattern_sig = WrapType(\"opset13.Relu\", \"opset13.Sigmoid\")\n\n    return pattern_sig\n```\n\n----------------------------------------\n\nTITLE: Link System Libraries\nDESCRIPTION: This links the specified system libraries (onnx_proto and onnx) to the `ov_onnx_frontend_tests` target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nov_link_system_libraries(ov_onnx_frontend_tests PUBLIC onnx_proto onnx)\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: Specifies the include directories for the target library. It adds the `include` subdirectory under the current source directory to the include path. The scope is set to PUBLIC and BUILD_INTERFACE to be used when building and installing the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/common/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}\n    PUBLIC\n        $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\n)\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINO Developer Scripts Package\nDESCRIPTION: This snippet locates the 'OpenVINODeveloperScripts' package.  It specifies the required path, disables root path searching, and defaults path.  This package likely contains utility functions and scripts needed for OpenVINO development.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(OpenVINO_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../\")\n\nfind_package(OpenVINODeveloperScripts REQUIRED\n             PATHS \"${OpenVINO_SOURCE_DIR}/cmake/developer_package\"\n             NO_CMAKE_FIND_ROOT_PATH\n             NO_DEFAULT_PATH)\n```\n\n----------------------------------------\n\nTITLE: Using WrapType with a list of node types\nDESCRIPTION: This C++ snippet demonstrates using WrapType to match either a Relu or a Sigmoid node in a pattern. The pattern_sig variable can represent either type, allowing for more versatile matching.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nshared_ptr<ov::Node> wrap_type_list() {\n    auto pattern_sig = make_shared<WrapType<opset13::Relu, opset13::Sigmoid>>();\n\n    return pattern_sig;\n}\n```\n\n----------------------------------------\n\nTITLE: Synchronous Inference in Python\nDESCRIPTION: This snippet highlights the `infer` method of the `InferRequest` object, which is used to perform synchronous inference.  The function blocks until the inference is complete.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/python/benchmark/sync_benchmark/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nopenvino.runtime.InferRequest.infer\n```\n\n----------------------------------------\n\nTITLE: Defining Plugin Class Header C++\nDESCRIPTION: This snippet shows the declaration of a plugin class, inheriting from `ov::IPlugin`. It defines the basic structure of a plugin, including member variables for the backend, wait executor, and configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nclass Plugin : public ov::IPlugin {\npublic:\n    std::shared_ptr<ov::ICompiledModel> compile_model(const std::shared_ptr<const ov::Model>& model, const ov::AnyMap& config = {}) override;\n    std::shared_ptr<ov::ICompiledModel> compile_model(const std::shared_ptr<const ov::Model>& model, const ov::RemoteContext& context, const ov::AnyMap& config = {}) override;\n\n    ov::SupportedOpsMap query_model(const std::shared_ptr<const ov::Model>& model, const ov::AnyMap& config = {}) const override;\n\n    void set_property(const ov::AnyMap& properties) override;\n    ov::Any get_property(const std::string& name, const ov::AnyMap& config = {}) const override;\n\n    ov::RemoteContext create_context(const ov::AnyMap& params) override;\n\n    void import_model(std::istream& model, const ov::RemoteContext& context, const ov::AnyMap& config = {}) override;\n    std::shared_ptr<ov::ICompiledModel> import_model(std::istream& model, const ov::AnyMap& config = {}) override;\n\n    void export_model(const std::shared_ptr<ov::ICompiledModel>& model, std::ostream& stream) override;\n\n    std::shared_ptr<ov::threading::IStreamsExecutor> get_stream_executor(const ov::AnyMap& config) const;\n\n    std::shared_ptr<ov::IAllocator> get_default_allocator() const override;\n\n    std::shared_ptr<ov::ISharedObjectManager> get_shared_object_manager() const override;\n\nprivate:\n    ov::runtime::Backend::Ptr m_backend;\n    ov::threading::ITaskExecutor::Ptr m_waitExecutor;\n    Configuration m_cfg;\n};\n\n```\n\n----------------------------------------\n\nTITLE: Defining Macro for Adding GPU Backend Target\nDESCRIPTION: Defines a macro `ov_gpu_add_backend_target` to simplify adding GPU backend targets. This macro parses arguments, sets target-specific include directories and link libraries, and calls `ov_add_target` to create the backend target. It also handles dependencies related to ONEDNN.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nmacro(ov_gpu_add_backend_target)\n    set(options\n    )\n    set(oneValueRequiredArgs\n        NAME\n    )\n    set(multiValueArgs\n        INCLUDES                      # Extra include directories\n        LINK_LIBRARIES                # Link libraries (in form of target name or file name)\n        BYPASS                        # All other args that must be passed as is to ov_add_target call\n    )\n    cmake_parse_arguments(ARG \"${options}\" \"${oneValueRequiredArgs}\" \"${multiValueArgs}\" ${ARGN} )\n\n    set(TARGET_INCLUDES ${COMMON_INCLUDE_DIRS} ${ARG_INCLUDES})\n    set(TARGET_LINK_LIBRARIES ${COMMON_LINK_LIBRARIES} ${ARG_LINK_LIBRARIES})\n    set(TARGET_DEFINITIONS \"\")\n\n    foreach(lib IN LISTS TARGET_LINK_LIBRARIES)\n        list(APPEND TARGET_INCLUDES $<TARGET_PROPERTY:${lib},INTERFACE_INCLUDE_DIRECTORIES>)\n        list(APPEND TARGET_DEFINITIONS $<TARGET_PROPERTY:${lib},INTERFACE_COMPILE_DEFINITIONS>)\n    endforeach()\n\n    ov_add_target(\n        NAME ${ARG_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        TYPE OBJECT\n        ADD_CPPLINT\n        INCLUDES\n            PRIVATE\n                ${TARGET_INCLUDES}\n        ${ARG_BYPASS}\n    )\n\n    get_property(CURRENT_LIBS GLOBAL PROPERTY EXTRA_LINK_LIBRARIES_GLOBAL)\n    list(APPEND CURRENT_LIBS ${ARG_LINK_LIBRARIES})\n    set_property(GLOBAL PROPERTY EXTRA_LINK_LIBRARIES_GLOBAL ${CURRENT_LIBS})\n\n    target_compile_options(${TARGET_NAME} PRIVATE ${COMMON_COMPILE_OPTIONS})\n    target_compile_definitions(${TARGET_NAME} PRIVATE ${TARGET_DEFINITIONS})\n    ov_set_threading_interface_for(${TARGET_NAME})\n\n    # We use onednn headers all over the graph module, so we have to append includes to all backends and add a dependency between targets\n    if (ENABLE_ONEDNN_FOR_GPU)\n        target_include_directories(${ARG_NAME} SYSTEM PRIVATE $<TARGET_PROPERTY:onednn_gpu_tgt,INTERFACE_INCLUDE_DIRECTORIES>)\n        add_dependencies(openvino_intel_gpu_${IMPL_TYPE}_obj onednn_gpu_tgt)\n    endif()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO with Coverage Support (CMake)\nDESCRIPTION: This snippet shows how to configure the OpenVINO build using CMake to enable code coverage reporting.  The `-DENABLE_COVERAGE=ON` flag instructs CMake to include coverage information during compilation, enabling the generation of coverage reports later.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/test_coverage.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake -DENABLE_COVERAGE=ON .\n```\n\n----------------------------------------\n\nTITLE: Python Sample Execution\nDESCRIPTION: This is an example of the output when running the Python version of the hello_reshape_ssd sample. It shows the steps of creating the OpenVINO Runtime Core, reading the model, reshaping it, loading it to the plugin, and performing synchronous inference. It also prints the detected objects with their confidence and coordinates.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-reshape-ssd.rst#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n[ INFO ] Creating OpenVINO Runtime Core\n[ INFO ] Reading the model: C:/test_data/models/mobilenet-ssd.xml\n[ INFO ] Reshaping the model to the height and width of the input image\n[ INFO ] Loading the model to the plugin\n[ INFO ] Starting inference in synchronous mode\n[ INFO ] Found: class_id = 52, confidence = 0.98, coords = (21, 98), (276, 210)\n[ INFO ] Image out.bmp was created!\n[ INFO ] This sample is an API example, for any performance measurements please use the dedicated benchmark_app tool\n```\n\n----------------------------------------\n\nTITLE: DepthToSpace Layer Example in XML\nDESCRIPTION: This XML code snippet provides an example of a DepthToSpace layer configuration, including the 'block_size' and 'mode' attributes, along with the input and output port dimensions. It showcases how to define the DepthToSpace operation in an OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/depth-to-space-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"DepthToSpace\" ...>\n    <data block_size=\"2\" mode=\"blocks_first\"/>\n    <input>\n        <port id=\"0\">\n            <dim>5</dim>\n            <dim>28</dim>\n            <dim>2</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>5</dim>  <!-- data.shape[0] -->\n            <dim>7</dim>  <!-- data.shape[1] / (block_size ^ 2) -->\n            <dim>4</dim>  <!-- data.shape[2] * block_size -->\n            <dim>6</dim>  <!-- data.shape[3] * block_size -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Running Image Classification Sample (Windows)\nDESCRIPTION: This command runs the `classification_sample_async.exe` application with a specified image (`dog.bmp`), model (`model.xml`), and device (`CPU`). It assumes the environment variables are set and the application is built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\ncd  %USERPROFILE%\\Documents\\Intel\\OpenVINO\\openvino_cpp_samples_build\\<architecture>\\Release\n.\\classification_sample_async.exe -i <path-to-input-image>\\dog.bmp -m <path-to-your-model>\\model.xml -d CPU\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Runtime with Homebrew (Shell)\nDESCRIPTION: Installs the OpenVINO Runtime using the Homebrew package manager. This command fetches and installs the necessary OpenVINO components and dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-brew.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nbrew install openvino\n```\n\n----------------------------------------\n\nTITLE: Setting Threading Interface for OpenVINO Core\nDESCRIPTION: This snippet sets the threading interface for the `openvino_core_obj` target. The `ov_set_threading_interface_for` function is a custom function that configures the appropriate threading library (e.g., TBB, OpenMP) based on the project's configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(openvino_core_obj)\n```\n\n----------------------------------------\n\nTITLE: export_model() Implementation C++\nDESCRIPTION: This code shows the implementation of the `export_model()` method. This method writes the backend-specific graph to the provided output stream (`model_stream`). This data is required to import the compiled model later using the `Plugin::import_model` method.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/compiled-model.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nvoid CompiledModel::export_model(std::ostream& model_stream) const {\n    OPENVINO_ASSERT(model_stream.good(), \"Output stream is not good.\");\n    // serialize m_model to model_stream\n    ov::serialize(m_model, model_stream, \"compiled_model\");\n}\n```\n\n----------------------------------------\n\nTITLE: Getting Tensor Shape in TypeScript\nDESCRIPTION: Explains how to use the `getShape` method to retrieve the tensor's shape as an array of numbers. The shape represents the dimensions of the tensor (e.g., [1, 3, 224, 224] for an image).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Tensor.rst#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ngetShape(): number[]\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for Tests in CMake\nDESCRIPTION: This snippet adds a subdirectory named `tests` to the build. This allows for the compilation and execution of unit tests for the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Faster Target Build with CMake Unity\nDESCRIPTION: This CMake snippet utilizes a custom function `ov_build_target_faster` to optimize the build process of the `ov_snippets_func_tests` target. It creates a unity build and specifies a precompiled header (`src/precomp.hpp`) for private use, reducing compilation time by combining multiple source files into fewer translation units.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME}\n    UNITY\n    PCH PRIVATE \"src/precomp.hpp\"\n)\n```\n\n----------------------------------------\n\nTITLE: Run Docker Image (CPU Only)\nDESCRIPTION: Runs the `openvino_notebooks` Docker image in interactive mode, mapping port 8888 on the host to port 8888 in the container.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_26\n\nLANGUAGE: console\nCODE:\n```\ndocker run -it -p 8888:8888 openvino_notebooks\n```\n\n----------------------------------------\n\nTITLE: Running a C++ Sample (Linux)\nDESCRIPTION: This command executes a C++ sample. It takes the path to the executable, the input media, the model, and the target device as arguments. This is for Linux.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_26\n\nLANGUAGE: sh\nCODE:\n```\n<sample.exe file> -i <path_to_media> -m <path_to_model> -d <target_device>\n```\n\n----------------------------------------\n\nTITLE: Configuring Threading Interface for OpenVINO Runtime in CMake\nDESCRIPTION: This snippet configures the threading interface for the OpenVINO runtime object library and links the TBB (Threading Building Blocks) library if found. This enables efficient parallel processing within the OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME}_obj)\nif (TBBBIND_2_5_FOUND)\n    target_link_libraries(${TARGET_NAME}_obj PRIVATE ${TBBBIND_2_5_IMPORTED_TARGETS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: SearchSorted XML Layer Definition\nDESCRIPTION: This XML snippet demonstrates the structure of a SearchSorted layer definition in an OpenVINO model. It showcases the layer type, data attribute (right_mode), input port dimensions for both the sorted sequence and the values to be searched, and the output port with its corresponding dimensions and precision (I64).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/search-sorted-15.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"SearchSorted\" ... >\n    <data right_mode=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>7</dim>\n            <dim>256</dim>\n            <dim>200</dim>\n            <dim>200</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>256</dim>\n            <dim>200</dim>\n            <dim>10</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"I64\">\n            <dim>7</dim>\n            <dim>256</dim>\n            <dim>200</dim>\n            <dim>10</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: NV12toBGR Layer Configuration (Dual Input)\nDESCRIPTION: This XML snippet demonstrates an NV12toBGR layer with two input ports: one for the Y plane and another for the UV plane. The Y plane input has dimensions of 1x480x640x1, and the UV plane input has dimensions of 1x240x320x2. The output BGR image has dimensions of 1x480x640x3.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/nv12-to-bgr-8.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"NV12toBGR\">\n    <input>\n        <port id=\"0\">  <!-- Y plane -->\n            <dim>1</dim>\n            <dim>480</dim>\n            <dim>640</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">  <!-- UV plane -->\n            <dim>1</dim>\n            <dim>240</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>480</dim>\n            <dim>640</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Loop Structure Example in XML\nDESCRIPTION: Provides an XML example of a typical Loop structure, showing the arrangement of input, output, port_map, back_edges, and body elements.  Illustrates how external ports are mapped to internal layers, and how back edges connect result layers to parameter layers within the loop body.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/loop-5.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"Loop\" ... >\n    <input> ... </input>\n    <output> ... </output>\n    <port_map>\n        <input external_port_id=\"0\" internal_layer_id=\"0\"/>\n        <input external_port_id=\"1\" internal_layer_id=\"1\"/>\n        <input external_port_id=\"-1\" internal_layer_id=\"2\" purpose=\"current_iteration\"/>\n        ...\n        <output external_port_id=\"3\" internal_layer_id=\"4\"/>\n        <output external_port_id=\"4\" internal_layer_id=\"10\" axis=\"1\"/>\n        <output external_port_id=\"-1\" internal_layer_id=\"22\" purpose=\"execution_condition\"/>\n        ...\n    </port_map>\n    <back_edges>\n        <edge from-layer=\"1\" to-layer=\"5\"/>\n        ...\n    </back_edges>\n    <body>\n        <layers> ... </layers>\n        <edges> ... </edges>\n    </body>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties and clang-format\nDESCRIPTION: This snippet sets the `INTERPROCEDURAL_OPTIMIZATION_RELEASE` property for the target, enabling link-time optimization (LTO). It also adds a clang-format target for the specified target for code formatting purposes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/plugin/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Remote Context Constructor Implementation C++\nDESCRIPTION: Implements the constructor for the TemplateRemoteContext class. It initializes the device name and properties of the remote context. These properties can be used to cast RemoteContext to device specific type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/remote-context.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nTemplateRemoteContext::TemplateRemoteContext(const std::string& device_name, const RemoteContextProperties& properties) : \n        m_name(device_name), m_property(properties) {}\n```\n\n----------------------------------------\n\nTITLE: BitwiseXor Numpy Broadcast Example in XML\nDESCRIPTION: Illustrates the BitwiseXor layer in XML format with numpy broadcasting. Input ports have different dimensions that are compatible for numpy-style broadcasting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-xor-13.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"BitwiseXor\">\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ROIPooling Layer Configuration in XML\nDESCRIPTION: This XML snippet demonstrates how to configure a ROIPooling layer. It shows the usage of attributes like `pooled_h`, `pooled_w`, and `spatial_scale` within the `<data>` tag. It also illustrates the input and output connections of the layer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/roi-pooling-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ROIPooling\" ... >\n           <data pooled_h=\"6\" pooled_w=\"6\" spatial_scale=\"0.062500\"/>\n           <input> ... </input>\n           <output> ... </output>\n       </layer>\n```\n\n----------------------------------------\n\nTITLE: Get/Set Tensor by Index in C++\nDESCRIPTION: This C++ code gets input and output tensors using their index numbers. It uses `get_input_tensor(index)` and `get_output_tensor(index)`. Requires OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\nov::InferRequest infer_request = compiled_model.create_infer_request();\n\nov::Tensor input_tensor = infer_request.get_input_tensor(0);\nstd::vector<float> input_data(input_tensor.get_size());\nstd::generate(input_data.begin(), input_data.end(), []() { return static_cast<float>(rand()) / RAND_MAX; });\nstd::memcpy(input_tensor.data(), input_data.data(), input_data.size() * sizeof(float));\n\ninfer_request.infer();\n\nov::Tensor output_tensor = infer_request.get_output_tensor(0);\nconst float* output_data = output_tensor.data<const float>();\n```\n\n----------------------------------------\n\nTITLE: Install Requirements (Linux/macOS)\nDESCRIPTION: This command installs required Python packages listed in `requirements.txt` within the activated virtual environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_16\n\nLANGUAGE: console\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running OpenVINO Setup Script on Linux\nDESCRIPTION: This command runs the `setupvars.sh` script to set up the OpenVINO environment variables on Linux. It is a prerequisite for using OpenVINO Runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n<INSTALL_DIR>/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: Topology Structure Definition in C++\nDESCRIPTION: This C++ snippet illustrates the structure of a `topology`, which represents a graph of primitives and their connections. It utilizes a map to store primitives, indexed by their unique `primitive_id`. The connections are defined through input primitives associated with each primitive.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/basic_data_structures.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nstruct topology{\n...\n    std::map<primitive_id, std::shared_ptr<primitive>> _primitives;\n...;\n}\n```\n\n----------------------------------------\n\nTITLE: List Homebrew Packages (Shell)\nDESCRIPTION: Lists all installed Homebrew packages to verify that OpenVINO Runtime has been successfully installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-brew.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nbrew list\n```\n\n----------------------------------------\n\nTITLE: Disabling all transformations\nDESCRIPTION: This example demonstrates how to disable all graph transformations using the 'transformations' option in the OV_CPU_DISABLE variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/feature_disabling.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_DISABLE=\"transformations\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Run PyTorch Layer Test (sh)\nDESCRIPTION: This example demonstrates how to run a PyTorch layer test for the `torch.linalg.cross` operation on CPU and GPU with `FP16` and `FP32` inference precisions.  It sets both `TEST_DEVICE` and `TEST_PRECISION` environment variables before running the test.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/layer_tests/README.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncd tests/layer_tests\nexport TEST_DEVICE=\"CPU;GPU\"\nexport TEST_PRECISION=\"FP32;FP16\"\npytest pytorch_tests/test_cross.py\n```\n\n----------------------------------------\n\nTITLE: Data allocation with specific layout\nDESCRIPTION: This code shows how to allocate input data with a specific layout (bfyx) using `engine.allocate_memory`. Note that the layout order in `engine.allocation_memory` is `bfxy`. The code shows how to correctly specify the dimensions for a `bfyx` layout.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_unit_test.md#_snippet_5\n\nLANGUAGE: c++\nCODE:\n```\nauto input = engine.allocate_memory({ data_types::f32, format::bfyx, { 1, 1, 5, 4 } });\n```\n\n----------------------------------------\n\nTITLE: Defining KleidiAI Usage\nDESCRIPTION: This snippet defines `-DOV_CPU_WITH_KLEIDIAI` and sets `OV_CPU_WITH_KLEIDIAI` to ON if `ENABLE_KLEIDIAI_FOR_CPU` is enabled, indicating support for KleidiAI.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_KLEIDIAI_FOR_CPU)\n    add_definitions(-DOV_CPU_WITH_KLEIDIAI)\n    set(OV_CPU_WITH_KLEIDIAI ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Unit Test Target with Dependencies in CMake\nDESCRIPTION: This snippet uses the ov_add_test_target macro to create the test target with dependencies on various libraries, including common_test_utils, openvino::reference, openvino::util, openvino::shape_inference, Threads::Threads, and openvino::runtime::dev. It defines excluded source paths, link libraries and labels for the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n    NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        EXCLUDED_SOURCE_PATHS\n            ${EXCLUDE_TESTS}\n        DEPENDENCIES\n            ${UNIT_TESTS_DEPENDENCIES}\n            # process models\n            test_model_zoo\n        LINK_LIBRARIES\n            common_test_utils\n            openvino::reference\n            openvino::util\n            openvino::shape_inference\n            ${CMAKE_DL_LIBS}\n            Threads::Threads\n            openvino::conditional_compilation\n            openvino::runtime::dev\n        ADD_CLANG_FORMAT\n        LABELS\n            OV UNIT CORE\n)\n```\n\n----------------------------------------\n\nTITLE: Validate and Infer Types in Python\nDESCRIPTION: This snippet demonstrates how to override the `validate_and_infer_types` method in Python for a custom OpenVINO operation.  This method is responsible for validating the input shapes and element types, and then setting the output shape and element type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-openvino-operations.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    def validate_and_infer_types(self):\n        \"\"\"Shape inference.\n\n        The shapes and element types of the output tensors are calculated based\n        on the shapes and element types of the input tensors.  The attributes\n        of the operation may also affect the output shapes, so they should be\n        validated first.\n        \"\"\"\n        # This example does not depend on attributes, so they are not validated here.\n        # But typically you should call self.get_attribute('attr_name') to get\n        # the value of the attribute, and then validate the attribute value.\n\n        # Input tensors can be accessed via self.get_input_element_type(i)\n        # and self.get_input_partial_shape(i) methods.\n\n        # In this example, the element type and shape of the output tensor\n        # is the same as the element type and shape of the input tensor.\n        output_shape = self.get_input_partial_shape(0)\n        output_type = self.get_input_element_type(0)\n        self.set_output_type(0, output_type, output_shape)\n```\n\n----------------------------------------\n\nTITLE: Adding gflags Subdirectory\nDESCRIPTION: This snippet adds the gflags subdirectory to the project if gflags is required. It checks for different locations of the gflags library and adds it accordingly. If gflags is not found, it reports a fatal error.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(TARGET gflags)\n    set(GFLAGS_TARGET gflags)\nelif(gflags_required)\n    set(GFLAGS_TARGET gflags_nothreads_static)\n    if(EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/thirdparty/gflags\")\n        add_subdirectory(thirdparty/gflags EXCLUDE_FROM_ALL)\n    elseif(EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/../../thirdparty/gflags\")\n        # Allow running samples CMakeLists.txt as stand alone from openvino sources\n        add_subdirectory(\"${CMAKE_CURRENT_SOURCE_DIR}/../../thirdparty/gflags\"\n                            \"${CMAKE_CURRENT_BINARY_DIR}/thirdparty/gflags\" EXCLUDE_FROM_ALL)\n    else()\n        message(FATAL_ERROR \"Failed to find 'gflags' library using '${gflags_component}' component\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Finding VA Libraries\nDESCRIPTION: Searches for VA-API libraries using `pkg_search_module`. If found, it adds the `ENABLE_LIBVA` definition and links against the VA-API library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/functional/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n# try to find VA libraries\nfind_package(PkgConfig QUIET)\nif(PkgConfig_FOUND)\n    pkg_search_module(libva QUIET IMPORTED_TARGET libva)\nendif()\n\nif(libva_FOUND)\n    target_compile_definitions(${TARGET_NAME} PRIVATE ENABLE_LIBVA)\n    target_link_libraries(${TARGET_NAME} PRIVATE PkgConfig::libva)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Specify Blob Dump Directory\nDESCRIPTION: Sets the environment variable to specify the directory where the dumped blobs will be stored for OpenVINO CPU executions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_DIR=<directory-name> binary ...\n```\n\n----------------------------------------\n\nTITLE: Creating OVSA directory on Guest VM\nDESCRIPTION: Creates a directory named OVSA in the user's home directory on the Guest VM. The -p flag ensures that parent directories are created if they don't already exist.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_31\n\nLANGUAGE: sh\nCODE:\n```\nmkdir -p ~/OVSA\n```\n\n----------------------------------------\n\nTITLE: CMake command to enable GPU unit tests\nDESCRIPTION: This command enables the unit tests during the CMake configuration process by setting the ENABLE_TESTS option to ON. This is a prerequisite for building the GPU unit tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_unit_test.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncmake -DCMAKE_BUILD_TYPE=Release -DENABLE_TESTS=ON ..\n```\n\n----------------------------------------\n\nTITLE: BitwiseLeftShift Layer Definition (Numpy Broadcast) XML\nDESCRIPTION: This XML snippet defines a BitwiseLeftShift layer using numpy broadcasting rules. The input ports have different dimensions, which are broadcasted to produce the output shape. The 'auto_broadcast' attribute implicitly uses the default value 'numpy'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-left-shift-15.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"BitwiseLeftShift\">\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Uninstall Specific OpenVINO Version (Example)\nDESCRIPTION: This is an example of uninstalling a specific version of OpenVINO (2025.1.0) using zypper. It removes the package matching the specified version number.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-zypper.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nsudo zypper remove *openvino-2025.1.0*\n```\n\n----------------------------------------\n\nTITLE: ReduceL2 with keep_dims=true in OpenVINO XML\nDESCRIPTION: This example demonstrates the ReduceL2 operation with the `keep_dims` attribute set to `true`. The operation reduces the input tensor along axes 2 and 3, maintaining the rank of the tensor by setting the reduced dimensions to 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-l2-4.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceL2\" ...>\n    <data keep_dims=\"true\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ReduceLogicalAnd XML Example with axis [1]\nDESCRIPTION: Demonstrates the ReduceLogicalAnd operation in XML, reducing along axis 1 (the second dimension). The 'keep_dims' attribute is 'false', causing the dimension to be removed from the output.  The reduction happens independently across the channel and spatial dimensions\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-logical-and-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceLogicalAnd\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [1] that means independent reduction in each channel and spatial dimensions -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Get Partial Shape of Port in OpenVINO (C)\nDESCRIPTION: This function retrieves the partial shape of a port. It requires a pointer to `ov_output_const_port_t` and returns the partial shape in the `ov_partial_shape_t* partial_shape` parameter. The function returns a status code indicating success or failure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_38\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_port_get_partial_shape(const ov_output_const_port_t* port, ov_partial_shape_t* partial_shape)\n```\n\n----------------------------------------\n\nTITLE: Configuring and building OpenVINO with CMake (Parallel Jobs)\nDESCRIPTION: These commands use CMake to configure and build the OpenVINO project. The first command generates Unix makefiles, and the second command initiates the build process using the generated makefiles. The `--parallel 8` flag specifies the number of parallel jobs to use during the build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_linux.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ncmake --build . --parallel 8\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library in CMake\nDESCRIPTION: This snippet installs the static library `TARGET_NAME` as part of the `OV_CPACK_COMP_CORE` component during the installation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_20\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: Defining the Test Target with CMake\nDESCRIPTION: This CMake snippet defines a test target named `ov_hetero_func_tests`. It specifies the root directory, dependencies (including mock_engine and openvino_hetero_plugin), link libraries (such as funcSharedTests, openvino::runtime::dev, gtest, common_test_utils), and include directories. It also adds clang-format checks and labels for the test.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/tests/functional/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME ov_hetero_func_tests)\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        DEPENDENCIES\n            mock_engine\n            openvino_hetero_plugin\n        LINK_LIBRARIES\n            openvino::funcSharedTests\n            openvino::runtime::dev\n            gtest\n            gtest_main\n            common_test_utils\n        INCLUDES\n            PUBLIC\n                $<TARGET_PROPERTY:openvino_hetero_plugin,SOURCE_DIR>/src\n        ADD_CLANG_FORMAT\n        LABELS\n            OV HETERO\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling Xbyak for RISCV64\nDESCRIPTION: Conditionally enables Xbyak code generation for RISC-V Vector Extension (RVV) instructions on RISCV64 architectures. It sets compile definitions, links the `xbyak_riscv` library, and adds its include directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_36\n\nLANGUAGE: cmake\nCODE:\n```\nif(RISCV64)\n    # Set `XBYAK_RISCV_V=1` to compile Xbyak-code for RVV-related instructions\n    target_compile_definitions(xbyak_riscv INTERFACE XBYAK_RISCV_V=1)\n    target_link_libraries(${TARGET_NAME} PRIVATE xbyak_riscv)\n    target_include_directories(${TARGET_NAME} SYSTEM INTERFACE $<TARGET_PROPERTY:xbyak_riscv::xbyak_riscv,INTERFACE_INCLUDE_DIRECTORIES>)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding C Samples Component in CMake\nDESCRIPTION: Adds a component for C samples to the OpenVINO packaging system. This component is marked as hidden and depends on the core C development component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nov_cpack_add_component(${OV_CPACK_COMP_C_SAMPLES}\n                       HIDDEN\n                       DEPENDS ${OV_CPACK_COMP_CORE_C_DEV})\n```\n\n----------------------------------------\n\nTITLE: Add SYCL Support to Target\nDESCRIPTION: This snippet calls the `add_sycl_to_target` CMake function to add SYCL support to the specified target. This function likely handles the inclusion of SYCL-related libraries, compiler flags, and configurations required for building SYCL kernels as part of the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/sycl/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_sycl_to_target(TARGET ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Loading Extensions from Library in Python\nDESCRIPTION: This snippet shows how to load extensions from a library into OpenVINO Runtime using Python.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncore = ov.Core()\n# [add_extension_lib]\ncore.add_extension(\"path_to_the_extension/libtemplate_extension.so\")\n# [add_extension_lib]\n```\n\n----------------------------------------\n\nTITLE: Create a Python virtual environment\nDESCRIPTION: These commands create a Python virtual environment. First, it installs the virtualenv package using pip. Then, it creates a new virtual environment in the specified directory, using the system's Python 3 interpreter.  Using virtual environments helps to manage dependencies for different projects independently.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/README.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npython3 -m pip install virtualenv\npython3 -m virtualenv -p `which python3` <directory_for_environment>\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Environment Variables (Linux/macOS)\nDESCRIPTION: This command sources the `setupvars.sh` script, which sets the necessary environment variables for using OpenVINO on Linux and macOS. `<INSTALLDIR>` is the OpenVINO installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nsource <INSTALLDIR>/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: Preprocessing dumped OpenCL source code\nDESCRIPTION: This command-line snippet demonstrates using the C preprocessor to clean up the dumped OpenCL source code by applying macros. This makes the code more readable and easier to debug.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_debug_utils.md#_snippet_6\n\nLANGUAGE: cl\nCODE:\n```\n$ cpp dumped_source.cl > clean_source.cl\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Tokenizers with Transformers Support\nDESCRIPTION: Installs the openvino-tokenizers package with the 'transformers' extra. This includes dependencies required for converting and using Hugging Face tokenizers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npip install openvino-tokenizers[transformers]\n```\n\n----------------------------------------\n\nTITLE: Defining access control for the model\nDESCRIPTION: Defines access control for the model and generates a master license using `ovsatool`.  It encrypts the IR files and generates a master license for use with the ISV's keystore.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_41\n\nLANGUAGE: sh\nCODE:\n```\nuuid=$(uuidgen)\n/opt/ovsa/bin/ovsatool controlAccess -i model/<name-of-the-model>.xml model/<name-of-the-model>.bin -n \"name of the model\" -d \"detailed name of the model\" -p <name-of-the-model>.dat -m <name-of-the-model>.masterlic -k isv_keystore -g $uuid\n```\n\n----------------------------------------\n\nTITLE: DeformablePSROIPooling Layer Configuration (Three Inputs)\nDESCRIPTION: This XML snippet shows a configuration for the DeformablePSROIPooling layer with three inputs: position score maps, regions of interest, and transformation values (offsets). It defines the structure of the layer, including the attributes and input/output port specifications. The third input represents the normalized offsets for ROI bin coordinates.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/deformable-psroi-pooling-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"DeformablePSROIPooling\" ... >\n    <data group_size=\"7\" mode=\"bilinear_deformable\" output_dim=\"8\" part_size=\"7\" spatial_bins_x=\"4\" spatial_bins_y=\"4\" spatial_scale=\"0.0625\" trans_std=\"0.1\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>392</dim>\n            <dim>38</dim>\n            <dim>63</dim>\n        </port>\n        <port id=\"1\">\n            <dim>300</dim>\n            <dim>5</dim>\n        </port>\n        <port id=\"2\">\n            <dim>300</dim>\n            <dim>2</dim>\n            <dim>7</dim>\n            <dim>7</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP32\">\n            <dim>300</dim>\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>7</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ONNX Frontend Class Diagram (Mermaid)\nDESCRIPTION: This Mermaid diagram illustrates the architecture and relationships between key classes in the OpenVINO ONNX Frontend, including the `FrontEnd`, `InputModel`, and `Extension` components. It shows the flow of data from an ONNX model through parsing and conversion to an OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/architecture.md#_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TB\n    fw_model[(ONNX)]\n    style fw_model fill:#427cb0\n\n    subgraph frontend [ov::frontend::onnx::FrontEnd]\n        direction TB\n        load_impl[\"load_impl\"]\n        convert[\"convert\"]\n        decode[\"decode\"]\n        add_extension[\"add_extension\"]\n        fe_name[\"get_name\"]\n    end\n\n    subgraph input_model [ov::frontend::onnx::InputModel]\n        get_place[\"get_place_by_tensor_name\"]\n        set_tensor_name[\"set_name_for_tensor\"]\n        others[\"other editing capabilities...\"]\n        set_pt_shape[\"set_partial_shape\"]\n        add_output[\"add_output\"]\n        extract_sub[\"extract_subgraph\"]\n    end\n\n    subgraph extension [ov::Extension]\n        so[\"SOExtension as path to *.so/*.dll\"]\n        ov::frontend::onnx::ConversionExtension\n        ov::frontend::onnx::OpExtension\n        others2[\"others...\"]\n    end\n\n    proto([onnx/protobuf libs])\n    ov_model[ov::Model]\n    ov_model_partial[ov::Model represented via ONNXFrameworkNodes]\n    onnx_name[onnx]\n    true_false[true/false]\n\n    fw_model--as stream/as path-->load_impl\n    load_impl--ParseFromIstream-->proto\n    proto--ModelProto-->load_impl\n\n    load_impl-->input_model\n    input_model-->convert\n    input_model-->decode\n    convert-->ov_model\n    decode-->ov_model_partial\n    extension-->add_extension\n    fe_name-->onnx_name\n```\n\n----------------------------------------\n\nTITLE: AVX2 Optimization Flags (CMake)\nDESCRIPTION: Conditionally sets AVX2 optimization flags and compiler options if AVX2 is enabled. It defines a source file specifically for AVX2 intrinsics and sets properties to enable compilation with AVX2 flags and skip precompiled headers. Also adds compile definitions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_AVX2)\n    ov_avx2_optimization_flags(avx2_flags)\n\n    set(OV_REFERENCE_X86_AVX2_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/op/convert_x86_intrinsics.cpp)\n    set_source_files_properties(${OV_REFERENCE_X86_AVX2_SRC} PROPERTIES COMPILE_OPTIONS \"${avx2_flags}\"\n                                                                        SKIP_UNITY_BUILD_INCLUSION ON\n                                                                        SKIP_PRECOMPILE_HEADERS ON)\n    target_compile_definitions(${TARGET_NAME} PRIVATE HAVE_AVX2)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name and Dependencies CMake\nDESCRIPTION: Sets the target name for the TensorFlow frontend tests and defines dependencies.  It checks for Intel CPU support and removes the corresponding plugin dependency if not enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"ov_tensorflow_frontend_tests\")\n\nlist(APPEND TF_TESTS_DEPENDENCIES tensorflow_test_models tensorflow_fe_standalone_build_test openvino_intel_cpu_plugin)\nif (NOT ENABLE_INTEL_CPU)\n    list(REMOVE_ITEM TF_TESTS_DEPENDENCIES openvino_intel_cpu_plugin)\n    set(EXCLUDED_TESTS ${CMAKE_CURRENT_SOURCE_DIR}/compilation.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: 2D GroupConvolutionBackpropData Example (OpenVINO XML)\nDESCRIPTION: Example of a 2D GroupConvolutionBackpropData layer configuration in OpenVINO XML format. This configuration specifies the layer's ID, name, type, attributes (dilations, pads_begin, pads_end, strides), input port dimensions, and output port dimensions with precision.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/group-convolution-backprop-data-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"5\" name=\"upsampling_node\" type=\"GroupConvolutionBackpropData\">\n    <data dilations=\"1,1\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>20</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>\n            <dim>5</dim>\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>8</dim>\n            <dim>447</dim>\n            <dim>447</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Set Include Directories\nDESCRIPTION: Sets the include directories for the target. It adds the current source directory's include directory to the target's include path, allowing the target to find header files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PRIVATE\n        ${CMAKE_CURRENT_SOURCE_DIR}/include/)\n```\n\n----------------------------------------\n\nTITLE: ReduceProd Layer with keep_dims=false (XML) - Example 2\nDESCRIPTION: This XML defines a ReduceProd layer where 'keep_dims' is 'false', reducing along axis 1. The input is 6x12x10x24, resulting in an output of 6x10x24 after removing the reduced dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-prod-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceProd\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [1] that means independent reduction in each channel and spatial dimensions -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: EmbeddingBagPacked with Sum Reduction, No Weights - XML\nDESCRIPTION: Demonstrates the EmbeddingBagPacked operation with 'sum' reduction and without per_sample_weights. The input embedding table, indices, and the resulting output are defined within the layer's input and output ports.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sparse/embedding-bag-packed-15.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"EmbeddingBagPacked\" ... >\n    <data reduction=\"sum\"/>\n    <input>\n        <port id=\"0\">     <!-- emb_table value is: [[-0.2, -0.6], [-0.1, -0.4], [-1.9, -1.8], [-1.,  1.5], [ 0.8, -0.7]] -->\n            <dim>5</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">     <!-- indices value is: [[0, 2], [1, 2], [3, 4]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">     <!-- output value is: [[-2.1, -2.4], [-2., -2.2], [-0.2, 0.8]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ReduceMax Operation Calculation in C++\nDESCRIPTION: This C++ code block illustrates how the ReduceMax operation calculates the maximum value along specified dimensions of an input tensor. The outer loop iterates through all valid indices of the output tensor, and the inner loop finds the maximum value along the specified dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-max-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i0, i1, ..., iN] = max[j0, ..., jN](x[j0, ..., jN]))\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Repository Structure\nDESCRIPTION: This code snippet shows the directory structure of the OpenVINO repository. It highlights key folders like `ci`, `github`, `cmake`, `docs`, `licensing`, `samples`, `scripts`, `src`, `tests`, `thirdparty`, and `tools`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/index.md#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<pre>\n <code>\n <a href=\"../../README.md\">openvino/</a>                  // OpenVINO Repository\n    .ci/                    // CI settings for Azure\n    .github/                // Github actions and PR templates\n    cmake/                  // Global CMake scripts\n    docs/                   // OpenVINO documentation\n    licensing/              // Licenses\n    samples/                // OpenVINO samples\n    scripts/                // Helper scripts\n    <a href=\"../../src/README.md\">src/</a>                    // Folder with core OpenVINO components\n    tests/                  // Infrastructure tests which validate full pipelines\n    thirdparty/             // Common third-party dependencies\n    tools/                  // OpenVINO tools\n </code>\n</pre>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties\nDESCRIPTION: This snippet sets the properties of the target `${PROJECT_NAME}` to define the prefix as empty and the suffix as `.node`. This is important to correctly create the Node.js addon file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${PROJECT_NAME} PROPERTIES\n    PREFIX \"\"\n    SUFFIX \".node\"\n)\n```\n\n----------------------------------------\n\nTITLE: Source Grouping CMake\nDESCRIPTION: This snippet organizes the source and header files into named groups within the Visual Studio project. It enhances project structure and improves code organization within the IDE.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Add Standalone Build Subdirectory\nDESCRIPTION: This includes the standalone_build subdirectory and creates a dependency for the ONNX frontend tests on onnx_fe_standalone_build_test.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(standalone_build)\nadd_dependencies(ov_onnx_frontend_tests onnx_fe_standalone_build_test)\n```\n\n----------------------------------------\n\nTITLE: Installing C++ Samples Directory (UNIX) in CMake\nDESCRIPTION: Installs the C++ samples directory to the specified destination under UNIX systems. It excludes certain file patterns (e.g., *.bat, *.ps1, *.sh, .clang-format) and includes the build_samples.sh script for building the samples.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(UNIX)\n    install(DIRECTORY cpp/\n            DESTINATION ${OV_CPACK_SAMPLESDIR}/cpp\n            COMPONENT ${OV_CPACK_COMP_CPP_SAMPLES}\n            ${OV_CPACK_COMP_CPP_SAMPLES_EXCLUDE_ALL}\n            PATTERN *.bat EXCLUDE\n            PATTERN *.ps1 EXCLUDE\n            PATTERN *.sh EXCLUDE\n            PATTERN .clang-format EXCLUDE)\n\n    install(PROGRAMS cpp/build_samples.sh\n            DESTINATION ${OV_CPACK_SAMPLESDIR}/cpp\n            COMPONENT ${OV_CPACK_COMP_CPP_SAMPLES}\n            ${OV_CPACK_COMP_CPP_SAMPLES_EXCLUDE_ALL})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories CMake\nDESCRIPTION: This snippet specifies the include directories for the C API library. The `target_include_directories` command sets the public include directory for the target, which is `$<$<BUILD_INTERFACE:${OpenVINO_C_API_SOURCE_DIR}/include>>`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC\n    $<BUILD_INTERFACE:${OpenVINO_C_API_SOURCE_DIR}/include>)\n```\n\n----------------------------------------\n\nTITLE: Clone and Initialize OpenVINO Repository\nDESCRIPTION: This snippet clones the OpenVINO repository from GitHub and initializes its submodules recursively. It's the first step in the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_webassembly.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/openvinotoolkit/openvino.git\ncd openvino\ngit submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Enable Performance Counters\nDESCRIPTION: This snippet enables performance counters within the OpenVINO project. Setting PERF_COUNT to YES activates the collection and reporting of performance metrics during inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/op_conformance_runner/config/config_example.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# PERFOMANCE_COUNTERS\nPERF_COUNT YES\n```\n\n----------------------------------------\n\nTITLE: Running benchmark_app with CPU Fallback Disabled\nDESCRIPTION: This snippet demonstrates running `benchmark_app` with CPU fallback disabled by loading the configuration from `config.json` using `-load_config ./config.json`. The configuration file contains the `ENABLE_STARTUP_FALLBACK` set to `NO`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/tests.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nopenvino/bin/intel64/Release$ ./benchark_app -m openvino/src/core/tests/models/ir/add_abc.xml -d AUTO -load_config ./config.json\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Component Responsibilities Diagram\nDESCRIPTION: This diagram illustrates the responsibilities of different OpenVINO components, including tools (OpenVINO Model Converter), tutorials (Samples, Demos, Notebooks), API (C++, C, Python), OV Plugins (AUTO, CPU, GPU), and OV Frontends (IR, ONNX, Paddle). It showcases how frontends read models, the OpenVINO library exposes the API, and plugins handle inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/docs/architecture.md#_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TB\n    subgraph tools [Tools]\n        ovc{{OpenVINO model converter}}\n    \n        style ovc fill:#6c9f7f\n    end\n    subgraph tutorials [Tutorials]\n        samples[Samples]\n        demos[Demos]\n        notebooks[Notebooks]\n    end\n    subgraph api [API]\n        cpp[C++]\n        subgraph bindings [Bindings]\n            c_api[C]\n            python_api[Python]\n        end\n    end\n    \n    subgraph plugins [OV Plugins]\n        auto([\"AUTO\"])\n        cpu([\"Intel_CPU\"])\n        gpu([\"Intel_GPU\"])\n    end\n    subgraph frontends [OV Frontends]\n        direction TB\n        ir_fe[\"IR Frontend\"]\n        onnx_fe[\"ONNX Frontend\"]\n        paddle_fe[\"Paddle Frontend\"]\n    end\n    openvino(openvino library)\n    \n    frontends--Read model--->openvino\n    openvino--API--->api\n    openvino--infer--->plugins\n```\n\n----------------------------------------\n\nTITLE: Jinja2 Template for Module Documentation\nDESCRIPTION: This Jinja2 template generates reStructuredText markup for documenting a Python module. It iterates through module attributes, functions, classes, exceptions, and submodules, generating autosummary directives for each. The template uses conditional blocks to include sections only if they exist.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/openvino_sphinx_theme/openvino_sphinx_theme/templates/custom-module-template.rst#_snippet_0\n\nLANGUAGE: Jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. automodule:: {{ fullname }}\n\n{% block attributes %}\n{% if attributes %}\n   .. rubric:: Module Attributes\n\n   .. autosummary::\n      :toctree:\n   {% for attr in attributes %}\n      {{ attr }}\n   {% endfor %}\n{% endif %}\n{% endblock %}\n\n{% block functions %}\n{% if functions %}\n   .. rubric:: Functions\n\n   .. autosummary::\n      :toctree:\n   {% for func in functions %}\n      {% if not func.startswith('_') %}\n         {{ func }}\n      {% endif %}\n   {%- endfor %}\n{% endif %}\n{% endblock %}\n\n{% block classes %}\n{% if classes %}\n   .. rubric:: Classes\n\n   .. autosummary::\n      :toctree:\n      :template: custom-class-template.rst\n   {% for cl in classes %}\n      {{ cl }}\n   {%- endfor %}\n{% endif %}\n{% endblock %}\n\n{% block exceptions %}\n{% if exceptions %}\n   .. rubric:: Exceptions\n\n   .. autosummary::\n      :toctree:\n   {% for item in exceptions %}\n      {{ item }}\n   {%- endfor %}\n{% endif %}\n{% endblock %}\n\n{% block modules %}\n{% if modules %}\n.. rubric:: Modules\n\n.. autosummary::\n   :toctree:\n   :template: custom-module-template.rst\n   :recursive:\n{% for mod in modules %}\n   {{ mod }}\n{%- endfor %}\n{% endif %}\n{% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Adding executable target with OpenVINO and OpenCV\nDESCRIPTION: Defines the executable target 'single-image-test' using the `ov_add_target` macro. It specifies the source directory and links necessary libraries, including OpenVINO runtime, OpenCV components, npu_tools_utils, and gflags.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_target(ADD_CPPLINT\n              TYPE EXECUTABLE\n              NAME ${TARGET_NAME}\n              ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n              LINK_LIBRARIES\n                  PRIVATE\n                      openvino::runtime\n                      opencv_core\n                      opencv_imgproc\n                      opencv_imgcodecs\n                      npu_tools_utils\n                      gflags)\n```\n\n----------------------------------------\n\nTITLE: Asin Layer XML Definition in OpenVINO\nDESCRIPTION: This XML snippet defines an Asin layer in OpenVINO. It specifies the input and output ports, along with their dimensions. The layer performs an element-wise arcsin operation on the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/asin-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Asin\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Arg Max/Min Primitive Creation Example in C++\nDESCRIPTION: This C++ code shows an example of creating an `arg_max_min` primitive using the cldnn namespace. It specifies the primitive ID, input, mode (max), top_k value, dimension, sort mode, and other configurations for the arg max/min operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/basic_data_structures.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ncldnn::arg_max_min top_k_prim = cldnn::arg_max_min(\"top_k\", { \"input\" }, arg_max_min::max, top_k, arg_max_min::y, arg_max_min::sort_by_values, false, \"\", padding(), data_types::f32);\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO with Ninja Build System\nDESCRIPTION: This snippet demonstrates how to build OpenVINO using the Ninja build system. It initializes the Visual Studio environment, configures CMake to use Ninja, and then builds the project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_windows.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ncall \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Auxiliary\\Build\\vcvars64.bat\"\ncmake -G Ninja -Wno-dev -DCMAKE_BUILD_TYPE=Release ..\ncmake --build . --parallel\n```\n\n----------------------------------------\n\nTITLE: Create Executable and Test Target\nDESCRIPTION: This snippet creates the executable target `ov_onnx_frontend_tests` using the compiled source files (`SRC`). A test target with the same name is then added and configured to exclude GPU tests. A property is set to label the test.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(ov_onnx_frontend_tests ${SRC})\nadd_test(NAME ov_onnx_frontend_tests COMMAND ov_onnx_frontend_tests --gtest_filter=-*IE_GPU*)\nset_property(TEST ov_onnx_frontend_tests PROPERTY LABELS OV UNIT ONNX_FE)\n```\n\n----------------------------------------\n\nTITLE: Marking Target as C++ in CMake\nDESCRIPTION: This snippet marks the object library `${TARGET_NAME}_obj` as a C++ target. This ensures that the appropriate compiler and linker settings are used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nov_mark_target_as_cc(${TARGET_NAME}_obj)\n```\n\n----------------------------------------\n\nTITLE: Creating Build Directory for OpenVINO Samples (macOS)\nDESCRIPTION: This command creates a new directory named `build` which will be used for building the OpenVINO samples. It assumes the user has write access to the current directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\nmkdir build\n```\n\n----------------------------------------\n\nTITLE: Build OpenVINO C samples\nDESCRIPTION: Executes the `build_samples.sh` script to build the OpenVINO C sample applications. Requires the OpenVINO Runtime and build tools to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\n/usr/share/openvino/samples/c/build_samples.sh\n```\n\n----------------------------------------\n\nTITLE: Example JavaScript test case\nDESCRIPTION: Illustrates the structure of a JavaScript test case for OpenVINO, including `describe`, `it`, and assertion blocks. It uses `node:test` module.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/test_examples.md#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst { describe, it, beforeEach } = require('node:test');\ndescribe('ov.Tensor tests', () => {\n  test('MyTensor.getElementType()', () => {\n    const tensor = new ov.MyTensor(ov.element.f32, [1, 3]);\n    assert.strictEqual(tensor.getElementType(), ov.element.f32);\n  });\n\n});\n \n```\n\n----------------------------------------\n\nTITLE: Setting Kernel Implementation with build_option in Unit Test (C++)\nDESCRIPTION: This code snippet demonstrates how to specify a particular kernel implementation within a unit test using `build_option::force_implementations`. This allows for targeted testing of specific kernel implementations, especially when a primitive has multiple kernels available.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_ops_enabling.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n...\nbuild_options options;\nimplementation_desc conv_impl = { format::fs_b_yx_fsv32, \"\" };\noptions.set_option(build_option::force_implementations({ {\"conv_fsv\", conv_impl} }));\nnetwork network(engine, topology, options);\n...\n\n```\n\n----------------------------------------\n\nTITLE: Defining Input Tensor Information (C++)\nDESCRIPTION: This snippet demonstrates how to define the format of the user's input data using the `ov::preprocess::PrePostProcessor::input` method in C++. It sets the precision to `U8`, the shape to `{1,480,640,3}`, the layout to `NHWC`, and the color format to `BGR`. This prepares the PrePostProcessor for handling data in this specific format. The code assumes that `ov::Layout` and `ov::ColorFormat` are defined within the `ov` namespace.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nppp.input().tensor()\n    .set_element_type(ov::element::u8)\n    .set_shape({1, 480, 640, 3})\n    .set_layout(\"NHWC\")\n    .set_color_format(ov::ColorFormat::BGR);\n```\n\n----------------------------------------\n\nTITLE: Running benchmark_app with Throughput Hint on AUTO plugin\nDESCRIPTION: This snippet demonstrates how to run the `benchmark_app` with the `-hint throughput` option, targeting the AUTO plugin.  It uses a sample model file (`add_abc.xml`) and specifies the target device as `AUTO`. This configuration aims to maximize throughput.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/tests.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nopenvino/bin/intel64/Release$ ./benchark_app -m openvino/src/core/tests/models/ir/add_abc.xml -d AUTO -hint throughput\n```\n\n----------------------------------------\n\nTITLE: L2 Normalization Formula (C++)\nDESCRIPTION: This C++ code snippet shows the mathematical formula for L2 normalization performed by the NormalizeL2 operation. It calculates the output by dividing the input element by the square root of the sum of squares along specified axes, with an epsilon value to prevent division by zero.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/normalize-l2-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i0, i1, ..., iN] = x[i0, i1, ..., iN] / sqrt(eps_mode(sum[j0,..., jN](x[j0, ..., jN]**2), eps))\n```\n\n----------------------------------------\n\nTITLE: TopK Layer Configuration Example in XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a TopK layer within an OpenVINO model. It showcases the specification of attributes such as axis, mode, and sort, as well as the input and output port dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/top-k-1.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"TopK\" ... >\n      <data axis=\"1\" mode=\"max\" sort=\"value\"/>\n      <input>\n          <port id=\"0\">\n              <dim>6</dim>\n              <dim>12</dim>\n              <dim>10</dim>\n              <dim>24</dim>\n          </port>\n          <port id=\"1\">\n              <!-- k = 3 -->\n          </port>\n      <output>\n          <port id=\"2\">\n              <dim>6</dim>\n              <dim>3</dim>\n              <dim>10</dim>\n              <dim>24</dim>\n          </port>\n      </output>\n  </layer>\n```\n\n----------------------------------------\n\nTITLE: IRDFT Layer Definition (4D Input, Signal Size) XML\nDESCRIPTION: Defines an IRDFT layer in XML for OpenVINO with a 4D input tensor and a signal_size input. The input tensor has dimensions 1x161x161x2, the axes tensor is [1, 2], and the signal_size tensor is [512, 100]. The output tensor will have dimensions 1x512x100.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/irdft-9.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IRDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>161</dim>\n            <dim>161</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- [1, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>2</dim> <!-- [512, 100] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>512</dim>\n            <dim>100</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Copying Generated Files (CMake)\nDESCRIPTION: This `add_custom_command` copies the generated files from the cache directory to the include directory if they have changed. This step helps optimize the build process by avoiding unnecessary recompilations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(OUTPUT \"${CODEGEN_INCDIR}/${PRIM_DB}\"\n  COMMAND \"${CMAKE_COMMAND}\" -E copy_if_different \"${CODEGEN_CACHE_DIR}/${PRIM_DB}\" \"${CODEGEN_INCDIR}/${PRIM_DB}\"\n  COMMAND \"${CMAKE_COMMAND}\" -E copy_if_different \"${CODEGEN_CACHE_DIR}/${PRIM_DB_BATCH_HEADERS}\" \"${CODEGEN_INCDIR}/${PRIM_DB_BATCH_HEADERS}\"\n  DEPENDS \"${CODEGEN_CACHE_DIR}/${PRIM_DB}\" \"${KERNELS}\" \"${CODEGEN_SCRIPT}\"\n  COMMENT \"Updating file if the file changed (${PRIM_DB}) ...\"\n)\n```\n\n----------------------------------------\n\nTITLE: Excluding Source Paths Based on Architecture (ARM)\nDESCRIPTION: This block excludes source paths containing ARM-specific code if the target architecture is not ARM or AARCH64. This prevents compilation errors on other architectures.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT (ARM OR AARCH64))\n    list(APPEND EXCLUDED_SOURCE_PATHS_FOR_UNIT_TEST\n      ${CMAKE_CURRENT_SOURCE_DIR}/transformations/arm\n      ${CMAKE_CURRENT_SOURCE_DIR}/snippets_transformations/arm)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Install RPATH in CMake\nDESCRIPTION: This CMake snippet sets the install RPATH for the pyopenvino module to specify the runtime library search path. It includes the path to OpenVINO's C++ libraries and the TBB (Threading Building Blocks) library, as pyopenvino depends on them through `openvino::offline_transformations` and TBB-optimized reference implementations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_install_rpath(${PROJECT_NAME} ${OV_CPACK_PYTHONDIR}/openvino\n                     # path to OpenVINO C++ libraries\n                     ${OV_CPACK_RUNTIMEDIR}\n                     # pyopenvino also depends on TBB because of:\n                     # pyopenvino => openvino::offline_transformations => TBB optimized openvino::reference\n                     ${TBB_LIB_INSTALL_DIR})\n```\n\n----------------------------------------\n\nTITLE: Run Hello Classification Sample - C\nDESCRIPTION: This command demonstrates how to run the Hello Classification sample in C. It takes the path to the model, image, and device as input. The model needs to be in OpenVINO's IR format.  The compiled C executable `hello_classification_c` must be present in the current directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-classification.rst#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nhello_classification_c alexnet.xml ./opt/intel/openvino/samples/scripts/car.png GPU\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Target\nDESCRIPTION: This snippet creates an executable target named '${TARGET_NAME}' using the source files defined in '${SOURCES_ALL}'. It then adds a compile definition to the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset(SOURCES_ALL\n    ${SOURCES_MAIN}\n    ${SOURCES_NATVIS}\n    ${TEST_COMMON_SOURCE_FILES}\n  )\n\nadd_executable(${TARGET_NAME} ${SOURCES_ALL})\ntarget_compile_definitions(${TARGET_NAME} PRIVATE CI_BUILD_NUMBER=\"\")\n```\n\n----------------------------------------\n\nTITLE: Running OpenVINO Setup Script in Command Prompt on Windows\nDESCRIPTION: These commands navigate to the OpenVINO installation directory and run the `setupvars.bat` script to set up the OpenVINO environment variables in the Command Prompt on Windows. It is a prerequisite for using OpenVINO Runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\ncd  <INSTALL_DIR>\nsetupvars.bat\n```\n\n----------------------------------------\n\nTITLE: Dump All Blobs by Port\nDESCRIPTION: Sets the environment variable to dump all blobs by specifying to dump both input and output blobs using 'ALL' port option during OpenVINO CPU execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_NODE_PORTS=ALL binary ...\n```\n\n----------------------------------------\n\nTITLE: Broadcast with Explicit Mode in XML\nDESCRIPTION: This XML snippet demonstrates the Broadcast operation with the 'explicit' mode. It broadcasts a tensor of shape [16] to the shape [1, 16, 50, 50] using an axes_mapping of [1]. The third input specifies the axes mapping.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/broadcast-3.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Broadcast\" ...>\n    <data mode=\"explicit\"/>\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n       </port>\n        <port id=\"1\">\n            <dim>4</dim>   <!--The tensor contains 4 elements: [1, 16, 50, 50] -->\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>   <!--The tensor contains 1 elements: [1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>16</dim>\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: BitwiseXor uint8 Example in Python\nDESCRIPTION: Illustrates the BitwiseXor operation on uint8 tensors. Input tensors `a` and `b` are uint8 arrays. The example details the conversion to binary, the bitwise XOR operation, and the conversion back to uint8.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-xor-13.rst#_snippet_1\n\nLANGUAGE: py\nCODE:\n```\n# For given uint8 inputs:\na = [21, 120]\nb = [3, 37]\n# Create a binary representation of uint8:\n# binary a: [00010101, 01111000]\n# binary b: [00000011, 00100101]\n# Perform bitwise XOR of corresponding elements in a and b:\n# [00010110, 01011101]\n# Convert binary values to uint8:\noutput = [22, 93]\n```\n\n----------------------------------------\n\nTITLE: 3D Convolution Example in OpenVINO XML\nDESCRIPTION: This XML snippet shows an example configuration for a 3D convolution layer in OpenVINO. It defines the layer type, data attributes (dilations, pads_begin, pads_end, strides, auto_pad), and input/output port dimensions. The 'explicit' auto_pad means that the pads_begin and pads_end values are used for padding.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/convolution-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"Convolution\" ...>\n    <data dilations=\"2,2,2\" pads_begin=\"0,0,0\" pads_end=\"0,0,0\" strides=\"3,3,3\" auto_pad=\"explicit\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>7</dim>\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>320</dim>\n        </port>\n        <port id=\"1\">\n            <dim>32</dim>\n            <dim>7</dim>\n            <dim>3</dim>\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>32</dim>\n            <dim>106</dim>\n            <dim>106</dim>\n            <dim>106</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Install Development Python API Dependencies\nDESCRIPTION: Installs additional Python dependencies required for building OpenVINO Python API from the `wheel/requirements-dev.txt` file in the source tree.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n% python3 -m pip install -r <openvino source tree>/src/bindings/python/wheel/requirements-dev.txt\n```\n\n----------------------------------------\n\nTITLE: Setting Common Link Libraries\nDESCRIPTION: Defines the common link libraries required for building the target. These libraries are linked to the target to resolve dependencies and provide required functionalities.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(COMMON_LINK_LIBRARIES openvino::shape_inference # for tensor accessor\n                          openvino_intel_gpu_kernels # for cl_kernel_data_serializer\n                          openvino_intel_gpu_runtime\n                          openvino::itt\n                          openvino::runtime::dev\n                          openvino::runtime\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Device for XMX Feature (Python)\nDESCRIPTION: This command executes the `hello_query_device.py` sample application to check if the Intel XMX (Xe Matrix Extension) feature is properly recognized.  XMX is a hardware feature to accelerate matrix operations. This script outputs information about the device capabilities, including whether `GPU_HW_MATMUL` is supported.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_driver_troubleshooting.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n$ ./hello_query_device.py\n...\n[ INFO ]                OPTIMIZATION_CAPABILITIES: FP32, BIN, FP16, INT8, GPU_HW_MATMUL, GPU_USM_MEMORY\n```\n\n----------------------------------------\n\nTITLE: StringTensorUnpack with 2D Input in XML\nDESCRIPTION: This XML snippet demonstrates the StringTensorUnpack operation with a 2D input tensor. It defines the input and output ports, showcasing the handling of multi-dimensional string tensors. The 'begins' and 'ends' tensors now have corresponding dimensions, reflecting the structure of the input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/string-tensor-unpack-15.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StringTensorUnpack\" ... >\n    <input>\n        <port id=\"0\" precision=\"STRING\">\n            <dim>2</dim>\n            <dim>2</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"I32\">\n            <dim>2</dim>     <!-- begins = [[0, 5], [13, 16]] -->\n            <dim>2</dim>\n        </port>\n        <port id=\"1\" precision=\"I32\">\n            <dim>2</dim>     <!-- ends = [[5, 13], [16, 21]] -->\n            <dim>2</dim>\n        </port>\n        <port id=\"2\" precision=\"U8\">\n            <dim>21</dim>    <!-- symbols = \"IntelOpenVINOOMZGenAI\" encoded in an utf-8 array -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: OVSA Keystore Generation\nDESCRIPTION: Generates a customer key store file, a certificate signing request (CSR), and a self-signed certificate using the ovsatool. It requires the ECDSA algorithm, organization name, keystore name, CSR name, and subject name as input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_48\n\nLANGUAGE: sh\nCODE:\n```\ncd $OVSA_RUNTIME_ARTEFACTS\n/opt/ovsa/bin/ovsatool keygen -storekey -t ECDSA -n Intel -k custkeystore -r  custkeystore.csr -e \"/C=IN/CN=localhost\"\n```\n\n----------------------------------------\n\nTITLE: List installed OpenVINO packages\nDESCRIPTION: Lists all installed packages that contain the word 'openvino' in their name.  Used to verify that OpenVINO has been successfully installed and to see which components are present.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\napt list --installed | grep openvino\n```\n\n----------------------------------------\n\nTITLE: Detect ABI Tag with setuptools.command.bdist_wheel - Python\nDESCRIPTION: This snippet executes a Python command to detect the ABI tag using the `setuptools.command.bdist_wheel` module. The output, representing the ABI tag, is stored in the `ABI_TAG` variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom setuptools.command.bdist_wheel import get_abi_tag; print(f'{get_abi_tag()}')\n```\n\n----------------------------------------\n\nTITLE: Reshape with dimension bounds - C\nDESCRIPTION: This C code snippet shows how to define dimension bounds using `ov_dimension` for reshaping a model. It gives the method to specify the range of an input dimension\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov_dynamic_shapes:reshape_bounds]\nov_dimension_t dimension = { .min = 1, .max = 10 };\nov_shape_t shape = { .dims = {dimension.min, 3, 224, 224}, .rank = 4 };\nconst char* input_name = ov_model_get_input_name(model, 0, &rc);\nov_reshape_model(model, &input_name, &shape, 1, &rc);\n//! [ov_dynamic_shapes:reshape_bounds]\n```\n\n----------------------------------------\n\nTITLE: PrePostProcessor output method TypeScript\nDESCRIPTION: Defines the `output` method for the PrePostProcessor interface. This method configures the output parameters. It accepts an optional index or tensor name (`idxOrTensorName`) as input, which can be either a string or a number, and returns an OutputInfo object for further configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PrePostProcessor.rst#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\noutput(idxOrTensorName?): OutputInfo\n```\n\nLANGUAGE: typescript\nCODE:\n```\nidxOrTensorName: string|number\n```\n\n----------------------------------------\n\nTITLE: Allocate USM host memory (C++)\nDESCRIPTION: This C++ snippet demonstrates how to allocate Unified Shared Memory (USM) on the host using the OpenVINO GPU plugin.  It allocates memory that is accessible by both the host and the GPU. The snippet requires OpenCL and OpenVINO libraries. The allocated USM memory can then be used as an input or output tensor in an OpenVINO inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\n// example usage\n{\n    size_t buffer_size;\n    auto remote_blob = context.create_tensor(ov::element::f32, ov::Shape{buffer_size}, ov::intel_gpu::memory_type::usm_host);\n    float* mapped_ptr = remote_blob.data<float>();\n    // use mapped_ptr\n}\n```\n\n----------------------------------------\n\nTITLE: GRUCell Layer Example (XML)\nDESCRIPTION: This XML snippet demonstrates how to define a GRUCell layer within an OpenVINO model. It specifies the `hidden_size` and `linear_before_reset` attributes, as well as the input and output port dimensions. The input ports correspond to the input data (X), initial hidden state, weights (W), recurrence weights (R), and biases (B).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/gru-cell-3.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"GRUCell\" ...>\n    <data hidden_size=\"128\" linear_before_reset=\"1\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>16</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n         <port id=\"2\">\n            <dim>384</dim>\n            <dim>16</dim>\n        </port>\n         <port id=\"3\">\n            <dim>384</dim>\n            <dim>128</dim>\n        </port>\n         <port id=\"4\">\n            <dim>768</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"5\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing Python Requirements for OpenVINO Samples (macOS)\nDESCRIPTION: This command is used to install the required Python packages using pip for a specific OpenVINO sample.  It is intended to be executed from the samples directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ncd <INSTALL_DIR>/samples/python/<SAMPLE_DIR>\npython3 -m pip install -r ./requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Run MyTensor Test Python\nDESCRIPTION: Executes the `test_mytensor.py` test file using pytest with increased verbosity.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\npytest tests/test_runtime/test_mytensor.py -v\n```\n\n----------------------------------------\n\nTITLE: CMake Subdirectory Inclusion\nDESCRIPTION: This CMake snippet adds subdirectories to the current project. It includes 'common', 'compile_tool', and 'single-image-test'.  If 'ENABLE_INTEL_NPU_PROTOPIPE' is enabled, it also includes the 'protopipe' subdirectory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(common)\nadd_subdirectory(compile_tool)\nadd_subdirectory(single-image-test)\n\nif (ENABLE_INTEL_NPU_PROTOPIPE)\n    add_subdirectory(protopipe)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Querying Available Devices\nDESCRIPTION: This shell command executes the `hello_query_device` sample to print the available devices, including the GPU devices. This allows users to identify the device names and indices to be used when compiling models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./hello_query_device\nAvailable devices:\n    Device: CPU\n...\n    Device: GPU.0\n...\n    Device: GPU.1\n```\n\n----------------------------------------\n\nTITLE: Divide Layer XML Definition (No Broadcast)\nDESCRIPTION: This XML snippet defines a Divide layer in OpenVINO with no auto-broadcasting. The inputs are two tensors of the same shape (256x56), and the output is a tensor of the same shape.  The `m_pythondiv` attribute is set to \"true\", indicating floor division.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/divide-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Divide\">\n    <data auto_broadcast=\"none\" m_pythondiv=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining External Components in components.yml (YAML)\nDESCRIPTION: This YAML snippet shows an example entry in `components.yml` for an external component, defining empty `revalidate` and `build` lists. This is part of the process of adding Smart CI rules for components outside the main OpenVINO repository.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_16\n\nLANGUAGE: YAML\nCODE:\n```\nNEW_EXTERNAL_COMPONENT:\n  revalidate: []\n  build: []\n```\n\n----------------------------------------\n\nTITLE: Setting LTO Properties in CMake\nDESCRIPTION: This snippet sets the `INTERPROCEDURAL_OPTIMIZATION_RELEASE` property for the object library `${TARGET_NAME}_obj`. This enables Link-Time Optimization (LTO) for release builds, which can improve performance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME}_obj PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Pad Output Example - Edge Mode - C++\nDESCRIPTION: Illustrates the output of the Pad operation in 'edge' mode, where the padded values are copied from the respective edge of the input tensor. It shows how the input tensor is extended based on pads_begin and pads_end attributes, using edge values for padding.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-1.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nINPUT =\n    [[ 1  2  3  4 ]\n    [ 5  6  7  8 ]\n    [ 9 10 11 12 ]]\n\npads_begin = [0, 1]\npads_end = [2, 3]\n\nOUTPUT =\n    [[ 1  1  2  3  4  4  4  4 ]\n    [ 5  5  6  7  8  8  8  8 ]\n    [ 9  9 10 11 12 12 12 12 ]\n    [ 9  9 10 11 12 12 12 12 ]\n    [ 9  9 10 11 12 12 12 12 ]]\n```\n\n----------------------------------------\n\nTITLE: Define TensorConstructor Interface\nDESCRIPTION: Defines the TensorConstructor interface with two constructor overloads. One constructor takes type and shape, while the other takes type, shape, and tensorData. These constructors are used to create Tensor objects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/TensorConstructor.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface TensorConstructor {\n    new TensorConstructor (type, shape): Tensor;\n    new TensorConstructor (type, shape, tensorData): Tensor;\n}\n```\n\n----------------------------------------\n\nTITLE: Faster Build Target CMake\nDESCRIPTION: This snippet optimizes the build process using the 'ov_build_target_faster' CMake function. It enables unity builds and sets a precompiled header (PCH) for private headers (src/precomp.hpp), improving compilation times.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/ov_helpers/ov_lpt_models/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME}\n    UNITY\n    PCH PRIVATE \"src/precomp.hpp\"\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Source Files with GLOB\nDESCRIPTION: This snippet uses `file(GLOB)` to collect source files (`.cpp`) and header files (`.hpp`) from the `src/` and `include/` directories, and `file(GLOB_RECURSE)` to collect source files from `npuw/` directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/plugin/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES \"src/*.cpp\" \"include/*.hpp\")\nfile(GLOB_RECURSE NPUW_SOURCES \"npuw/*.cpp\" \"npuw/*.hpp\")\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties\nDESCRIPTION: This snippet sets the INTERPROCEDURAL_OPTIMIZATION_RELEASE property for the target, enabling link-time optimization (LTO) based on the ENABLE_LTO flag.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/src/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Variable Declarations\nDESCRIPTION: This rule enforces the use of `let` or `const` instead of `var` for variable declarations in JavaScript and TypeScript code. It promotes modern JavaScript practices and is enforced by ESLint with the configuration `no-var: ['error']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nno-var: ['error']\n```\n\n----------------------------------------\n\nTITLE: Get Property C++\nDESCRIPTION: This snippet demonstrates the `get_property` method, used to retrieve the value of a specified property key.  It accesses the internal configuration and returns the property value as an `ov::Any` object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nov::Any Plugin::get_property(const std::string& name, const ov::AnyMap& /*config*/) const {\n    return m_cfg.Get(name);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Build Target Faster Using Precompiled Headers\nDESCRIPTION: This CMake code utilizes the 'ov_build_target_faster' macro to enable precompiled header support for the 'sharedTestClasses' target. It specifies 'src/precomp.hpp' as the precompiled header file. This can significantly speed up compilation times.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/shared_test_classes/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME}\n        PCH PRIVATE \"src/precomp.hpp\"\n        )\n```\n\n----------------------------------------\n\nTITLE: Get OpenVINO Model in C++\nDESCRIPTION: This snippet demonstrates how to obtain an OpenVINO Model in C++. It reads the model from a specified path using `core.read_model()`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"path_to_your_model.xml\");\n```\n\n----------------------------------------\n\nTITLE: ReduceL2 with keep_dims=false in OpenVINO XML\nDESCRIPTION: This example demonstrates the ReduceL2 operation with the `keep_dims` attribute set to `false`. The operation reduces the input tensor along axes 2 and 3, removing the reduced dimensions from the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-l2-4.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceL2\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: OneHot Implementation Logic (Conceptual) C++\nDESCRIPTION: This conceptual C++ code demonstrates the core logic of the OneHot operation.  It iterates through the input indices and sets the corresponding output elements to `on_value` or `off_value` based on whether the index matches the current dimension. The `depth` parameter determines the size of the one-hot dimension, and the `axis` attribute specifies the insertion point for the new dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/one-hot-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[:, ... ,:, i, :, ... ,:] = on_value if (indices[:, ..., :, :, ..., :] == i) else off_value\n```\n\n----------------------------------------\n\nTITLE: Running Python OpenModelZoo Tests (Installation Layout)\nDESCRIPTION: This command runs the Python OpenModelZoo tests using pytest in the installation layout. It specifies the CPU backend, enables parallel execution with 4 workers, excludes CUDA tests, and sets the model zoo directory. Replace <OV_INSTALL_DIR> with the OpenVINO installation directory and <ONNX_MODELS_DIR> with the directory containing the ONNX models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\npytest --backend=CPU <OV_INSTALL_DIR>/tests/pyopenvino/tests/test_onnx/test_zoo_models.py -v -n 4 --forked -k 'not _cuda' --model_zoo_dir=<ONNX_MODELS_DIR>\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO from Source in Snapcraft (CMake)\nDESCRIPTION: This snippet shows how to configure a part in the application's snapcraft.yaml to build and install OpenVINO libraries from source using the CMake plugin. It specifies the source repository, branch, CMake generator, parameters, build environment, and required packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/integrate-openvino-with-ubuntu-snap.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nopenvino-build:\n  plugin: cmake\n  source-type: git\n  source: https://github.com/openvino.git\n  source-branch: master\n  cmake-generator: Ninja\n  cmake-parameters:\n    - -DENABLE_SAMPLES:BOOL=OFF\n    - -DENABLE_TESTS:BOOL=OFF\n  build-environment:\n    - CMAKE_BUILD_PARALLEL_LEVEL: ${SNAPCRAFT_PARALLEL_BUILD_COUNT}\n    - CMAKE_BUILD_TYPE: Release\n  build-packages:\n    - build-essential\n    - ninja-build\n    - pkg-config\n    - gzip\n```\n\n----------------------------------------\n\nTITLE: Setting Threading Interface (CMake)\nDESCRIPTION: Configures the threading interface for the target. This ensures that the unit tests use the correct threading model for the OpenVINO environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/tests/unit/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Running Model Creation Sample (C++)\nDESCRIPTION: This console command demonstrates how to run the `model_creation_sample` executable. It requires the path to the LeNet weights file (`lenet.bin`) and the device name (e.g., `GPU`).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/model-creation.rst#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nmodel_creation_sample <path_to_weights_file> <device_name>\n```\n\n----------------------------------------\n\nTITLE: Installing NPU AL Library and Exporting Targets\nDESCRIPTION: This snippet installs the static library defined by `TARGET_NAME` into the `NPU_PLUGIN_COMPONENT`.  It also exports the target `openvino::npu_al` for use in developer packages, specifying the installation include directories for external users of the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/al/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${NPU_PLUGIN_COMPONENT})\n\nov_developer_package_export_targets(TARGET openvino::npu_al\n                                    INSTALL_INCLUDE_DIRECTORIES\n                                        ${CMAKE_CURRENT_SOURCE_DIR}/include)\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Logic (C++)\nDESCRIPTION: This code demonstrates the logic behind the ScatterElementsUpdate operation. It illustrates how the output tensor is updated based on the indices and updates tensors, considering the specified axis. This example is for a 3D tensor case, where the update corresponding to the `[i][j][k]` entry is performed based on the axis value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-3.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[indices[i][j][k]][j][k] = updates[i][j][k] if axis = 0,\noutput[i][indices[i][j][k]][k] = updates[i][j][k] if axis = 1,\noutput[i][j][indices[i][j][k]] = updates[i][j][k] if axis = 2\n```\n\n----------------------------------------\n\nTITLE: Creating Symbolic Link to OpenVINO Installation Directory\nDESCRIPTION: This command creates a symbolic link named `openvino_2025` in the `/opt/intel` directory, pointing to the OpenVINO installation directory. This simplifies referencing the installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-macos.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nsudo ln -s /opt/intel/openvino_2025.1.0 /opt/intel/openvino_2025\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Package from Specific Source\nDESCRIPTION: This command installs the OpenVINO package from a specific mirror using pip. This is useful for users in China who may experience issues downloading from the default PyPI repository.  The `-i` flag specifies the alternate index URL.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/pypi_publish/pypi-openvino-rt.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npip install openvino -i https://mirrors.aliyun.com/pypi/simple/\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target in CMake\nDESCRIPTION: This command sets up a clang-format target for code style checks. It creates a target to automatically format the C++ and header files using clang-format, enforcing a consistent code style throughout the project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/template_extension/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE template_extension_src \"${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\" \"${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\")\nov_add_clang_format_target(openvino_template_extension_clang FOR_SOURCES ${template_extension_src})\n```\n\n----------------------------------------\n\nTITLE: Verify OpenVINO Installation\nDESCRIPTION: This command verifies that the OpenVINO package is installed correctly by importing the 'Core' class from the 'openvino' module and printing the available devices. A successful installation will display a list of available devices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/pypi_publish/pypi-openvino-rt.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npython -c \"from openvino import Core; print(Core().available_devices)\"\n```\n\n----------------------------------------\n\nTITLE: Add OpenVINO Node Dependency (JSON)\nDESCRIPTION: This JSON snippet demonstrates how to add the `openvino-node` package as a dependency in a project's `package.json` file.  It uses a file path to indicate a local package dependency. This will allow the module to be used within the scope of that project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/README.md#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n\"openvino-node\": \"file:*path-to-current-directory*\"\n```\n\n----------------------------------------\n\nTITLE: Define setup.py Packaging Variables (CMake)\nDESCRIPTION: This macro defines variables used by setup.py for packaging the OpenVINO Python API. It sets the Python version, wheel version, build number, and various directories for libraries and binaries. It also defines the setup_py_env variable, which contains the environment variables required for running setup.py, including version information, build locations, and build type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nmacro(ov_define_setup_py_packaging_vars)\n    # Python3_VERSION_MAJOR and Python3_VERSION_MINOR are defined inside pybind11\n    set(pyversion python${Python3_VERSION_MAJOR}.${Python3_VERSION_MINOR})\n\n    # define version (syncronize with tools/openvino_dev/CMakeLists.txt)\n    if(DEFINED ENV{CI_BUILD_DEV_TAG} AND NOT \"$ENV{CI_BUILD_DEV_TAG}\" STREQUAL \"\")\n        set(WHEEL_VERSION \"${OpenVINO_VERSION}.$ENV{CI_BUILD_DEV_TAG}\" CACHE STRING \"Version of this release\" FORCE)\n        set(wheel_pre_release ON)\n    else()\n        set(WHEEL_VERSION ${OpenVINO_VERSION} CACHE STRING \"Version of this release\" FORCE)\n    endif()\n    set(WHEEL_BUILD \"${OpenVINO_VERSION_BUILD}\" CACHE STRING \"Build number of this release\" FORCE)\n\n    # Common vars used by setup.py\n    set(PY_PACKAGES_DIR ${OV_CPACK_PYTHONDIR})\n    set(TBB_LIBS_DIR runtime/3rdparty/tbb/lib)\n    if(WIN32)\n        set(TBB_LIBS_DIR runtime/3rdparty/tbb/bin)\n    endif()\n    set(PUGIXML_LIBS_DIR runtime/3rdparty/pugixml/lib)\n\n    if(USE_BUILD_TYPE_SUBFOLDER)\n        set(build_type ${CMAKE_BUILD_TYPE})\n    else()\n        set(build_type $<CONFIG>)\n    endif()\n\n    # define setup.py running environment\n    set(setup_py_env ${CMAKE_COMMAND} -E env\n        # for cross-compilation\n        SETUPTOOLS_EXT_SUFFIX=${PYTHON_MODULE_EXTENSION}\n        # versions\n        WHEEL_VERSION=${WHEEL_VERSION}\n        WHEEL_BUILD=${WHEEL_BUILD}\n        # build locations\n        OPENVINO_BINARY_DIR=${OpenVINO_BINARY_DIR}\n        OPENVINO_PYTHON_BINARY_DIR=${OpenVINOPython_BINARY_DIR}\n        # to create proper directories for BA, OVC tools\n        CPACK_GENERATOR=${CPACK_GENERATOR}\n        # propogate build type\n        BUILD_TYPE=${build_type}\n        # variables to reflect cpack locations\n        OV_RUNTIME_LIBS_DIR=${OV_CPACK_RUNTIMEDIR}\n        TBB_LIBS_DIR=${TBB_LIBS_DIR}\n        PUGIXML_LIBS_DIR=${PUGIXML_LIBS_DIR}\n        PY_PACKAGES_DIR=${PY_PACKAGES_DIR})\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Using Optional as the top node in a pattern\nDESCRIPTION: This C++ snippet shows how to use Optional as the top node of a pattern, meaning the entire pattern may or may not exist in the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\nshared_ptr<ov::Node> pattern_optional_top() {\n    // Creating nodes\n    auto relu_node = make_shared<WrapType<opset13::Relu>>();\n    auto optional_node = make_shared<Optional>(relu_node);\n\n    return optional_node;\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Command List Example\nDESCRIPTION: Shows an example of a custom command list for building the target application. Includes git commands for cleaning, resetting, and checking out code, as well as commands for initializing submodules and building with make.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/commit_slider/README.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n\"commandList\" : [\n    {\"cmd\" : \"git rm --cached -r .\", \"path\" : \"{gitPath}\", \"tag\" : \"clean\"},\n    {\"cmd\" : \"git reset --hard\", \"path\" : \"{gitPath}\", \"tag\" : \"clean\"},\n    {\"cmd\" : \"git rm .gitattributes\", \"path\" : \"{gitPath}\", \"tag\" : \"clean\"},\n    {\"cmd\" : \"git reset .\", \"path\" : \"{gitPath}\", \"tag\" : \"clean\"},\n    {\"cmd\" : \"git checkout -- .\", \"path\" : \"{gitPath}\", \"tag\" : \"clean\"},\n    {\"cmd\" : \"git rm --cached -r .\", \"path\" : \"{gitPath}\", \"tag\" : \"clean\"},\n    {\"cmd\" : \"git reset --hard\", \"path\" : \"{gitPath}\", \"tag\" : \"clean\"},\n    {\"cmd\" : \"git rm .gitattributes\", \"path\" : \"{gitPath}\", \"tag\" : \"clean\"},\n    {\"cmd\" : \"git reset .\", \"path\" : \"{gitPath}\", \"tag\" : \"clean\"},\n    {\"cmd\" : \"git checkout -- .\", \"path\" : \"{gitPath}\"},\n    {\"cmd\" : \"git clean -fxd\", \"path\" : \"{gitPath}\", \"tag\" : \"clean\"},\n    {\"cmd\" : \"mkdir -p build\", \"path\" : \"{gitPath}\"},\n    {\"cmd\" : \"git checkout {commit}\", \"catchMsg\" : \"error\", \"path\" : \"{gitPath}\"},\n    {\"cmd\" : \"git submodule init\", \"path\" : \"{gitPath}\"},\n    {\"cmd\" : \"git submodule update --recursive\", \"path\" : \"{buildPath}\"},\n    {\"cmd\" : \"{makeCmd}\", \"catchMsg\" : \"CMake Error\", \"path\" : \"{buildPath}\"},\n    {\"cmd\" : \"make --jobs=4\", \"path\" : \"{buildPath}\"},\n    {\"cmd\" : \"git checkout -- .\", \"path\" : \"{gitPath}\"}\n]\n```\n\n----------------------------------------\n\nTITLE: MatcherPass Transformation Template (C++)\nDESCRIPTION: This code snippet shows the template for a MatcherPass transformation class in C++.  It defines the structure of the class, including the constructor and the `register_matcher` method for registering the pattern and callback.  This template serves as a starting point for implementing custom pattern-based transformations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/matcher-pass.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// [graph_rewrite:template_transformation_hpp]\n#include <openvino/core/rt_info.hpp>\n#include <openvino/opsets/opset10.hpp>\n#include <openvino/pass/matcher_pass.hpp>\n\n#include \"openvino/pass/graph_rewrite.hpp\"\n\nusing namespace std;\nusing namespace ov::opset10;\n\nnamespace {\nclass TemplateTransformation final : public ov::pass::MatcherPass {\npublic:\n    TemplateTransformation() {\n        MATCHER_SCOPE(TemplateTransformation);\n        // Pattern\n        auto input0 = ov::pattern::any_input();\n        auto input1 = ov::pattern::any_input();\n        auto add = ov::pattern::wrap_type<Add>({input0, input1});\n\n        // Matcher\n        ov::matcher_pass_callback callback = [=](ov::pass::Matcher& m) {\n            const auto& pattern_to_output = m.get_pattern_value_map();\n            auto add_node = std::dynamic_pointer_cast<Add>(pattern_to_output.at(add).get_node_shared_ptr());\n\n            if (!add_node) {\n                return false;\n            }\n\n            // Transformation code\n\n            return true;\n        };\n\n        auto m = std::make_shared<ov::pass::Matcher>(add, \"TemplateTransformation\");\n        register_matcher(m, callback);\n    }\n};\n}  // namespace\n// [graph_rewrite:template_transformation_hpp]\n```\n\n----------------------------------------\n\nTITLE: Run Full Test Suite with tox Shell\nDESCRIPTION: Executes the full test suite using the `tox` command.  `tox` is a generic virtualenv management and test command line tool.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ntox\n```\n\n----------------------------------------\n\nTITLE: Creating a Customer License Configuration\nDESCRIPTION: Creates a customer license configuration file.  It sets a time limit for the license, the license server address, and adds the server certificate for validation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_43\n\nLANGUAGE: sh\nCODE:\n```\ncd $OVSA_DEV_ARTEFACTS\n/opt/ovsa/bin/ovsatool licgen -t TimeLimit -l30 -n \"Time Limit License Config\" -v 1.0 -u \"<isv-developer-vm-ip-address>:<license_server-port>\" /opt/ovsa/certs/server.crt  -k isv_keystore -o 30daylicense.config\n```\n\n----------------------------------------\n\nTITLE: ReduceMin Layer Configuration (XML, keep_dims=true)\nDESCRIPTION: This XML snippet shows a configuration for the ReduceMin layer with `keep_dims` set to true. The input tensor has dimensions 6x12x10x24, and reduction is performed along axes 2 and 3, resulting in an output tensor with dimensions 6x12x1x1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-min-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceMin\" ...>\n    <data keep_dims=\"true\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Registering Plugins\nDESCRIPTION: This snippet conditionally registers the plugins using the `ov_register_plugins` function if `ENABLE_TEMPLATE_REGISTRATION` is enabled. This step updates the `plugins.xml` file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/src/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_TEMPLATE_REGISTRATION)\n    # Update the plugins.xml file\n    ov_register_plugins(MAIN_TARGET ${TARGET_NAME})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enabling KleidiAI for CPU\nDESCRIPTION: This snippet determines whether to enable KleidiAI for the CPU plugin based on whether the architecture is AARCH64.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(AARCH64)\n    set(ENABLE_KLEIDIAI_FOR_CPU_DEFAULT ON)\nelse()\n    set(ENABLE_KLEIDIAI_FOR_CPU_DEFAULT OFF)\nendif()\nov_dependent_option(ENABLE_KLEIDIAI_FOR_CPU \"Enable KleidiAI for OpenVINO CPU Plugin\" ${ENABLE_KLEIDIAI_FOR_CPU_DEFAULT} \"AARCH64\" OFF)\n```\n\n----------------------------------------\n\nTITLE: RandomUniform Example (double)\nDESCRIPTION: Example of RandomUniform output with initial_seed = 80, output_type = double, and alignment = PYTORCH. The input shape is [2, 2], minval = 2, and maxval = 10.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\ninput_shape = [ 2, 2 ]\nminval = 2\nmaxval = 10\noutput  = [[8.34928537 6.12348725] \\\n           [3.76852914 2.89564172]]\n```\n\n----------------------------------------\n\nTITLE: Upgrading PIP using Python\nDESCRIPTION: This command upgrades the pip package installer to the latest version.  It uses the `-m` flag to directly execute the pip module within the currently active Python environment. This ensures the upgrade is performed for the correct Python installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/troubleshooting-install-config.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython -m pip install --upgrade pip\n```\n\n----------------------------------------\n\nTITLE: Parsing Input Arguments Shell Script Example\nDESCRIPTION: This snippet shows a stage in the benchmark application process: parsing input arguments. It involves interpreting and validating the parameters provided by the user via the command line to configure the benchmark run.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\n[Step 1/11] Parsing and validating input arguments\n[ INFO ] Parsing input parameters\n```\n\n----------------------------------------\n\nTITLE: Add Subdirectories using CMake\nDESCRIPTION: This snippet adds the specified subdirectories to the current CMake build process.  Each `add_subdirectory` call instructs CMake to process the `CMakeLists.txt` file located within the named subdirectory.  This is commonly used to structure larger projects into manageable modules.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(c)\nadd_subdirectory(js)\nadd_subdirectory(python)\n```\n\n----------------------------------------\n\nTITLE: Marking Target as C/C++ in CMake\nDESCRIPTION: This snippet marks the `openvino_core_obj` target as a C/C++ target. The `ov_mark_target_as_cc` function is a custom function that likely sets the appropriate properties to ensure that the target is treated as a C/C++ library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nov_mark_target_as_cc(openvino_core_obj)\n```\n\n----------------------------------------\n\nTITLE: Correct Padding Function\nDESCRIPTION: The `correct_pad` static method adjusts the padding array to match the rank of the input data. If the padding length is less than the rank, it pads with zeros. If it's greater, it truncates the padding.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\ndef correct_pad(pad, rank):\n    pad_len = len(pad)\n    if pad_len < rank:\n        return np.pad(pad, (0, rank - pad_len), 'constant').astype(np.int64)\n    elif pad_len > rank:\n        return np.array(pad[: rank - 1]).astype(np.int64)\n    else:\n        return np.array(pad, dtype=np.int64)\n```\n\n----------------------------------------\n\nTITLE: Sqrt Layer Definition with Multi-Dimensional Input in OpenVINO (XML)\nDESCRIPTION: This XML snippet demonstrates the Sqrt layer configuration for a multi-dimensional input tensor in OpenVINO. It highlights the input and output port definitions, specifying the dimensions of the tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/sqrt-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Sqrt\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Enabling Snippets LibXSMM TPP\nDESCRIPTION: Conditionally enables the Snippets LibXSMM TPP library for the CPU plugin. It defines compile definitions, links the `xsmm` library and adds its include directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_33\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_SNIPPETS_LIBXSMM_TPP)\n    target_compile_definitions(xsmm PRIVATE __BLAS=0)\n    target_link_libraries(${TARGET_NAME} PRIVATE xsmm)\n    target_include_directories(${TARGET_NAME} SYSTEM PRIVATE $<TARGET_PROPERTY:xsmm,INCLUDE_DIRECTORIES>)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: DepthToSpace with blocks_first Mode in Python\nDESCRIPTION: This Python code snippet demonstrates the transformation of the input tensor when the 'mode' attribute is set to 'blocks_first'. It reshapes and transposes the input data to move depth information into spatial dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/depth-to-space-1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nx' = reshape(data, [N, block_size, block_size, ..., block_size, C / (block_size ^ K), D1, D2, ..., DK])\nx'' = transpose(x', [0,  K + 1,  K + 2, 1, K + 3, 2, K + 4, 3, ..., K + (K + 1), K])\ny = reshape(x'', [N, C / (block_size ^ K), D1 * block_size, D2 * block_size, D3 * block_size, ..., DK * block_size])\n```\n\n----------------------------------------\n\nTITLE: Setting Protobuf MSVC Runtime\nDESCRIPTION: This snippet configures the MSVC runtime library to be used with Protobuf. It ensures that Protobuf and other libraries are built with compatible runtime settings (/MD or /MT). If `protobuf_MSVC_STATIC_RUNTIME` is not already defined, it sets it to `OFF`, enforcing the use of dynamic linking for the runtime library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/protobuf/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED protobuf_MSVC_STATIC_RUNTIME)\n    set(protobuf_MSVC_STATIC_RUNTIME OFF CACHE BOOL \"Link protobuf to static runtime libraries\" FORCE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Disable NPU Memory Allocation (Linux)\nDESCRIPTION: This snippet shows how to disable NPU memory allocation by setting the `DISABLE_OPENVINO_GENAI_NPU_L0` environment variable to `1` in a Linux environment. This can be useful for compatibility with older NPU drivers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_19\n\nLANGUAGE: console\nCODE:\n```\nexport DISABLE_OPENVINO_GENAI_NPU_L0=1\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Negative Pads Example - C++\nDESCRIPTION: Shows padding with negative values in pads_begin and pads_end, effectively cropping the input tensor. This example is applicable to all pad_mode attribute options.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\npads_begin = [-1, -1]\npads_end = [-1, -1]\n\nDATA =\n[[1,  2,  3,  4]\n[5,  6,  7,  8]\n[9, 10, 11, 12]]\nShape(3, 4)\n\n\npad_mode = \"constant\"\npad_mode = \"edge\"\npad_mode = \"reflect\"\npad_mode = \"symmetric\"\n\nOUTPUT =\n[[ 6, 7 ]]\nShape(1, 2)\n```\n\n----------------------------------------\n\nTITLE: Multiclass Non-Maximum Suppression Layer Configuration (2 Inputs)\nDESCRIPTION: This example demonstrates the configuration of a MulticlassNonMaxSuppression layer within the OpenVINO framework using two input tensors: boxes and scores. It shows how to define the layer's attributes, input ports with dimension specifications, and output ports with precision details.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/multiclass-non-max-suppression-9.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"MulticlassNonMaxSuppression\" ... >\n    <data sort_result=\"score\" output_type=\"i64\" sort_result_across_batch=\"false\" iou_threshold=\"0.2\" score_threshold=\"0.5\" nms_top_k=\"-1\" keep_top_k=\"-1\" background_class=\"-1\"    normalized=\"false\" nms_eta=\"0.0\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>100</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>\n            <dim>5</dim>\n            <dim>100</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"5\" precision=\"FP32\">\n            <dim>-1</dim> <!-- \"-1\" means a undefined dimension calculated during the model inference -->\n            <dim>6</dim>\n        </port>\n        <port id=\"6\" precision=\"I64\">\n            <dim>-1</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"7\" precision=\"I64\">\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: Defines the target name for the library being built. This target name ('mock_engine') is used throughout the CMake file to refer to the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/unit_test_utils/mocks/mock_engine/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset (TARGET_NAME \"mock_engine\")\n```\n\n----------------------------------------\n\nTITLE: Reshape with dimension bounds - C++\nDESCRIPTION: This C++ code snippet demonstrates how to define dimension bounds using `ov::Dimension` for reshaping a model.  It showcases setting the lower and upper bounds for a specific dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov_dynamic_shapes:reshape_bounds]\nauto model = core.read_model(\"model.xml\");\n\nmodel->reshape({{model->input().get_any_name(), {ov::Dimension(1, 10), 3, 224, 224}}});\n//! [ov_dynamic_shapes:reshape_bounds]\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files in CMake\nDESCRIPTION: This snippet uses the `file(GLOB_RECURSE)` command to find all `.cpp` and `.hpp` files in the specified source directories and header files in the include directories. These files are then stored in the `LIBRARY_SRC`, `PUBLIC_HEADERS`, and `DEV_HEADERS` variables for later use in defining the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/*.hpp)\nfile(GLOB_RECURSE PUBLIC_HEADERS ${OV_CORE_INCLUDE_PATH}/*.hpp)\nfile(GLOB_RECURSE DEV_HEADERS ${OV_CORE_DEV_API_PATH}/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Compare BF16 and FP32 Precision\nDESCRIPTION: Compares precision between BF16 and FP32 dumps using the CPU plugin. Replace `/path/model.xml` with the path to the model. Uses the previously generated `dump_bf16` and `dump_fp32` files for comparison.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/dump_check/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 dump_check.py -m /path/model.xml  ./dump_bf16 ./dump_fp32\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Environment Variables on macOS\nDESCRIPTION: This script sets up the OpenVINO environment variables on a macOS system. It sources the `setupvars.sh` script located in the OpenVINO installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\nsource <INSTALL_DIR>/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: Add Custom Target for Wheel Build - CMake\nDESCRIPTION: This CMake code adds a custom target named `ie_wheel` which depends on the wheel file and the fdupes report (if generated).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(ie_wheel ALL DEPENDS ${ie_wheel_deps})\n```\n\n----------------------------------------\n\nTITLE: Add cpuUtils Library\nDESCRIPTION: This snippet defines a static library named 'cpuUtils'. It includes header and source files related to runtime information, memory formats, and precision support. It also conditionally includes RISC-V 64-bit specific files based on the RISCV64 variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/functional/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(cpuUtils STATIC\n    $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/src/utils/rt_info/memory_formats_attribute.hpp\n    $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/src/utils/rt_info/memory_formats_attribute.cpp\n    $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/src/utils/precision_support.h\n    $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/src/utils/precision_support.cpp\n    ${CPU_ISA_TRAITS_RV64})\n```\n\n----------------------------------------\n\nTITLE: Static Dimension C++\nDESCRIPTION: These code snippets illustrate how to create static Dimension objects in OpenVINO using C++.\nThey initialize `ov::Dimension` objects representing a dimension with a fixed, static value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/docs/shape_propagation.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nov::Dimension{3};\nov::Dimension{3, 3};\n```\n\n----------------------------------------\n\nTITLE: Importing OpenVINO properties - C++\nDESCRIPTION: This C++ snippet shows the necessary includes and using directives at the beginning of code snippets that use OpenVINO properties. This is used as a header in other snippets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nusing namespace ov::hint;\nusing namespace ov::intel_auto;\nusing namespace ov::device;\nusing namespace ov::properties;\n```\n\n----------------------------------------\n\nTITLE: Program Structure Definition in C++\nDESCRIPTION: Defines the structure of the `program` class, which encapsulates the computational graph and related resources. It includes members for managing program nodes, kernels, passes, inputs, outputs, and the execution order.  The `engine` and `kernels_cache` are used for executing compiled kernels.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/basic_data_structures.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nstruct program {\n...\n    uint32_t prog_id = 0;\n    engine& _engine;\n    std::unique_ptr<kernels_cache> _kernels_cache;\n    std::list<program_node*> inputs;\n    std::vector<program_node*> outputs;\n    nodes_ordering processing_order;\n    std::unique_ptr<pass_manager> pm;\n    std::map<primitive_id, std::shared_ptr<program_node>> nodes_map;\n...\n};\n```\n\n----------------------------------------\n\nTITLE: IDFT Layer XML Configuration (4D input, no signal_size)\nDESCRIPTION: This XML snippet configures an IDFT layer in OpenVINO with a 4D input tensor and without the optional signal_size input. It defines the input and output ports with their respective dimensions for the inverse discrete Fourier transform.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/idft-7.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>\t<!-- [1, 2] -->\n        </port>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Copying Generated Code to Include Directory\nDESCRIPTION: This CMake snippet creates a custom command to copy the generated kernel sources and headers from the code generation cache directory to the include directory. The `copy_if_different` command ensures that the files are only copied if they have changed, further optimizing the build process. This makes the generated code available for compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/cm/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(OUTPUT \"${CODEGEN_INCDIR}/${KERNEL_SOURCES}\"\n  COMMAND \"${CMAKE_COMMAND}\" -E copy_if_different \"${CODEGEN_CACHE_DIR}/${KERNEL_SOURCES}\" \"${CODEGEN_INCDIR}/${KERNEL_SOURCES}\"\n  COMMAND \"${CMAKE_COMMAND}\" -E copy_if_different \"${CODEGEN_CACHE_DIR}/${KERNEL_HEADERS}\" \"${CODEGEN_INCDIR}/${KERNEL_HEADERS}\"\n  DEPENDS \"${CODEGEN_CACHE_DIR}/${KERNEL_SOURCES}\" \"${KERNELS}\" \"${HEADERS}\" \"${CODEGEN_SCRIPT}\"\n  COMMENT \"Updating file if the file changed (${CODEGEN_INCDIR}/${KERNEL_SOURCES}) ...\"\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Public and Private Include Directories for OpenVINO Core\nDESCRIPTION: This snippet defines the public and private include directories for the `openvino_core_obj` target. Public include directories are added to the interface of the target and are visible to consumers of the library, while private include directories are only used internally.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(openvino_core_obj PUBLIC $<BUILD_INTERFACE:${OV_CORE_INCLUDE_PATH}>\n                                             PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/src)\n```\n\n----------------------------------------\n\nTITLE: Creating Shared Library\nDESCRIPTION: This snippet creates a shared library named `${PROJECT_NAME}` (ov_node_addon) and includes all the specified C++ source files.  It also includes the platform-specific delay load hook source on Windows.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${PROJECT_NAME} SHARED\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/node_output.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/async_reader.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/preprocess/preprocess.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/preprocess/pre_post_process_wrap.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/preprocess/preprocess_steps.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/preprocess/input_info.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/preprocess/output_info.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/preprocess/input_tensor_info.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/preprocess/output_tensor_info.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/preprocess/input_model_info.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/preprocess/resize_algorithm.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/errors.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/helper.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/type_validation.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/tensor.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/infer_request.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/compiled_model.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/core_wrap.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/model_wrap.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/addon.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/element_type.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/partial_shape_wrap.cpp\n\n    ${CMAKE_JS_SRC}\n)\n```\n\n----------------------------------------\n\nTITLE: GenerateProposals Layer Configuration XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a GenerateProposals layer in OpenVINO. It specifies the layer's type, version, attributes such as `min_size`, `nms_threshold`, `post_nms_count`, `pre_nms_count`, and `roi_num_type`, along with input and output port dimensions and precision.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/generate-proposals-9.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"GenerateProposals\" version=\"opset9\">\n    <data min_size=\"0.0\" nms_threshold=\"0.699999988079071\" post_nms_count=\"1000\" pre_nms_count=\"1000\" roi_num_type=\"i32\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>3</dim>\n        </port>\n        <port id=\"1\">\n            <dim>50</dim>\n            <dim>84</dim>\n            <dim>3</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>12</dim>\n            <dim>50</dim>\n            <dim>84</dim>\n        </port>\n        <port id=\"3\">\n            <dim>8</dim>\n            <dim>3</dim>\n            <dim>50</dim>\n            <dim>84</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"4\" precision=\"FP32\">\n            <dim>-1</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"5\" precision=\"FP32\">\n            <dim>-1</dim>\n        </port>\n        <port id=\"6\" precision=\"I32\">\n            <dim>8</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Dump IR with multiple options OpenVINO (sh)\nDESCRIPTION: This example demonstrates how to use multiple options with OV_CPU_DUMP_IR, including specifying transformation stages and the output directory.  It combines transformations and dir options for finer control.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/graph_serialization.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_DUMP_IR=\"transformations=snippets dir=path/dumpDir\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Setting Interprocedural Optimization (CMake)\nDESCRIPTION: This snippet sets the interprocedural optimization property for the library in the release configuration. It enables link-time optimization if `ENABLE_LTO` is set.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: FakeConvert Operation in Python\nDESCRIPTION: This Python code demonstrates the mathematical operation performed by FakeConvert. It shows how the input data is scaled, shifted, converted to a destination type, and then converted back to the original data type. This operation is used to emulate the behavior of low-precision floating-point types.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/quantization/fake-convert-13.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\ndata = data * scale - shift\nConvertLike(Convert(data, destination_type), data)\ndata = (data + shift) / scale\n```\n\n----------------------------------------\n\nTITLE: ConvolutionBackpropData Padding Calculation with Auto Padding\nDESCRIPTION: These formulas calculate total padding and how to distribute it between pads_begin and pads_end when the output shape is specified and auto_pad is enabled. The distribution depends on whether auto_pad is SAME_UPPER or not, ensuring proper alignment of input and output tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/convolution-backprop-data-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\ntotal_padding[i] = stride[i] * (X_i - 1) + ((K_i - 1) * dilations[i] + 1) - output_shape[i] + output_padding[i]\nif auto_pads != SAME_UPPER:\n    pads_begin[i] = total_padding[i] // 2\n    pads_end[i] = total_padding[i] - pads_begin[i]\nelse:\n    pads_end[i] = total_padding[i] // 2\n    pads_begin[i] = total_padding[i] - pads_end[i]\n```\n\n----------------------------------------\n\nTITLE: Sign Layer XML Configuration in OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of a Sign layer within an OpenVINO model. It specifies the input and output ports with their respective dimensions. The Sign layer calculates the sign of each element in the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/sign-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Sign\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Building oneDNN GPU Library with CMake\nDESCRIPTION: Configures and builds the oneDNN GPU library using the `ExternalProject_Add` CMake function. It sets various CMake arguments to control the build process, including compiler flags, enabled primitives, ISA, and build type. It also handles platform-specific configurations and installs the built library and associated PDB files (on Windows).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/thirdparty/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_ONEDNN_FOR_GPU)\n    function(build_onednn_gpu)\n        include(ExternalProject)\n        set(ONEDNN_BUILD_DIR \"${CMAKE_CURRENT_BINARY_DIR}/onednn_gpu_build\")\n        set(ONEDNN_INSTALL_DIR \"${CMAKE_CURRENT_BINARY_DIR}/onednn_gpu_install\" CACHE PATH \"Installation path for oneDNN GPU library\")\n        set(ONEDNN_PREFIX_DIR \"${CMAKE_CURRENT_BINARY_DIR}/onednn_gpu_root\")\n        set(ONEDNN_ENABLED_PRIMITIVES \"CONCAT;CONVOLUTION;DECONVOLUTION;INNER_PRODUCT;MATMUL;REORDER;POOLING;REDUCTION;SDPA;RNN\")\n        set(ONEDNN_ENABLED_ISA \"XELP;XEHP;XEHPG;XEHPC;XE2;XE3\")\n        set(DNNL_GPU_LIBRARY_NAME \"openvino_onednn_gpu\" CACHE STRING \"Name of oneDNN library for Intel GPU Plugin\")\n\n        if(X86_64)\n            set(ONEDNN_TARGET_ARCH \"X64\" CACHE STRING \"\" FORCE)\n        elseif(X86)\n            set(ONEDNN_TARGET_ARCH \"X86\" CACHE STRING \"\" FORCE)\n        elseif(RISCV64)\n            set(ONEDNN_TARGET_ARCH \"RV64\" CACHE STRING \"\" FORCE)\n        elseif(ARM)\n            set(ONEDNN_TARGET_ARCH \"ARM\" CACHE STRING \"\" FORCE)\n        elseif(AARCH64)\n            set(ONEDNN_TARGET_ARCH \"AARCH64\" CACHE STRING \"\" FORCE)\n        else()\n            message(WARNING \"Intel GPU plugin unsupported architecture: ${CMAKE_SYSTEM_PROCESSOR}\")\n        endif()\n\n        # oneDNN specific compile flags\n        if(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG OR (OV_COMPILER_IS_INTEL_LLVM AND UNIX))\n            ov_add_compiler_flags(-Wno-undef)\n            ov_add_compiler_flags(-Wno-missing-declarations)\n            if(CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 11 AND CMAKE_COMPILER_IS_GNUCXX)\n                ov_add_compiler_flags(-Wno-array-bounds)\n                ov_add_compiler_flags(-Wno-stringop-overflow)\n                if(CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 12)\n                    ov_add_compiler_flags(-Wno-restrict)\n                endif()\n            endif()\n        endif()\n\n        if(SUGGEST_OVERRIDE_SUPPORTED)\n            set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-suggest-override\")\n        endif()\n\n        # remove CMAKE_COMPILE_WARNING_AS_ERROR for onednn_gpu\n        if(WIN32 AND CMAKE_COMPILE_WARNING_AS_ERROR AND CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\" AND CMAKE_VERSION VERSION_LESS 3.24)\n            ov_add_compiler_flags(/WX-)\n        endif()\n\n        # pass common variables\n        foreach(cmake_var IN ITEMS CMAKE_SYSTEM_NAME CMAKE_SYSTEM_VERSION\n                                   CMAKE_SYSTEM_PROCESSOR CMAKE_TOOLCHAIN_FILE\n                                   CMAKE_VERBOSE_MAKEFILE CMAKE_GENERATOR\n                                   CMAKE_CXX_COMPILER CMAKE_C_COMPILER\n                                   CMAKE_CXX_COMPILER_LAUNCHER CMAKE_C_COMPILER_LAUNCHER)\n            if(${cmake_var})\n                list(APPEND cmake_extra_args \"-D${cmake_var}=${${cmake_var}}\")\n            endif()\n        endforeach()\n\n        # pass compilation flags\n        foreach(lang IN ITEMS C CXX)\n            foreach(build_type IN ITEMS \"\" \"_DEBUG\" \"_MINSIZEREL\" \"_RELEASE\" \"_RELWITHDEBINFO\")\n                set(flag_var \"CMAKE_${lang}_FLAGS${build_type}\")\n                if(${flag_var})\n                    list(APPEND cmake_extra_args \"-D${flag_var}=${${flag_var}}\")\n                endif()\n            endforeach()\n        endforeach()\n\n        # pass build type\n        if(OV_GENERATOR_MULTI_CONFIG)\n            if(CMAKE_GENERATOR STREQUAL \"Ninja Multi-Config\")\n                list(APPEND cmake_extra_args \"-DCMAKE_CONFIGURATION_TYPES=${CMAKE_DEFAULT_BUILD_TYPE}\")\n                list(APPEND cmake_extra_args \"-DCMAKE_DEFAULT_BUILD_TYPE=${CMAKE_DEFAULT_BUILD_TYPE}\")\n            endif()\n        else()\n            list(APPEND cmake_extra_args \"-DCMAKE_BUILD_TYPE=${CMAKE_BUILD_TYPE}\")\n        endif()\n\n        if(CMAKE_VERSION VERSION_GREATER_EQUAL 3.15 AND DEFINED CMAKE_MSVC_RUNTIME_LIBRARY)\n            list(APPEND cmake_extra_args \"-DCMAKE_MSVC_RUNTIME_LIBRARY=${CMAKE_MSVC_RUNTIME_LIBRARY}\")\n        endif()\n\n        # propogate OpenCL if explicitly specified\n        if(OpenCL_LIBRARY)\n            list(APPEND cmake_extra_args \"-DOpenCL_LIBRARY=${OpenCL_LIBRARY}\")\n        endif()\n        if(OpenCL_INCLUDE_DIR)\n            list(APPEND cmake_extra_args \"-DOpenCL_INCLUDE_DIR=${OpenCL_INCLUDE_DIR}\")\n        endif()\n\n        set(onednn_gpu_lib \"${CMAKE_STATIC_LIBRARY_PREFIX}${DNNL_GPU_LIBRARY_NAME}${CMAKE_STATIC_LIBRARY_SUFFIX}\")\n        set(ONEDNN_GPU_LIB_PATH ${ONEDNN_INSTALL_DIR}/lib/${onednn_gpu_lib} CACHE FILEPATH \"Path to oneDNN GPU library\")\n\n        ExternalProject_Add(onednn_gpu_build\n            # Directory Options:\n            PREFIX \"${ONEDNN_PREFIX_DIR}\"\n            SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/onednn_gpu\"\n            BINARY_DIR \"${ONEDNN_BUILD_DIR}\"\n            INSTALL_DIR \"${ONEDNN_INSTALL_DIR}\"\n            # Configure Step Options:\n            CMAKE_ARGS\n                ${cmake_extra_args}\n                \"-DCMAKE_INTERPROCEDURAL_OPTIMIZATION_RELEASE=${ENABLE_LTO}\"\n                \"-DCMAKE_POLICY_DEFAULT_CMP0069=NEW\"\n                \"-DDNNL_TARGET_ARCH=${ONEDNN_TARGET_ARCH}\"\n                \"-DDNNL_CPU_RUNTIME=NONE\"\n                \"-DDNNL_GPU_RUNTIME=OCL\"\n                \"-DDNNL_LIBRARY_NAME=${DNNL_GPU_LIBRARY_NAME}\"\n                \"-DCMAKE_INSTALL_PREFIX=${ONEDNN_INSTALL_DIR}\"\n                \"-DDNNL_ENABLE_CONCURRENT_EXEC=ON\"\n                \"-DDNNL_ENABLE_PRIMITIVE_CACHE=ON\"\n                \"-DDNNL_ENABLE_WORKLOAD=INFERENCE\"\n                \"-DDNNL_ENABLE_JIT_PROFILING=${BUILD_SHARED_LIBS}\"\n                \"-DDNNL_ENABLE_ITT_TASKS=${BUILD_SHARED_LIBS}\"\n                \"-DDNNL_BUILD_TESTS=OFF\"\n                \"-DDNNL_BUILD_EXAMPLES=OFF\"\n                \"-DDNNL_BLAS_VENDOR=NONE\"\n                \"-DDNNL_LIBRARY_TYPE=STATIC\"\n                \"-DDNNL_EXPERIMENTAL_PROFILING=ON\"\n                \"-DONEDNN_BUILD_GRAPH=OFF\"\n                # specifically for Conan, because it overrides CMAKE_PREFIX_PATH and oneDNN's FindOpenCL.cmake is ignored\n                # Conan's FindOpenCL.cmake module does not set OpenCL_INCLUDE_DIRS, so we need to set it manually\n                \"-DOpenCL_INCLUDE_DIRS=$<TARGET_PROPERTY:OpenCL::OpenCL,INTERFACE_INCLUDE_DIRECTORIES>\"\n                # Conan calls cmake with default value for CMP0091, so we have to bypass it to oneDNN build\n                # because we bypass conan_toolchain.cmake via CMAKE_TOOLCHAIN_FILE\n                \"-DCMAKE_POLICY_DEFAULT_CMP0091=NEW\"\n            CMAKE_CACHE_ARGS\n                # The arguments below requires list to be passed as argument\n                # which doesn't work properly when passed to CMAKE_ARGS.\n                # Thus we pass it via CMAKE_CACHE_ARGS\n                \"-DDNNL_ENABLE_PRIMITIVE:STRING=${ONEDNN_ENABLED_PRIMITIVES}\"\n                \"-DDNNL_ENABLE_PRIMITIVE_GPU_ISA:STRING=${ONEDNN_ENABLED_ISA}\"\n            # Build Step Options:\n            BUILD_BYPRODUCTS ${ONEDNN_GPU_LIB_PATH}\n            # Target Options:\n            EXCLUDE_FROM_ALL ON\n        )\n\n        if(WIN32 AND NOT BUILD_SHARED_LIBS)\n            ExternalProject_Add_Step(onednn_gpu_build post-build\n                COMMAND ${CMAKE_COMMAND}\n                    -D \"BUILD_DIR=${ONEDNN_BUILD_DIR}/src\"\n                    -D \"OUTPUT_DIR=${OPENVINO_STATIC_PDB_OUTPUT_DIRECTORY}\"\n                    -P \"${CMAKE_CURRENT_SOURCE_DIR}/copy_pdb.cmake\"\n                COMMENT \"Copy oneDNN for GPU PDB files\"\n                DEPENDEES install  # Ensures this runs after install\n            )\n        endif()\n\n        set(LIB_INCLUDE_DIRS \"${ONEDNN_INSTALL_DIR}/include\" \"${CMAKE_CURRENT_SOURCE_DIR}/onednn_gpu/src\" \"${CMAKE_CURRENT_SOURCE_DIR}/onednn_gpu/src/gpu/intel/jit/ngen\" \"${CMAKE_CURRENT_SOURCE_DIR}/onednn_gpu/third_party/ngen\")\n        set(LIB_DEFINITIONS ENABLE_ONEDNN_FOR_GPU\n                            DNNL_DLL\n                            DNNL_DLL_EXPORTS\n                            DNNL_ENABLE_CPU_ISA_HINTS\n                            DNNL_ENABLE_MAX_CPU_ISA\n                            DNNL_X64=1)\n        add_library(onednn_gpu_tgt INTERFACE)\n        set_target_properties(onednn_gpu_tgt PROPERTIES\n            INTERFACE_LINK_LIBRARIES $<BUILD_INTERFACE:${ONEDNN_GPU_LIB_PATH}>\n            INTERFACE_INCLUDE_DIRECTORIES \"$<BUILD_INTERFACE:${LIB_INCLUDE_DIRS}>\"\n            INTERFACE_SYSTEM_INCLUDE_DIRECTORIES \"${LIB_INCLUDE_DIRS}\"\n            INTERFACE_COMPILE_DEFINITIONS \"${LIB_DEFINITIONS}\"\n        )\n        add_dependencies(onednn_gpu_tgt onednn_gpu_build)\n\n        if(NOT BUILD_SHARED_LIBS)\n            ov_install_static_lib(onednn_gpu_tgt ${OV_CPACK_COMP_CORE})\n\n            # we need to install library explicitly and set_target_properties in OpenVINOConfig.cmake for 'onednn_gpu_tgt'\n            # to point to installation location of this file\n            install(FILES \"${ONEDNN_GPU_LIB_PATH}\"\n                    DESTINATION ${OV_CPACK_ARCHIVEDIR}\n                    COMPONENT ${OV_CPACK_COMP_CORE})\n        endif()\n    endfunction()\n    build_onednn_gpu()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Analyzing Code Coverage (Bash)\nDESCRIPTION: This script generates a code coverage report after the fuzz test execution. It uses `llvm-profdata` to merge the profiling data and `llvm-cov` to generate the HTML report. The object file path and instr-profile path must be specified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nllvm-profdata merge -sparse *.profraw -o default.profdata && \\\nllvm-cov show ./read_network-fuzzer -object=lib/libopenvino.so -instr-profile=default.profdata -format=html -output-dir=read_network-coverage\n```\n\n----------------------------------------\n\nTITLE: GatherND with Leading Dimensions Example\nDESCRIPTION: This example illustrates GatherND operating with leading dimensions in the indices tensor.  The indices tensor's shape affects how the operation selects and arranges slices from the data tensor in the output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-5.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nindices = [[[1]], [[0]]]\ndata    = [[1, 2],\n              [3, 4]]\noutput  = [[[3, 4]],\n              [[1, 2]]]\n```\n\n----------------------------------------\n\nTITLE: Creating OVSA User on Guest VM\nDESCRIPTION: Adds a user named `ovsa` to the guest VM. Includes creating the user's home directory and setting a password for the new user.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_33\n\nLANGUAGE: sh\nCODE:\n```\nsudo useradd -m ovsa\nsudo passwd ovsa\n```\n\n----------------------------------------\n\nTITLE: Eye Operation Example 1 (XML)\nDESCRIPTION: Illustrates the Eye operation with specified input ports and data attributes. It configures the layer to generate a 5x5 identity matrix of type i8.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/eye-9.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... name=\"Eye\" type=\"Eye\">\n    <data output_type=\"i8\"/>\n    <input>\n        <port id=\"0\" precision=\"I32\"/>  <!-- num rows: 5 -->\n        <port id=\"1\" precision=\"I32\"/>  <!-- num columns: 5 -->\n        <port id=\"2\" precision=\"I32\"/>  <!-- diagonal index -->\n    </input>\n    <output>\n        <port id=\"3\" precision=\"I8\" names=\"Eye:0\">\n            <dim>5</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating a Constant Subgraph (C++)\nDESCRIPTION: This code snippet shows how to construct a constant subgraph. Creating constant subgraphs is a common task during transformations, and they often need to be folded using `ov::pass::ConstantFolding` after the transformation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov:constant_subgraph]\nauto const_node = ov::op::v0::Constant::create(ov::element::f32, ov::Shape{1}, {1.0f});\nauto add = std::make_shared<ov::op::v1::Add>(input, const_node);\n//! [ov:constant_subgraph]\n```\n\n----------------------------------------\n\nTITLE: Setting MSVC Compiler Flags for OpenMP Vectorization\nDESCRIPTION: This snippet sets the `/openmp:experimental` flag for the MSVC compiler when using Visual Studio 2017 or later. This flag enables SIMD vectorization, specifically when using `#pragma omp simd` directives.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/thirdparty/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\" AND MSVC_TOOLSET_VERSION GREATER_EQUAL 141)\n    # Visual Studio 2017 (v141 toolset)\n    # This flag is needed for enabling SIMD vectorization with command '#pragma omp simd'.\n    # Compilation with '/openmp:experimental' key allow us to enable vectorizatikon capability in MSVC.\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /openmp:experimental\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: SpaceToDepth with depth_first mode in Python\nDESCRIPTION: This Python code snippet demonstrates the transformation performed by the SpaceToDepth operation when mode is set to 'depth_first'. It reshapes the input tensor, transposes it, and reshapes it again to obtain the output tensor. The input tensor has K spatial dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/space-to-depth-1.rst#_snippet_1\n\nLANGUAGE: py\nCODE:\n```\nx' = reshape(data, [N, C, D1 / block_size, block_size, D2 / block_size, block_size, ..., DK / block_size, block_size])\n\nx'' = transpose(x', [0,  1, 3, 5, ..., K + (K + 1),  2, 4, ..., K + K])\n\ny = reshape(x'', [N, C * (block_size ^ K), D1 / block_size, D2 / block_size, ..., DK / block_size])\n```\n\n----------------------------------------\n\nTITLE: Reactivating a Virtual Environment (Linux/macOS)\nDESCRIPTION: This command reactivates a Python virtual environment named `openvino_env` on Linux or macOS. It uses the `activate` script located within the `bin` directory of the environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/run-notebooks.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsource openvino_env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Conditional Test Subdirectory Inclusion in CMake\nDESCRIPTION: This CMake block conditionally adds the 'tests' subdirectory to the build process. If the ENABLE_TESTS variable is set to true, the CMakeLists.txt file in the 'tests' directory will be included, allowing for test compilation and execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/ir/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditional Developer Checks and Hook Installation (CMake)\nDESCRIPTION: This CMake code block conditionally executes checks for pybind11-stubgen and pre-commit packages. If these packages are missing, a warning is issued. It then attempts to install a pre-commit hook from a YAML configuration file for Python stub generation, logging any errors or success messages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT (DEFINED ENV{CI} OR DEFINED ENV{TF_BUILD} OR DEFINED ENV{JENKINS_URL}))\n\n    # check pybind11-stubgen package\n    execute_process(COMMAND ${Python3_EXECUTABLE} -m pip show pybind11-stubgen\n                    OUTPUT_VARIABLE pybind11_stubgen_info OUTPUT_STRIP_TRAILING_WHITESPACE\n                    RESULT_VARIABLE pybind11_stubgen_result)\n    string(REGEX MATCH \"Version: ([\\\\.0-9]+)\" pybind11_stubgen_version \"${pybind11_stubgen_info}\")\n    if(pybind11_stubgen_result OR NOT pybind11_stubgen_version)\n        message(WARNING \"pybind11-stubgen package is not installed or version is not detected. This package is necessary for developing Python API.\")\n    endif()\n\n    # check pre-commit package    \n    execute_process(COMMAND ${Python3_EXECUTABLE} -m pip show pre-commit\n                    OUTPUT_VARIABLE precommit_pkg_info OUTPUT_STRIP_TRAILING_WHITESPACE\n                    RESULT_VARIABLE precommit_pkg_result)\n    string(REGEX MATCH \"Version: ([\\\\.0-9]+)\" precommit_pkg_version \"${precommit_pkg_info}\")\n    if(precommit_pkg_result OR NOT precommit_pkg_version)\n        message(WARNING \"pre-commit package is not installed or version is not detected. This package is necessary for developing Python API.\")\n    endif()\n\n    # attach the pre-commit hook for Python stub generation\n    execute_process(COMMAND pre-commit install -c \"${CMAKE_CURRENT_SOURCE_DIR}/scripts/stub_generation_precommit.yaml\"\n                    RESULT_VARIABLE precommit_install_result\n                    ERROR_VARIABLE precommit_install_error\n                    OUTPUT_STRIP_TRAILING_WHITESPACE\n                    ERROR_STRIP_TRAILING_WHITESPACE)\n    if(NOT precommit_install_result EQUAL \"0\")\n        message(WARNING \"Python stub generation pre-commit hook install failed with error: ${precommit_install_error}. It is necessary for developing Python API.\")\n    else()\n        message(STATUS \"pre-commit hook for Python API stub generation has been installed\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Wrapping with C++ Object in C\nDESCRIPTION: This code snippet demonstrates how to wrap a C++ class in C by creating a struct containing an instance of the C++ class.  This approach is suitable for simple objects. The struct `ov_ClassName` contains an instance of the `ov::ClassName` class.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/how_to_wrap_openvino_objects_with_c.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nstruct ov_ClassName {\n    ov::ClassName object;\n};\n```\n\n----------------------------------------\n\nTITLE: Install Python and GIT on Linux\nDESCRIPTION: This code snippet provides the commands to update, upgrade, and install necessary packages, including python3-venv, build-essential, python3-dev, git-all, libgl1-mesa-dev, and ffmpeg, on Ubuntu-based Linux systems. These packages are essential for running OpenVINO notebooks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get install python3-venv build-essential python3-dev git-all libgl1-mesa-dev ffmpeg\n```\n\n----------------------------------------\n\nTITLE: Adding Unit Test Target\nDESCRIPTION: This snippet adds the unit test target using the `ov_add_test_target` macro. It defines the target name, root directory, include directories (public and private), excluded source paths, object files, link libraries, and labels.  Key libraries included are gtest, dnnl, openvino_runtime_s, unit_test_utils, ov_snippets_models, snippets_test_utils, MLAS_LIBRARY and SHL_LIBRARY.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/vectorized/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        INCLUDES\n            PUBLIC\n                $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/src\n                $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/src/nodes\n                $<TARGET_PROPERTY:openvino::conditional_compilation,INTERFACE_INCLUDE_DIRECTORIES>\n            PRIVATE\n                $<TARGET_PROPERTY:openvino::snippets,SOURCE_DIR>/include\n        EXCLUDED_SOURCE_PATHS\n            ${EXCLUDED_SOURCE_PATHS_FOR_UNIT_TEST}\n        OBJECT_FILES\n            ${OBJ_LIB}\n        LINK_LIBRARIES\n            gtest\n            gtest_main\n            dnnl\n            gmock\n            openvino_runtime_s\n            unit_test_utils\n            ov_snippets_models\n            snippets_test_utils\n            ${MLAS_LIBRARY}\n            ${SHL_LIBRARY}\n        ADD_CPPLINT\n        LABELS\n            OV UNIT CPU\n)\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Example 2 - XML\nDESCRIPTION: This XML snippet provides another example of the ScatterElementsUpdate operation with use_init_val set to 'false' and reduction set to 'sum'. It shows the input data, indices, updates, and the resulting output tensor, highlighting the difference with use_init_val=true.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_12\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... use_init_val=\"false\" reduction=\"sum\" type=\"ScatterElementsUpdate\">\n    <input>\n        <port id=\"0\">>  <!-- data -->\n            <dim>4</dim>  <!-- values: [2, 3, 4, 6] -->\n        </port>\n        <port id=\"1\">  <!-- indices -->\n            <dim>6</dim>  <!-- values: [1, 0, 0, 2, 3, 2] -->\n        </port>\n        <port id=\"2\">>  <!-- updates -->\n            <dim>6</dim>  <!-- values: [10, 20, 30, 40, 70, 60] -->\n        </port>\n        <port id=\"3\">     <!-- values: [0] -->\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"4\" precision=\"FP32\">\n            <dim>4</dim>  <!-- values: [50, 10, 100, 70] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: StridedSlice Negative Slicing Example in XML\nDESCRIPTION: Shows how to use negative indices in the StridedSlice layer for slicing tensors. The example performs slicing on a 3D array, where one dimension utilizes a negative index for the end value. This is equivalent to array[0:2, 0:2, 0:-1].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/strided-slice-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StridedSlice\" ...>\n    <data/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim> <!-- begin: [0, 0, 0] -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim> <!-- end: [2, 2, -1] - -1 will be replaced by 4 - 1 = 3 -->\n        </port>\n        <port id=\"3\">\n            <dim>3</dim> <!-- stride: [1, 1, 1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>2</dim> <!-- element ids: [0, 1] -->\n            <dim>2</dim> <!-- element ids: [0, 1] -->\n            <dim>3</dim> <!-- element ids: [0, 1, 2] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ScatterNDUpdate Example 2 (XML)\nDESCRIPTION: This XML snippet demonstrates a ScatterNdUpdate layer configuration with reduction set to 'sum'.  It showcases how the input data is updated by summing the update values at the specified indices.  The updated values are added at the indices [0, 2, -3, -3, 0].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-nd-update-15.rst#_snippet_4\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... reduction=\"sum\" type=\"ScatterNdUpdate\">\n    <input>\n        <port id=\"0\" precision=\"FP16\">  <!-- data -->\n            <dim>4</dim>  <!-- values: [1, 2, 3, 4] -->\n        </port>\n        <port id=\"1\" precision=\"I32\">  <!-- indices -->\n            <dim>5</dim>  <!-- values: [0, 2, -3, -3, 0] -->\n        </port>\n        <port id=\"2\" precision=\"FP16\">  <!-- updates -->\n            <dim>5</dim>  <!-- values: [10, 20, 30, 40, 50] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP16\">\n            <dim>4</dim>  <!-- values: [61, 72, 23, 4] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Removing Excluded Files\nDESCRIPTION: This snippet uses `file(GLOB_RECURSE)` to collect files in the `EXCLUDE_PATHS` and then removes them from the `SOURCES` and `HEADERS` lists.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_26\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE FILES_TO_REMOVE ${EXCLUDE_PATHS})\nlist(REMOVE_ITEM SOURCES ${FILES_TO_REMOVE})\nlist(REMOVE_ITEM HEADERS ${FILES_TO_REMOVE})\n```\n\n----------------------------------------\n\nTITLE: Adding API Validator Post-Build Step\nDESCRIPTION: This snippet adds a post-build step to the target, using `ov_add_api_validator_post_build_step`. This step validates the generated API of the plugin after the build process completes, ensuring API consistency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/plugin/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_api_validator_post_build_step(TARGET ${NPU_PLUGIN_TARGET})\n```\n\n----------------------------------------\n\nTITLE: PReLU layer definition with 4D input in XML\nDESCRIPTION: This example defines a PReLU layer in XML for a 4D input tensor 'data' and a 1D 'slope' tensor. The 'data' tensor's dimensions are 1, 20, 128, and 128. The 'slope' tensor has a dimension of 20. The output tensor mirrors the shape of the input 'data' tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/prelu-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Prelu\">\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>20</dim>\n            <dim>128</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"1\">\n            <dim>20</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>20</dim>\n            <dim>128</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for TensorFlow Frontend (CMake)\nDESCRIPTION: This snippet conditionally adds the 'tensorflow' subdirectory to the build if the ENABLE_OV_TF_FRONTEND CMake option is enabled. This includes the TensorFlow frontend functionality.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_OV_TF_FRONTEND)\n    add_subdirectory(tensorflow)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions for OpenVINO Runtime in CMake\nDESCRIPTION: This code defines compile definitions for the OpenVINO runtime object library, including enabling the OpenVINO runtime API and proxy plugin support. It conditionally enables debug capabilities based on the `ENABLE_DEBUG_CAPS` flag.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME}_obj PRIVATE\n    IMPLEMENT_OPENVINO_RUNTIME_API\n    $<$<TARGET_EXISTS:openvino_proxy_plugin_obj>:PROXY_PLUGIN_ENABLED>)\n\nif(ENABLE_DEBUG_CAPS)\n    target_compile_definitions(${TARGET_NAME}_obj PUBLIC ENABLE_DEBUG_CAPS)\nendif()\n```\n\n----------------------------------------\n\nTITLE: ScatterNDUpdate Example 2 (C++)\nDESCRIPTION: This C++ snippet shows the update of two slices of a 4x4 shape in the 'data' array, with the 'reduction' attribute set to 'none'. It demonstrates how entire slices of the 'data' array are replaced with corresponding slices from the 'updates' array based on the provided indices. No dependencies required as it is example data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-nd-update-15.rst#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ndata    = [[[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n          [[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n          [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]],\n          [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]]]\nindices = [[0], [2]]\nupdates = [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n          [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]]]\noutput  = [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n          [[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n          [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],\n          [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]]]\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch ScriptModule to OpenVINO IR with `convert_model`\nDESCRIPTION: This code snippet demonstrates how to convert a PyTorch `torch.jit.ScriptModule` model to OpenVINO IR using the `convert_model` function from the `openvino` library.  The `example_input` parameter is required since `torch.jit.ScriptModule` objects are saved in an untraced state. It assumes that you have a `script_module.pt` file containing the model and that the `torch` and `openvino` libraries are installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-pytorch.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom openvino import convert_model\nimport torch\n\nconvert_model(input_model='script_module.pt', example_input=torch.rand(1, 10))\n```\n\n----------------------------------------\n\nTITLE: AvgPool Configuration: valid padding, exclude-pad true\nDESCRIPTION: This XML snippet configures an AvgPool layer with 'valid' auto_pad and 'exclude-pad' set to true. It uses a kernel size of 5x5, 1x1 padding at both the beginning and end, and a 2x2 stride. With 'valid' padding, no padding is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/avg-pool-14.rst#_snippet_4\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"AvgPool\" ... >\n    <data auto_pad=\"valid\" exclude-pad=\"true\" kernel=\"5,5\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>14</dim>\n            <dim>14</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: I420toBGR Layer Configuration (Multiple Inputs)\nDESCRIPTION: This XML snippet defines an I420toBGR layer configuration with three input ports for Y, U, and V planes. It specifies the dimensions for each input plane as well as the output BGR tensor, describing batch size, height, width and channels.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/i420-to-bgr-8.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"I420toBGR\">\n    <input>\n        <port id=\"0\"> \t<!-- Y plane -->\n            <dim>1</dim>\n            <dim>480</dim>\n            <dim>640</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\"> \t<!-- U plane -->\n            <dim>1</dim>\n            <dim>240</dim>\n            <dim>320</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"2\"> \t<!-- V plane -->\n          <dim>1</dim>\n          <dim>240</dim>\n          <dim>320</dim>\n          <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>480</dim>\n            <dim>640</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Reshape Tensor with Dimension Calculation in OpenVINO XML\nDESCRIPTION: This XML snippet demonstrates reshaping a tensor while preserving the first dimension, calculating the second, and fixing the third dimension using the Reshape operation in OpenVINO. The 'special_zero' attribute is set to 'true', enabling dimension copying from the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/reshape-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Reshape\" ...>\n    <data special_zero=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>5</dim>\n            <dim>5</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>   <!--The tensor contains 3 elements: 0, -1, 4 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>2</dim>\n            <dim>150</dim>\n            <dim>4</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name and Dependencies in CMake\nDESCRIPTION: This snippet defines the target name for the functional tests and initializes the list of dependencies. Additional dependencies are added based on enabled features like IR frontend, HETERO, AUTO and AUTO_BATCH plugins.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/tests/functional/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_inference_functional_tests)\n\nif(SUGGEST_OVERRIDE_SUPPORTED)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-suggest-override\")\nendif()\n\nset(DEPENDENCIES\n    mock_engine\n    openvino_template_extension\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Target and README with CMake\nDESCRIPTION: This snippet installs the 'compile_tool' executable and its README file (if it exists). It specifies the destination directory, component, and exclusion flags for the installation.  The destination is under tools/compile_tool and it uses the npu internal component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/compile_tool/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n        RUNTIME DESTINATION \"tools/${TARGET_NAME}\"\n        COMPONENT ${NPU_INTERNAL_COMPONENT}\n        ${OV_CPACK_COMP_NPU_INTERNAL_EXCLUDE_ALL})\n\nif(EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/README.md\")\n    install(FILES \"${CMAKE_CURRENT_SOURCE_DIR}/README.md\"\n            DESTINATION \"tools/${TARGET_NAME}\"\n            COMPONENT ${NPU_INTERNAL_COMPONENT}\n            ${OV_CPACK_COMP_NPU_INTERNAL_EXCLUDE_ALL})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries\nDESCRIPTION: This snippet links the target '${TARGET_NAME}' with several libraries, including 'openvino_intel_gpu_graph', 'OpenCL::OpenCL', 'gtest', 'gtest_main', 'gflags', 'common_test_utils', 'openvino::reference', and 'gmock'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino_intel_gpu_graph\n                                             OpenCL::OpenCL\n                                             gtest\n                                             gtest_main\n                                             gflags\n                                             common_test_utils\n                                             openvino::reference\n                                             gmock)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for Common Components\nDESCRIPTION: This snippet adds the 'common' and 'common_translators' subdirectories to the build. These directories likely contain shared code and translator implementations used by multiple frontends.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(common)\n\nadd_subdirectory(common_translators)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing ONNX Models for OpenModelZoo Tests\nDESCRIPTION: This command preprocesses the ONNX models for use in the OpenModelZoo tests. It uses the 'model_zoo_preprocess.sh' script to prepare the models located in the directory specified by '-d <ONNX_MODELS_DIR>'. The '-o' flag specifies that the output should be written to the same directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nmodel_zoo_preprocess.sh -d <ONNX_MODELS_DIR> -o\n```\n\n----------------------------------------\n\nTITLE: Building Target Faster with Unity Builds in CMake\nDESCRIPTION: This snippet utilizes the `ov_build_target_faster` function to optimize the build process for 'ov_snippets_models'. It enables unity builds and specifies a precompiled header file \"src/precomp.hpp\" to reduce compilation time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/ov_helpers/ov_snippets_models/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME}\n    UNITY\n    PCH PRIVATE \"src/precomp.hpp\"\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories to CMake Project\nDESCRIPTION: This CMake code uses the `add_subdirectory` command to include several subdirectories in the project build. These subdirectories represent different components of the OpenVINO test runner, such as conformance infrastructure, API conformance, operation conformance, and subgraph dumping tools. This allows CMake to process the CMakeLists.txt files within each subdirectory and include their targets in the overall build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(test_runner/conformance_infra)\nadd_subdirectory(test_runner/api_conformance_runner)\nadd_subdirectory(test_runner/op_conformance_runner)\nadd_subdirectory(subgraphs_dumper)\nadd_subdirectory(op_conformance_utils)\nadd_subdirectory(subgraphs_dumper/tests)\n```\n\n----------------------------------------\n\nTITLE: DetectionOutput Layer Configuration XML - OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of a DetectionOutput layer in OpenVINO. It showcases various attributes like `background_label_id`, `code_type`, `confidence_threshold`, `keep_top_k`, and `nms_threshold` that control the behavior of the layer. The snippet also shows the input and output port definitions with their dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/detectionoutput-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"DetectionOutput\" ... >\n    <data background_label_id=\"1\" code_type=\"caffe.PriorBoxParameter.CENTER_SIZE\" confidence_threshold=\"0.019999999552965164\" input_height=\"1\" input_width=\"1\" keep_top_k=\"200\" nms_threshold=\"0.44999998807907104\" normalized=\"true\" num_classes=\"2\" share_location=\"true\" top_k=\"200\" variance_encoded_in_target=\"false\" clip_after_nms=\"false\" clip_before_nms=\"false\" objectness_score=\"0\" decrease_label_id=\"false\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>5376</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>2688</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>2</dim>\n            <dim>5376</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>200</dim>\n            <dim>7</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Add Tests Subdirectory (CMake)\nDESCRIPTION: This snippet conditionally adds the tests subdirectory if ENABLE_TESTS is set.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configure Backend-Specific Tests\nDESCRIPTION: This loop iterates through the active backends (CPU, GPU, INTERPRETER), sets up the source manifest, renames it, and configures backend-specific test source files. It copies the manifest to the output directory and configures the test source files by replacing variables like ${BACKEND_NAME}. Finally, it appends the processed source files to the SRC variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(BACKEND_NAME IN LISTS ACTIVE_BACKEND_LIST)\n    string(TOLOWER ${BACKEND_NAME} BACKEND_DIR)\n    string(REGEX REPLACE \"([a-z0-9]+):(.*)\" \"\\\\1\" BACKEND_DIR ${BACKEND_DIR})\n    set(SRC_MANIFEST ${CMAKE_CURRENT_SOURCE_DIR}/runtime/${BACKEND_DIR}/unit_test.manifest)\n    set(MANIFEST_RENAME \"${BACKEND_DIR}_unit_test.manifest\")\n    set(MANIFEST \"${TEST_MODEL_ZOO}/onnx/${MANIFEST_RENAME}\")\n\n    list(APPEND custom_commands\n         COMMAND ${CMAKE_COMMAND} -E copy \"${SRC_MANIFEST}\"\n                                          \"${TEST_MODEL_ZOO_OUTPUT_DIR}/onnx/${MANIFEST_RENAME}\")\n\n    foreach(TEST_SRC IN LISTS MULTI_TEST_SRC)\n        string(REPLACE \":\" \"_\" BACKEND_NAME ${BACKEND_NAME})\n        string(REPLACE \".in.\" \"_${BACKEND_NAME}.\" TARGET_NAME ${TEST_SRC})\n        configure_file(${TEST_SRC} ${TARGET_NAME})\n        set(SRC ${CMAKE_CURRENT_BINARY_DIR}/${TARGET_NAME} ${SRC})\n    endforeach()\n\n    message(STATUS \"Adding unit test for backend ${BACKEND_NAME}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Layer Configuration (Example 1)\nDESCRIPTION: Defines the ScatterElementsUpdate layer's input and output ports, specifying the dimensions for each port. The input ports include 'values', 'indices', and 'updates'. The output port defines the precision as I32.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"ScatterElementsUpdate\">\n        <input>\n            <port id=\"0\">\n                <dim>3</dim>\n                <dim>4</dim>  <!-- values: [[2, 2, 2, 2],\n                                             [2, 2, 2, 2],\n                                             [2, 2, 2, 2]] -->\n            </port>\n            <port id=\"1\">  <!-- indices -->\n                <dim>2</dim>\n                <dim>2</dim>  <!-- values: [[1, 1],\n                                             [0, 3]] -->\n            </port>\n            <port id=\"2\">  <!-- updates -->\n                <dim>2</dim>\n                <dim>2</dim>  <!-- values: [[11, 12],\n                                             [13, 14]]) -->\n            </port>\n            <port id=\"3\">     <!-- values: [1] -->\n                <dim>1</dim>\n            </port>\n        </input>\n        <output>\n            <port id=\"4\" precision=\"I32\">\n                <dim>3</dim>\n                <dim>4</dim>  <!-- values: [[  2, 264,   2,   2],\n                                             [ 26,   2,   2,  28],\n                                             [  2,   2,   2,   2]] -->\n            </port>\n        </output>\n    </layer>\n```\n\n----------------------------------------\n\nTITLE: infer_postprocess() Method C++\nDESCRIPTION: This code snippet shows the implementation of the `infer_postprocess()` method. It converts backend-specific tensors back to tensors passed by the user, ensuring that the output data is in a format that the user can readily consume.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/synch-inference-request.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nvoid InferRequest::infer_postprocess() {\n    // Converts backend specific tensors to tensors passed by user\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Memory Statistics Path to CSV file\nDESCRIPTION: This environment variable dumps memory usage statistics to a *.csv file when the compiled model is destructed. The filename includes the model name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nOV_CPU_MEMORY_STATISTICS_PATH=<file_path>.csv\n```\n\n----------------------------------------\n\nTITLE: DeformableConvolution Layer XML Example\nDESCRIPTION: This XML snippet demonstrates the configuration of a DeformableConvolution layer in OpenVINO. It includes the layer type, data attributes such as dilations, pads, strides, auto_pad, group, and deformable_group, as well as input and output port specifications including dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/deformable-convolution-8.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"DeformableConvolution\" ...>\n    <data dilations=\"1,1\" pads_begin=\"0,0\" pads_end=\"0,0\" strides=\"1,1\" auto_pad=\"explicit\" group=\"1\" deformable_group=\"1\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>50</dim>\n            <dim>220</dim>\n            <dim>220</dim>\n        </port>\n        <port id=\"2\">\n            <dim>64</dim>\n            <dim>4</dim>\n            <dim>5</dim>\n            <dim>5</dim>\n        </port>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>25</dim>\n            <dim>220</dim>\n            <dim>220</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"4\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>220</dim>\n            <dim>220</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Shape Prediction Usage Example C++\nDESCRIPTION: This code snippet demonstrates how to use the ShapePredictor to predict the shape for input/output preallocation. It calls `predict_preallocation_shape` to get a predicted shape, then calls `can_preallocate` to check if the predicted shape's memory can be allocated. If both are successful, it returns the predicted shape; otherwise, it returns the current shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/dynamic_shape/memory_preallocation.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nov::Shape predict_shape(const std::string& name,\n                        const ov::Shape current_shape,\n                        ov::element::Type element_type,\n                        cldnn::ShapePredictor& shape_predictor) {\n    // Request prediction for `current_shape` and `element_type` data type\n    auto prealloc_info = shape_predictor.predict_preallocation_shape(name, current_shape, element_type.bitwidth(), false);\n    const auto& preallocation_shape = prealloc_info.second;\n    // Check if shape was successfully predicted and there is enough free memory for preallocation\n    auto can_preallocate_buffer = prealloc_info.first &&\n                                  shape_predictor.can_preallocate(cldnn::ceil_div(ov::shape_size(preallocation_shape) * element_type.bitwidth(), 8));\n    if (can_preallocate_buffer) {\n        return preallocation_shape;\n    }\n\n    return current_shape;\n}\n```\n\n----------------------------------------\n\nTITLE: Chat with Vision Language Model in C++\nDESCRIPTION: This C++ code provides a basic setup for performing inference with a Vision Language Model (VLM) using OpenVINO GenAI.  It includes the necessary header files and checks for the correct number of command-line arguments (model directory and an input). The code is incomplete as it only defines a main function with argument parsing and an empty try block. It requires implementation details for image loading, VLM pipeline initialization, and text generation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n#include \"load_image.hpp\"\n#include <openvino/genai/visual_language/pipeline.hpp>\n#include <filesystem>\n\n            bool print_subword(std::string&& subword) {\n                return !(std::cout << subword << std::flush);\n            }\n\n            int main(int argc, char* argv[]) try {\n                if (3 != argc) {\n```\n\n----------------------------------------\n\nTITLE: Squeeze 1D Tensor to 0D Tensor (Constant) using OpenVINO XML\nDESCRIPTION: This example shows how to squeeze a 1D tensor with a single element into a 0D tensor (constant). The 'allow_axis_skip' attribute is false. Input port 0 has dimension 1, and port 1 specifies that axis 0 is to be squeezed, resulting in a 0D tensor as output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/squeeze-15.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Squeeze\" version=\"opset15\">\n    <data allow_axis_skip=\"false\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n        </port>\n    </input>\n    <input>\n        <port id=\"1\">\n            <dim>1</dim>  <!-- value is [0] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Glob Source and Header Files (CMake)\nDESCRIPTION: This snippet uses file(GLOB) to find all C++ source and header files in the src directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/src/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Get Tensor by Name or Output Port TypeScript\nDESCRIPTION: Retrieves an input or output tensor from the InferRequest by name or output port. If a tensor with the specified name or port is not found, an exception is thrown. Returns a Tensor object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ngetTensor(nameOrOutput: string | Output): Tensor;\n```\n\n----------------------------------------\n\nTITLE: Optimizing Build Speed with UNITY\nDESCRIPTION: This snippet uses the `ov_build_target_faster` macro to optimize the build speed of the `ov_op_conformance_tests` target by employing the UNITY build system. This allows for faster compilation times by combining multiple source files into a single compilation unit.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/op_conformance_runner/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME} UNITY)\n```\n\n----------------------------------------\n\nTITLE: BitwiseXor No Broadcast Example in XML\nDESCRIPTION: Shows an example of the BitwiseXor layer in XML format with no broadcasting.  Both input ports have the same dimensions, ensuring element-wise operation without broadcasting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-xor-13.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"BitwiseXor\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO GenAI from Archive - ARM 64-bit\nDESCRIPTION: These commands download and extract the OpenVINO GenAI archive for ARM 64-bit systems.  The `curl` command downloads the tarball, and the `tar` command extracts its contents.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-genai.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino_genai/packages/2025.1/linux/openvino_genai_ubuntu20_2025.1.0.0_arm64.tar.gz -O openvino_genai_2025.1.0.0.tgz\ntar -xf openvino_genai_2025.1.0.0.tgz\n```\n\n----------------------------------------\n\nTITLE: Gather-7 Example with batch_dims=1 in shell\nDESCRIPTION: Illustrates the Gather-7 operation with batch_dims set to 1 and axis to 1. This example shows how the indices are applied to each batch in the data tensor to produce the corresponding output batch.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-7.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 1\naxis = 1\n\nindices = [[0, 0, 4], <-- this is applied to the first batch\n           [4, 0, 0]]  <-- this is applied to the second batch\nindices_shape = (2, 3)\n\ndata    = [[1, 2, 3, 4, 5],  <-- the first batch\n           [6, 7, 8, 9, 10]]  <-- the second batch\ndata_shape = (2, 5)\n\noutput  = [[ 1, 1, 5],\n           [10, 6, 6]]\noutput_shape = (2, 3)\n```\n\n----------------------------------------\n\nTITLE: Upgrade PIP and Install Requirements (Azure ML)\nDESCRIPTION: This command upgrades pip and installs the required packages from `requirements.txt`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_22\n\nLANGUAGE: console\nCODE:\n```\npython -m pip install --upgrade pip\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Compiler Version Check (GCC)\nDESCRIPTION: This snippet checks the GCC compiler version and exits if it's less than 7.0. It skips the build because the JS API isn't supported with older GCC versions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX AND LINUX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.0)\n    message(WARNING \"JS API is not support gcc compiler version less than 7, skipping\")\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: RNNCell Layer Configuration (XML)\nDESCRIPTION: This XML snippet demonstrates the layer configuration for an RNNCell-3 operation within the OpenVINO framework. It specifies the hidden size, input port dimensions (batch size, input size, hidden size), and output port dimensions. This XML configuration defines how the RNNCell layer is structured and connected within a larger model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/rnn-cell-3.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"RNNCell\" ...>\n       <data hidden_size=\"128\"/>\n       <input>\n           <port id=\"0\">\n               <dim>1</dim>\n               <dim>16</dim>\n           </port>\n           <port id=\"1\">\n               <dim>1</dim>\n               <dim>128</dim>\n           </port>\n           <port id=\"2\">\n               <dim>128</dim>\n               <dim>16</dim>\n           </port>\n           <port id=\"3\">\n               <dim>128</dim>\n               <dim>128</dim>\n           </port>\n           <port id=\"4\">\n               <dim>128</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">\n               <dim>1</dim>\n               <dim>128</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Downloading and Installing OpenVINO (RHEL 8)\nDESCRIPTION: These commands download the OpenVINO Runtime archive for RHEL 8, extract it, and move the extracted directory to `/opt/intel`. It uses `curl` to download the archive, `tar` to extract it, and `sudo mv` to move the extracted folder with root privileges.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino/packages/2025.1/linux/openvino_toolkit_rhel8_2025.1.0.18503.6fec06580ab_x86_64.tgz --output openvino_2025.1.0.tgz\ntar -xf openvino_2025.1.0.tgz\nsudo mv openvino_toolkit_rhel8_2025.1.0.18503.6fec06580ab_x86_64 /opt/intel/openvino_2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Copy libraries and model to Android device and run benchmark\nDESCRIPTION: Copies OpenVINO libraries, OneTBB libraries, the shared STL library, the example model, and the OpenVINO benchmark_app tool to the /data/local/tmp/ directory on the Android device using ADB. Finally, it executes the benchmark_app tool on the device to test the OpenVINO installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_android.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n# Copy OpenVINO™ libraries to android device\n$ANDROID_TOOLS_PATH/adb push --sync $OPV_HOME_DIR/openvino-install/runtime/lib/aarch64/* /data/local/tmp/\n# Copy OneTBB libraries to android device\n$ANDROID_TOOLS_PATH/adb push --sync $OPV_HOME_DIR/one-tbb-install/lib/* /data/local/tmp/\n# Copy shared STL library to android device\n$ANDROID_TOOLS_PATH/adb push --sync $ANDROID_NDK_PATH/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android/libc++_shared.so /data/local/tmp/\n# Copy example model files\n$ANDROID_TOOLS_PATH/adb push --sync $OPV_HOME_DIR/mobelinet-v3-tf /data/local/tmp/\n# Copy OpenVINO™ benchmark_app tool to android device\n$ANDROID_TOOLS_PATH/adb push --sync $OPV_HOME_DIR/openvino/bin/aarch64/Release/benchmark_app /data/local/tmp/\n# Run OpenVINO™ benchmark_app tool\n$ANDROID_TOOLS_PATH/adb shell \"LD_LIBRARY_PATH=/data/local/tmp ./data/local/tmp/benchmark_app -m /data/local/tmp/mobelinet-v3-tf/v3-small_224_1.0_float.xml -hint latency\"\n```\n\n----------------------------------------\n\nTITLE: Conditional Proxy Build Enablement in CMake\nDESCRIPTION: This snippet checks if the `ENABLE_PROXY` option is enabled. If not, it immediately returns, skipping the rest of the configuration for the proxy plugin.  This allows for conditional compilation of the proxy plugin during the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT ENABLE_PROXY)\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Globbing Natvis Files for MSVC\nDESCRIPTION: This snippet uses file(GLOB) to find '.natvis' files in the current source directory when using MSVC.  These files are used for debugging with Visual Studio.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif (MSVC)\n  file(GLOB SOURCES_NATVIS\n    \"${CMAKE_CURRENT_SOURCE_DIR}/float16.natvis\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuration for Disabling CPU Fallback in AUTO plugin (JSON)\nDESCRIPTION: This JSON snippet shows the configuration to disable CPU fallback at startup for the AUTO plugin using the `ENABLE_STARTUP_FALLBACK` property set to `NO`. This will prevent the CPU from being used as a helper device at the beginning.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/tests.md#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"AUTO\": {\n            \"ENABLE_STARTUP_FALLBACK\": \"NO\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Clone OpenVINO Repository\nDESCRIPTION: Clones the OpenVINO repository from GitHub with submodules and a specific branch (master).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_raspbian.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --recurse-submodules --single-branch --branch=master https://github.com/openvinotoolkit/openvino.git \n```\n\n----------------------------------------\n\nTITLE: Setting NPU Build Properties CMake\nDESCRIPTION: This snippet sets the build properties for the NPU plugin, including the device name, plugin component name (lowercased), and internal component name. It also sets the NPU_PLUGIN_SOURCE_DIR variable to the current source directory. These properties are used throughout the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(NPU_DEVICE_NAME \"NPU\")\nstring(TOLOWER \"${NPU_DEVICE_NAME}\" NPU_PLUGIN_COMPONENT)\nset(NPU_INTERNAL_COMPONENT \"${NPU_PLUGIN_COMPONENT}_internal\")\n\nset(NPU_PLUGIN_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Protopipe Build Inside OpenVINO\nDESCRIPTION: These commands build Protopipe as part of the OpenVINO build process. It assumes you are inside the OpenVINO source tree, creates a build directory for Protopipe, configures CMake to use the specified OpenCV installation, and then builds the `protopipe` target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_23\n\nLANGUAGE: cmake\nCODE:\n```\ncd <OpenVINO-root-dir>\nmkdir \"src/plugins/intel_npu/tools/protopipe/build\" && cd \"src/plugins/intel_npu/tools/protopipe/build\"\ncmake ../ -DOpenCV_DIR=<opencv-build-dir> -DCMAKE_BUILD_TYPE=Release\ncmake --build . --config Release --target protopipe --parallel\n```\n\n----------------------------------------\n\nTITLE: Class Diagram: JIT Emitters Architecture\nDESCRIPTION: This diagram illustrates the class relationships involved in the JIT emitter architecture. It shows how jit_kernel interacts with jit_operation_emitter, which inherits from jit_emitter, which in turn inherits from Emitter.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/src/emitters/README.md#_snippet_0\n\nLANGUAGE: Mermaid\nCODE:\n```\nclassDiagram\njit_kernel *-- jit_operation_emitter\njit_operation_emitter --|> jit_emitter\njit_emitter --|> Emitter\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Artifact Usage Example in YAML\nDESCRIPTION: This YAML snippet illustrates how to use the outputs from the OpenVINO provider action to download and install the OpenVINO Python wheel.  It uses the `actions/download-artifact` action to retrieve the artifacts and then uses `pip` to install the OpenVINO package with the correct version and wheel source obtained from the provider action's outputs.  This example shows usage within a separate job that depends on the first.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/openvino_provider.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nopenvino_tokenizers_tests:\n  name: OpenVINO tokenizers tests\n  needs: [ openvino_download ]\n  defaults:\n    run:\n      shell: bash\n  runs-on: ubuntu-22.04\n    steps:\n    ...\n    - name: Download OpenVINO package\n      uses: actions/download-artifact@95815c38cf2ff2164869cbab79da8d1f422bc89e # v4.2.1\n      with:\n        name: ${{ needs.openvino_download.outputs.ov_artifact_name }}\n        path: ${{ env.INSTALL_DIR }}\n        merge-multiple: true\n\n    - name: Install OpenVINO Python wheel from pre-built artifacts\n      run: |\n        python3 -m pip install openvino==${{ needs.openvino_download.outputs.ov_version }} ${{ needs.openvino_download.outputs.ov_wheel_source }}\n      working-directory: ${{ env.INSTALL_DIR }}\n```\n\n----------------------------------------\n\nTITLE: Slicing 1D Tensor XML\nDESCRIPTION: This XML snippet defines a Slice layer that extracts a portion of a 1D tensor. It specifies the start, stop, step, and axes for the slicing operation. The input data has a dimension of 10, and the output is also a 1D tensor with a dimension of 10 after the slicing operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-8.rst#_snippet_8\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Slice\" ...>\n       <input>\n           <port id=\"0\">       <!-- data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -->\n               <dim>10</dim>\n           </port>\n           <port id=\"1\">       <!-- start: [100] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"2\">       <!-- stop: [-100] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"3\">       <!-- step: [-1] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"4\">       <!-- axes: [0] -->\n             <dim>1</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">       <!-- output: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] -->\n               <dim>10</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Defining Edges between Layers XML\nDESCRIPTION: This XML snippet defines the edges, or connections, between layers in the OpenVINO model.  The `from-layer`, `from-port`, `to-layer`, and `to-port` attributes specify the source and destination of the data flow, connecting different layers for computation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/tensor-iterator-1.rst#_snippet_8\n\nLANGUAGE: xml\nCODE:\n```\n<edges>\n    <edge from-layer=\"0\" from-port=\"0\" to-layer=\"2\" to-port=\"0\"/>\n    <edge from-layer=\"1\" from-port=\"1\" to-layer=\"2\" to-port=\"1\"/>\n    <edge from-layer=\"2\" from-port=\"2\" to-layer=\"7\" to-port=\"0\"/>\n    <edge from-layer=\"3\" from-port=\"0\" to-layer=\"7\" to-port=\"1\"/>\n    <edge from-layer=\"4\" from-port=\"0\" to-layer=\"7\" to-port=\"2\"/>\n    <edge from-layer=\"5\" from-port=\"1\" to-layer=\"7\" to-port=\"3\"/>\n    <edge from-layer=\"6\" from-port=\"1\" to-layer=\"7\" to-port=\"4\"/>\n    <edge from-layer=\"7\" from-port=\"6\" to-layer=\"8\" to-port=\"0\"/>\n    <edge from-layer=\"7\" from-port=\"5\" to-layer=\"9\" to-port=\"0\"/>\n    <edge from-layer=\"7\" from-port=\"5\" to-layer=\"11\" to-port=\"0\"/>\n    <edge from-layer=\"10\" from-port=\"1\" to-layer=\"11\" to-port=\"1\"/>\n    <edge from-layer=\"11\" from-port=\"2\" to-layer=\"12\" to-port=\"0\"/>\n</edges>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Compile Options and Threading Interface\nDESCRIPTION: Applies the common compile options to the target and sets the threading interface using `ov_set_threading_interface_for`. This configures the threading model used by the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_options(${TARGET_NAME} PRIVATE ${COMMON_COMPILE_OPTIONS})\n\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Installing Directory\nDESCRIPTION: Installs the directory `${OpenVINOPython_SOURCE_DIR}/src/openvino/test_utils` to the destination `tests/${OV_CPACK_PYTHONDIR}/openvino`. It is installed under the tests component and excluded from all by default.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY ${OpenVINOPython_SOURCE_DIR}/src/openvino/test_utils\n        DESTINATION tests/${OV_CPACK_PYTHONDIR}/openvino\n        COMPONENT tests\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Run Bert Benchmark Script\nDESCRIPTION: This command executes the `bert_benchmark.py` script, initiating the BERT model benchmarking process. The script will load the model, prepare the dataset, and run the inference benchmark using OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/bert-benchmark.rst#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\npython bert_benchmark.py\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenVINO Repository and Submodules\nDESCRIPTION: This snippet clones the OpenVINO repository from GitHub and initializes all submodules recursively. It is the first step in the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_windows.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/openvinotoolkit/openvino.git\ncd openvino\ngit submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Validating the Subgraphs Dumper tool with tests\nDESCRIPTION: These commands configure and run unit tests for the `ov_subgraphs_dumper` tool. The `gtest_filter` argument allows specifying which tests to run.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/subgraphs_dumper/README.md#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -DENABLE_FUNCTIONAL_TESTS=ON .\nmake ov_subgraphs_dumper_tests\n./ov_subgraphs_dumper_tests --gtest_filter=*\n```\n\n----------------------------------------\n\nTITLE: tests/functional/CMakeLists.txt for Plugin Tests\nDESCRIPTION: This CMake snippet shows how to build functional tests for the plugin. It links the plugin tests against common functional test utilities provided by the OpenVINO Developer Package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/build-plugin-using-cmake.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nadd_executable(${PROJECT_NAME}\n        ${CMAKE_CURRENT_SOURCE_DIR}/template_plugin.cpp)\n\ntarget_link_libraries(${PROJECT_NAME} PRIVATE\n        openvino::funcSharedTests\n        openvino::gtest\n        openvino::gtest_main)\n```\n\n----------------------------------------\n\nTITLE: Defining Runner via 'with' Parameter in YAML\nDESCRIPTION: This code demonstrates how to define a runner using the `with` parameter in a YAML workflow file. The example shows the C++ unit tests job using the `aks-linux-4-cores-16gb` runner, configured via the `runner` key within the `with` section.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/runners.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nCXX_Unit_Tests:\n  name: C++ unit tests\n  ...\n  with:\n    runner: aks-linux-4-cores-16gb\n    ...\n```\n\n----------------------------------------\n\nTITLE: Run Specific TensorFlow 1 Layer Test\nDESCRIPTION: This command runs a specific TensorFlow 1 layer test (in this case, `test_tf_Unique.py`) using py.test. This allows for focused testing of individual TensorFlow operations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npy.test tensorflow_tests/test_tf_Unique.py\n```\n\n----------------------------------------\n\nTITLE: NonMaxSuppression Layer Configuration in C++\nDESCRIPTION: This C++ code snippet demonstrates the configuration of a NonMaxSuppression layer within a model. It defines the layer's attributes (box_encoding, sort_result_descending, output_type), input ports (boxes, scores, max_output_boxes_per_class, iou_threshold, score_threshold), and output ports (selected_indices, selected_scores, valid_outputs).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/no-max-suppression-5.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"NonMaxSuppression\" ... >\n    <data box_encoding=\"corner\" sort_result_descending=\"1\" output_type=\"i64\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>100</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>\n            <dim>5</dim>\n            <dim>100</dim>\n        </port>\n        <port id=\"2\"></port> <!-- 10 -->\n        <port id=\"3\"></port>\n        <port id=\"4\"></port>\n    </input>\n    <output>\n        <port id=\"5\" precision=\"I64\">\n            <dim>150</dim> <!-- min(100, 10) * 3 * 5 -->\n            <dim>3</dim>\n        </port>\n        <port id=\"6\" precision=\"FP32\">\n            <dim>150</dim> <!-- min(100, 10) * 3 * 5 -->\n            <dim>3</dim>\n        </port>\n        <port id=\"7\" precision=\"I64\">\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Compile Definitions to Target in CMake\nDESCRIPTION: Adds the compile definitions from the `DEFINES` list to the `ov_capi_test` target. These definitions control various aspects of the compilation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/tests/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME} PRIVATE ${DEFINES})\n```\n\n----------------------------------------\n\nTITLE: Example Model Creation Sample (Python)\nDESCRIPTION: This console command provides an example of running the `model_creation_sample.py` script with the `lenet.bin` weights file and the `GPU` device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/model-creation.rst#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\npython model_creation_sample.py lenet.bin GPU\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Edge Mode Example (Positive Pads) - C++\nDESCRIPTION: Demonstrates padding with the edge mode, where new elements are copied from the respective edge of the input data tensor. pads_begin and pads_end define the padding amounts for each dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\npads_begin = [0, 1]\npads_end = [2, 3]\n\nDATA =\n[[1,  2,  3,  4]\n[5,  6,  7,  8]\n[9, 10, 11, 12]]\n\npad_mode = \"edge\"\n\nOUTPUT =\n    [[ 1,  1,  2,  3,  4,  4,  4,  4 ]\n    [ 5,  5,  6,  7,  8,  8,  8,  8 ]\n    [ 9,  9, 10, 11, 12, 12, 12, 12 ]\n    [ 9,  9, 10, 11, 12, 12, 12, 12 ]\n    [ 9,  9, 10, 11, 12, 12, 12, 12 ]]\n```\n\n----------------------------------------\n\nTITLE: Custom Operation Mapping (C++)\nDESCRIPTION: Demonstrates mapping a custom operation directly, assuming attribute names match in both the framework and OpenVINO representations.  This simplifies the extension process when attribute names and types are consistent.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/frontend-extensions.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nCore core;\ncore.add_extension(std::make_shared<ov::frontend::OpExtension<CustomOperation>>());\nauto model = core.read_model(\"model.onnx\");\n```\n\n----------------------------------------\n\nTITLE: InferRequest Interface Definition TypeScript\nDESCRIPTION: Defines the InferRequest interface, which provides methods to interact with a compiled OpenVINO model for inference. This includes methods to get the compiled model, get input and output tensors, perform synchronous and asynchronous inference, and set input and output tensors by index or name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface InferRequest {\n    getCompiledModel(): CompiledModel;\n    getInputTensor(): Tensor;\n    getInputTensor(idx): Tensor;\n    getOutputTensor(): Tensor;\n    getOutputTensor(idx?): Tensor;\n    getTensor(nameOrOutput): Tensor;\n    infer(): {\n        [outputName: string]: Tensor;\n    };\n    infer(inputData): {\n        [outputName: string]: Tensor;\n    };\n    infer(inputData): {\n        [outputName: string]: Tensor;\n    };\n    inferAsync(inputData): Promise<{ [outputName: string]: Tensor }>;\n    setInputTensor(tensor): void;\n    setInputTensor(idx, tensor): void;\n    setOutputTensor(tensor): void;\n    setOutputTensor(idx, tensor): void;\n    setTensor(name, tensor): void;\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Output Name Based on Target Architecture\nDESCRIPTION: This snippet sets the output name of the target based on the target platform architecture. If the architecture is ARM or AARCH64, the output name is set to \"openvino_arm_cpu_plugin\". If the architecture is RISCV64, the output name is set to \"openvino_riscv_cpu_plugin\".\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_28\n\nLANGUAGE: cmake\nCODE:\n```\nif(ARM OR AARCH64)\n    set_target_properties(${TARGET_NAME} PROPERTIES OUTPUT_NAME \"openvino_arm_cpu_plugin\")\nelif(RISCV64)\n    set_target_properties(${TARGET_NAME} PROPERTIES OUTPUT_NAME \"openvino_riscv_cpu_plugin\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for JAX Frontend (CMake)\nDESCRIPTION: This snippet conditionally adds the 'jax' subdirectory to the build if the ENABLE_OV_JAX_FRONTEND CMake option is enabled. This includes the JAX frontend functionality.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_OV_JAX_FRONTEND)\n    add_subdirectory(jax)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding User to Video and Render Groups (Linux)\nDESCRIPTION: These commands add the current Linux user to the `video` and `render` groups, granting necessary permissions to work with the GPU device.  The specific group required may vary depending on the Linux distribution. This command uses `sudo` to elevate privileges and `usermod` to modify group memberships.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_driver_troubleshooting.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo usermod -a -G video \"$(whoami)\"\nsudo usermod -a -G render \"$(whoami)\"\n```\n\n----------------------------------------\n\nTITLE: MatMul Matrix-Matrix Multiplication (FullyConnected) XML Configuration\nDESCRIPTION: Configuration example demonstrating MatMul operation for matrix-matrix multiplication with batch size 1, similar to a FullyConnected layer. The input ports define the dimensions of the input matrices (1x1024 and 1024x1000). The output port defines the resulting matrix (1x1000).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/matmul-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MatMul\">\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>1024</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1024</dim>\n            <dim>1000</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>1000</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Include OpenVINO Op in Python\nDESCRIPTION: This snippet shows how to include the OpenVINO Op class in Python, which is required for creating custom operations. It imports the necessary modules from the openvino library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-openvino-operations.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.runtime import Model, op\nfrom openvino.runtime.op import Constant\nfrom openvino.runtime.exceptions import UserInputError\n```\n\n----------------------------------------\n\nTITLE: Compiling C++ code with cnpy\nDESCRIPTION: This command compiles a C++ source file (mycode.cpp) that uses the cnpy library. It links the cnpy library and the zlib library (required by cnpy). The `-L` flag specifies the path to the directory where the cnpy library is installed, and the `-lcnpy` flag links the cnpy library. The `--std=c++11` flag ensures that the code is compiled using the C++11 standard.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/cnpy/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ng++ -o mycode mycode.cpp -L/path/to/install/dir -lcnpy -lz --std=c++11\n```\n\n----------------------------------------\n\nTITLE: Col2Im Example with Default Parameters (XML)\nDESCRIPTION: This example demonstrates the Col2Im operation with default parameters. It showcases the input and output port configurations, including dimensions for batch size, channels, and output size. The kernel size is also specified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/col2im-15.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Col2Im\" ... >\n    <input>\n        <port id=\"0\" precision=\"I32\">\n            <dim>3</dim>     <!-- batch_axis -->\n            <dim>12</dim>    <!-- C * Product(kernel_size) -->\n            <dim>225</dim>   <!-- L -->\n        </port>\n        <port id=\"1\" precision=\"I32\">\n            <dim>2</dim>     <!-- output_size -->\n        </port>\n        <port id=\"2\" precision=\"I32\">\n            <dim>2</dim>     <!-- kernel_size -->\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"I32\">\n            <dim>3</dim>     <!-- batch_axis -->\n            <dim>3</dim>     <!-- C -->\n            <dim>16</dim>    <!-- output_size[0] -->\n            <dim>16</dim>    <!-- output_size[1] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: AvgPool Layer Configuration with explicit auto_pad\nDESCRIPTION: This XML snippet demonstrates the configuration of an AvgPool layer using explicit padding (`auto_pad=\"explicit\"`). The `kernel` size is 5x5, `pads_begin` and `pads_end` are set to 1x1, and the stride is 3x3.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/avg-pool-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"AvgPool\" ... >\n    <data auto_pad=\"explicit\" exclude-pad=\"true\" kernel=\"5,5\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"3,3\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate 3D Tensor Update - C++ (axis=0)\nDESCRIPTION: This code snippet shows how the ScatterElementsUpdate operation updates a 3D tensor when axis is set to 0. It demonstrates the element update logic based on the indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\noutput[indices[i][j][k]][j][k] = reduction(updates[i][j][k], output[indices[i][j][k]][j][k]) if axis = 0\n```\n\n----------------------------------------\n\nTITLE: Enable Pattern Matching Logging\nDESCRIPTION: This snippet demonstrates how to enable pattern matching logging by setting the `OV_MATCHER_LOGGING` environment variable to `true` before executing a program. It enables the basic logging output during the pattern matching process, allowing for observation and debugging.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/docs/debug_capabilities/matcher_logging.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOV_MATCHER_LOGGING=true ./your_amazing_program\n```\n\n----------------------------------------\n\nTITLE: Setting pybind11 Minimum Version and Finding Package\nDESCRIPTION: This snippet sets the minimum required version of pybind11 and attempts to find the package. If pybind11 is not found, it adds the pybind11 subdirectory from the thirdparty directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CROSSCOMPILING)\n    set(pybind11_min_version 2.12.0)\nelse()\n    set(pybind11_min_version 2.12.0)\nendif()\n# search for FindPython3.cmake instead of legacy modules\nset(PYBIND11_FINDPYTHON ON)\n\nov_find_python3(REQUIRED)\nfind_package(pybind11 ${pybind11_min_version} QUIET)\n\nif(NOT pybind11_FOUND)\n    add_subdirectory(thirdparty/pybind11 EXCLUDE_FROM_ALL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Executable Target in CMake\nDESCRIPTION: Installs the `ov_capi_test` executable to the `tests` directory. This specifies where the executable should be placed after building. The target is also excluded from the 'all' target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/tests/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n    RUNTIME DESTINATION tests\n    COMPONENT tests\n    EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Unique Operation with Axis Input - XML Configuration\nDESCRIPTION: This XML snippet demonstrates the Unique operation with an axis input connected to a constant containing zero. The operation finds unique elements along the specified axis. The `sorted` attribute is set to false, and the `index_element_type` is set to i32.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/unique-10.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Unique\" ... >\n    <data sorted=\"false\" index_element_type=\"i32\"/>\n    <input>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <input>\n        <port id=\"1\" precision=\"I64\">\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>-1</dim>\n            <dim>3</dim>\n        </port>\n        <port id=\"3\" precision=\"I32\">\n            <dim>-1</dim>\n        </port>\n        <port id=\"4\" precision=\"I32\">\n            <dim>3</dim>\n        </port>\n        <port id=\"5\" precision=\"I64\">\n            <dim>-1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Set Up OpenVINO YUM Repository\nDESCRIPTION: This snippet creates a YUM repository file for OpenVINO in the /tmp directory, then moves it to the YUM configuration directory (/etc/yum.repos.d). It uses `tee` to create the file and `sudo mv` to move it to the correct location.  Requires sudo privileges for moving the file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ntee > /tmp/openvino.repo << EOF\n[OpenVINO]\nname=Intel(R) Distribution of OpenVINO\nbaseurl=https://yum.repos.intel.com/openvino\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://yum.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB\nEOF\n```\n\n----------------------------------------\n\nTITLE: MVN Operation Parameters Definition in C++\nDESCRIPTION: This code defines the `mvn_params` structure, which inherits from `base_params`, to hold the parameters specific to the MVN operation. It includes parameters such as `mvnMode`, `mvnNormalizeVariance`, and `epsilon`. The `GetParamsKey()` method generates a key based on these parameters for quick kernel applicability checks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_kernels.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nstruct mvn_params : public base_params {\n    mvn_params() : base_params(KernelType::MVN) {}\n\n    MVNMode mvnMode = MVNMode::WITHIN_CHANNELS;\n    bool mvnNormalizeVariance = true;\n    float epsilon = 1e-10f;\n\n    virtual ParamsKey GetParamsKey() const {\n        ParamsKey k = base_params::GetParamsKey();\n\n        k.EnableMVNMode(mvnMode);\n\n        if (mvnNormalizeVariance)\n            k.EnableMVNNormalizeVariance();\n\n        return k;\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Inference with OpenVINO (C++)\nDESCRIPTION: This code snippet demonstrates asynchronous inference using `ov::InferRequest::start_async()` and `ov::InferRequest::wait()`. The `NEXT` request is populated in the main application thread while the `CURRENT` request is being processed asynchronously by the inference engine, allowing parallel execution and improved resource utilization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/general-optimizations.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// Main application thread\n{\n    // Capture frame for NEXT request\n    cv::Mat frame = cv::imread(\"image.bmp\");\n    cv::resize(frame, resized_image, cv::Size(width, height), cv::INTER_LINEAR);\n\n    // Set input tensor for NEXT request\n    ov::Tensor input_tensor = ov::Tensor(input_type, input_shape, resized_image.data);\n    next_infer_request.set_input_tensor(input_tensor);\n\n    // Start asynchronous inference for NEXT request\n    next_infer_request.start_async();\n\n    // Wait for CURRENT request to complete\n    current_infer_request.wait();\n\n    // Process output tensor of CURRENT request\n    const ov::Tensor& output_tensor = current_infer_request.get_output_tensor();\n\n    // Swap requests for the next iteration\n    std::swap(current_infer_request, next_infer_request);\n}\n```\n\n----------------------------------------\n\nTITLE: Exporting Developer Package Targets (CMake)\nDESCRIPTION: Exports the targets for the developer package and specifies the install include directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET openvino::reference\n                                    INSTALL_INCLUDE_DIRECTORIES \"${REF_IMPL_INCLUDE_DIR}/\")\n```\n\n----------------------------------------\n\nTITLE: Setting Warnings as Errors for MSVC CMake\nDESCRIPTION: This snippet sets the CMAKE_COMPILE_WARNING_AS_ERROR variable to ON for MSVC compilers, treating warnings as errors. It also removes the /wd4996 flag to disable a specific warning and adds /WX flag when CMake version is less than 3.24. Additionally, it sets the linker flags to treat linker warnings as errors for shared libraries, modules and executables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    set(CMAKE_COMPILE_WARNING_AS_ERROR ON)\n    remove_compiler_flags(/wd4996)\n    if(CMAKE_VERSION VERSION_LESS 3.24)\n        ov_add_compiler_flags(/WX)\n    endif()\n\n    set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} /WX\")\n    set(CMAKE_MODULE_LINKER_FLAGS \"${CMAKE_MODULE_LINKER_FLAGS} /WX\")\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} /WX\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Inference Pipeline Diagram\nDESCRIPTION: This diagram shows the OpenVINO inference pipeline, detailing the interaction between the `Core` object (with `read_model()` and `compile_model()` methods), framework models, OpenVINO models, compiled models (CPU, GPU, AUTO), and InferRequests. It illustrates how models are read, compiled for different devices, and used to create infer requests for running inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/docs/architecture.md#_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TB\n    subgraph core [ov::Core core]\n        read_model[\"core.read_model()\"]\n        compile_model[\"core.compile_model()\"]\n    end\n    fw_model[(Framework model)]\n    ov_model[ov::Model]\n    \n    subgraph compiled_models [Compiled models]\n        cpu_model[\"CPU\"]\n        gpu_model[\"GPU\"]\n        auto_model[\"AUTO\"]\n    end\n    subgraph infer_reguests [InferRequests]\n        cpu_req[\"CPU\"]\n        gpu_req[\"GPU\"]\n        auto_req[\"AUTO\"]\n    end\n   \n    result[Results]\n    \n    fw_model--->read_model\n    read_model--->ov_model\n    ov_model-->compile_model\n    \n    fw_model--->compile_model\n    \n    compile_model--\"compile_model(CPU)\"--->cpu_model\n    compile_model--\"compile_model(GPU)\"--->gpu_model\n    compile_model--\"compile_model(AUTO)\"--->auto_model\n    \n    cpu_model--\"create_infer_request()\"--->cpu_req\n    gpu_model--\"create_infer_request()\"--->gpu_req\n    auto_model--\"create_infer_request()\"--->auto_req\n    \n    cpu_req--\"infer()\"-->result\n    gpu_req--\"infer()\"-->result\n    auto_req--\"infer()\"-->result\n```\n\n----------------------------------------\n\nTITLE: Get Input Size of Model in OpenVINO (C)\nDESCRIPTION: This function is used to get the input size of an OpenVINO model. It requires a pointer to the `ov_model_t` and returns the input size through the `input_size` parameter. The return value indicates the status of the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_30\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_model_inputs_size(const ov_model_t* model, size_t* input_size);\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO\nDESCRIPTION: This command installs the built OpenVINO artifacts to a specified installation location. The `CMAKE_INSTALL_PREFIX` variable sets the destination directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_INSTALL_PREFIX=<installation location> -P cmake_install.cmake\n```\n\n----------------------------------------\n\nTITLE: GELU Layer Definition with tanh Approximation - XML\nDESCRIPTION: This XML snippet defines a Gelu layer in OpenVINO using the 'tanh' approximation mode. It specifies the input and output tensor dimensions for the layer. The 'approximation_mode' attribute within the 'data' element controls the GELU approximation used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/gelu-7.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Gelu\">\n    <data approximation_mode=\"tanh\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: NV12toRGB XML Layer Configuration Example 1\nDESCRIPTION: This XML snippet demonstrates the configuration of an NV12toRGB layer within an OpenVINO model, taking a single-plane NV12 input and producing an RGB output. The input tensor has dimensions 1x720x640x1, and the output tensor has dimensions 1x480x640x3. This configuration assumes the input NV12 image is provided as a single plane.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/nv12-to-rgb-8.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"NV12toRGB\">\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>720</dim>\n            <dim>640</dim>\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>480</dim>\n            <dim>640</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties in CMake\nDESCRIPTION: This CMake snippet sets target properties for the pyopenvino module. It enables interprocedural optimization (IPO) for release builds based on the `ENABLE_LTO` variable and sets the output name of the module to `_pyopenvino`. This ensures the module is optimized and named correctly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${PROJECT_NAME} PROPERTIES\n    INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO}\n    OUTPUT_NAME \"_pyopenvino\")\n```\n\n----------------------------------------\n\nTITLE: Adding Test Target in CMake\nDESCRIPTION: This snippet uses the `ov_add_test_target` macro to define the test target. It specifies the target name, root directory, include directories, linked libraries, and labels. It also enables cpplint.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/api_conformance_runner/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/src\"\n        ADD_CPPLINT\n        INCLUDES\n            PRIVATE\n                \"${CMAKE_CURRENT_SOURCE_DIR}/include\"\n        LINK_LIBRARIES\n            PUBLIC\n                conformance_shared\n        LABELS\n            OV API_CONFORMANCE\n)\n```\n\n----------------------------------------\n\nTITLE: Linking Atomic Library on RISC-V Architecture in CMake\nDESCRIPTION: This snippet conditionally links the atomic library to the target if the RISCV64 architecture is detected. This is required for std::atomic_bool.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(RISCV64)\n    # for std::atomic_bool\n    target_link_libraries(${TARGET_NAME} PRIVATE atomic)\nendif()\n```\n\n----------------------------------------\n\nTITLE: LogicalXor XML Layer Definition (Numpy Broadcast)\nDESCRIPTION: This XML snippet defines a LogicalXor layer with two input ports of different shapes, demonstrating numpy broadcasting. The output port shows the resulting broadcasted shape. This example showcases how the LogicalXor operation handles input tensors with different shapes, automatically adjusting their dimensions using numpy-style broadcasting rules to produce a consistent output shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/logical/logical-xor-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"LogicalXor\">\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Layer Definition (XML)\nDESCRIPTION: This XML code snippet provides an example of how to define a ScatterElementsUpdate layer within an OpenVINO model.  It shows the input and output port configurations, including dimensions, which are necessary for specifying the layer's behavior. This configuration includes defining dimensions for data, indices, updates, and the axis input tensors, as well as the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-3.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ScatterElementsUpdate\">\n    <input>\n        <port id=\"0\">\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>7</dim>\n            <dim>7</dim>\n        </port>\n        <port id=\"1\">\n            <dim>125</dim>\n            <dim>20</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n        </port>\n        <port id=\"2\">\n            <dim>125</dim>\n            <dim>20</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n        </port>\n        <port id=\"3\">     <!-- value [0] -->\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"4\" precision=\"FP32\">\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>7</dim>\n            <dim>7</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions for Unit Test Target in CMake\nDESCRIPTION: This snippet defines compile definitions for the unit test target, including shared library prefixes and suffixes, and frontend library prefixes and suffixes. These definitions are used during the compilation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME}\n    PRIVATE\n        SHARED_LIB_PREFIX=\\\"${CMAKE_SHARED_LIBRARY_PREFIX}\\\"\n        SHARED_LIB_SUFFIX=\\\"${OV_BUILD_POSTFIX}${CMAKE_SHARED_LIBRARY_SUFFIX}\\\"\n        FRONTEND_LIB_PREFIX=\\\"${CMAKE_SHARED_LIBRARY_PREFIX}${FRONTEND_NAME_PREFIX}\\\"\n        # Assume <lib>.so is an existed symlink to <lib><version>.so (or <lib>.so<version>\n        FRONTEND_LIB_SUFFIX=\\\"${FRONTEND_NAME_SUFFIX}${OV_BUILD_POSTFIX}${CMAKE_SHARED_LIBRARY_SUFFIX}\\\"\n        )\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Symmetric Mode Example (Mixed Pads) - C++\nDESCRIPTION: Demonstrates symmetric padding with mixed positive and negative pad values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nOUTPUT =\n[[6, 7, 8, 8, 7, 6],\n[2, 3, 4, 4, 3, 2],\n[2, 3, 4, 4, 3, 2],\n[6, 7, 8, 8, 7, 6]]\nShape(4, 6)\n```\n\n----------------------------------------\n\nTITLE: Defining ACL Usage\nDESCRIPTION: This snippet defines `-DOV_CPU_WITH_ACL` and sets the `OV_CPU_WITH_ACL` variable to ON if `DNNL_USE_ACL` is enabled, indicating that the CPU plugin will be built with ARM Compute Library (ACL) support.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_USE_ACL)\n    add_definitions(-DOV_CPU_WITH_ACL)\n    set(OV_CPU_WITH_ACL ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Proposal Layer Configuration XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a Proposal layer in OpenVINO. It includes attributes such as base_size, feat_stride, min_size, nms_thresh, and pre/post_nms_topn, along with example values for ratio and scale.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/proposal-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Proposal\" ... >\n    <data base_size=\"16\" feat_stride=\"16\" min_size=\"16\" nms_thresh=\"0.6\" post_nms_topn=\"200\" pre_nms_topn=\"6000\"\n    ratio=\"2.67\" scale=\"4.0,6.0,9.0,16.0,24.0,32.0\"/>\n    <input> ... </input>\n    <output> ... </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining Include Paths for OpenVINO Core\nDESCRIPTION: This snippet defines the include paths for the OpenVINO core library. `OV_CORE_INCLUDE_PATH` points to the main include directory, and `OV_CORE_DEV_API_PATH` points to the development API include directory. These paths are used later to specify where the compiler should look for header files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OV_CORE_INCLUDE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/include)\nset(OV_CORE_DEV_API_PATH ${CMAKE_CURRENT_SOURCE_DIR}/dev_api)\n```\n\n----------------------------------------\n\nTITLE: ReduceMean Calculation\nDESCRIPTION: Illustrates the calculation of the ReduceMean operation, showcasing how the output is derived from the input data by averaging along specified axes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-mean-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i0, i1, ..., iN] = mean[j0, ..., jN](x[j0, ..., jN]))\n```\n\n----------------------------------------\n\nTITLE: Defining OpenVINO Snap Slots\nDESCRIPTION: This snippet demonstrates how to define slots in the OpenVINO snap's snapcraft.yaml file. Slots expose interfaces for other snaps to connect to, in this case, providing access to OpenVINO libraries and third-party libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/integrate-openvino-with-ubuntu-snap.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nslots:\n  openvino-libs:\n    interface: content\n    content: openvino-libs\n    read:\n      - $SNAP/usr/local/li\n  openvino-3rdparty-libs:\n    interface: content\n    content: openvino-extra-libs\n    read:\n      - $SNAP/usr/local/runtime/3rdparty/tbb/lib\n```\n\n----------------------------------------\n\nTITLE: Exporting GPTQ Model\nDESCRIPTION: This snippet shows how to export a GPTQ model for OpenVINO using Optimum-Intel. GPTQ models don't require specifying quantization parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_8\n\nLANGUAGE: Console\nCODE:\n```\noptimum-cli export openvino -m TheBloke/Llama-2-7B-Chat-GPTQ\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard\nDESCRIPTION: This snippet sets the C++ standard to C++17 and disables compiler extensions. This ensures that the code is compiled according to the C++17 standard, promoting portability and consistency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset (CMAKE_CXX_STANDARD 17)\nset (CMAKE_CXX_EXTENSIONS OFF)\nset (CMAKE_CXX_STANDARD_REQUIRED ON)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This snippet sets the target name for the library to `openvino_transformations`. This name is used throughout the CMake file to refer to the library being built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_transformations\")\n```\n\n----------------------------------------\n\nTITLE: Col2Im Example with Non-Default Parameters (XML)\nDESCRIPTION: This example demonstrates the Col2Im operation with non-default dilations, padding, and strides. It shows how to configure the `data` layer with specific values for these parameters to control the sliding blocks' behavior.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/col2im-15.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Col2Im\" ... >\n    <data dilations=\"2,2\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\" precision=\"I32\">\n            <dim>1</dim>     <!-- batch_axis -->\n            <dim>27/dim</dim>     <!-- C * Product(kernel_size) -->\n            <dim>25</dim>    <!-- L -->\n        </port>\n        <port id=\"1\" precision=\"I32\">\n            <dim>2</dim>     <!-- output_size -->\n        </port>\n        <port id=\"2\" precision=\"I32\">\n            <dim>2</dim>     <!-- kernel_size -->\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"I32\">\n            <dim>1</dim>     <!-- batch_axis -->\n            <dim>3</dim>     <!-- C -->\n            <dim>16</dim>    <!-- output_size[0] -->\n            <dim>16</dim>    <!-- output_size[1] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Library\nDESCRIPTION: This snippet uses the `add_library` command to create the shared library. It specifies the target name, the type of library (MODULE), and the source files to be compiled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/test_builtin_extensions/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${TARGET_NAME} MODULE ${LIBRARY_SRC} ${LIBRARY_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: ISTFT Example: 3D input, 1D output, center=false, default length (XML)\nDESCRIPTION: Example configuration for the ISTFT operation with a 3D data input, resulting in a 1D output signal. The 'center' attribute is set to 'false', and the signal length is determined by default calculation. This example showcases the basic structure and parameters required for an ISTFT layer in OpenVINO, including input and output port dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/istft-16.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ISTFT\" ... >\n    <data center=\"false\" ... />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>16</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n        </port>\n        <port id=\"2\"></port> <!-- frame_size value: 11 -->\n        <port id=\"3\"></port> <!-- frame_step value: 3 -->\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for Profiling (Windows)\nDESCRIPTION: This snippet configures the OpenVINO build for profiling on Windows using cmake.  Key options include disabling various frontends and enabling the collection stage for selective build.  It also sets various other build options, such as disabling GPU and other execution modes, and enables ITT profiling.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\ncmake -G Ninja -Wno-dev -DCMAKE_BUILD_TYPE=Debug -DENABLE_CPPLINT=OFF -DCMAKE_VERBOSE_MAKEFILE=ON -DCMAKE_COMPILE_WARNING_AS_ERROR=OFF -DENABLE_FASTER_BUILD=ON -DENABLE_SANITIZER=OFF -DTHREADING=TBB -DBUILD_SHARED_LIBS=OFF -DENABLE_PROFILING_ITT=ON -DSELECTIVE_BUILD=COLLECT -DENABLE_INTEL_GPU=OFF -DENABLE_MULTI=OFF -DENABLE_AUTO=OFF -DENABLE_AUTO_BATCH=OFF -DENABLE_HETERO=OFF -DENABLE_TEMPLATE=OFF -DENABLE_OV_ONNX_FRONTEND=OFF -DENABLE_OV_PADDLE_FRONTEND=OFF -DENABLE_OV_PYTORCH_FRONTEND=OFF -DENABLE_OV_JAX_FRONTEND=OFF -DENABLE_OV_TF_FRONTEND=OFF -DCMAKE_INSTALL_PREFIX=install ..\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name and Compiler Optimization Flags in CMake\nDESCRIPTION: This snippet sets the target name, disables interprocedural optimization for release builds, and tries to use the gold linker. It also adds a definition for the serialized zoo.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_core_unit_tests)\n\nset(CMAKE_INTERPROCEDURAL_OPTIMIZATION_RELEASE OFF)\n\nov_try_use_gold_linker()\n\nadd_definitions(-DSERIALIZED_ZOO=\\\"${TEST_MODEL_ZOO}/core/models\\\")\n```\n\n----------------------------------------\n\nTITLE: Link System Libraries CMake\nDESCRIPTION: Links the target against the `onnx_proto` and `onnx` system libraries. The libraries are linked publicly, making them available to downstream targets that link to this library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/onnx_common/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_link_system_libraries(${TARGET_NAME} PUBLIC onnx_proto onnx)\n```\n\n----------------------------------------\n\nTITLE: Compiling Model on Default GPU Python\nDESCRIPTION: This Python snippet demonstrates how to compile a model on the default GPU device using `ov::Core::compile_model()`.  It utilizes the `compile_model_default_gpu` fragment from the specified Python file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# [compile_model_default_gpu]\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model=model, device_name=\"GPU\")\n# [compile_model_default_gpu]\n```\n\n----------------------------------------\n\nTITLE: Pad Output Example - Reflect Mode - C++\nDESCRIPTION: Illustrates the output of the Pad operation in 'reflect' mode, where the padded values are a reflection of the input tensor (values on edges are not duplicated). It shows how the input tensor is extended based on pads_begin and pads_end attributes, using reflected values for padding.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-1.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nINPUT =\n    [[ 1  2  3  4 ]\n    [ 5  6  7  8 ]\n    [ 9 10 11 12 ]]\n\npads_begin = [0, 1]\npads_end = [2, 3]\n\nOUTPUT =\n    [[  2  1  2  3  4  3  2  1 ]\n    [  6  5  6  7  8  7  6  5 ]\n    [ 10  9 10 11 12 11 10  9 ]\n    [  6  5  6  7  8  7  6  5 ]\n    [  2  1  2  3  4  3  2  1 ]]\n```\n\n----------------------------------------\n\nTITLE: Concatenating Output Tensors in TensorIterator (C++) - Reversed\nDESCRIPTION: This code demonstrates how the output tensors are concatenated based on the 'stride' attribute in the TensorIterator operation when stride is negative. It concatenates the tensors in the reverse order. This snippet shows the logic for forming the final output from the intermediate results of the TensorIterator's body when iterating backward.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/tensor-iterator-1.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\noutput = Concat(S[N-1], S[N-2], ..., S[0])\n```\n\n----------------------------------------\n\nTITLE: Setting Multi-threading Properties in C++ OpenVINO\nDESCRIPTION: This snippet demonstrates setting the number of threads, scheduling core type, and enabling hyper-threading for the CPU device in OpenVINO using C++. It sets the `ov::inference_num_threads`, `ov::hint::scheduling_core_type`, and `ov::hint::enable_hyper_threading` properties to control CPU multi-threading for inference execution. These properties configure the number of threads used by OpenVINO for inference, the preferred core type for scheduling, and whether to enable hyper-threading.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device/performance-hint-and-thread-scheduling.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// ! [ov:intel_cpu:multi_threading:part0]\n#include <openvino/runtime.hpp>\n\nint main() {\n    ov::Core core;\n    core.set_property(\"CPU\", ov::inference_num_threads(20));\n    core.set_property(\"CPU\", ov::hint::scheduling_core_type(ov::CoreType::ANY));\n    core.set_property(\"CPU\", ov::hint::enable_hyper_threading(true));\n    return 0;\n}\n// ! [ov:intel_cpu:multi_threading:part0]\n```\n\n----------------------------------------\n\nTITLE: Uninstall Specific OpenVINO Version\nDESCRIPTION: This command removes a specific version of OpenVINO using ZYPPER. Replace <VERSION>, <UPDATE>, and <PATCH> with the specific version numbers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-zypper.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nsudo zypper remove *openvino-<VERSION>.<UPDATE>.<PATCH>*\n```\n\n----------------------------------------\n\nTITLE: Root CMakeLists.txt for Plugin Build\nDESCRIPTION: This CMake snippet demonstrates the structure of the root CMakeLists.txt file for building a plugin. It finds the OpenVINO Developer Package and includes subdirectories for plugin source and tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/build-plugin-using-cmake.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nfind_package(OpenVINODeveloperPackage REQUIRED)\n\nadd_subdirectory(src)\n\nif (ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: AdaptiveAvgPool Layer in XML\nDESCRIPTION: This XML snippet demonstrates how to define an AdaptiveAvgPool layer within an OpenVINO model. It specifies the input and output dimensions, along with the output type. The layer takes two inputs: the input tensor and the desired output shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/adaptive-avg-pool-8.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"AdaptiveAvgPool\" ... >\n    <data output_type=\"i64\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <input>\n        <port id=\"1\">\n            <dim>2</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>16</dim>\n            <dim>16</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Inference Precision Pattern\nDESCRIPTION: This environment variable sets the inference precision pattern for specific layers.  The example provided targets `FullyConnected` layers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nOV_CPU_INFER_PRC_POS_PATTERN=\"^FullyConnected@\"\n```\n\n----------------------------------------\n\nTITLE: Creating OpenCV C Wrapper Library CMake\nDESCRIPTION: This CMake snippet sets up the project, defines the target library name, includes source and header files, and creates a static library. It also attempts to find the OpenCV library and sets necessary compile definitions and link libraries if OpenCV is found.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/c/common/opencv_c_wrapper/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(opencv_c_wrapper)\n\nset(TARGET_NAME ${PROJECT_NAME})\n\nfile(GLOB SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp\n                  ${CMAKE_CURRENT_SOURCE_DIR}/src/*.h\n                  ${CMAKE_CURRENT_SOURCE_DIR}/src/*.c)\nfile(GLOB HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/include/*.h)\n\n# create library\nadd_library(${TARGET_NAME} STATIC ${HEADERS} ${SOURCES})\n\n# Find OpenCV components if exist\nfind_package(OpenCV QUIET COMPONENTS core imgproc imgcodecs)\nif(NOT OpenCV_FOUND OR NOT OpenCV_VERSION VERSION_GREATER_EQUAL 3)\n    message(WARNING \"OpenCV ver. 3.0+ is not found, ${TARGET_NAME} is built without OPENCV support\")\nelse()\n    target_compile_definitions(${TARGET_NAME} PRIVATE USE_OPENCV)\n    target_link_libraries(${TARGET_NAME} PRIVATE ${OpenCV_LIBRARIES})\nendif()\n\ntarget_include_directories(${TARGET_NAME} PUBLIC \"${CMAKE_CURRENT_SOURCE_DIR}/include\"\n                                          PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}/src\")\n\nset_target_properties(${TARGET_NAME} PROPERTIES FOLDER c_samples)\n\nif(COMMAND ov_add_clang_format_target)\n    ov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\nendif()\n```\n\n----------------------------------------\n\nTITLE: NV12 VAAPI Surface Consumption (C) with OpenVINO\nDESCRIPTION: Demonstrates how to directly consume an NV12 VAAPI video decoder surface on Linux with OpenVINO using the C API. This enables processing video frames decoded by VAAPI using OpenVINO models. Requires OpenVINO runtime and VAAPI.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_45\n\nLANGUAGE: c\nCODE:\n```\n//! [context_sharing_va]\n// Create remote context\nov_property_t context_properties[1];\ncontext_properties[0].key = ov::intel_gpu::va_display().c_str();\ncontext_properties[0].value.type = ov::Any::Type::POINTER;\ncontext_properties[0].value.ptr = display;\nov_remote_context_t remote_context = {0};\nov_core_create_context_with_properties(core, \"GPU\", context_properties, 1, &remote_context);\n\n// Create remote tensor\nov_property_t tensor_properties[1];\ntensor_properties[0].key = ov::intel_gpu::va_surface().c_str();\ntensor_properties[0].value.type = ov::Any::Type::POINTER;\ntensor_properties[0].value.ptr = surface;\nsize_t shape[2] = {height + height / 2, width};\nov_remote_tensor_t remote_tensor = {0};\nov_remote_context_create_tensor_with_properties(remote_context, OV_TYPE_U8, shape, 2, tensor_properties, 1, &remote_tensor);\n\n// Set remote tensor to InferRequest\nov_infer_request_set_tensor(infer_request, input_tensor_name, remote_tensor);\nov_infer_request_infer(infer_request);\n//! [context_sharing_va]\n```\n\n----------------------------------------\n\nTITLE: Interpolate Layer Configuration\nDESCRIPTION: This XML configuration defines an Interpolate layer within OpenVINO. It specifies input and output port dimensions, data scaling, axes, and the chosen mode (linear).  The data input ports 1, 2, and 3 specify the scaling factors and axes for the interpolation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_16\n\nLANGUAGE: xml\nCODE:\n```\n   <layer ... type=\"Interpolate\" ...>\n       <data shape_calculation_mode=\"scales\" pads_begin=\"0\" pads_end=\"0\" mode=\"linear\"/>\n       <input>\n           <port id=\"0\">\n               <dim>1</dim>\n               <dim>2</dim>\n               <dim>48</dim>\n               <dim>80</dim>\n           </port>\n           <port id=\"1\">\n               <dim>2</dim> \t<!--The values in this input are [24, 160] -->\n           </port>\n           <port id=\"2\">\n               <dim>2</dim> \t<!--The values in this input are [0.5, 2.0] -->\n           </port>\n           <port id=\"3\">\n               <dim>2</dim> \t<!--The values in this input are [2, 3] (axes). -->\n           </port>\n       </input>\n       <output>\n           <port id=\"0\"  precision=\"FP32\">\n               <dim>1</dim>\n               <dim>2</dim>\n               <dim>24</dim>\n               <dim>160</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions (CMake)\nDESCRIPTION: This snippet sets a compile definition when building a static library. This definition is used to conditionally compile code based on whether the library is being built as a static library or a shared library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/shape_inference/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME} PUBLIC OPENVINO_STATIC_LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Define ov_shape struct in C\nDESCRIPTION: This struct represents the shape of a tensor. It includes the rank (number of dimensions) and an array of dimension sizes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_5\n\nLANGUAGE: C\nCODE:\n```\ntypedef struct ov_shape {\n\n    int64_t rank;\n\n    int64_t* dims;\n\n} ov_shape_t;\n```\n\n----------------------------------------\n\nTITLE: ScaledDotProductAttention Implementation (Python)\nDESCRIPTION: This Python code provides a pseudo-code implementation of the ScaledDotProductAttention operation using other OpenVINO operations and NumPy. It defines the ScaledDotProductAttention function which calculates attention weights and applies them to the value tensor, considering optional attention masks, causal masking, and scaling factors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/scaled-dot-product-attention.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\ndef ScaledDotProductAttention(query, key, value, attn_mask=None, scale=None, *, causal):\n    L, S = Gather(ShapeOf(query), -2), Gather(ShapeOf(key), -2)\n    if scale is None:\n        scale = 1.0 / Sqrt(ConvertLike(Gather(ShapeOf(query), -1), query))\n    attn_bias = Broadcast(ConvertLike(0, query), [L, S])\n    if causal:\n        attn_bias = numpy.triu(Broadcast(ConvertLike(-inf, query), [L, S]), k=1)\n    elif attn_mask is not None:\n        if attn_mask.element_type == boolean:\n            attn_bias = Select(LogicalNot(attn_mask), ConvertLike(-inf, query), ConvertLike(0, query))\n        else:\n            attn_bias += attn_mask\n    attn_weight = MatMul(query, Transpose(key, [-2, -1])) * scale\n    attn_weight += attn_bias\n    attn_weight = Softmax(attn_weight, axis=-1)\n    return MatMul(attn_weight, value)\n```\n\n----------------------------------------\n\nTITLE: Navigating to Build Directory for OpenVINO Samples (Linux)\nDESCRIPTION: This command changes the current directory to the `build` directory previously created, preparing the environment for the CMake build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncd build\n```\n\n----------------------------------------\n\nTITLE: Waiting for Inference Completion in Python\nDESCRIPTION: This Python code demonstrates waiting for asynchronous inference to complete using `ov::InferRequest::wait`.  It starts inference asynchronously and then blocks until the inference is finished. Requires OpenVINO and a compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\ncompiled_model = ie.compile_model(model, \"CPU\")\ninfer_request = compiled_model.create_infer_request()\n\ninput_tensor = infer_request.get_input_tensor()\ninput_tensor.data[:] = np.random.normal(0, 1, input_tensor.shape)\n\ninfer_request.start_async()\ninfer_request.wait()\n```\n\n----------------------------------------\n\nTITLE: Range Generation with Negative Step in OpenVINO (XML)\nDESCRIPTION: This XML snippet demonstrates the Range operation with a negative step, generating a sequence from start (23) to stop (2) with a step of -3. The output is an integer tensor of dimension 7.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/range-4.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Range\">\n    <data output_type=\"i32\">\n    <input>\n        <port id=\"0\">  <!-- start value: 23 -->\n        </port>\n        <port id=\"1\">  <!-- stop value: 2 -->\n        </port>\n        <port id=\"2\">  <!-- step value: -3 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>7</dim> <!-- [23, 20, 17, 14, 11, 8, 5] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Running Python OpenModelZoo Tests (Build Layout)\nDESCRIPTION: This command runs the Python OpenModelZoo tests using pytest in the build layout. It specifies the CPU backend, enables parallel execution with 4 workers, excludes CUDA tests, and sets the model zoo directory. Replace <OV_REPO_DIR> with the OpenVINO repository directory and <ONNX_MODELS_DIR> with the directory containing the ONNX models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\npytest --backend=CPU <OV_REPO_DIR>/src/bindings/python/tests/test_onnx/test_zoo_models.py -v -n 4 --forked -k 'not _cuda' --model_zoo_dir=<ONNX_MODELS_DIR>\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Nightly Wheels (Master Branch) with pip\nDESCRIPTION: This command installs the nightly build of OpenVINO using pip, specifying the extra index URL to the OpenVINO nightly wheels repository on AWS S3. The `--pre` flag allows the installation of pre-release versions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/release-notes-openvino/release-policy.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\npip install --pre openvino --extra-index-url\nhttps://storage.openvinotoolkit.org/simple/wheels/nightly\n```\n\n----------------------------------------\n\nTITLE: Verify OpenVINO YUM Repository Setup\nDESCRIPTION: This command verifies that the OpenVINO repository has been correctly set up by listing available repositories and filtering the output for 'openvino'.  The grep command filters the output of `yum repolist` to display only the lines containing 'openvino'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nyum repolist | grep -i openvino\n```\n\n----------------------------------------\n\nTITLE: Install and Export the Library\nDESCRIPTION: This snippet installs the static library and developer package export targets for conditional compilation. It also sets the install include directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n\nov_developer_package_export_targets(TARGET openvino::conditional_compilation\n                                    INSTALL_INCLUDE_DIRECTORIES \"${CMAKE_CURRENT_SOURCE_DIR}/include/\")\n```\n\n----------------------------------------\n\nTITLE: Adding Post-Build Command\nDESCRIPTION: Adds a custom command to copy the contents of the `src/openvino/test_utils` directory from the `OpenVINOPython_SOURCE_DIR` to the `CMAKE_LIBRARY_OUTPUT_DIRECTORY` after the target is built. This copies necessary files for the library to function correctly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(TARGET ${TARGET_NAME}\n        POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy_directory ${OpenVINOPython_SOURCE_DIR}/src/openvino/test_utils ${CMAKE_LIBRARY_OUTPUT_DIRECTORY}\n        )\n```\n\n----------------------------------------\n\nTITLE: FloorMod Example - No Broadcasting - XML\nDESCRIPTION: This XML snippet demonstrates the FloorMod operation with no broadcasting enabled. The input tensors have the same shape, so element-wise FloorMod is applied directly. The `auto_broadcast` attribute is set to 'none'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/floormod-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"FloorMod\">\n    <data auto_broadcast=\"none\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: FakeConvert Layer in XML\nDESCRIPTION: This XML code shows an example of how the FakeConvert layer is defined. It specifies the destination type, input ports with their dimensions, and the output port with its dimensions. The input ports represent the data, scale, and shift tensors, while the output port represents the resulting tensor after the FakeConvert operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/quantization/fake-convert-13.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer … type=\"FakeConvert\"…>\n    <data destination_type=\"f8e4m3\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>56</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>56</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Importing Custom Helper Function\nDESCRIPTION: This code snippet demonstrates how to import the `top1_index` function from the `custom_helpers` module into the `custom_module` package. This step is necessary to make the function accessible from other parts of the OpenVINO™ API. It highlights the modular structure and import mechanisms in Python.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.helpers.custom_module.custom_helpers import top1_index\n```\n\n----------------------------------------\n\nTITLE: Installing Constraints File\nDESCRIPTION: This command installs the 'constraints.txt' file to the 'tests' destination. It adds the installed file to the 'tests' component, excluding it from default build targets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/samples_tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(FILES ../constraints.txt DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Installing a specific Python version using Brew\nDESCRIPTION: This command uses the `brew install` command to install a specific Python version (e.g., 3.11) using the Homebrew package manager.  It is a necessary step for setting up the OpenVINO Runtime Python API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n% brew install python@3.11\n```\n\n----------------------------------------\n\nTITLE: Including Thirdparty Subdirectory CMake\nDESCRIPTION: This snippet adds the thirdparty subdirectory to the build process, excluding it from the ALL target. This means that the third-party libraries will be built as part of the NPU plugin build but not when building the entire project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(thirdparty EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Linking oneDNN Library\nDESCRIPTION: If oneDNN is enabled for GPU (`ENABLE_ONEDNN_FOR_GPU` is true), this snippet links the `onednn_gpu_tgt` library to the target library as a system library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_ONEDNN_FOR_GPU)\n  ov_target_link_libraries_as_system(${TARGET_NAME} PUBLIC onednn_gpu_tgt)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Python Build Condition Check Function\nDESCRIPTION: This function `ov_check_python_build_conditions` checks for Python requirements (interpreter, development modules), sets build flags based on whether Python is enabled and the build type, and performs compiler compatibility checks. If Python is not found, it disables Python API builds. It uses macros like `ov_find_python3`, `ov_detect_python_module_extension` to find Python components and set the Python module extension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ov_check_python_build_conditions)\n    # user explicitly specified ENABLE_PYTHON=ON\n    if(ENABLE_PYTHON)\n        set(find_package_mode REQUIRED)\n        set(message_mode FATAL_ERROR)\n    else()\n        set(find_package_mode QUIET)\n        set(message_mode WARNING)\n    endif()\n\n    ov_find_python3(${find_package_mode})\n\n    if(Python3_Development.Module_FOUND OR Python3_Development_FOUND)\n        message(STATUS \"Python3 executable: ${Python3_EXECUTABLE}\")\n        message(STATUS \"Python3 version: ${Python3_VERSION}\")\n        if(Python3_PyPy_VERSION)\n            message(STATUS \"Python3 PyPy version: ${Python3_PyPy_VERSION}\")\n        endif()\n        message(STATUS \"Python3 interpreter ID: ${Python3_INTERPRETER_ID}\")\n        if(Python3_SOABI)\n            message(STATUS \"Python3 SOABI: ${Python3_SOABI}\")\n        endif()\n        ov_detect_python_module_extension()\n        if(PYTHON_MODULE_EXTENSION)\n            set(PYTHON_MODULE_EXTENSION ${PYTHON_MODULE_EXTENSION} PARENT_SCOPE)\n            message(STATUS \"PYTHON_MODULE_EXTENSION: ${PYTHON_MODULE_EXTENSION}\")\n        endif()\n        message(STATUS \"Python3 include dirs: ${Python3_INCLUDE_DIRS}\")\n        message(STATUS \"Python3 libraries: ${Python3_LIBRARIES}\")\n    else()\n        message(${message_mode} \"Python 3.x Interpreter and Development.Module components are not found. OpenVINO Python API will be turned off (ENABLE_PYTHON is OFF)\")\n    endif()\n\n    if(NOT OV_GENERATOR_MULTI_CONFIG AND CMAKE_BUILD_TYPE STREQUAL \"Debug\" AND CMAKE_DEBUG_POSTFIX)\n        set(python_debug ON)\n        message(${message_mode} \"Building python bindings in debug configuration is not supported on your platform (ENABLE_PYTHON is OFF)\")\n    else()\n        set(python_debug OFF)\n    endif()\n\n    set(is_compiler_supported ON)\n    # Python bindings compiled with the 14.28 toolset (19.28 compiler) could not be imported by another Pybind module linked with OpenVINO\n    if(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\" AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 19.29)\n        set(is_compiler_supported OFF)\n        message(${message_mode} \"MSVC toolset version 14.28 or lower (19.28 compiler) is not supported. Please update your toolset (${CMAKE_CXX_COMPILER_VERSION}).\")\n    endif()\n\n    if((Python3_Development.Module_FOUND OR Python3_Development_FOUND) AND NOT python_debug AND is_compiler_supported)\n        set(ENABLE_PYTHON_DEFAULT ON PARENT_SCOPE)\n    else()\n        set(ENABLE_PYTHON_DEFAULT OFF PARENT_SCOPE)\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Preprocessing and Saving Model to IR - Python\nDESCRIPTION: Demonstrates how to load an ONNX model, define preprocessing steps (resizing, color conversion, mean/scale adjustments), and save the modified model as an OpenVINO IR file in Python. The preprocessing steps configure the model to accept BGR images of varying sizes and convert them to the expected RGB format with specified mean and scale values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details/integrate-save-preprocessing-use-case.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef save_model():\n    core = Core()\n    model = core.read_model(\"model.onnx\")\n\n    ppp = PrePostProcessor(model)\n    ppp.input().tensor().\n        set_shape([1, 360, 640, 3]).\n        set_layout(Layout(\"NHWC\")).\n        set_color_format(ColorFormat.BGR)\n\n    ppp.input().preprocess().\n        resize(ov.preprocess.ResizeAlgorithm.RESIZE_LINEAR).\n        convert_color(ColorFormat.RGB).\n        mean([0.485, 0.456, 0.406]).\n        scale([0.229, 0.224, 0.225])\n\n    ppp.input().model().\n        set_layout(Layout(\"NCHW\"))\n\n    model = ppp.build()\n    serialize(model, \"ov_model.xml\")\n```\n\n----------------------------------------\n\nTITLE: GatherND Complex Batch Dimensions Slice Example\nDESCRIPTION: Demonstrates GatherND with batch_dims = 2 and leading dimensions in the indices, focusing on slice extraction. This example showcases a more complex scenario where both batching and multi-dimensional indices influence the output. It highlights how to handle intricate data gathering scenarios.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-8.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 2\nindices = [[[[1]], <--- this is applied to the first batch\n               [[0]],\n               [[2]]],\n              [[[0]],\n               [[2]],\n               [[2]]] <--- this is applied to the sixth batch\n             ], shape = (2, 3, 1, 1)\ndata    = [[[ 1,  2,  3,  4], <--- this is the first batch\n               [ 5,  6,  7,  8],\n               [ 9, 10, 11, 12]]\n              [[13, 14, 15, 16],\n               [17, 18, 19, 20],\n               [21, 22, 23, 24]] <--- this is the sixth batch\n             ] <--- the second batch, shape = (2, 3, 4)\noutput  = [[[ 2], [ 5], [11]], [[13], [19], [23]]], shape = (2, 3, 1)\n```\n\n----------------------------------------\n\nTITLE: SegmentMax with num_segments > max(segment_ids) + 1 in OpenVINO XML\nDESCRIPTION: This example showcases the SegmentMax operation when `num_segments` is greater than `max(segment_ids) + 1`. The output tensor's first dimension will be equal to the value of `num_segments`, with padding for the extra segments.  `fill_mode` is set to `ZERO` meaning that padded segments will contain zero values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/segment-max-16.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"SegmentMax\" ... >\n    <data empty_segment_value=\"ZERO\">\n        <input>\n            <port id=\"0\" precision=\"F32\">   <!-- data -->\n                <dim>5</dim>\n            </port>\n            <port id=\"1\" precision=\"I32\">   <!-- segment_ids with 4 segments: [0, 0, 2, 3, 3] -->\n                <dim>5</dim> \n            </port>\n            <port id=\"2\" precision=\"I64\">   <!-- number of segments: 8 -->\n                <dim>0</dim> \n            </port>\n        </input>\n        <output>\n            <port id=\"3\" precision=\"F32\">\n                <dim>8</dim>\n            </port>\n        </output>\n    </layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for Interface Library\nDESCRIPTION: This snippet sets the include directory for the 'openvino_conditional_compilation' library to the 'include' directory within the current source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} INTERFACE\n    $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>)\n```\n\n----------------------------------------\n\nTITLE: Developer Package Export Targets CMake\nDESCRIPTION: This snippet calls the `ov_developer_package_export_targets` function to export the target for developer packages. It specifies the target to export and the installation include directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET ${TARGET_NAME}\n                                    INSTALL_INCLUDE_DIRECTORIES \"${PUBLIC_HEADERS_DIR}/\")\n```\n\n----------------------------------------\n\nTITLE: Running Hello Reshape SSD Node.js Sample\nDESCRIPTION: This command demonstrates how to run the Hello Reshape SSD Node.js sample. It requires specifying the path to the model file (.xml), the path to the image, and the target device (e.g., AUTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/js/node/hello_reshape_ssd/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnode hello_reshape_ssd.js ../../assets/models/road-segmentation-adas-0001.xml ../../assets/images/empty_road_mapillary.jpg AUTO\n```\n\n----------------------------------------\n\nTITLE: Modifying kernel_base.cpp for LayerID in release build\nDESCRIPTION: This C++ snippet shows how to modify the base jitter method to append `LayerID` in the release build. This is useful for debugging source dumps.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_debug_utils.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n// inference-engine/thirdparty/clDNN/kernel_selector/core/kernel_base.cpp\nJitConstants KernelBase::MakeBaseParamsJitConstants(const base_params& params) const {\n    // ...\n#ifndef NDEBUG                             <--- should be removed\n    jit.AddConstant(MakeJitConstant(\"LayerID\", params.layerID));\n#endif\n}\n```\n\n----------------------------------------\n\nTITLE: Running cpuFuncTests on Sandy Bridge with SDE\nDESCRIPTION: This snippet shows how to run cpuFuncTests on an older architecture (Sandy Bridge) using Intel SDE. This is useful for testing compatibility and debugging issues on specific CPU architectures.  It requires the SDE binary and the cpuFuncTests executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/cpu_emulation.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n/path/to/sde -snd -- ./cpuFuncTests\n```\n\n----------------------------------------\n\nTITLE: Enabling IR Frontend Support\nDESCRIPTION: This CMake snippet conditionally adds dependencies and a compiler definition based on whether the OpenVINO IR frontend is enabled. If `ENABLE_OV_IR_FRONTEND` is true, it adds `openvino_ir_frontend` as a dependency and defines a private compiler definition `IR_FRONTEND_ENABLED` for the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/tests/functional/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_OV_IR_FRONTEND)\n    add_dependencies(${TARGET_NAME} openvino_ir_frontend)\n    target_compile_definitions(${TARGET_NAME} PRIVATE IR_FRONTEND_ENABLED)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Quantize model weights to E2M1 format using NNCF\nDESCRIPTION: This code snippet shows how to compress model weights to the MXFP4 (E2M1) data format using the `compress_weights` function from the `nncf` library. It sets the `mode` to `CompressWeightsMode.E2M1` and `group_size` to 32.  The `all_layers=True` argument indicates that all eligible layers should be compressed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression/microscaling-quantization.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom nncf import compress_weights, CompressWeightsMode\ncompressed_model = compress_weights(model, mode=CompressWeightsMode.E2M1, group_size=32, all_layers=True)\n```\n\n----------------------------------------\n\nTITLE: Single Image Test Usage Message\nDESCRIPTION: This snippet shows the usage message of the `single-image-test.exe` application when run with the `-help` option. It lists all available command-line flags and their descriptions, data types, and default values. This helps users understand the available options for configuring the application.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/README.md#_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\nsingle-image-test.exe: Usage: Release\\single-image-test.exe[<options>]\n\n  Flags from C:\\Users\\mdoronin\\work\\applications.ai.vpu-accelerators.vpux-plugin\\tools\\single-image-test\\main.cpp:\n    -box_tolerance (Box tolerance for 'detection' mode) type: double\n      default: 0.0001\n    -classes (Number of classes for Yolo V3) type: int32 default: 80\n    -color_format (Color format for input: RGB or BGR) type: string\n      default: \"BGR\"\n    -compiled_blob (Output compiled network file (compiled result blob))\n      type: string default: \"\"\n    -confidence_threshold (Confidence threshold for Detection mode)\n      type: double default: 0.0001\n    -config (Path to the configuration file (optional)) type: string\n      default: \"\"\n    -coords (Number of coordinates for Yolo V3) type: int32 default: 4\n    -cosim_threshold (Threshold for 'cosim' mode) type: double\n      default: 0.90000000000000002\n    -dataset (The dataset used to train the model. Useful for instances such as\n      semantic segmentation to visualize the accuracy per-class) type: string\n      default: \"NONE\"\n    -device (Device to use) type: string default: \"\"\n    -il (Input layout) type: string default: \"\"\n    -img_as_bin (Force binary input even if network expects an image)\n      type: bool default: false\n    -img_bin_precision (Specify the precision of the binary input files. Eg:\n      'FP32,FP16,I32,I64,U8') type: string default: \"\"\n    -iml (Model input layout) type: string default: \"\"\n    -input (Input file(s)) type: string default: \"\"\n    -ip (Input precision (default: U8, available: FP32, FP16, I32, I64, U8))\n      type: string default: \"\"\n    -is_tiny_yolo (Is it Tiny Yolo or not (true or false)?) type: bool\n      default: false\n    -log_level (IE logger level (optional)) type: string default: \"\"\n    -mean_values (Optional. Mean values to be used for the input image per\n      channel. Values to be provided in the [channel1,channel2,channel3]\n      format. Can be defined for desired input of the model, for example:\n      \"--mean_values data[255,255,255],info[255,255,255]\". The exact meaning\n      and order of channels depend on how the original model was trained.\n      Applying the values affects performance and may cause type conversion)\n      type: string default: \"\"\n    -mode (Comparison mode to use) type: string default: \"\"\n    -network (Network file (either XML or pre-compiled blob)) type: string\n      default: \"\"\n    -normalized_image (Images in [0, 1] range or not) type: bool default: false\n    -nrmse_loss_threshold (Threshold for 'nrmse' mode) type: double default: 1\n    -num (Number of scales for Yolo V3) type: int32 default: 3\n    -ol (Output layout) type: string default: \"\"\n    -oml (Model output layout) type: string default: \"\"\n    -op (Output precision (default: FP32, available: FP32, FP16, I32, I64, U8))\n      type: string default: \"\"\n    -override_model_batch_size (Enforce a model to be compiled for batch size)\n      type: uint32 default: 1\n    -pc (Report performance counters) type: bool default: false\n    -prob_tolerance (Probability tolerance for 'classification/ssd/yolo' mode)\n      type: double default: 0.0001\n    -psnr_reference (PSNR reference value in dB) type: double default: 30\n    -psnr_tolerance (Tolerance for 'psnr' mode) type: double default: 0.0001\n    -raw_tolerance (Tolerance for 'raw' mode (absolute diff)) type: double\n      default: 0.0001\n    -rrmse_loss_threshold (Threshold for 'rrmse' mode) type: double\n      default: 1.7976931348623157e+308\n    -run_test (Run the test (compare current results with previously dumped))\n      type: bool default: false\n    -scale_border (Scale border) type: uint32 default: 4\n    -scale_values (Optional. Scale values to be used for the input image per\n      channel. Values are provided in the [channel1,channel2,channel3] format.\n      Can be defined for desired input of the model, for example:\n      \"--scale_values data[255,255,255],info[255,255,255]\". The exact meaning\n      and order of channels depend on how the original model was trained. If\n      both --mean_values and --scale_values are specified, the mean is\n      subtracted first and then scale is applied regardless of the order of\n      options in command line. Applying the values affects performance and may\n      cause type conversion) type: string default: \"\"\n    -sem_seg_classes (Number of classes for semantic segmentation) type: uint32\n      default: 12\n    -sem_seg_ignore_label (The number of the label to be ignored) type: uint32\n      default: 4294967295\n    -sem_seg_threshold (Threshold for 'semantic segmentation' mode)\n      type: double default: 0.97999999999999998\n    -top_k (Top K parameter for 'classification' mode) type: uint32 default: 1\n```\n\n----------------------------------------\n\nTITLE: Downloading, Extracting and Moving OpenVINO ARM Archive\nDESCRIPTION: These commands download the OpenVINO Runtime archive file for macOS (ARM, 64-bit), extract the files, rename the extracted folder, and move it to the `/opt/intel/openvino_2025.1.0` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-macos.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino/packages/2025.1/macos/openvino_toolkit_macos_12_6_2025.1.0.18503.6fec06580ab_arm64.tgz --output openvino_2025.1.0.tgz\ntar -xf openvino_2025.1.0.tgz\nsudo mv openvino_toolkit_macos_12_6_2025.1.0.18503.6fec06580ab_arm64 /opt/intel/openvino_2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Apply Naming Style\nDESCRIPTION: This snippet uses the `ov_ncc_naming_style` macro to apply a naming style for the target. It specifies the target name, source directories, and compile definitions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/frontend/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_ncc_naming_style(FOR_TARGET ${TARGET_NAME}\n                    SOURCE_DIRECTORIES \"${${TARGET_NAME}_INCLUDE_DIR}\"\n                    DEFINITIONS\n                        $<TARGET_PROPERTY:onnx,INTERFACE_COMPILE_DEFINITIONS>)\n```\n\n----------------------------------------\n\nTITLE: Importing MyTensor with Alias in Python\nDESCRIPTION: Imports the MyTensor class from the _pyopenvino.mymodule module and creates an alias for it named MyTensorBase.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino._pyopenvino.mymodule import MyTensor as MyTensorBase\n```\n\n----------------------------------------\n\nTITLE: Configuring ONNXRT Model Parameters with YAML\nDESCRIPTION: This code snippet shows how to configure ONNX Runtime model parameters using YAML in Protopipe, including specifying the execution provider (EP) and session options.  It demonstrates how to enable the OpenVINO Execution Provider and set its device type and parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- { name: model.onnx, framework: onnxrt } # Default (MLAS) EP will be used\n- { name: model.onnx, framework: onnxrt, session_options: { session.disable_cpu_ep_fallback: 1 } } # Default (MLAS) EP with the sessions options will be used\n- { name: model.onnx, framework: onnxrt, ep: { name: OV, device_type: NPU_U8, params: { enable_qdq_optimizer: False, model_priority: LOW } } } # OpenVINO EP will be used\n```\n\n----------------------------------------\n\nTITLE: PSROIPooling Layer Configuration in XML\nDESCRIPTION: This XML snippet demonstrates how to configure a PSROIPooling layer within an OpenVINO model. It shows the usage of attributes like `group_size`, `mode`, `output_dim`, `spatial_bins_x`, `spatial_bins_y`, and `spatial_scale`. It also defines the input and output port dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/psroi-pooling-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"PSROIPooling\" ... >\n    <data group_size=\"6\" mode=\"bilinear\" output_dim=\"360\" spatial_bins_x=\"3\" spatial_bins_y=\"3\" spatial_scale=\"1\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3240</dim>\n            <dim>38</dim>\n            <dim>38</dim>\n        </port>\n        <port id=\"1\">\n            <dim>100</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>100</dim>\n            <dim>360</dim>\n            <dim>6</dim>\n            <dim>6</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Set Output Tensor by Index and Tensor Object TypeScript\nDESCRIPTION: Sets the output tensor for the InferRequest by index. The `idx` parameter specifies the index of the output tensor. The tensor's type and shape must match the model's output requirements. This method returns void.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nsetOutputTensor(idx, tensor): void\n```\n\n----------------------------------------\n\nTITLE: Defining Test Configuration Path\nDESCRIPTION: Appends a definition to the DEFINES list, specifying the path to the custom operator configuration file used during testing. This allows the tests to locate and use custom operators.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/functional/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND DEFINES TEST_CUSTOM_OP_CONFIG_PATH=\"${CMAKE_CURRENT_SOURCE_DIR}/custom_op/custom_op.xml\")\n```\n\n----------------------------------------\n\nTITLE: OVSA Environment Preparation\nDESCRIPTION: Prepares the environment for running the model server by copying necessary files and creating a directory for the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_52\n\nLANGUAGE: sh\nCODE:\n```\ncd $OVSA_RUNTIME_ARTEFACTS/..\ncp /opt/ovsa/example_runtime ovms -r\ncd ovms\nmkdir -vp model/fd/1\n```\n\n----------------------------------------\n\nTITLE: GRUCell Formula\nDESCRIPTION: This code snippet shows the mathematical formulas used to calculate the output of the GRUCell operation. It defines the update gate (zt), reset gate (rt), candidate hidden state (ht), and final hidden state (Ht) based on input (Xt), previous hidden state (Ht-1), weights (W, R), and biases (Wb, Rb).  The linear_before_reset attribute affects the ht calculation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/gru-cell-3.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nFormula:\n  *  - matrix multiplication\n (.) - Hadamard product(element-wise)\n [,] - concatenation\n  f, g - are activation functions.\n   zt = f(Xt*(Wz^T) + Ht-1*(Rz^T) + Wbz + Rbz)\n   rt = f(Xt*(Wr^T) + Ht-1*(Rr^T) + Wbr + Rbr)\n   ht = g(Xt*(Wh^T) + (rt (.) Ht-1)*(Rh^T) + Rbh + Wbh) # default, when linear_before_reset = 0\n   ht = g(Xt*(Wh^T) + (rt (.) (Ht-1*(Rh^T) + Rbh)) + Wbh) # when linear_before_reset != 0\n   Ht = (1 - zt) (.) ht + zt (.) Ht-1\n```\n\n----------------------------------------\n\nTITLE: Add GPU Backend Target\nDESCRIPTION: This snippet calls the `ov_gpu_add_backend_target` CMake function, providing the defined target name.  This custom function likely sets up necessary build configurations specific to the GPU backend within the OpenVINO project. It ensures the target is correctly configured as a GPU backend component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/sycl/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_gpu_add_backend_target(\n    NAME ${TARGET_NAME}\n)\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Example 1 - XML\nDESCRIPTION: This XML snippet provides an example of the ScatterElementsUpdate operation with use_init_val set to 'true' and reduction set to 'sum'. It shows the input data, indices, updates, and the resulting output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_11\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... use_init_val=\"true\" reduction=\"sum\" type=\"ScatterElementsUpdate\">\n    <input>\n        <port id=\"0\">  <!-- data -->\n            <dim>4</dim>  <!-- values: [2, 3, 4, 6] -->\n        </port>\n        <port id=\"1\">  <!-- indices (negative values allowed) -->\n            <dim>6</dim>  <!-- values: [1, 0, 0, -2, -1, 2] -->\n        </port>\n        <port id=\"2\">>  <!-- updates -->\n            <dim>6</dim>  <!-- values: [10, 20, 30, 40, 70, 60] -->\n        </port>\n        <port id=\"3\">     <!-- values: [0] -->\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"4\" precision=\"FP32\">\n            <dim>4</dim>  <!-- values: [52, 13, 104, 76] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Graph Serialization Path\nDESCRIPTION: This environment variable specifies the path to serialize the execution graph to. This is useful for analyzing the graph structure and verifying transformations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nOV_CPU_EXEC_GRAPH_PATH=graph.xml\n```\n\n----------------------------------------\n\nTITLE: Exporting Targets for Developer Package in CMake\nDESCRIPTION: This code snippet configures the installation and export of the library for inclusion in a developer package.  `ov_developer_package_export_targets` likely comes from an OpenVINO custom CMake module.  It installs include directories for the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/offline_transformations/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET ${TARGET_NAME}\n                                    INSTALL_INCLUDE_DIRECTORIES \"${CMAKE_CURRENT_SOURCE_DIR}/include/\")\n```\n\n----------------------------------------\n\nTITLE: Append ONNX Tests Dependencies\nDESCRIPTION: This code appends openvino_template_extension to the list of ONNX test dependencies. It then checks if Intel CPU, GPU or TEMPLATE is enabled. If enabled, the corresponding backend is added to the active backend list and the corresponding plugin is appended to the ONNX test dependencies when strict dependencies are enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND ONNX_TESTS_DEPENDENCIES openvino_template_extension)\n\nif (ENABLE_INTEL_CPU)\n    set(ACTIVE_BACKEND_LIST ${ACTIVE_BACKEND_LIST} \"IE:CPU\")\n    if (ENABLE_STRICT_DEPENDENCIES)\n        # For convinience add a runtime dependency to build along with this target.\n        # Warning: Parallel build with -GNinja may not be efficient.\n        list(APPEND ONNX_TESTS_DEPENDENCIES openvino_intel_cpu_plugin)\n    endif()\nendif()\n\nif (ENABLE_INTEL_GPU)\n    set(ACTIVE_BACKEND_LIST ${ACTIVE_BACKEND_LIST} \"IE:GPU\")\n    if (ENABLE_STRICT_DEPENDENCIES)\n        # For convinience add a runtime dependency to build along with this target.\n        # Warning: Parallel build with -GNinja may not be efficient.\n        list(APPEND ONNX_TESTS_DEPENDENCIES openvino_intel_gpu_plugin)\n    endif()\nendif()\n\nif (ENABLE_TEMPLATE)\n    set(ACTIVE_BACKEND_LIST ${ACTIVE_BACKEND_LIST} INTERPRETER)\n    if (ENABLE_STRICT_DEPENDENCIES)\n        list(APPEND ONNX_TESTS_DEPENDENCIES openvino_template_plugin)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Custom Post-processing Step in OpenVINO (C++)\nDESCRIPTION: This C++ snippet demonstrates adding a custom post-processing step to the execution graph. Post-processing steps are integrated into the graph and executed on a selected device. The flow is: Model output -> Steps -> User tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_27\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nauto model = core.read_model(MODEL_PATH);\n\nauto out = model->output();\n\n// Define a custom postprocessing operation: insert a node after the output\nauto scale_node = [](const ov::Output<ov::Node>& node) {\n    // node: ov::Output<ov::Node> - OpenVINO node to insert after\n    // Constant 2.0\n    auto const_node = ov::op::v0::Constant::create(ov::element::f32, ov::Shape{}, {2.0f});\n    // node * 2.0\n    auto mul = std::make_shared<ov::op::v1::Multiply>(node, const_node);\n    // Result\n    return mul->output(0);\n};\n\n// 1. Create Preprocessor object\nov::preprocess::Preprocessor pp(model);\n// 2. Get a Tensor object, that describes output of the model\nauto output_tensor = pp.output(out.get_any_name());\n// 3. Set custom postprocessing operation.\noutput_tensor.postprocess().custom(scale_node);\n// 4. Apply preprocessing modification to the model\nmodel = pp.build();\n\nauto compiled_model = core.compile_model(model, DEVICE_NAME);\n```\n\n----------------------------------------\n\nTITLE: Disable NPU Memory Allocation (Windows)\nDESCRIPTION: This snippet shows how to disable NPU memory allocation by setting the `DISABLE_OPENVINO_GENAI_NPU_L0` environment variable to `1` in a Windows environment. This can be useful for compatibility with older NPU drivers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_20\n\nLANGUAGE: console\nCODE:\n```\nset DISABLE_OPENVINO_GENAI_NPU_L0=1\n```\n\n----------------------------------------\n\nTITLE: Clone OpenVINO Repository and Submodules\nDESCRIPTION: Clones the OpenVINO repository and initializes its submodules. This ensures that all necessary components for building the documentation are available. Requires git to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/documentation_build_instructions.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n$ git clone <openvino_repository_url> <repository_path>\n$ cd <repository_path>\n$ git submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Docker Image Definition in GHA Workflow (YAML)\nDESCRIPTION: This code snippet shows how to define a Docker image within a GitHub Actions workflow using the `container` key.  It specifies the image to use and defines a volume mount.  The volume allows the container to access files on the host machine.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/docker_images.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nBuild:\n  ...\n  container:\n    image: openvinogithubactions.azurecr.io/dockerhub/ubuntu:20.04\n    volumes:\n      - /mount:/mount\n  ...\n```\n\n----------------------------------------\n\nTITLE: Create Library Aliases\nDESCRIPTION: Creates aliases for the original target names, pointing to the new target names. This ensures that existing code using the old names continues to work.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/common_test_utils/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n# Add new names to use them from openvino repo\nadd_library(${NEW_TARGET_NAME} ALIAS ${TARGET_NAME})\nadd_library(${NEW_TARGET_NAME}_s ALIAS ${TARGET_NAME}_s)\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target in CMake\nDESCRIPTION: This snippet adds a custom target named `${TARGET_NAME}_clang` to run clang-format on the source code of the `TARGET_NAME` library. This helps enforce a consistent code style.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_18\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: GatherND with batch_dims=3 Example\nDESCRIPTION: Shows an example of GatherND operation with batch_dims set to 3. This example further illustrates how the 'batch_dims' attribute affects the slicing and gathering of data, specifically with higher-dimensional input tensors and indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-8.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 3\nindices = [[[[1],\n                [0]],\n               [[3],\n                [2]]]\n               ], shape = (1, 2, 2, 1)\ndata    = [[[[ 1  2  3  4],\n                [ 5  6  7  8]],\n               [[ 9 10 11 12],\n                [13 14 15 16]]]\n             ], shape = (1, 2, 2, 4)\noutput  = [[[ 2  5],\n               [12 15]]\n             ], shape = (1, 2, 2)\n```\n\n----------------------------------------\n\nTITLE: Get Output Port (Single Output Model) in OpenVINO (C)\nDESCRIPTION: This function retrieves a single output port from an OpenVINO model, assuming the model has only one output. It accepts a pointer to an `ov_model_t` and returns a pointer to the `ov_output_port_t`. The function returns a status code indicating the result.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_29\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_model_output(const ov_model_t* model, ov_output_port_t** output_port);\n```\n\n----------------------------------------\n\nTITLE: Set Minimum CMake Version\nDESCRIPTION: Specifies the minimum required version of CMake for this project. It ensures that the build system has the necessary features to correctly process the CMake configuration files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/ovc/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required (VERSION 3.13)\n```\n\n----------------------------------------\n\nTITLE: Clip Coordinate Function\nDESCRIPTION: Clips a coordinate value to be within the bounds of the input shape for a given axis.  This prevents out-of-bounds access.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef clip_coord(self, coord, axis):\n    return max(0, min(coord, self.input_shape[axis] - 1))\n```\n\n----------------------------------------\n\nTITLE: Testing Selective Compile with Benchmark\nDESCRIPTION: This command executes the benchmark application to test the selectively compiled OpenVINO build. The `-m <model_path>` option specifies the path to the model, and the `-d CPU` option specifies the CPU as the inference device. This verifies that the selectively built OpenVINO can correctly run inference for the target model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/selective_build.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd ./bin/intel64/Release\n\n./benchmark_app -niter 1 -nireq 1 \\\n-m <model_path> -d CPU\n```\n\n----------------------------------------\n\nTITLE: Appending Libraries CMake\nDESCRIPTION: This snippet appends required libraries to the `LIBRARIES` list. These libraries are linked to the `ov_subgraphs_dumper` target during the build process. The libraries include gflags, op_conformance_utils, and openvino core development libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/subgraphs_dumper/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND LIBRARIES\n        gflags\n        op_conformance_utils\n        openvino::core::dev\n)\n```\n\n----------------------------------------\n\nTITLE: CumSum Example 2: Exclusive Summation in OpenVINO (XML)\nDESCRIPTION: This XML example demonstrates the CumSum operation with exclusive summation (exclusive=\"1\") and forward direction (reverse=\"0\").  It calculates the exclusive cumulative sum of the input tensor [1., 2., 3., 4., 5.] along axis 0, resulting in the output tensor [0., 1., 3., 6., 10.].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/cumsum-3.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"CumSum\" exclusive=\"1\" reverse=\"0\">\n    <input>\n        <port id=\"0\">     <!-- input value is: [1., 2., 3., 4., 5.] -->\n            <dim>5</dim>\n        </port>\n        <port id=\"1\"/>     <!-- axis value is: 0 -->\n    </input>\n    <output>\n        <port id=\"2\">     <!-- output value is: [0., 1., 3., 6., 10.] -->\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files\nDESCRIPTION: This snippet uses `file(GLOB)` to find all C++ source files in the `ops` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB OPS_SRC \"${CMAKE_CURRENT_SOURCE_DIR}/ops/*.cpp\")\n```\n\n----------------------------------------\n\nTITLE: Einsum Layer with explicit equation XML Configuration\nDESCRIPTION: This XML snippet demonstrates the configuration of an Einsum layer with the equation `ij,ij->i`. The layer takes two inputs, each with dimensions 2x64, and produces an output with dimension 2. This example showcases an explicit equation specifying the input and output indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/einsum-7.rst#_snippet_9\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Einsum\" version=\"opset7\">\n    <data equation=\"ij,ij->i\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>64</dim>\n        </port>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>64</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Element Type of Input Tensor in TypeScript\nDESCRIPTION: This code snippet shows the `setElementType` method of the InputTensorInfo interface. This method is used to define the data type (element type) of the input tensor. It accepts either an `elementTypeString` or an `element` enum value as a parameter and returns an `InputTensorInfo` object for chaining.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InputTensorInfo.rst#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nsetElementType(elementType): InputTensorInfo\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files\nDESCRIPTION: This snippet uses `file(GLOB)` to collect all header (`.h`, `.hpp`) and source (`.cpp`) files in the current source directory and its `ocl` subdirectory. These files are then stored in the `LIBRARY_SOURCES_MAIN` and `LIBRARY_SOURCES_OCL` variables, respectively. These variables are later combined into `LIBRARY_SOURCES_ALL`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB LIBRARY_SOURCES_MAIN\n    \"${CMAKE_CURRENT_SOURCE_DIR}/*.h\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\"\n  )\n\nfile(GLOB LIBRARY_SOURCES_OCL\n    \"${CMAKE_CURRENT_SOURCE_DIR}/ocl/*.h\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/ocl/*.hpp\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/ocl/*.cpp\"\n)\n\nset(LIBRARY_SOURCES_ALL\n    ${LIBRARY_SOURCES_MAIN}\n    ${LIBRARY_SOURCES_OCL}\n  )\n```\n\n----------------------------------------\n\nTITLE: Enabling Kleidiai Integration\nDESCRIPTION: Conditionally enables the Kleidiai library for the CPU plugin by linking it to the target. The `ENABLE_KLEIDIAI_FOR_CPU` variable controls whether Kleidiai is enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_35\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_KLEIDIAI_FOR_CPU)\n    target_link_libraries(${TARGET_NAME} PRIVATE kleidiai)\nendif()\n```\n\n----------------------------------------\n\nTITLE: ScatterNDUpdate Example 4 (XML)\nDESCRIPTION: This XML snippet demonstrates a ScatterNdUpdate layer configuration with reduction set to 'prod'. It defines the layer to multiply the update values at the specified indices. Multiplication is performed at the indices [0, 2, -3, -3, 0].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-nd-update-15.rst#_snippet_6\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... reduction=\"prod\" type=\"ScatterNdUpdate\">\n    <input>\n        <port id=\"0\" precision=\"FP32\">  <!-- data -->\n            <dim>4</dim>  <!-- values: [1, 2, 3, 4] -->\n        </port>\n        <port id=\"1\" precision=\"I32\">  <!-- indices -->\n            <dim>5</dim>  <!-- values: [0, 2, -3, -3, 0] -->\n        </port>\n        <port id=\"2\" precision=\"FP32\">  <!-- updates -->\n            <dim>5</dim>  <!-- values: [10, 20, 30, 40, 50] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP32\">\n            <dim>4</dim>  <!-- values: [500, 3600, 40, 4] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Add Clang Format Target for OpenVINO Proxy Plugin\nDESCRIPTION: This snippet adds a clang-format target for the OpenVINO proxy plugin. This target is used to automatically format the code according to a predefined style guide.  `ov_add_clang_format_target` is a custom CMake function (presumably defined elsewhere).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to OpenVINO Runtime Object Library in CMake\nDESCRIPTION: This code links required libraries to the OpenVINO runtime object library, including internal and external dependencies such as `openvino::itt`, `openvino::util`, `openvino::core::dev`, and `nlohmann_json::nlohmann_json`. It also marks the target as a C/C++ target and ensures ABI stability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME}_obj PRIVATE openvino::itt openvino::util openvino::core::dev nlohmann_json::nlohmann_json)\nov_mark_target_as_cc(${TARGET_NAME}_obj)\n\n# OpenVINO Runtime is public API => need to mark this library as important for ABI free\nov_abi_free_target(${TARGET_NAME}_obj)\n```\n\n----------------------------------------\n\nTITLE: Creating Source Groups in CMake\nDESCRIPTION: Uses the source_group command to organize the source and header files into named groups (src and include) within the IDE project. This improves project organization and readability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Promote Floats using ConvertPromoteTypes in OpenVINO\nDESCRIPTION: This example demonstrates the ConvertPromoteTypes operation promoting FP16 and FP32 inputs to FP32. The 'promote_unsafe' and 'pytorch_scalar_promotion' attributes are set to false, and 'u64_integer_promotion_target' is set to f32. This example demonstrates how to convert the data type of tensor inputs to a common data type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/convert-promote-types-14.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ConvertPromoteTypes\">\n    <data promote_unsafe=\"false\" pytorch_scalar_promotion=\"false\" u64_integer_promotion_target=\"f32\"/>\n    <input>\n        <port id=\"0\" precision=\"FP16\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\" precision=\"FP32\">\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\", names=\"ConvertPromoteTypes:0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"3\" precision=\"FP32\", names=\"ConvertPromoteTypes:1\">\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: OVSA Model Server Start\nDESCRIPTION: Starts the secure OpenVINO Model Server using a shell script.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_55\n\nLANGUAGE: sh\nCODE:\n```\n./start_secure_ovsa_model_server.sh\n```\n\n----------------------------------------\n\nTITLE: ReadValue XML Example in OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of a ReadValue layer within an OpenVINO model. It shows how to define the variable_id, variable_type (f32), and variable_shape for the operation, as well as the input and output port dimensions. The 'force' directive in the code-block ensures the code is displayed as-is without interpretation by the documentation system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/read-value-6.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ReadValue\" ...>\n    <data variable_id=\"lstm_state_1\" variable_type=\"f32\" variable_shape=\"1,3,224,224\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: GELU Layer Definition with erf Approximation - XML\nDESCRIPTION: This XML snippet defines a Gelu layer in OpenVINO using the 'erf' approximation mode.  It specifies the input and output tensor dimensions for the layer. The 'approximation_mode' attribute in the 'data' element selects the Gauss error function approximation for the GELU calculation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/gelu-7.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Gelu\">\n    <data approximation_mode=\"erf\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>7</dim>\n            <dim>9</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>3</dim>\n            <dim>7</dim>\n            <dim>9</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Disabling Warnings for ONNX Targets\nDESCRIPTION: This snippet calls a custom function `ov_disable_all_warnings` to disable all warnings for the `onnx` and `onnx_proto` targets. This reduces the amount of warning output during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/onnx/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nov_disable_all_warnings(onnx onnx_proto)\n```\n\n----------------------------------------\n\nTITLE: Conditional Frontend Dependencies\nDESCRIPTION: These snippets conditionally add dependencies and definitions based on whether specific frontends (ONNX, TensorFlow, TensorFlow Lite, Paddle, PyTorch) are enabled. If a frontend is enabled, its corresponding library is added to the `DEPENDENCIES` list and a definition is added to the `DEFINITIONS` list.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/test_builtin_extensions/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (ENABLE_OV_ONNX_FRONTEND)\n    list(APPEND DEPENDENCIES openvino::frontend::onnx)\n    list(APPEND DEFINITIONS ENABLE_OV_ONNX_FRONTEND)\nendif()\n\nif (ENABLE_OV_TF_FRONTEND)\n    list(APPEND DEPENDENCIES openvino::frontend::tensorflow)\n    list(APPEND DEFINITIONS ENABLE_OV_TF_FRONTEND)\nendif()\n\nif (ENABLE_OV_TF_LITE_FRONTEND)\n    list(APPEND DEPENDENCIES openvino::frontend::tensorflow_lite)\n    list(APPEND DEFINITIONS ENABLE_OV_TF_LITE_FRONTEND)\nendif()\n\nif (ENABLE_OV_PADDLE_FRONTEND)\n    list(APPEND DEPENDENCIES openvino::frontend::paddle)\n    list(APPEND DEFINITIONS ENABLE_OV_PADDLE_FRONTEND)\nendif()\n\nif (ENABLE_OV_PYTORCH_FRONTEND)\n    list(APPEND DEPENDENCIES openvino::frontend::pytorch)\n    list(APPEND DEFINITIONS ENABLE_OV_PYTORCH_FRONTEND)\nendif()\n```\n\n----------------------------------------\n\nTITLE: STFT Configuration for 1D Signal, transpose_frames=false (XML)\nDESCRIPTION: This XML snippet demonstrates the configuration of the STFT operation for a 1D signal input where the transpose_frames attribute is set to false.  The input signal has a dimension of 56, the window has a dimension of 7, frame_size is 11 and frame_step is 3. The output shape is [16, 6, 2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/stft-15.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"STFT\" ... >\n    <data transpose_frames=\"false\"/>\n    <input>\n        <port id=\"0\">\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n        </port>\n        <port id=\"2\"></port> <!-- value: 11 -->\n        <port id=\"3\"></port> <!-- value: 3 -->\n    <output>\n        <port id=\"4\">\n            <dim>16</dim>\n            <dim>6</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Globbing SYCL Source Files\nDESCRIPTION: This snippet uses `file(GLOB_RECURSE)` to collect all SYCL-related source files (`sycl_*.cpp`) in the `ocl` subdirectory and stores them in the `SYCL_SOURCES` variable. This is used for conditional compilation based on the compiler type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SYCL_SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/ocl/sycl_*.cpp\")\n```\n\n----------------------------------------\n\nTITLE: Installing C++ CMakeLists.txt for C Samples in CMake\nDESCRIPTION: Installs the CMakeLists.txt file from the C++ samples directory into the C samples directory. This is likely done to provide a base CMakeLists.txt for the C samples to use or modify.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(FILES cpp/CMakeLists.txt\n        DESTINATION ${OV_CPACK_SAMPLESDIR}/c\n        COMPONENT ${OV_CPACK_COMP_C_SAMPLES}\n        ${OV_CPACK_COMP_C_SAMPLES_EXCLUDE_ALL})\n```\n\n----------------------------------------\n\nTITLE: Adding Static Library CMake\nDESCRIPTION: Creates a static library named `${TARGET_NAME}` using the source files listed in the `SRC` variable. The `add_library` command defines the library build target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/lib/src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${SRC})\n```\n\n----------------------------------------\n\nTITLE: Dumping Layout to String in C++\nDESCRIPTION: This snippet demonstrates how to convert a layout object to a string representation in C++ using `layout.to_string()`. This is useful for debugging and serialization purposes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nov::Layout layout(\"NCHW\");\nstd::string layout_str = layout.to_string();\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories CMake\nDESCRIPTION: This code snippet adds the 'functional' and 'unit' subdirectories to the CMake project. This allows CMake to process the CMakeLists.txt files within those subdirectories and include their contents in the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(functional)\nadd_subdirectory(unit)\n```\n\n----------------------------------------\n\nTITLE: Attribute Renaming in Custom Operation Mapping (C++)\nDESCRIPTION: Illustrates mapping a custom operation, renaming attributes during the process. The `named_attributes` parameter of the `OpExtension` constructor specifies the mapping between framework attribute names and OpenVINO attribute names.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/frontend-extensions.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nCore core;\ncore.add_extension(std::make_shared<ov::frontend::OpExtension<CustomOperation>>(std::unordered_map<std::string, std::string>{\n    {\"fw_attr1\", \"attr1\"},\n    {\"fw_attr2\", \"attr2\"}}));\nauto model = core.read_model(\"model.onnx\");\n```\n\n----------------------------------------\n\nTITLE: Install Python prerequisites on Ubuntu\nDESCRIPTION: This command installs the necessary Python prerequisites on Ubuntu, including Python 3, the Python development package, setuptools, and the pip package manager. This is a necessary first step for installing and using OpenVINO Python tools.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/README.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nsudo apt-get install python3 python3-dev python3-setuptools python3-pip\n```\n\n----------------------------------------\n\nTITLE: OpenVINO C API Project Setup with CMake\nDESCRIPTION: This CMake snippet defines the OpenVINO_C_API project and adds subdirectories for source code and tests (if enabled). It configures the overall structure of the C API build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(OpenVINO_C_API)\n\nadd_subdirectory(src)\n\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Extensions to OpenVINO IR Frontend\nDESCRIPTION: This snippet describes how to add extensions to the OpenVINO IR Frontend using the `ov::frontend::ir::Frontend::add_extension()` API. It lists the supported extension types, including `ov::TelemetryExtension`, `ov::BaseOpExtension`, and `ov::detail::SOExtension`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/ir/docs/architecture.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov::frontend::ir::Frontend::add_extension()\n```\n\n----------------------------------------\n\nTITLE: Applying GNU GPL Notices\nDESCRIPTION: This code snippet demonstrates how to attach the GNU General Public License (GPL) notices to the start of each source file in a new program. It includes the copyright line, a statement about the software being free and redistributable, a disclaimer of warranty, and a link to the full GPL license.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/onetbb_third-party-programs.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n<one line to give the program's name and a brief idea of what it does.>\nCopyright (C) <year>  <name of author>\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <https://www.gnu.org/licenses/>.\n```\n\n----------------------------------------\n\nTITLE: 3D GroupConvolution XML Example\nDESCRIPTION: This XML configuration shows a 3D GroupConvolution layer in OpenVINO.  It provides tensor dimensions for inputs and outputs, and the crucial parameters dilations, padding values, and strides. This illustrates how to implement a 3D convolution operation with grouped channels.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/group-convolution-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"GroupConvolution\" ...>\n    <data dilations=\"1,1,1\" pads_begin=\"2,2,2\" pads_end=\"2,2,2\" strides=\"1,1,1\" auto_pad=\"explicit\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>12</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>5</dim>\n            <dim>5</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Promote Integers Unsafely using ConvertPromoteTypes in OpenVINO\nDESCRIPTION: This example showcases the ConvertPromoteTypes operation performing an unsafe promotion of I16 and U32 inputs to I64.  The 'promote_unsafe' attribute is set to true, allowing the potentially unsafe conversion. 'pytorch_scalar_promotion' is false and 'u64_integer_promotion_target' is f32. This example demonstrates an integer type promotion scenario.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/convert-promote-types-14.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ConvertPromoteTypes\">\n    <data promote_unsafe=\"true\" pytorch_scalar_promotion=\"false\" u64_integer_promotion_target=\"f32\"/>\n    <input>\n        <port id=\"0\" precision=\"I16\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\" precision=\"U32\">\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"I64\", names=\"ConvertPromoteTypes:0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"3\" precision=\"I64\", names=\"ConvertPromoteTypes:1\">\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: AvgPool Configuration: same_upper, exclude-pad false\nDESCRIPTION: This XML snippet configures an AvgPool layer with 'same_upper' auto_pad and 'exclude-pad' set to false. It uses a kernel size of 5x5, stride of 2x2, and defines input/output port dimensions. 'exclude-pad' being false means zero-values that came from padding are included in averaging calculation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/avg-pool-14.rst#_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"AvgPool\" ... >\n    <data auto_pad=\"same_upper\" exclude-pad=\"false\" kernel=\"5,5\" pads_begin=\"0,0\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Compiler-Specific Flag Adjustments\nDESCRIPTION: This code block adjusts compiler flags based on the detected compiler. It removes the `-Zi` flag (program database for edit and continue) for MSVC, and adds `-Wno-undef` and `-Wno-deprecated-copy` flags for GCC and Clang to suppress specific warnings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/gtest/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(target gtest gtest_main gmock gmock_main)\n    if(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n        get_target_property(_target_cxx_flags ${target} COMPILE_FLAGS)\n        if(_target_cxx_flags)\n            string(REPLACE \"-Zi\" \" \" _target_cxx_flags ${_target_cxx_flags})\n            set_target_properties(${target} PROPERTIES COMPILE_FLAGS \"${_target_cxx_flags}\")\n        endif()\n    elseif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG OR (OV_COMPILER_IS_INTEL_LLVM AND UNIX))\n        target_compile_options(${target} PRIVATE -Wno-undef)\n        if(CMAKE_COMPILER_IS_GNUCXX)\n            target_compile_options(${target} PRIVATE -Wno-deprecated-copy)\n        endif()\n    endif()\n    # disable warnings\n    ov_disable_all_warnings(${target})\n    set_target_properties(${target} PROPERTIES FOLDER thirdparty)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Mermaid Diagram of Dynamic Shape Execution Flow\nDESCRIPTION: This Mermaid diagram visualizes the control flow of dynamic shape model execution within OpenVINO, illustrating decision points related to shape changes, kernel fusion, and memory allocation. It shows the interaction between different functions and processes during inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/dynamic_shape/overall_flow.md#_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    A[\"primitive_inst::execute()\"] --> B{\"is dynamic?\"}\n    B --> |No  | H[\"Execute impl\"]\n    B --> |Yes | C[\"runtime in_place_concat\"]\n    C --> D[\"update_shape()\"]\n    D --> E{\"shape changed from <br/> previous inference?\"}\n    E --> |No | H[\"Execute impl\"]\n    E --> |Yes| G{\"Valid fusion?\"}\n    G --> |No | I{\"Create unfused subgraph\"}\n    I --> II[\"Execute subgraph\"]\n    G --> |Yes | J[\"update_impl()\"]\n    J --> JJ{\"Impl changed?\"}\n    JJ --> |No | L[\"Set arguments\"]\n    L --> H\n    JJ --> |Yes | KK{\"preferred weight format <br/> changed?\"}\n    KK --> |Yes | M[\"update_weights()\"]\n    KK --> |No  | O{\"Is current memory enough <br/> for the new shape?\"}\n    M --> O\n    O --> |No |P[\"reallocate output memory\"]\n    O --> |Yes | L\n    P --> L\n```\n\n----------------------------------------\n\nTITLE: BitBake Configuration\nDESCRIPTION: Configures BitBake to enable specific features and packages.  It sets the `MACHINE` variable for the target architecture, enables clDNN GPU plugin, OpenVINO Python API, and includes OpenVINO related libraries, samples and the Python API package in the target image.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yocto.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# Build with SSE4.2, AVX2 etc. extensions\nMACHINE = \"intel-skylake-64\"\n# Enable clDNN GPU plugin when needed.\n# This requires meta-clang and meta-oe layers to be included in bblayers.conf\n# and is not enabled by default.\nPACKAGECONFIG:append:pn-openvino-inference-engine = \" opencl\"\n# Enable building OpenVINO Python API.\n# This requires meta-python layer to be included in bblayers.conf.\nPACKAGECONFIG:append:pn-openvino-inference-engine = \" python3\"\n# This adds OpenVINO related libraries in the target image.\nCORE_IMAGE_EXTRA_INSTALL:append = \" openvino-inference-engine\"\n# This adds OpenVINO samples in the target image.\nCORE_IMAGE_EXTRA_INSTALL:append = \" openvino-inference-engine-samples\"\n# Include OpenVINO Python API package in the target image.\nCORE_IMAGE_EXTRA_INSTALL:append = \" openvino-inference-engine-python3\"\n```\n\n----------------------------------------\n\nTITLE: SliceScatter Example 1: Slice over axis 0\nDESCRIPTION: This XML code demonstrates an example of using SliceScatter to fill a slice over axis 0. The data tensor has shape (2, 5), the updates tensor has shape (1, 5), and the slice is defined by start=0, stop=1, step=1, and axis=0.  The first row of the data tensor is replaced with the values from the updates tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-scatter-15.rst#_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"SliceScatter\">\n    <input>\n        <port id=\"0\" precision=\"FP32\">  <!-- data -->\n            <dim>2</dim>\n            <dim>5</dim>  <!-- values: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]] -->\n        </port>\n        <port id=\"1\" precision=\"FP32\">  <!-- updates -->\n            <dim>1</dim>\n            <dim>5</dim>  <!-- values: [[10, 20, 30, 40, 50]] -->\n        </port>\n        <port id=\"2\" precision=\"I32\">  <!-- start -->\n            <dim>1</dim>  <!-- values: [0] -->\n        </port>\n        <port id=\"3\" precision=\"I32\">  <!-- stop -->\n            <dim>1</dim>  <!-- values: [1] -->\n        </port>\n        <port id=\"4\" precision=\"I32\">  <!-- step -->\n            <dim>1</dim>  <!-- values: [1] -->\n        </port>\n        <port id=\"5\" precision=\"I32\">  <!-- axes -->\n            <dim>1</dim>  <!-- values: [0] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"6\" precision=\"FP32\">\n            <dim>2</dim>\n            <dim>5</dim>  <!-- values: [[10, 20, 30, 40, 50], [5, 6, 7, 8, 9]] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: PriorBox Layer Configuration XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a PriorBox layer. It includes settings for aspect ratios, clipping, densities, fixed ratios/sizes, flipping, min/max sizes, offset, step, and variance. The input and output port dimensions are also defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/prior-box-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"PriorBox\" ...>\n    <data aspect_ratio=\"2.0\" clip=\"false\" density=\"\" fixed_ratio=\"\" fixed_size=\"\" flip=\"true\" max_size=\"38.46\" min_size=\"16.0\" offset=\"0.5\" step=\"16.0\" variance=\"0.1,0.1,0.2,0.2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>        <!-- values: [24, 42] -->\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>        <!-- values: [384, 672] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>2</dim>\n            <dim>16128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Conditional Snippets LIBXSMM TPP Definitions\nDESCRIPTION: This snippet conditionally adds definitions and configures linking for LIBXSMM TPP support in snippets. If `ENABLE_SNIPPETS_LIBXSMM_TPP` is enabled, it adds compile definitions, compile definitions to disable blas and links xsmm library.  It also adds include directories for xsmm.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/vectorized/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_SNIPPETS_LIBXSMM_TPP)\n    add_definitions(-DSNIPPETS_LIBXSMM_TPP -DLIBXSMM_DEFAULT_CONFIG)\n    target_compile_definitions(xsmm PRIVATE __BLAS=0)\n    target_link_libraries(${TARGET_NAME} PRIVATE xsmm)\n    target_include_directories(${TARGET_NAME} SYSTEM PRIVATE $<TARGET_PROPERTY:xsmm,INCLUDE_DIRECTORIES>)\nendif()\n```\n\n----------------------------------------\n\nTITLE: MaxPool Layer Definition with 'valid' Auto Pad (XML)\nDESCRIPTION: This example demonstrates the XML definition of a MaxPool layer in OpenVINO using the 'valid' auto_pad attribute. The kernel size, padding, and strides are specified. The input and output ports define the dimensions of the tensors, leading to a reduced output size due to the 'valid' padding.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-8.rst#_snippet_10\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MaxPool\" ... >\n       <data auto_pad=\"valid\" kernel=\"2,2\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n       <input>\n           <port id=\"0\">\n               <dim>1</dim>\n               <dim>3</dim>\n               <dim>32</dim>\n               <dim>32</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"1\">\n               <dim>1</dim>\n               <dim>3</dim>\n               <dim>16</dim>\n               <dim>16</dim>\n           </port>\n           <port id=\"2\">\n               <dim>1</dim>\n               <dim>3</dim>\n               <dim>16</dim>\n               <dim>16</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Notebooks Dockerfile\nDESCRIPTION: This Dockerfile sets up an environment for running OpenVINO notebooks. It specifies the base image, labels, environment variables, and commands to install necessary dependencies, including Node.js, mesa-libGL, dos2unix, GPU drivers and updates.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_6\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM quay.io/thoth-station/s2i-thoth-ubi8-py38:v0.29.0\n\nLABEL name=\"OpenVINO(TM) Notebooks\" \\\n  maintainer=\"helena.kloosterman@intel.com\" \\\n  vendor=\"Intel Corporation\" \\\n  version=\"0.2.0\" \\\n  release=\"2021.4\" \\\n  summary=\"OpenVINO(TM) Developer Tools and Jupyter Notebooks\" \\\n  description=\"OpenVINO(TM) Notebooks Container\"\n\nENV JUPYTER_ENABLE_LAB=\"true\" \\\n  ENABLE_MICROPIPENV=\"1\" \\\n  UPGRADE_PIP_TO_LATEST=\"1\" \\\n  WEB_CONCURRENCY=\"1\" \\\n  THOTH_ADVISE=\"0\" \\\n  THOTH_ERROR_FALLBACK=\"1\" \\\n  THOTH_DRY_RUN=\"1\" \\\n  THAMOS_DEBUG=\"0\" \\\n  THAMOS_VERBOSE=\"1\" \\\n  THOTH_PROVENANCE_CHECK=\"0\"\n\nUSER root\n\n# Upgrade NodeJS > 12.0\n# Install dos2unix for line end conversion on Windows\nRUN dnf --disableplugin=subscription-manager remove -y nodejs && \\\n  dnf --disableplugin=subscription-manager module -y reset nodejs && \\\n  dnf --disableplugin=subscription-manager module -y enable nodejs:20 && \\\n  dnf --disableplugin=subscription-manager install -y nodejs mesa-libGL dos2unix libsndfile && \\\n  dnf --disableplugin=subscription-manager -y update-minimal --security --sec-severity=Important --sec-severity=Critical --sec-severity=Moderate\n\n# GPU drivers\nRUN dnf --disableplugin=subscription-manager install -y 'dnf-command(config-manager)' && \\\n    dnf --disableplugin=subscription-manager config-manager --add-repo  https://repositories.intel.com/gpu/rhel/8.6/lts/2350/unified/intel-gpu-8.6.repo\n\nRUN rpm -ivh https://vault.centos.org/centos/8/AppStream/x86_64/os/Packages/mesa-filesystem-21.1.5-1.el8.x86_64.rpm && \\\n    dnf --disableplugin=subscription-manager install --refresh -y \\\n\n```\n\n----------------------------------------\n\nTITLE: Streaming Output in C++\nDESCRIPTION: This C++ code demonstrates streaming output from an LLMPipeline.  It defines a lambda function `streamer` that prints each generated word immediately to the console with `std::flush`.  It returns `false` to indicate that generation should continue.  The `pipe.generate` function is called with the streamer and a maximum number of new tokens.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai.rst#_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/genai/llm_pipeline.hpp\"\n#include <iostream>\n\nint main(int argc, char* argv[]) {\n   std::string model_path = argv[1];\n   ov::genai::LLMPipeline pipe(model_path, \"CPU\");\n\n   auto streamer = [](std::string word) {\n      std::cout << word << std::flush;\n      // Return flag indicating whether generation should be stopped.\n      // false means continue generation.\n      return false;\n   };\n   pipe.generate(\"The Sun is yellow because\", ov::genai::streamer(streamer), ov::genai::max_new_tokens(100));\n}\n```\n\n----------------------------------------\n\nTITLE: Install Custom Sphinx Sitemap\nDESCRIPTION: Installs the custom OpenVINO Sphinx sitemap. This sitemap is located in the `docs/openvino_custom_sphinx_sitemap` directory.  It uses pip within the activated virtual environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/documentation_build_instructions.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\n(env) $ pip install docs/openvino_custom_sphinx_sitemap\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Example 3 - XML\nDESCRIPTION: This XML snippet provides an example of the ScatterElementsUpdate operation with use_init_val set to 'true' and reduction set to 'none'. It showcases the overwriting behavior with multi-dimensional tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_13\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... use_init_val=\"true\" reduction=\"none\" type=\"ScatterElementsUpdate\">\n    <input>\n        <port id=\"0\">>  <!-- data -->\n            <dim>3</dim>\n            <dim>4</dim>  <!-- values: [[0, 0, 0, 0],\n                                         [0, 0, 0, 0],\n                                         [0, 0, 0, 0]] -->\n        </port>\n        <port id=\"1\">  <!-- indices -->\n            <dim>2</dim>\n            <dim>2</dim>  <!-- values: [[1, 2],\n                                         [0, 3]] -->\n        </port>\n        <port id=\"2\">>  <!-- updates -->\n            <dim>2</dim>\n            <dim>2</dim>  <!-- values: [[11, 12],\n                                         [13, 14]]) -->\n        </port>\n        <port id=\"3\">     <!-- values: [1] -->\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"4\" precision=\"I32\">\n            <dim>3</dim>\n            <dim>4</dim>  <!-- values:  [[ 0, 11, 12,  0],\n                                          [13,  0,  0, 14],\n                                          [ 0,  0,  0,  0]] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License to Work\nDESCRIPTION: This snippet provides the boilerplate notice required to apply the Apache License to a project. The user needs to replace the bracketed fields with their own identifying information, such as the copyright year and owner's name. The text should be enclosed in the appropriate comment syntax for the file format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/runtime-third-party-programs.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: OpenVINO IR Frontend Flowchart\nDESCRIPTION: This flowchart illustrates the data flow within the OpenVINO IR Frontend. It shows how the IR file is read by the frontend to create an ov::Model, which is then used by the OpenVINO library. It uses Mermaid syntax to visually represent the components and their interactions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/ir/README.md#_snippet_0\n\nLANGUAGE: Mermaid\nCODE:\n```\nflowchart LR\n    ir[(\"IR (*.xml)\")]\n\n    style ir fill:#427cb0\n\n    ir_fe[\"OpenVINO IR Frontend\"]\n\n    openvino(openvino library)\n    ir--Read ir---ir_fe\n    ir_fe--Create ov::Model--->openvino\n    click ir \"https://docs.openvino.ai/2025/documentation/openvino-ir-format/operation-sets.html\"\n```\n\n----------------------------------------\n\nTITLE: StringTensorPack Example 4 (2D)\nDESCRIPTION: This example demonstrates the StringTensorPack operation with 2D tensors for 'begins' and 'ends'. It packs the string \"IntelOpenVINOOMZGenAI\" into a 2D string tensor [[\"Intel\", \"OpenVINO\"], [\"OMZ\", \"GenAI\"]] based on the provided begin and end indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/string-tensor-pack-15.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StringTensorPack\" ... >\n    <input>\n        <port id=\"0\" precision=\"I64\">\n            <dim>2</dim>     <!-- begins = [[0, 5], [13, 16]] -->\n            <dim>2</dim>\n        </port>\n        <port id=\"1\" precision=\"I64\">\n            <dim>2</dim>     <!-- ends = [[5, 13], [16, 21]] -->\n            <dim>2</dim>\n        </port>\n        <port id=\"2\" precision=\"U8\">\n            <dim>21</dim>    <!-- symbols = \"IntelOpenVINOOMZGenAI\" -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"STRING\">\n            <dim>2</dim>     <!-- output = [[\"Intel\", \"OpenVINO\"], [\"OMZ\", \"GenAI\"]] -->\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Filtering Install Interface Properties\nDESCRIPTION: This function filters the install interface properties for the specified target (gtest, gtest_main, gmock, gmock_main) to correctly handle include directories. It distinguishes between build interface and install interface include directories, ensuring proper include paths during both the build and installation phases. The function iterates through the interface include directories, replaces parts of generator expressions and sets the target properties accordingly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/gtest/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(_ov_gtest_filter_install_interface TARGET TYPE)\n    set(final_include_dirs \"$<INSTALL_INTERFACE:developer_package/include/${TYPE}>\")\n\n    get_target_property(include_dirs ${TARGET} INTERFACE_INCLUDE_DIRECTORIES)\n    foreach(include_dir IN LISTS include_dirs)\n        if(NOT include_dir MATCHES \".*INSTALL_INTERFACE.*\")\n            # remove leading and trailing parts of generator expressions\n            string(REPLACE \"$<BUILD_INTERFACE:\" \"\" include_dir \"${include_dir}\")\n            string(REPLACE \">\" \"\" include_dir \"${include_dir}\")\n            # wrap to BUILD_INTERFACE again\n            list(APPEND final_include_dirs \"$<BUILD_INTERFACE:${include_dir}>\")\n        endif()\n    endforeach()\n\n    set_target_properties(${TARGET} PROPERTIES\n        INTERFACE_INCLUDE_DIRECTORIES \"${final_include_dirs}\"\n        INTERFACE_SYSTEM_INCLUDE_DIRECTORIES \"${final_include_dirs}\")\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Conditional MLAS Library Setting\nDESCRIPTION: If 'ENABLE_MLAS_FOR_CPU' is enabled, this sets the 'MLAS_LIBRARY' variable to 'mlas'. Otherwise, the MLAS tests are excluded. The library is then linked to the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT ENABLE_MLAS_FOR_CPU)\n    list(APPEND EXCLUDED_SOURCE_PATHS_FOR_UNIT_TEST ${CMAKE_CURRENT_SOURCE_DIR}/gemm_api_test.cpp)\nelse()\n    set(MLAS_LIBRARY \"mlas\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: MatMul Matrix-Vector Multiplication XML Configuration\nDESCRIPTION: Configuration example demonstrating MatMul operation for matrix-vector multiplication. The input ports define the dimensions of the input matrix (1000x1024) and the input vector (1024). The output port defines the resulting vector (1000).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/matmul-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MatMul\">\n    <input>\n        <port id=\"0\">\n            <dim>1000</dim>\n            <dim>1024</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1024</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1000</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Filter Blob Dumps by Node Ports\nDESCRIPTION: Filters the blobs to be dumped based on the node ports (IN, OUT, or ALL) for OpenVINO CPU executions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_NODE_PORTS='<ports_kind>' binary ...\n```\n\n----------------------------------------\n\nTITLE: Defining ARM Target Architectures\nDESCRIPTION: This snippet defines the target architecture for ARM and AArch64. The target architecture and available sub-architectures are set based on whether the platform is ARM, AARCH64, or Apple Silicon. It also factors in the `OV_CPU_AARCH64_USE_MULTI_ISA` option.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(OV_CPU_ARM_TARGET_GENERIC_ARCHS armv8a\n                                    armv8.2-a\n                                    armv8.6-a armv8.6-a-sve armv8.6-a-sve2 armv8.6-a-sve2-sme2\n                                    armv8r64 # the same as armv8.4-a\n)\nif(ARM)\n    set(OV_CPU_ARM_TARGET_ARCH_DEFAULT armv7a)\n    set(OV_CPU_ARM_TARGET_ARCHS armv7a armv7a-hf\n                                # requires estate=32\n                                ${OV_CPU_ARM_TARGET_GENERIC_ARCHS})\nelif(AARCH64)\n    if(APPLE)\n        set(OV_CPU_ARM_TARGET_ARCH_DEFAULT arm64-v8.2-a)\n    else()\n        if(OV_CPU_AARCH64_USE_MULTI_ISA)\n            # set v8a even we want fp16 kernels, because\n            # we use multi_isa=1 in ACLConfig.cmake to enable both fp16 and fp32 kernels\n            # actual kernel is selected in runtime based on runtime capabilities\n            set(OV_CPU_ARM_TARGET_ARCH_DEFAULT arm64-v8a)\n        else()\n            set(OV_CPU_ARM_TARGET_ARCH_DEFAULT arm64-v8.2-a)\n        endif()\n    endif()\n    set(OV_CPU_ARM_TARGET_ARCHS arm64-v8a\n                                arm64-v8.2-a arm64-v8.2-a-sve arm64-v8.2-a-sve2\n                                # used with estate=64\n                                ${OV_CPU_ARM_TARGET_GENERIC_ARCHS})\nendif()\nset(OV_CPU_ARM_TARGET_ARCH ${OV_CPU_ARM_TARGET_ARCH_DEFAULT} CACHE STRING \"Architecture for ARM ComputeLibrary\")\nset_property(CACHE OV_CPU_ARM_TARGET_ARCH PROPERTY STRINGS ${OV_CPU_ARM_TARGET_ARCHS})\n```\n\n----------------------------------------\n\nTITLE: GatherND Layer Configuration Example 3\nDESCRIPTION: Shows an XML configuration example for GatherND with batch_dims set to 3, and the input/output port dimensions. It specifies the shape of the data and indices tensors and resulting output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-8.rst#_snippet_9\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"GatherND\" version=\"opset8\">\n    <data batch_dims=\"3\" />\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>64</dim>\n            <dim>320</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>64</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>64</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flag for Intel LLVM on Windows\nDESCRIPTION: This snippet adds a compiler flag to suppress Microsoft include warnings when using the Intel LLVM compiler on Windows.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nelseif (OV_COMPILER_IS_INTEL_LLVM AND WIN32)\n    ov_add_compiler_flags(\"/Wno-microsoft-include\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Maximum Reduction - C++\nDESCRIPTION: This code snippet demonstrates the 'max' reduction mode for the ScatterElementsUpdate operation. The maximum value between the corresponding element from the updates tensor and the element in the data tensor at the specified index is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\noutput[indices[i]] = max(updates[i], output[indices[i]]) axis = 0\n```\n\n----------------------------------------\n\nTITLE: Finding OpenCV in CMake\nDESCRIPTION: Uses `find_package` to locate the OpenCV library with specific components (core, imgproc, imgcodecs).  It checks if OpenCV is found and if the version is greater than or equal to 3. If OpenCV is not found or the version is insufficient, a warning message is displayed. If OpenCV is found, it links the library and defines a compilation definition.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/format_reader/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenCV QUIET COMPONENTS core imgproc imgcodecs)\nif(NOT OpenCV_FOUND OR NOT OpenCV_VERSION VERSION_GREATER_EQUAL 3)\n    message(WARNING \"OpenCV ver. 3.0+ is not found, ${TARGET_NAME} will be built without OpenCV support\")\nelse()\n    target_link_libraries(${TARGET_NAME} PRIVATE ${OpenCV_LIBRARIES} ie_samples_utils)\n    if(UNIX AND NOT APPLE)\n        # Workaround issue that rpath-link is missing for PRIVATE dependencies\n        # Fixed in cmake 3.16.0 https://gitlab.kitware.com/cmake/cmake/issues/19556\n        target_link_libraries(${TARGET_NAME} INTERFACE \"-Wl,-rpath-link,${OpenCV_INSTALL_PATH}/lib\")\n    endif()\n    target_compile_definitions(${TARGET_NAME} PRIVATE USE_OPENCV)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enable Sparse Weights Decompression in C++\nDESCRIPTION: This C++ snippet shows how to enable the sparse weights decompression feature in OpenVINO by setting the `sparse_weights_decompression_rate` property using `SetConfig` before compiling the model. The property controls the sparse rate threshold for applying the feature. A value of 1 disables the feature. The property must be set before `compile_model()` is called.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\n// Example of sparse weights decompression rate property usage\n\n#include <openvino/openvino.hpp>\n\nint main() {\n    // Create OpenVINO Core object\n    ov::Core core;\n\n    // Set the property before calling compile_model()\n    core.set_property(\"CPU\", ov::properties({{ov::intel_cpu::sparse_weights_decompression_rate(0.8)}}));\n\n    // Read the model\n    std::shared_ptr<ov::Model> model = core.read_model(\"path_to_your_model/your_model.xml\");\n\n    // Compile the model\n    ov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\n\n    // Create an inference request\n    ov::InferRequest infer_request = compiled_model.create_infer_request();\n\n    // omitted: Prepare input\n\n    // Start inference\n    infer_request.infer();\n\n    return 0;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Wrap USM pointer (C++)\nDESCRIPTION: This snippet shows how to create an `ov::RemoteTensor` by wrapping an existing USM (Unified Shared Memory) pointer using the OpenVINO C++ API. It retrieves the `ClContext` and creates a tensor from the pointer. Requires OpenVINO runtime, intel_gpu ocl, and CL/cl_ext.hpp.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n//! [wrap_usm_pointer]\n#include <openvino/runtime/core.hpp>\n#include <openvino/runtime/intel_gpu/ocl/ocl.hpp>\n\n#if defined(OPENVINO_USE_OPENCL)\n#include <CL/cl.hpp>\n#include <CL/cl_ext.hpp>\n#endif\n\nvoid wrap_usm_ptr(void* usm_ptr, size_t size) {\n#if defined(CL_VERSION_2_0)\n    // Wrap usm pointer to remote tensor\n    ov::Core core;\n    auto context = core.get_default_context(\"GPU\").as<ov::intel_gpu::ocl::ClContext>();\n    ov::Tensor remote_tensor = context.create_tensor(ov::element::f32, {ov::Dimension(size)}, usm_ptr);\n    (void)remote_tensor;\n#endif\n}\n//! [wrap_usm_pointer]\n```\n\n----------------------------------------\n\nTITLE: ISTFT Example: 3D input, 1D output, center=true, default length (XML)\nDESCRIPTION: Example of ISTFT operation with a 3D data input producing a 1D output signal.  The 'center' attribute is set to 'true', indicating that padding was applied to the original signal. The signal length is calculated by default based on the 'center' attribute being true.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/istft-16.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ISTFT\" ... >\n    <data center=\"true\" ... />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>16</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n        </port>\n        <port id=\"2\"></port> <!-- frame_size value: 11 -->\n        <port id=\"3\"></port> <!-- frame_step value: 3 -->\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>45</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: NormalizeL2 Example (XML) - Channel Normalization\nDESCRIPTION: This XML snippet demonstrates the usage of the NormalizeL2 operation for normalizing over the channel dimension in an NCHW layout. It defines a layer with input and output ports, and specifies the 'eps' and 'eps_mode' attributes. The axes input indicates that normalization is performed along the channel dimension (axis 1).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/normalize-l2-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"NormalizeL2\" ...>\n    <data eps=\"1e-8\" eps_mode=\"add\"/>\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- axes list [1] means normalization over channel dimension -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: MaxPool with 4D input, 2D kernel, same_upper padding (sh)\nDESCRIPTION: Presents MaxPool operation on a 4D input tensor with a 2D kernel and 'same_upper' auto padding. The code block includes the input tensor, strides, kernel size, rounding type, auto padding, and the resulting output tensor and indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-8.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[-1, 2, 3],\n                 [4, 5, -6],\n                 [-7, 8, 9]],\n                [[2, -1, 5],\n                 [6, -7, 1],\n                 [8, 2, -3]]]]\nstrides = [1, 1]\nkernel = [2, 2]\nrounding_type = \"floor\"\nauto_pad = \"same_upper\"\noutput0 = [[[[5, 5, 3],\n                   [8, 9, 9]\n                   [8, 9, 9]],\n                  [[6, 5, 5],\n                   [8, 2, 1],\n                   [8, 2, -3]]]]\noutput1 = [[[[4, 4, 2],\n                   [7, 8, 8]\n                   [7, 8, 8]],\n                  [[12, 11, 11],\n                   [15, 16, 14],\n                   [15, 16, 17]]]]\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for Statistics Collection (First Build)\nDESCRIPTION: This code snippet shows how to configure CMake for the first build, which collects statistics about the components used during inference. The `-DSELECTIVE_BUILD=COLLECT` option enables the collection of these statistics. The collected statistics are used to determine which components should be included in the second, selective build. The `-DENABLE_PROFILING_ITT=ON` option is also enabled for profiling.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/selective_build.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd ./build\n\ncmake .. \\\n-DCMAKE_BUILD_TYPE=Release \\\n-DCMAKE_BUILD_TYPE=Release \\\n-DENABLE_CPU_DEBUG_CAPS=ON \\\n-DENABLE_PROFILING_ITT=ON \\\n-DSELECTIVE_BUILD=COLLECT \\\n-DCMAKE_INSTALL_PREFIX=`pwd`/install \\\n-DCMAKE_INSTALL_RPATH=`pwd`/install/runtime/3rdparty/tbb/lib:`pwd`/install/runtime/lib/intel64\n\ncmake --build . --config Release -j 8\n```\n\n----------------------------------------\n\nTITLE: Defining CPU Debug Caps\nDESCRIPTION: This snippet conditionally defines the `CPU_DEBUG_CAPS` macro if `ENABLE_CPU_DEBUG_CAPS` is enabled, likely for debugging purposes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_CPU_DEBUG_CAPS)\n    add_definitions(-DCPU_DEBUG_CAPS)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Set Target Name\nDESCRIPTION: Defines the target name for the TensorFlow Lite frontend tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"ov_tensorflow_lite_frontend_tests\")\n```\n\n----------------------------------------\n\nTITLE: Adding Test Target in CMake\nDESCRIPTION: This snippet uses the `ov_add_test_target` macro to create a test target for the proxy plugin. It specifies the name, root directory, dependencies, link libraries, and labels for the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        DEPENDENCIES\n            ${DEPENDENCIES}\n        LINK_LIBRARIES\n            openvino::runtime::dev\n            gtest\n            gtest_main\n            common_test_utils\n        ADD_CLANG_FORMAT\n        LABELS\n            OV UNIT PROXY\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Model Inputs and Outputs using Properties in OpenVINO Python\nDESCRIPTION: This snippet showcases how to access inputs and outputs of `Model` and `CompiledModel` objects using properties in the OpenVINO Python API. This provides a more Pythonic way to interact with model metadata compared to the C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\n\n# access inputs\nfor input_node in model.inputs:\n    print(f\"Input name: {input_node.get_any_name()}\")\n\n# access outputs\nfor output_node in model.outputs:\n    print(f\"Output name: {output_node.get_any_name()}\")\n```\n\n----------------------------------------\n\nTITLE: Setting DNNL Include Directories\nDESCRIPTION: Sets the include directories for the DNNL (Deep Neural Network Library) dependency of the target, ensuring the compiler can locate the necessary header files. These include directories are added as system include directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_37\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} SYSTEM PRIVATE $<TARGET_PROPERTY:dnnl,INCLUDE_DIRECTORIES>)\n```\n\n----------------------------------------\n\nTITLE: Defining a Property in C++\nDESCRIPTION: This C++ code snippet demonstrates the definition of a static constexpr Property.  It is used for accessing device properties in OpenVINO. The full_name property, for instance, can be used to query the full device name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/api_reference.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstatic constexpr Property<std::string> full_name{\"FULL_DEVICE_NAME\"};\n```\n\n----------------------------------------\n\nTITLE: ScaledDotProductAttention Example 3 (XML)\nDESCRIPTION: This XML snippet demonstrates the use of batch dimension broadcasting within the ScaledDotProductAttention layer. The example configures input ports (query, key, value, attention_mask) to showcase how dimensions are broadcasted, with N1 repeating 4 times, N2 repeating 1 time and N3 repeating 1 time for key. The 'causal' attribute is set to false.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/scaled-dot-product-attention.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"287\" name=\"aten::scaled_dot_product_attention_0\" type=\"ScaledDotProductAttention\" version=\"opset13\">\n\t\t\t<data causal=\"false\" />\n\t\t\t<input>\n\t\t\t\t<!-- Multiple batch dimensions, broadcastable to the following values: N1 = 4, N2 = 6, N3 = 10-->\n\t\t\t\t<port id=\"0\" precision=\"FP32\"> <!-- query -->\n\t\t\t\t\t<dim>4</dim> <!-- N1 (repeat 1 time) -->\n\t\t\t\t\t<dim>6</dim> <!-- N2 (repeat 1 time)-->\n\t\t\t\t\t<dim>10</dim> <!-- N3 (repeat 1 time)-->\n\t\t\t\t\t<dim>-1</dim> <!-- L -->\n\t\t\t\t\t<dim>80</dim> <!-- E -->\n\t\t\t\t</port>\n\t\t\t\t<port id=\"1\" precision=\"FP32\"> <!-- key -->\n\t\t\t\t\t<dim>1</dim> <!-- N1 (repeat 4 times) -->\n\t\t\t\t\t<dim>6</dim> <!-- N2 (repeat 1 time) -->\n\t\t\t\t\t<dim>10</dim> <!-- N3 (repeat 1 time) -->\n\t\t\t\t\t<dim>-1</dim> <!-- S -->\n\t\t\t\t\t<dim>80</dim> <!-- E -->\n\t\t\t\t</port>\n\t\t\t\t<port id=\"2\" precision=\"FP32\"> <!-- value -->\n\t\t\t\t\t<dim>1</dim> <!-- N1 (repeat 4 times)-->\n\t\t\t\t\t<dim>1</dim> <!-- N2 (repeat 6 times)-->\n\t\t\t\t\t<dim>1</dim> <!-- N3 (repeat 10 times)-->\n\t\t\t\t\t<dim>-1</dim> <!-- S -->\n\t\t\t\t\t<dim>80</dim> <!-- Ev -->\n\t\t\t\t</port>\n\t\t\t\t<port id=\"3\" precision=\"FP32\"> <!-- attention_mask -->\n\t\t\t\t\t<dim>1</dim> <!-- N1 (repeat 4 times)-->\n\t\t\t\t\t<dim>1</dim> <!-- N2 (repeat 6 times)-->\n\t\t\t\t\t<dim>1</dim> <!-- N3 (repeat 10 times)-->\n\t\t\t\t\t<dim>-1</dim> <!-- L -->\n\t\t\t\t\t<dim>-1</dim> <!-- S -->\n\t\t\t\t</port>\n\t\t\t</input>\n\t\t\t<output>\n\t\t\t\t<!-- Output contains broadcasted dimensions N1 = 4, N2 = 6, N3 = 10-->\n\t\t\t\t<port id=\"4\" precision=\"FP32\">\n\t\t\t\t\t<dim>4</dim> <!-- N1 -->\n\t\t\t\t\t<dim>6</dim> <!-- N2 -->\n\t\t\t\t\t<dim>10</dim> <!-- N3 -->\n\t\t\t\t\t<dim>-1</dim> <!-- L -->\n\t\t\t\t\t<dim>80</dim> <!-- Ev -->\n\t\t\t\t</port>\n\t\t\t</output>\n\t\t</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Source Directory in CMake\nDESCRIPTION: Sets the NPU_UTILS_SOURCE_DIR variable to the current CMake source directory. This is used to reference the location of the NPU utilities source files within the project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(NPU_UTILS_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Adding C++ Samples Component in CMake\nDESCRIPTION: Adds a component for C++ samples to the OpenVINO packaging system. This component is marked as hidden and depends on the core development component, ensuring that the core development tools are installed alongside the C++ samples.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nov_cpack_add_component(${OV_CPACK_COMP_CPP_SAMPLES}\n                       HIDDEN\n                       DEPENDS ${OV_CPACK_COMP_CORE_DEV})\n```\n\n----------------------------------------\n\nTITLE: MaxPool with 4D input, 2D kernel, valid padding, ceil rounding (sh)\nDESCRIPTION: Illustrates MaxPool operation on a 4D input tensor with a 2D kernel, valid padding, and ceiling rounding type. It demonstrates the input tensor, strides, kernel size, rounding type, auto padding, and the resulting output tensor and indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-8.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[-1, 2, 3],\n                 [4, 5, -6],\n                 [-7, 8, 9]]]]\nstrides = [2, 2]\nkernel = [2, 2]\nrounding_type = \"ceil\"\nauto_pad = \"valid\"\noutput0 = [[[[5, 3],\n                   [8, 9]]]]\noutput1 = [[[[4, 2],\n                   [7, 8]]]]\n```\n\n----------------------------------------\n\nTITLE: LessEqual Layer Configuration (No Broadcast) - XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a LessEqual layer in OpenVINO with no broadcasting enabled. The `auto_broadcast` attribute is set to 'none', indicating that the input tensors must have matching shapes. The input and output ports define the tensor dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/lessequal-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"LessEqual\">\n    <data auto_broadcast=\"none\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: RNNSequence XML Layer Configuration\nDESCRIPTION: This XML snippet demonstrates the configuration of an RNNSequence layer within an OpenVINO model. It defines the input and output port dimensions, and specifies the hidden_size attribute. The input ports define the shapes for input data (X), initial hidden state (H), sequence lengths, weights (W), recurrence weights (R), and biases (B), while the output ports specify the shapes for the output hidden states (Y) and the last hidden state (Ho).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/rnn-sequence-5.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"RNNSequence\" ...>\n    <data hidden_size=\"128\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>16</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n        </port>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>128</dim>\n            <dim>16</dim>\n        </port>\n        <port id=\"4\">\n            <dim>1</dim>\n            <dim>128</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"5\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"6\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"7\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Excluding X64 Paths\nDESCRIPTION: This snippet excludes x64-specific paths if the target architecture is not x86_64.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT X86_64)\n    list(APPEND EXCLUDE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/nodes/executors/x64/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/nodes/kernels/x64/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/emitters/plugin/x64/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/emitters/snippets/x64/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/emitters/tpp/x64/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/transformations/cpu_opset/x64/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/transformations/tpp/x64/*)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Torchvision Requirements File in CMake\nDESCRIPTION: This CMake snippet installs the `torchvision/requirements.txt` file from the OpenVINOPython source directory to `${OV_CPACK_PYTHONDIR}/openvino/preprocess/torchvision`, assigning it to the `${OV_CPACK_COMP_OPENVINO_REQ_FILES}` component and potentially excluding it from other components. This manages dependencies specific to the torchvision preprocessing module.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES ${OpenVINOPython_SOURCE_DIR}/src/openvino/preprocess/torchvision/requirements.txt\n        DESTINATION ${OV_CPACK_PYTHONDIR}/openvino/preprocess/torchvision\n        COMPONENT ${OV_CPACK_COMP_OPENVINO_REQ_FILES}\n        ${OV_CPACK_COMP_OPENVINO_REQ_FILES_EXCLUDE_ALL})\n```\n\n----------------------------------------\n\nTITLE: NonMaxSuppression layer definition in C++\nDESCRIPTION: Defines a NonMaxSuppression layer with specific attributes and input/output ports. The layer configuration includes box encoding, sorting order, and output data type, along with input tensor dimensions for boxes, scores, and thresholds. The output specifies a tensor containing selected box indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/non-max-suppression-4.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"NonMaxSuppression\" ... >\n    <data box_encoding=\"corner\" sort_result_descending=\"1\" output_type=\"i64\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>100</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>\n            <dim>5</dim>\n            <dim>100</dim>\n        </port>\n        <port id=\"2\"/> <!-- 10 -->\n        <port id=\"3\"/>\n        <port id=\"4\"/>\n    </input>\n    <output>\n        <port id=\"5\" precision=\"I64\">\n            <dim>150</dim> <!-- min(100, 10) * 3 * 5 -->\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating Tensor from String Array in OpenVINO (C)\nDESCRIPTION: This function creates an OpenVINO tensor from a string array. It takes the string array, the array size, tensor shape, and a pointer to a tensor pointer as input. The function returns a status code indicating success or failure during tensor creation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_43\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_tensor_create_from_string_array(const char** string_array, const size_t array_size, const ov_shape_t shape, ov_tensor_t** tensor)\n```\n\n----------------------------------------\n\nTITLE: Adding Target with ov_add_target in CMake\nDESCRIPTION: This snippet uses the `ov_add_target` function to define the target 'ov_snippets_models'. It specifies the target type as STATIC, sets the root include directory, defines public includes and link libraries including openvino::runtime::dev, common_test_utils, openvino::snippets, and ov_lpt_models, and configures cpplint.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/ov_helpers/ov_snippets_models/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_target(\n        NAME ${TARGET_NAME}\n        TYPE STATIC\n        ROOT ${PUBLIC_HEADERS_DIR}\n        INCLUDES\n            PUBLIC\n                \"$<BUILD_INTERFACE:${PUBLIC_HEADERS_DIR}>\"\n                \"$<BUILD_INTERFACE:${COMMON_TEST_UTILS_INCLUDES}>\"\n        ADDITIONAL_SOURCE_DIRS\n            ${CMAKE_CURRENT_SOURCE_DIR}/src\n        LINK_LIBRARIES\n            PUBLIC\n                openvino::runtime::dev\n                common_test_utils\n                openvino::snippets\n                ov_lpt_models\n        ADD_CPPLINT\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Static Library in CMake\nDESCRIPTION: This CMake snippet defines a static library called `snippets_test_utils`. It includes several header files (lowering_utils.hpp, lir_test_utils.hpp, lir_comparator.hpp) and corresponding source files.  It also configures include directories and links the library with `common_test_utils` and `ov_snippets_models`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(snippets_test_utils STATIC ${CMAKE_CURRENT_SOURCE_DIR}/include/lowering_utils.hpp\n                                       ${CMAKE_CURRENT_SOURCE_DIR}/include/lir_test_utils.hpp\n                                       ${CMAKE_CURRENT_SOURCE_DIR}/include/lir_comparator.hpp\n                                       ${CMAKE_CURRENT_SOURCE_DIR}/src/lowering_utils.cpp\n                                       ${CMAKE_CURRENT_SOURCE_DIR}/src/lir_test_utils.cpp\n                                       ${CMAKE_CURRENT_SOURCE_DIR}/src/lir_comparator.cpp)\ntarget_include_directories(snippets_test_utils PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include)\ntarget_link_libraries(snippets_test_utils PRIVATE common_test_utils ov_snippets_models)\n```\n\n----------------------------------------\n\nTITLE: Gather Operation Example 1\nDESCRIPTION: Illustrates the Gather operation with default batch_dims and axis values. The indices tensor specifies which elements to gather from the data tensor, resulting in the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-8.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 0\naxis = 0\n\nindices = [0, 0, 4]\ndata    = [1, 2, 3, 4, 5]\noutput  = [1, 1, 5]\n```\n\n----------------------------------------\n\nTITLE: Deformable Convolution Example (deformable_group=4) in XML\nDESCRIPTION: This XML snippet illustrates a DeformableConvolution layer with `deformable_group` set to 4. It provides configuration details for the layer, specifying attributes like dilations, pads, strides, and auto_pad. The input port dimensions and output port dimensions are also defined, which determines the expected data shape for inputs and the resulting output data shape, with FP32 precision specified for the output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/deformable-convolution-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"DeformableConvolution\" ...>\n    <data dilations=\"1,1\" pads_begin=\"0,0\" pads_end=\"0,0\" strides=\"1,1\" auto_pad=\"explicit\"  group=\"1\" deformable_group=\"4\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>200</dim>\n            <dim>220</dim>\n            <dim>220</dim>\n        </port>\n        <port id=\"2\">\n            <dim>64</dim>\n            <dim>4</dim>\n            <dim>5</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>220</dim>\n            <dim>220</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target\nDESCRIPTION: Adds a custom target for running clang-format on the source files of the specified target. The `ov_add_clang_format_target` macro configures a target that will format the code according to the project's style guidelines.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/common/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: LSTMCell XML Layer Example\nDESCRIPTION: This XML snippet demonstrates how to define an LSTMCell layer in an OpenVINO model. It specifies the hidden size, input port dimensions, and output port dimensions. This configuration shows a specific example of how the LSTMCell operation can be integrated into a larger neural network.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/lstm-cell-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"LSTMCell\" ...>\n    <data hidden_size=\"128\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>16</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n         <port id=\"3\">\n            <dim>512</dim>\n            <dim>16</dim>\n        </port>\n         <port id=\"4\">\n            <dim>512</dim>\n            <dim>128</dim>\n        </port>\n         <port id=\"5\">\n            <dim>512</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"6\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"7\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Showing BitBake Layers\nDESCRIPTION: Shows the currently added layers in the BitBake configuration. This step is optional but it allows the user to verify that the layers were added correctly before proceeding with the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yocto.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nbitbake-layers show-layers\n```\n\n----------------------------------------\n\nTITLE: Implicit Concat Marking C++\nDESCRIPTION: In cases where concat operations are optimized out, the corresponding layer is marked as optimized-out within the graph. This prevents the layer from being executed during runtime while maintaining the integrity of the node connections within the computational graph. The marking process ensures that the optimized concat layer doesn't perform redundant computations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/execution_of_inference.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// Concat layer is just marked as **optimized-out** and not executed during runtime.\n```\n\n----------------------------------------\n\nTITLE: Creating CPU Plugin with OpenVINO CMake Macro\nDESCRIPTION: This snippet creates a plugin named `${TARGET_NAME}` for the CPU device, specifying it as an extension and using the `VERSION_DEFINES_FOR` parameter to generate version definitions from the `src/plugin.cpp` file.  It also specifies source and header files, and options to include clang-format and clang-tidy.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_27\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_plugin(NAME ${TARGET_NAME}\n              DEVICE_NAME \"CPU\"\n              AS_EXTENSION\n              VERSION_DEFINES_FOR src/plugin.cpp\n              SOURCES ${SOURCES} ${HEADERS}\n              ADD_CLANG_FORMAT\n              ADD_CLANG_TIDY)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for Tests CMake\nDESCRIPTION: This snippet adds subdirectories to the current CMake project, enabling the building of the layer_tests, model_hub_tests, samples_tests, and e2e_tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(layer_tests)\nadd_subdirectory(model_hub_tests)\nadd_subdirectory(samples_tests)\nadd_subdirectory(e2e_tests)\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for Specific Sources in CMake (Passes)\nDESCRIPTION: This snippet sets include directories for specific source files related to passes (transformations) like `smart_reshape`, `rt_info`, and others. It uses `set_source_files_properties` to append the interface include directories from `openvino::core::dev` to these files, allowing them to access the necessary headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_18\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE smart_reshape_srcs ${CMAKE_CURRENT_SOURCE_DIR}/src/pass/smart_reshape/*.cpp)\nfile(GLOB_RECURSE rt_info_srcs ${CMAKE_CURRENT_SOURCE_DIR}/src/pass/rt_info/*.cpp)\nset_source_files_properties(\"${CMAKE_CURRENT_SOURCE_DIR}/src/pass/convert_precision.cpp\"\n                            \"${CMAKE_CURRENT_SOURCE_DIR}/src/pass/convert_fp32_to_fp16.cpp\"\n                            \"${CMAKE_CURRENT_SOURCE_DIR}/src/pass/init_node_info.cpp\"\n                            \"${CMAKE_CURRENT_SOURCE_DIR}/src/pass/serialize.cpp\"\n                            \"${CMAKE_CURRENT_SOURCE_DIR}/src/op/type_relaxed.cpp\"\n                            \"${CMAKE_CURRENT_SOURCE_DIR}/src/preprocess/preprocess_steps_impl.cpp\"\n                            \"${CMAKE_CURRENT_SOURCE_DIR}/src/model.cpp\" # for SmartReshape\n                            ${smart_reshape_srcs} ${rt_info_srcs}\n        PROPERTIES INCLUDE_DIRECTORIES $<TARGET_PROPERTY:openvino::core::dev,INTERFACE_INCLUDE_DIRECTORIES>)\n```\n\n----------------------------------------\n\nTITLE: ReduceProd Operation Calculation (C++)\nDESCRIPTION: This C++ code snippet illustrates the calculation performed by the ReduceProd operation. It demonstrates how the output is computed by multiplying elements along specified dimensions of the input tensor, while preserving other dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-prod-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i0, i1, ..., iN] = prod[j0, ..., jN](x[j0, ..., jN]))\n```\n\n----------------------------------------\n\nTITLE: Azure Blob Storage: Installing sccache\nDESCRIPTION: This YAML snippet shows how to install `sccache` using the `mozilla-actions/sccache-action` action within a GitHub Actions workflow. It specifies the desired version of `sccache` to install. This step needs to be executed before the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/caches.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n- name: Install sccache\n  uses: mozilla-actions/sccache-action@v0.0.3\n  with:\n    version: \"v0.5.4\"\n```\n\n----------------------------------------\n\nTITLE: NonZero Layer Definition in XML - OpenVINO\nDESCRIPTION: This XML snippet demonstrates the definition of a NonZero layer in OpenVINO. It specifies the output type as i64 and defines the input and output ports with their respective dimensions. The input tensor has dimensions 3x10x100x200, and the output tensor represents the indices of non-zero elements, having dimensions 4x-1, where -1 signifies a dynamic dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/condition/nonzero-3.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"NonZero\">\n <data output_type=\"i64\"/>\n <input>\n <port id=\"0\">\n <dim>3</dim>\n <dim>10</dim>\n <dim>100</dim>\n <dim>200</dim>\n </port>\n </input>\n <output>\n <port id=\"1\">\n <dim>4</dim>\n <dim>-1</dim>\n </port>\n </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Exporting Target for Developer Package in CMake\nDESCRIPTION: This snippet uses the `ov_developer_package_export_targets` function to configure the export of the 'ov_snippets_models' target for inclusion in a developer package. It specifies the target and the installation include directories for the public headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/ov_helpers/ov_snippets_models/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET ${TARGET_NAME}\n                                    INSTALL_INCLUDE_DIRECTORIES \"${PUBLIC_HEADERS_DIR}/\")\n```\n\n----------------------------------------\n\nTITLE: Adding the Library\nDESCRIPTION: This snippet uses `add_library` to create a static library named `openvino_intel_gpu_runtime` from the collected source files. The `STATIC` keyword specifies that a static library should be built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${LIBRARY_SOURCES_ALL})\n```\n\n----------------------------------------\n\nTITLE: src/CMakeLists.txt for Plugin Library Build\nDESCRIPTION: This CMake snippet showcases the CMakeLists.txt file for building the plugin shared library. It includes OpenVINO targets from the OpenVINO Developer Package to link with the plugin sources.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/build-plugin-using-cmake.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nadd_library(${PROJECT_NAME} SHARED\n        ${CMAKE_CURRENT_SOURCE_DIR}/template_plugin.cpp)\n\ntarget_link_libraries(${PROJECT_NAME} PRIVATE\n        openvino::runtime\n        openvino::runtime::dev)\n```\n\n----------------------------------------\n\nTITLE: MaxPool Layer with 'valid' auto_pad (XML)\nDESCRIPTION: This XML snippet defines a MaxPool layer with 'valid' auto_pad, a kernel size of 2x2, padding of 1x1 on both sides (though the padding is ignored because of 'valid'), and strides of 2x2. It processes a 4D input tensor with dimensions 1x3x32x32 and produces a 4D output tensor with dimensions 1x3x16x16, along with a second output port for indices. 'valid' padding means no padding is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-14.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MaxPool\" ... >\n    <data auto_pad=\"valid\" kernel=\"2,2\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>16</dim>\n            <dim>16</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>16</dim>\n            <dim>16</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ReduceMax Layer Configuration in XML (axes=[1], keep_dims=false)\nDESCRIPTION: This XML configuration defines a ReduceMax layer with `keep_dims` set to `false`. The input tensor has dimensions 6x12x10x24, and the reduction is performed along axis 1 (value [1]). The output tensor removes the reduced dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-max-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceMax\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [1] that means independent reduction in each channel and spatial dimensions -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name and Finding Python Interpreter (CMake)\nDESCRIPTION: This snippet defines the target library name and finds the Python 3 interpreter, which is required for code generation during the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_intel_gpu_kernels\")\n\nfind_host_package(Python3 REQUIRED COMPONENTS Interpreter)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This line sets the name of the target to 'gtest_main_manifest'. This name is used to refer to the target in subsequent CMake commands.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/gtest_main_manifest/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME \"gtest_main_manifest\")\n```\n\n----------------------------------------\n\nTITLE: Define OpenVINO GPU Plugin Properties (C)\nDESCRIPTION: These macros define properties specific to the GPU plugin for OpenVINO. They control the underlying OpenCL context, memory sharing, and tile selection.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_16\n\nLANGUAGE: C\nCODE:\n```\nOPENVINO_C_VAR(const char*) ov_property_key_intel_gpu_context_type;\n\nOPENVINO_C_VAR(const char*) ov_property_key_intel_gpu_ocl_context;\n\nOPENVINO_C_VAR(const char*) ov_property_key_intel_gpu_ocl_context_device_id;\n\nOPENVINO_C_VAR(const char*) ov_property_key_intel_gpu_tile_id;\n\nOPENVINO_C_VAR(const char*) ov_property_key_intel_gpu_ocl_queue;\n\nOPENVINO_C_VAR(const char*) ov_property_key_intel_gpu_va_device;\n\nOPENVINO_C_VAR(const char*) ov_property_key_intel_gpu_shared_mem_type;\n\nOPENVINO_C_VAR(const char*) ov_property_key_intel_gpu_mem_handle;\n\nOPENVINO_C_VAR(const char*) ov_property_key_intel_gpu_dev_object_handle;\n\nOPENVINO_C_VAR(const char*) ov_property_key_intel_gpu_va_plane;\n```\n\n----------------------------------------\n\nTITLE: Add Node.js Subdirectory CMake\nDESCRIPTION: This snippet adds the 'node' subdirectory to the build. This subdirectory contains the CMakeLists.txt file that defines how to build the Node.js portion of the OpenVINO JavaScript API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(node)\n```\n\n----------------------------------------\n\nTITLE: br0 Interface Up Script\nDESCRIPTION: This script, `br0-qemu-ifup`, brings up the specified network interface (`$nic`) and adds it to the `br0` bridge. It reads configuration from `/etc/default/qemu-kvm` if the file exists.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\n#!/bin/sh\nnic=$1\nif [ -f /etc/default/qemu-kvm ]; then\n\t. /etc/default/qemu-kvm\nfi\nswitch=br0\nifconfig $nic 0.0.0.0 up\nbrctl addif ${switch} $nic\n```\n\n----------------------------------------\n\nTITLE: Adding Main Target\nDESCRIPTION: Adds the main target 'openvino_intel_gpu_graph' as a static library. It includes object files, include directories, and link libraries.  It depends on 'run_codegen_test'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_target(\n    NAME ${TARGET_NAME}\n    TYPE STATIC\n    ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n    EXCLUDED_SOURCE_PATHS\n        ${EXCLUDE_PATHS}\n    OBJECT_FILES\n        ${OBJ_FILES}\n    INCLUDES\n        PUBLIC\n            ${COMMON_INCLUDE_DIRS}\n    LINK_LIBRARIES\n        PUBLIC\n            ${COMMON_LINK_LIBRARIES}\n            ${EXTRA_LINK_LIBRARIES}\n            openvino::reference # for loop primitive subroutines\n    DEPENDENCIES\n        run_codegen_test\n    ADD_CPPLINT\n)\n```\n\n----------------------------------------\n\nTITLE: Set Public Headers Directory CMake\nDESCRIPTION: This snippet sets the variable `PUBLIC_HEADERS_DIR` to the directory containing the public header files for the library.  This is used later when setting the include directories for the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(PUBLIC_HEADERS_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: ScatterNDUpdate Example 3 (XML)\nDESCRIPTION: This XML snippet demonstrates a ScatterNdUpdate layer configuration with reduction set to 'sub'. It shows how to define the layer to subtract the update values at the specific indices. The values are subtracted at the indices [0, 2, -3, -3, 0].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-nd-update-15.rst#_snippet_5\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... reduction=\"sub\" type=\"ScatterNdUpdate\">\n    <input>\n        <port id=\"0\" precision=\"I32\">  <!-- data -->\n            <dim>4</dim>  <!-- values: [1, 2, 3, 4] -->\n        </port>\n        <port id=\"1\" precision=\"I32\">  <!-- indices -->\n            <dim>5</dim>  <!-- values: [0, 2, -3, -3, 0] -->\n        </port>\n        <port id=\"2\" precision=\"I32\">  <!-- updates -->\n            <dim>5</dim>  <!-- values: [10, 20, 30, 40, 50] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"I32\">\n            <dim>4</dim>  <!-- values: [-59, -68, -17, 4] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating zlib Static Library\nDESCRIPTION: This snippet creates a static library named 'openvino_zlib' using the specified source and header files. It also creates an alias for the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/zlib/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${zlib_srcs} ${zlib_hdrs} ${lib_ext_hdrs})\nadd_library(openvino::zlib ALIAS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: query_state() Method C++\nDESCRIPTION: This code snippet shows the implementation of the `query_state()` method, which retrieves variable states from the model. This is particularly important for stateful models where previous inference outputs influence subsequent inferences.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/synch-inference-request.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nstd::vector<VariableState> InferRequest::query_state() {\n    // Implementation for querying variable states\n    return m_variable_states;\n}\n```\n\n----------------------------------------\n\nTITLE: FloorMod Example - Numpy Broadcasting - XML\nDESCRIPTION: This XML snippet showcases the FloorMod operation with numpy broadcasting enabled. The input tensors have different shapes, and numpy broadcasting rules are applied to determine the output shape. The `auto_broadcast` attribute is set to 'numpy'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/floormod-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"FloorMod\">\n    <data auto_broadcast=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Example Docker Pull Command\nDESCRIPTION: This command demonstrates pulling a Docker image from Docker Hub.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/docker_images.md#_snippet_2\n\nLANGUAGE: docker\nCODE:\n```\ndocker pull ubuntu:22.04\n```\n\n----------------------------------------\n\nTITLE: Slice: Slicing Backwards with Step -2 in OpenVINO XML\nDESCRIPTION: This example showcases slicing a 1D tensor backwards with a step of -2. This means elements are selected in reverse order, skipping one element at a time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-8.rst#_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Slice\" ...>\n       <input>\n           <port id=\"0\">       <!-- data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -->\n             <dim>10</dim>\n           </port>\n           <port id=\"1\">       <!-- start: [9] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"2\">       <!-- stop: [-11] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"3\">       <!-- step: [-2] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"4\">       <!-- axes: [0] -->\n             <dim>1</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">       <!-- output: [9, 7, 5, 3, 1] -->\n               <dim>5</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: PReLU layer definition with 2D input in XML\nDESCRIPTION: This example demonstrates the XML representation of a PReLU layer processing a 2D input tensor 'data' and a 1D 'slope' tensor. The 'data' tensor has dimensions 20 and 128, and the 'slope' tensor has a dimension of 128. The output tensor maintains the same shape as the input 'data' tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/prelu-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Prelu\">\n    <input>\n        <port id=\"0\">\n            <dim>20</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"1\">\n            <dim>128</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>20</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation Definitions Based on SELECTIVE_BUILD\nDESCRIPTION: This snippet conditionally defines compile definitions based on the value of the 'SELECTIVE_BUILD' variable. If set to 'COLLECT', it defines 'SELECTIVE_BUILD_ANALYZER'. If set to 'ON', it checks for 'SELECTIVE_BUILD_STAT' and defines 'SELECTIVE_BUILD'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(SELECTIVE_BUILD STREQUAL \"COLLECT\")\n    target_compile_definitions(${TARGET_NAME} INTERFACE SELECTIVE_BUILD_ANALYZER)\nelseif(SELECTIVE_BUILD STREQUAL \"ON\")\n    if(NOT DEFINED SELECTIVE_BUILD_STAT)\n        message(FATAL_ERROR \"In case SELECTIVE_BUILD is enabled, the SELECTIVE_BUILD_STAT variable should contain the path to the collected IntelSEAPI statistics.\\\n Usage: -DSELECTIVE_BUILD=ON -DSELECTIVE_BUILD_STAT=/path/*.csv\")\n    endif()\n    find_host_package (Python3 REQUIRED COMPONENTS Interpreter)\n\n    file(TO_CMAKE_PATH ${SELECTIVE_BUILD_STAT} CMAKE_SELECTIVE_BUILD_STAT)\n    file(GLOB STAT_FILES ${CMAKE_SELECTIVE_BUILD_STAT})\n    if(NOT STAT_FILES)\n        message(FATAL_ERROR \"SELECTIVE_BUILD_STAT (${SELECTIVE_BUILD_STAT}) path doesn't contain valid csv files!\")\n    endif()\n\n    target_compile_definitions(${TARGET_NAME} INTERFACE SELECTIVE_BUILD)\n```\n\n----------------------------------------\n\nTITLE: Explicit Commit List Configuration\nDESCRIPTION: Example showing how to explicitly define a list of commit hashes to be analyzed by the commit slider. Useful when you don't want to use a git range.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/commit_slider/README.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n\"commitList\" : {\n    \"explicitList\" : [\"hash_1\", \"hash_2\", ... , \"hash_N\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Cubic Interpolation Coefficient Calculation (Python)\nDESCRIPTION: This Python code defines a function `get_cubic_coeff` that calculates the coefficients used in cubic interpolation. It takes `s` (distance from the sample point) and `a` (a parameter controlling the interpolation kernel) as input. The function returns an array of four coefficients that are used to weight the neighboring pixels during cubic interpolation. Dependency: numpy.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef get_cubic_coeff(s, a):\n    abs_s = abs(s)\n    coeff = np.zeros(4)\n    coeff[0] = a * (abs_s - 1.0) * (abs_s - 1.0) * abs_s\n    coeff[1] = ((a + 2.0) * abs_s - (a + 3.0)) * abs_s * abs_s + 1.0\n    coeff[2] = (((-a -2.0) * abs_s+ (2.0 * a + 3.0)) * abs_s - a) * abs_s\n    coeff[3] = - a * abs_s * abs_s * (abs_s - 1.0)\n    return coeff\n```\n\n----------------------------------------\n\nTITLE: Defining LSTM Cell Layer XML\nDESCRIPTION: This XML snippet defines an LSTMCell layer with specific inputs and outputs, along with data about the hidden size of the LSTM. The input and output ports specify the data dimensions expected by the LSTM cell.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/tensor-iterator-1.rst#_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"7\" type=\"LSTMCell\" ...>\n    <data hidden_size=\"256\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>512</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>256</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>256</dim>\n        </port>\n        <port id=\"3\">\n            <dim>1024</dim>\n            <dim>768</dim>\n        </port>\n        <port id=\"4\">\n            <dim>1024</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"5\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>256</dim>\n        </port>\n        <port id=\"6\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>256</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing Executable Target in CMake\nDESCRIPTION: This snippet installs the `StressMemLeaksTests` executable to the `tests` directory within the installation prefix. `RUNTIME DESTINATION` specifies the installation directory for executables. `COMPONENT tests` assigns the target to the tests component for installation management. `EXCLUDE_FROM_ALL` prevents this target from being built as part of an \"all\" build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/memleaks_tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n        RUNTIME DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Check New Models Support on CPU\nDESCRIPTION: This command checks new models' support (as IRs) on the CPU plugin, saves the results to a custom directory, and creates Conformance IRs from models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython3 run_conformance.py -m /path/to/new/model_irs -s=1 -w /path/to/working/dir -d CPU\n```\n\n----------------------------------------\n\nTITLE: Result Layer XML Configuration in OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of a Result layer in OpenVINO. The layer takes a single input tensor of type T and uses it to specify the output of the model. The input port specifies the dimensions of the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/result-1.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"Result\" ...>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </input>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Build C++ OpenVINO Samples\nDESCRIPTION: This command executes the build_samples.sh script located in the C++ samples directory of the OpenVINO installation. It compiles the C++ sample applications.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-zypper.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n/usr/share/openvino/samples/cpp/build_samples.sh\n```\n\n----------------------------------------\n\nTITLE: Adding OpenVINO Sample with CMake\nDESCRIPTION: This CMake command adds a new OpenVINO sample application named `sync_benchmark`.  It specifies `main.cpp` as the source file and declares a dependency on the `ie_samples_utils` library.  The `ov_add_sample` macro is used to simplify the process of configuring and building OpenVINO sample applications.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/benchmark/sync_benchmark/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_sample(NAME sync_benchmark\n              SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/main.cpp\"\n              DEPENDENCIES ie_samples_utils)\n```\n\n----------------------------------------\n\nTITLE: Initialize NV12 two-plane preprocessing (C++)\nDESCRIPTION: This C++ code snippet demonstrates how to initialize preprocessing for NV12 two-plane video input within an OpenVINO pipeline. It sets the color format, memory type, and element type for the input tensor, preparing the model for NV12 video data. Requires OpenVINO library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_30\n\nLANGUAGE: cpp\nCODE:\n```\nusing namespace ov::preprocess;\nauto p = PrePostProcessor(model);\np.input().tensor().set_element_type(ov::element::u8)\n                  .set_color_format(ov::preprocess::ColorFormat::NV12_TWO_PLANES, {\"y\", \"uv\"})\n                  .set_memory_type(ov::intel_gpu::memory_type::surface);\np.input().preprocess().convert_color(ov::preprocess::ColorFormat::BGR);\np.input().model().set_layout(\"NCHW\");\nmodel = p.build();\n```\n\n----------------------------------------\n\nTITLE: Running Python ONNX Standard Compliance Tests (Build Layout)\nDESCRIPTION: This command runs the Python tests to confirm operator compliance with the ONNX standard using pytest in the build layout.  It excludes tests related to CUDA. Replace <OV_REPO_DIR> with the OpenVINO repository directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\npytest <OV_REPO_DIR>/src/bindings/python/tests/test_onnx/test_backend.py -sv -k 'not cuda'\n```\n\n----------------------------------------\n\nTITLE: Detecting Dynamic Output Shapes - C\nDESCRIPTION: This C code snippet demonstrates how to verify if a compiled OpenVINO model's output shape is dynamic using the C API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_18\n\nLANGUAGE: C\nCODE:\n```\n#include <openvino/c/openvino.h>\n#include <stdio.h>\n#include <stdlib.h>\n\n#define OV_CHECK(status, message)                                  \\\n    if (status != 0) {                                            \\\n        printf(\"%s\\n\", message);                               \\\n        return 1;                                                  \\\n    }\n\nint main() {\n    // ! [ov_dynamic_shapes:detect_dynamic]\n    ov_core_t* core = NULL;\n    OV_CHECK(ov_core_create(&core), \"Failed to create Core\");\n\n    ov_model_t* model = NULL;\n    OV_CHECK(ov_core_read_model(core, \"model.xml\", &model), \"Failed to read model\");\n\n    ov_compiled_model_t* compiled_model = NULL;\n    OV_CHECK(ov_core_compile_model(core, model, \"CPU\", &compiled_model), \"Failed to compile model\");\n\n    ov_output_port output_port = {0};\n    OV_CHECK(ov_compiled_model_output(compiled_model, 0, &output_port), \"Failed to get output port\");\n\n    ov_partial_shape partial_shape;\n    OV_CHECK(ov_output_port_get_partial_shape(output_port, &partial_shape), \"Failed to get partial shape\");\n\n    ov_bool_t is_dynamic = 0;\n    OV_CHECK(ov_partial_shape_is_dynamic(&partial_shape, &is_dynamic), \"Failed to check if partial shape is dynamic\");\n\n    if (is_dynamic) {\n        printf(\"Output shape is dynamic\\n\");\n    } else {\n        printf(\"Output shape is static\\n\");\n    }\n\n    ov_output_port_free(&output_port);\n    ov_compiled_model_free(compiled_model);\n    ov_model_free(model);\n    ov_core_free(core);\n    // ! [ov_dynamic_shapes:detect_dynamic]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: ReverseSequence Layer Configuration XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a ReverseSequence layer in OpenVINO. It specifies the batch_axis and seq_axis attributes, as well as the input and output port dimensions. The input ports define the dimensions for the data tensor and the sequence lengths tensor, while the output port defines the dimensions for the resulting tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/reverse-sequence-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ReverseSequence\">\n    <data batch_axis=\"0\" seq_axis=\"1\"/>\n    <input>\n        <port id=\"0\">       <!-- data -->\n            <dim>4</dim>    <!-- batch_axis -->\n            <dim>10</dim>   <!-- seq_axis -->\n            <dim>100</dim>\n            <dim>200</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>    <!-- seq_lengths value: [2, 4, 8, 10] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>4</dim>\n            <dim>10</dim>\n            <dim>100</dim>\n            <dim>200</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Configuring Unit Test Target in CMake\nDESCRIPTION: This uses the `ov_add_test_target` function to configure the unit test target. It specifies the name, root directory, source directories, include directories, linked libraries, and dependencies.  It sets labels to categorize the tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/unit/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        ADDITIONAL_SOURCE_DIRS\n            ${OpenVINO_SOURCE_DIR}/src/plugins/auto/src\n            ${OpenVINO_SOURCE_DIR}/src/plugins/auto/src/utils\n        INCLUDES\n            ${OpenVINO_SOURCE_DIR}/src/plugins/auto\n            ${CMAKE_CURRENT_SOURCE_DIR}\n            ${OpenVINO_SOURCE_DIR}/src/plugins/auto/src\n        LINK_LIBRARIES\n            unit_test_utils\n        ADD_CPPLINT\n        DEPENDENCIES\n            openvino_template_extension\n            mock_engine\n        LABELS\n            OV UNIT MULTI AUTO\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Operation Constructor in C++\nDESCRIPTION: This snippet demonstrates how to define a constructor for a custom operation in C++ using the OpenVINO API. It includes default constructor and constructor that takes operation inputs and attributes as parameters. It uses `OPENVINO_OP` macro to define the type info of the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-openvino-operations.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nclass Identity : public ov::op::Op\n{\npublic:\n    OPENVINO_OP(\"Identity\", \"template\");\n    Identity() = default;\n\n    Identity(const ov::Output<ov::Node>& arg) : ov::op::Op({arg}) {\n        constructor_validate_and_infer_types();\n    }\n\n    std::shared_ptr<ov::Node> clone_with_new_inputs(const ov::OutputVector& new_args) const override {\n        OPENVINO_ASSERT(new_args.size() == 1, \"Expected 1 element in new_args, but got \", new_args.size());\n        return std::make_shared<Identity>(new_args.at(0));\n    };\n\n    void validate_and_infer_types() override {\n        OPENVINO_ASSERT(get_input_size() == 1, \"Expected 1 input, but got \", get_input_size());\n        set_output_type(0, get_input_element_type(0), get_input_partial_shape(0));\n    }\n\n    bool visit_attributes(ov::AttributeVisitor& visitor) override {\n        return true;\n    }\n};\n\n```\n\n----------------------------------------\n\nTITLE: StringTensorPack Example 3 (Skipped Symbols)\nDESCRIPTION: This example shows how to skip symbols using the StringTensorPack operation. The 'begins' and 'ends' tensors are set up such that only the first and last characters of the 'symbols' tensor are included in the output strings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/string-tensor-pack-15.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StringTensorPack\" ... >\n    <input>\n        <port id=\"0\" precision=\"I64\">\n            <dim>2</dim>     <!-- begins = [0, 8] -->\n        </port>\n        <port id=\"1\" precision=\"I64\">\n            <dim>2</dim>     <!-- ends = [1, 9] -->\n        </port>\n        <port id=\"2\" precision=\"U8\">\n            <dim>9</dim>     <!-- symbols = \"123456789\" encoded in an utf-8 array -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"STRING\">\n            <dim>5</dim>     <!-- output = [\"1\", \"9\"] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Acos Layer XML Example\nDESCRIPTION: This XML snippet defines an Acos layer in OpenVINO, demonstrating how to specify input and output ports with their dimensions. The input and output tensors are of the same type and dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/acos-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Acos\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: MaxPool with 3D input, 1D kernel, valid padding (sh)\nDESCRIPTION: Shows the MaxPool operation on a 3D input tensor with a 1D kernel and valid padding. It presents the input tensor, strides, kernel size, rounding type, auto padding, and the corresponding output tensor and indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-8.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[-1, 2, 3, 5, -7, 9, 1]]]\nstrides = [1]\nkernel = [3]\nrounding_type = \"floor\"\nauto_pad = \"valid\"\noutput0 = [[[3, 5, 5, 9, 9]]]\noutput1 = [[[2, 3, 3, 5, 5]]]\n```\n\n----------------------------------------\n\nTITLE: Installing OVSA model hosting components on Guest VM\nDESCRIPTION: Navigates to the ~/OVSA directory on the guest VM, unpacks the ovsa-model-hosting.tar.gz archive, navigates into the unpacked directory, and executes the install.sh script with sudo privileges to install the OVSA model hosting components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_37\n\nLANGUAGE: sh\nCODE:\n```\ncd ~/OVSA\ntar xvfz ovsa-model-hosting.tar.gz\ncd ovsa-model-hosting\nsudo ./install.sh\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Kernel Configuration File - Python\nDESCRIPTION: This Python snippet demonstrates how to set the custom kernel configuration file path using `ov::Core::set_property()` before loading the network. This allows the OpenVINO plugin to use the custom operations defined in the configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-gpu-operations.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncore = ov.Core()\ncore.set_property(\"GPU\", {\"CONFIG_FILE\": \"path_to_config_file\"})\nmodel = core.read_model(\"path_to_model\")\ncompiled_model = core.compile_model(model, \"GPU\")\n```\n\n----------------------------------------\n\nTITLE: Synchronous Inference with InferRequest TypeScript\nDESCRIPTION: Performs inference in synchronous mode, using previously set input tensors via `setTensor` or `setInputTensor`.  It returns an object where keys are output names and values are Tensor objects.  The model must have inputs already specified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\ninfer(): { [outputName: string] : Tensor};\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name and Include Directory (CMake)\nDESCRIPTION: This snippet defines the target name for the library and sets the include directory containing the header files. These variables are used in subsequent commands to configure the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/shape_inference/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_shape_inference\")\n\nset(SHAPE_INFER_INCLUDE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: BSD License Text\nDESCRIPTION: This snippet represents a BSD-style license. It permits redistribution and use in source and binary forms, with or without modification, provided that the copyright notice, list of conditions, and disclaimer are retained in source code and reproduced in binary distributions. The license also disclaims warranties and limits liability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/runtime-third-party-programs.txt#_snippet_4\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n   ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n   WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n   DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n   ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n   (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n   LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n   ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n   SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: CMake Subdirectory Inclusion\nDESCRIPTION: This CMake snippet adds the 'common', 'unit', and 'functional' directories as subdirectories to the current CMake project.  This allows CMake to process the CMakeLists.txt files within those subdirectories and incorporate their build targets into the overall project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(common)\nadd_subdirectory(unit)\nadd_subdirectory(functional)\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Environment Initialization\nDESCRIPTION: This command initializes the OpenVINO environment by sourcing the `setupvars.bat` script from the OpenVINO installation directory. This sets up the necessary environment variables for using OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_20\n\nLANGUAGE: batch\nCODE:\n```\n \"<OpenVINO-install-dir>/setupvars.bat\"\n```\n\n----------------------------------------\n\nTITLE: GatherElements Layer XML Example\nDESCRIPTION: Example of a GatherElements layer configuration in XML format.  Specifies the input and output tensor dimensions and the axis attribute.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-elements-6.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<... type=\"GatherElements\" ...>\n    <data axis=\"1\" />\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>7</dim>\n            <dim>5</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>\n            <dim>10</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>3</dim>\n            <dim>10</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Threading Interface\nDESCRIPTION: This snippet calls a function `ov_set_threading_interface_for` to configure the threading interface for the specified target ('${TARGET_NAME}').  The details of this function are not provided in the context.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Define OpenVINO AUTO Plugin Properties (C)\nDESCRIPTION: These macros define properties specific to the AUTO plugin for OpenVINO. They control aspects like device binding, and fallback behavior.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_15\n\nLANGUAGE: C\nCODE:\n```\nOPENVINO_C_VAR(const char*) ov_property_key_intel_auto_device_bind_buffer;\n\nOPENVINO_C_VAR(const char*) ov_property_key_intel_auto_enable_startup_fallback;\n\nOPENVINO_C_VAR(const char*) ov_property_key_intel_auto_enable_runtime_fallback;\n```\n\n----------------------------------------\n\nTITLE: Benchmarking asl-recognition model on CPU in latency mode (Python, repeated)\nDESCRIPTION: This snippet demonstrates how to run the OpenVINO Benchmark Tool to measure the latency of the 'asl-recognition' model on a CPU. It uses the `benchmark_app` command with the `-m` option to specify the model path, `-d` to select the CPU device, and `-hint` to set the performance hint to latency. The model is expected to be in the OpenVINO Intermediate Representation (IR) format ('.xml' file). This is a repeated example.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app -m omz_models/intel/asl-recognition-0004/FP16/asl-recognition-0004.xml -d CPU -hint latency\n```\n\n----------------------------------------\n\nTITLE: Defining GPU Runner for Tests in YAML\nDESCRIPTION: This snippet shows how to specify a self-hosted GPU runner for a job using the `runs-on` key. It also defines a container with specific device access and volume mounts, necessary for GPU-enabled workloads within Docker. The example showcases the use of the `gpu` label to run on any available GPU runner.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/runners.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nGPU:\n    name: GPU Tests\n    needs: [ Build, Smart_CI ]\n    runs-on: [ self-hosted, gpu ]\n    container:\n      image: ubuntu:20.04\n      options: --device /dev/dri:/dev/dri --group-add 109 --group-add 44\n      volumes:\n        - /dev/dri:/dev/dri\n  ...\n```\n\n----------------------------------------\n\nTITLE: Linux Foundation BSD License Text\nDESCRIPTION: This snippet is a BSD-style license used by The Linux Foundation.  It permits redistribution and use in source and binary forms, with or without modification, provided that the copyright notice, list of conditions, and disclaimer are retained in source code and reproduced in binary distributions.  There may be limitations described in the disclaimer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/runtime-third-party-programs.txt#_snippet_5\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted (subject to the limitations in the\ndisclaimer below) provided that the following conditions are met:\n\n   * Redistributions of source code must retain the above copyright\n     notice, this list of conditions and the following disclaimer.\n\n   * Redistributions in binary form must reproduce the above\n     copyright notice, this list of conditions and the following\n     disclaimer in the documentation and/or other materials provided\n     with the distribution.\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINO Runtime\nDESCRIPTION: This snippet searches for the OpenVINO Runtime library. It first attempts to use PkgConfig if available and if not cross-compiling and in Release mode. If PkgConfig fails or the conditions aren't met, it uses the find_package(OpenVINO) command. The appropriate link libraries are then set based on which method succeeds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/thread_local/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# Search OpenVINO Runtime installed\nfind_package(PkgConfig QUIET)\n# TODO: fix cross-compilation later\nif(PkgConfig_FOUND AND NOT CMAKE_CROSSCOMPILING AND CMAKE_BUILD_TYPE STREQUAL \"Release\")\n    pkg_search_module(openvino REQUIRED\n                      IMPORTED_TARGET\n                      openvino)\n    set(ov_link_libraries PkgConfig::openvino)\nelse()\n    find_package(OpenVINO REQUIRED COMPONENTS Runtime)\n    set(ov_link_libraries openvino::runtime)\nendif()\n```\n\n----------------------------------------\n\nTITLE: 2D ConvolutionBackpropData with output_padding Example\nDESCRIPTION: Configuration of a 2D ConvolutionBackpropData layer with output padding. It shows an upsampling operation, with dilations, padding, strides and non-zero output_padding defined. The input and output port dimensions illustrate the effect of the backprop data operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/convolution-backprop-data-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"5\" name=\"upsampling_node\" type=\"ConvolutionBackpropData\">\n    <data dilations=\"1,1\" pads_begin=\"0,0\" pads_end=\"0,0\" strides=\"3,3\" output_padding=\"2,2\" auto_pad=\"explicit\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>20</dim>\n            <dim>2</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>20</dim>\n            <dim>10</dim>\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>10</dim>\n            <dim>8</dim>\n            <dim>8</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: BitwiseRightShift Layer No Broadcast XML Configuration\nDESCRIPTION: This XML configuration demonstrates a BitwiseRightShift layer where no broadcasting is performed. Both input tensors have the same dimensions (256x56), and the output tensor also has the same dimensions.  The auto_broadcast attribute is implicitly set to \"numpy\" by default, but since the shapes are matching, it effectively behaves as \"none\".\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-right-shift-15.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"BitwiseRightShift\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Install Ubuntu on Guest VM\nDESCRIPTION: This command starts the QEMU system emulator to install Ubuntu 18.04 on the Guest VM.  It specifies the disk image, ISO image, memory allocation, CPU settings, network configuration using TAP devices and up/down scripts, and VNC for accessing the VM's console.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\nsudo qemu-system-x86_64 -m 8192 -enable-kvm \\\n-cpu host \\\n-drive if=virtio,file=<path-to-disk-image>/ovsa_isv_dev_vm_disk.qcow2,cache=none \\\n-cdrom <path-to-iso-image>/ubuntu-18.04.5-live-server-amd64.iso \\\n-device e1000,netdev=hostnet1,mac=52:54:00:d1:66:5f \\\n-netdev tap,id=hostnet1,script=<path-to-scripts>/virbr0-qemu-ifup,downscript=<path-to-scripts>/virbr0-qemu-ifdown \\\n-vnc :1\n```\n\n----------------------------------------\n\nTITLE: Fix Automatic Affinities for Heterogeneous Execution with OpenVINO (C++)\nDESCRIPTION: This C++ snippet shows how to 'fix' automatically assigned affinities in OpenVINO's heterogeneous execution mode.  It ensures that if an operation has been automatically assigned to the 'CPU', it retains that assignment, which can help optimize memory transfers. The OpenVINO runtime library is required.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/hetero-execution.rst#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    std::shared_ptr<ov::Model> model = core.read_model(\"path_to_your_model.xml\");\n\n    // [fix_automatic_affinities]\n    for (const auto& node : model->get_ops()) {\n        auto rt_info = node->get_rt_info();\n        auto it = rt_info.find(\"affinity\");\n        if (it != rt_info.end() && it->second.as<std::string>() == \"CPU\") {\n            node->get_rt_info()[\"affinity\"] = \"CPU\";\n        }\n    }\n    // [fix_automatic_affinities]\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example 5 (ceil_torch rounding)\nDESCRIPTION: This example demonstrates MaxPool with a 4D input, 2D kernel, and `rounding_type` set to 'ceil_torch'. It showcases the input tensor, strides, kernel dimensions, padding values, and resulting output after applying the MaxPool operation with the specified rounding behavior.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/pooling_shape_rules.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]]]]   # shape: (1, 1, 3, 3)\n      strides = [2, 2]\n      kernel = [2, 2]\n      pads_begin = [1, 1]\n      pads_end = [1, 1]\n      rounding_type = \"ceil_torch\"\n      output0 = [[[[1, 3],\n                   [7, 9]]]]   # shape: (1, 1, 2, 2)\n      output1 = [[[[0, 2],\n                   [6, 8]]]]   # shape: (1, 1, 2, 2)\n```\n\n----------------------------------------\n\nTITLE: LSTMCell Formula\nDESCRIPTION: This snippet represents the mathematical formula for the LSTMCell operation. It shows how the input, hidden state, and cell state are used to compute the output for the current time step. The formula details the calculations for the input gate, forget gate, cell state update, output gate, and the final hidden state.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/lstm-cell-1.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nFormula:\n *  - matrix multiplication\n (.) - Hadamard product (element-wise)\n [,] - concatenation\n f, g, h - are activation functions.\n it = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Wbi + Rbi)\n ft = f(Xt*(Wf^T) + Ht-1*(Rf^T) + Wbf + Rbf)\n ct = g(Xt*(Wc^T) + Ht-1*(Rc^T) + Wbc + Rbc)\n Ct = ft (.) Ct-1 + it (.) ct\n ot = f(Xt*(Wo^T) + Ht-1*(Ro^T) + Wbo + Rbo)\n Ht = ot (.) h(Ct)\n```\n\n----------------------------------------\n\nTITLE: Wrap cl::Buffer (C++)\nDESCRIPTION: This snippet shows how to create an `ov::RemoteTensor` by wrapping an existing OpenCL buffer (`cl::Buffer`) using the OpenVINO C++ API. It retrieves the `ClContext` and creates a tensor from the buffer. Requires OpenVINO runtime, intel_gpu ocl, and CL/cl.hpp.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\n//! [wrap_cl_buffer]\n#include <openvino/runtime/core.hpp>\n#include <openvino/runtime/intel_gpu/ocl/ocl.hpp>\n\n#if defined(OPENVINO_USE_OPENCL)\n#include <CL/cl.hpp>\n#endif\n\nvoid wrap_cl_buffer(cl::Buffer buffer_obj) {\n    // Wrap cl::Buffer to remote tensor\n    ov::Core core;\n    auto context = core.get_default_context(\"GPU\").as<ov::intel_gpu::ocl::ClContext>();\n    ov::Tensor remote_tensor = context.create_tensor(ov::element::f32, {ov::Dimension(buffer_obj.getInfo<CL_BUFFER_SIZE>())}, buffer_obj);\n    (void)remote_tensor;\n}\n//! [wrap_cl_buffer]\n```\n\n----------------------------------------\n\nTITLE: ReduceSum XML Example (keep_dims=true)\nDESCRIPTION: Demonstrates the usage of ReduceSum in an OpenVINO model using XML configuration when `keep_dims` is set to `true`. The input tensor has dimensions 6x12x10x24, the reduction axes are 2 and 3, and the output tensor has dimensions 6x12x1x1, preserving the reduced dimensions with size 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-sum-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceSum\" ...>\n    <data keep_dims=\"true\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Generate Header File from CSV Statistics\nDESCRIPTION: This snippet shows how to use the `ccheader.py` script to generate a C++ header file from the CSV statistics files. The script takes the CSV files as input (`${csv_files}`) and outputs a header file (`conditional_compilation_gen.h`) containing preprocessor definitions indicating which code regions were used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython3.8 ../../src/common/conditional_compilation/scripts/ccheader.py --stat ${csv_files} --out conditional_compilation_gen.h\n```\n\n----------------------------------------\n\nTITLE: Setting GITHUB_TOKEN Permissions (read-all)\nDESCRIPTION: This snippet demonstrates setting the `GITHUB_TOKEN` permissions to `read-all` as a starting point for restricting access in GitHub Actions workflows. This provides relatively safe permissions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/security.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npermissions: read-all\n```\n\n----------------------------------------\n\nTITLE: Hello Classification Arguments\nDESCRIPTION: This describes the arguments accepted by the hello_classification.js script.  It requires the path to the model file, the path to the image file, and the device to run the inference on as arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/js/node/hello_classification/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnode hello_classification.js *path_to_model_file* *path_to_img* *device*\n```\n\n----------------------------------------\n\nTITLE: ScaledDotProductAttention Example 5 (XML)\nDESCRIPTION: This XML example illustrates attention mask broadcasting in the ScaledDotProductAttention layer. It provides sample configurations for dimensions N, L, S, E, and Ev, demonstrating how attention masks with smaller dimensions are broadcasted to match the dimensions of L and S. 'causal' attribute is set to false.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/scaled-dot-product-attention.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"285\" name=\"aten::scaled_dot_product_attention_0\" type=\"ScaledDotProductAttention\" version=\"opset13\">\n\t\t\t<data causal=\"false\" />\n\t\t\t<input>\n\t\t\t\t<!-- Example with simple dimensions, with N = 2, L = 16, S = 32, E = 80, Ev = 80-->\n\t\t\t\t<port id=\"0\" precision=\"FP32\"> <!-- query -->\n\t\t\t\t\t<dim>2</dim>  <!-- N -->\n\t\t\t\t\t<dim>16</dim> <!-- L -->\n\t\t\t\t\t<dim>80</dim> <!-- E -->\n\t\t\t\t</port>\n\t\t\t\t<port id=\"1\" precision=\"FP32\"> <!-- key -->\n\t\t\t\t\t<dim>2</dim>  <!-- N -->\n\t\t\t\t\t<dim>32</dim> <!-- S -->\n\t\t\t\t\t<dim>80</dim> <!-- E -->\n\t\t\t\t</port>\n\t\t\t\t<port id=\"2\" precision=\"FP32\"> <!-- value -->\n\t\t\t\t\t<dim>2</dim>  <!-- N -->\n\t\t\t\t\t<dim>32</dim> <!-- S -->\n\t\t\t\t\t<dim>80</dim> <!-- Ev -->\n\t\t\t\t</port>\n\t\t\t\t<port id=\"3\" precision=\"FP32\"> <!-- attention_mask -->\n\t\t\t\t\t<dim>2</dim>  <!-- N -->\n\t\t\t\t\t<dim>1</dim>  <!-- to be broadcasted to L -->\n\t\t\t\t\t<dim>1</dim> <!-- to be broadcasted to S -->\n\t\t\t\t</port>\n\t\t\t</input>\n\t\t\t<output>\n\t\t\t\t<port id=\"4\" precision=\"FP32\">\n\t\t\t\t\t<dim>2</dim>  <!-- N -->\n\t\t\t\t\t<dim>16</dim> <!-- L -->\n\t\t\t\t\t<dim>80</dim> <!-- Ev -->\n\t\t\t\t</port>\n\t\t\t</output>\n\t\t</layer>\n```\n\n----------------------------------------\n\nTITLE: IsNaN Layer Definition in XML - OpenVINO\nDESCRIPTION: This XML snippet defines an IsNaN layer in OpenVINO. It takes a tensor of FP32 precision as input and outputs a boolean tensor of the same shape. The layer checks each element of the input tensor for NaN values and sets the corresponding element in the output tensor to True if it is NaN, and False otherwise.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/isnan-10.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IsNaN\">\n    <input>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n     </input>\n    <output>\n        <port id=\"1\" precision=\"BOOL\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Example 4 - XML\nDESCRIPTION: This XML snippet provides an example of the ScatterElementsUpdate operation with use_init_val set to 'true' and reduction set to 'sum' using multi-dimensional tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_14\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... use_init_val=\"true\" reduction=\"sum\" type=\"ScatterElementsUpdate\">\n    <input>\n        <port id=\"0\">>  <!-- data -->\n            <dim>3</dim>\n            <dim>4</dim>  <!-- values: [[1, 1, 1, 1],\n                                         [1, 1, 1, 1],\n                                         [1, 1, 1, 1]] -->\n        </port>\n        <port id=\"1\">  <!-- indices -->\n            <dim>2</dim>\n            <dim>2</dim>  <!-- values: [[1, 1],\n                                         [0, 3]] -->\n        </port>\n        <port id=\"2\">>  <!-- updates -->\n            <dim>2</dim>\n            <dim>2</dim>  <!-- values: [[11, 12],\n                                         [13, 14]]) -->\n        </port>\n        <port id=\"3\">     <!-- values: [1] -->\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"4\" precision=\"I32\">\n            <dim>3</dim>\n            <dim>4</dim>  <!-- values: [[ 1, 24,  1,  1],\n                                          [14,  1,  1, 15],\n                                          [ 1,  1,  1,  1]] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Naming Style in CMake\nDESCRIPTION: This snippet sets the naming style for the `TARGET_NAME` library, using the specified source directories for guidance. This helps enforce a consistent naming convention.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\nov_ncc_naming_style(FOR_TARGET ${TARGET_NAME}\n                    SOURCE_DIRECTORIES ${UTIL_INCLUDE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Dump BF16 Precision\nDESCRIPTION: Dumps BF16 precision data using the CPU plugin. It filters by the 'Convolution' layer. Replace `/path/model.xml` with the path to the model and saves the dump to `./dump_bf16`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/dump_check/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 dump_check.py -m /path/model.xml -bf16 -f Convolution ./dump_bf16\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Environment in Apps Section\nDESCRIPTION: This snippet shows how to set the OpenVINO environment variables in the apps section of the application's snapcraft.yaml. This ensures that the application has access to the OpenVINO libraries at runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/integrate-openvino-with-ubuntu-snap.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\napps:\n  app:\n    command: usr/local/app\n    environment:\n      LD_LIBRARY_PATH: $LD_LIBRARY_PATH:$SNAP/openvino-libs:$SNAP/openvino-extra-libs\n```\n\n----------------------------------------\n\nTITLE: Getting Tensor Element Type in TypeScript\nDESCRIPTION: Demonstrates the `getElementType` method for obtaining the tensor's element type, which is represented by the `element` type. The return value indicates the data type of the tensor's elements (e.g., float32, int32).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Tensor.rst#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ngetElementType(): element\n```\n\n----------------------------------------\n\nTITLE: Setting LTO based on Compiler Version CMake\nDESCRIPTION: This code snippet conditionally sets the interprocedural optimization (LTO) flag based on the GCC compiler version. If the compiler is GCC and the version is 9.0 or greater, it sets the CMAKE_INTERPROCEDURAL_OPTIMIZATION_RELEASE variable to the value of ENABLE_LTO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 9.0)\n    set(CMAKE_INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enumerating Devices for Multi-Device Mode in C++\nDESCRIPTION: This C++ code snippet demonstrates how to enumerate available devices and use them with the multi-device mode in OpenVINO. It shows how to obtain device information and configure the inference runtime to utilize multiple devices. It assumes the existence of OpenVINO Runtime API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n//! [part2]\nauto available_devices = core.get_available_devices();\nstd::string device_string;\nstd::string supported_devices_string;\nfor (auto&& available_device : available_devices) {\n    if (available_device.find(\"GPU\") != std::string::npos) {\n        if (!device_string.empty()) {\n            device_string += \",\";\n            supported_devices_string += \",\";\n        }\n        device_string += available_device;\n        supported_devices_string += available_device;\n    }\n}\nif (supported_devices_string.empty()) {\n    throw std::runtime_error(\"GPU device not found!\");\n}\ncore.set_property(\"MULTI\", ov::device::priorities(supported_devices_string));\n//! [part2]\n```\n\n----------------------------------------\n\nTITLE: Compiling TensorFlow Lite Model in OpenVINO using C\nDESCRIPTION: This C code snippet demonstrates compiling a TensorFlow Lite model with OpenVINO. It calls `ov_core_compile_model_from_file` to compile '<INPUT_MODEL>.tflite' and stores the result in `compiled_model`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_10\n\nLANGUAGE: c\nCODE:\n```\nov_compiled_model_t* compiled_model = NULL;\nov_core_compile_model_from_file(core, \"<INPUT_MODEL>.tflite\", \"AUTO\", 0, &compiled_model);\n```\n\n----------------------------------------\n\nTITLE: Gather Operation Example 2\nDESCRIPTION: Demonstrates the Gather operation with a non-default batch_dims value. The indices tensor is a 2D array, applied to the batches in the data tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-8.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 1\naxis = 1\n\nindices = [[0, 0, 4], <-- this is applied to the first batch\n           [4, 0, 0]]  <-- this is applied to the second batch\nindices_shape = (2, 3)\n\ndata    = [[1, 2, 3, 4, 5],  <-- the first batch\n           [6, 7, 8, 9, 10]]  <-- the second batch\ndata_shape = (2, 5)\n\noutput  = [[ 1, 1, 5],\n           [10, 6, 6]]\noutput_shape = (2, 3)\n```\n\n----------------------------------------\n\nTITLE: DFT Layer XML Definition (signal_size, 3D input)\nDESCRIPTION: Defines a DFT layer in XML format with the signal_size input for a 3-dimensional input tensor. This snippet shows how to utilize signal_size to modify the output dimensions of the DFT operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/dft-7.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"DFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- axes input contains [0, 1] -->\n        </port>\n        <port id=\"2\">\n            <dim>2</dim> <!-- signal_size input contains [512, 100] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>512</dim>\n            <dim>100</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: GELU Layer Configuration XML - OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of a Gelu layer in OpenVINO. It specifies the input and output ports with their corresponding dimensions, defining the data flow through the layer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/gelu-2.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Gelu\">\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Checking NPU State with dmesg (Linux)\nDESCRIPTION: This snippet shows an example of the message that should appear in the console output when the Intel NPU driver is successfully initialized on Linux. It uses the `dmesg` command to check the kernel logs for NPU-related messages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/configurations-intel-npu.rst#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n[  797.193201] [drm] Initialized intel_vpu 0.<version number> for 0000:00:0b.0 on minor 0\n```\n\n----------------------------------------\n\nTITLE: Transformation Filter Specification (Bash)\nDESCRIPTION: This code snippet shows how to specify the main graph transformation stages using the 'transformations' filter.  The tokens define the specific transformations to apply, like preLpt, lpt, postLpt, snippets, or specific transformations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/debug_caps_filters.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ntransformations=<comma_separated_tokens>\n```\n\n----------------------------------------\n\nTITLE: MatMul Matrix-Matrix Multiplication with Batch XML\nDESCRIPTION: Configuration example demonstrating MatMul operation for matrix-matrix multiplication with batch size 10, similar to a batched FullyConnected layer. The input ports define the dimensions of the input matrices (10x1024 and 1024x1000). The output port defines the resulting matrix (10x1000).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/matmul-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MatMul\">\n    <input>\n        <port id=\"0\">\n            <dim>10</dim>\n            <dim>1024</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1024</dim>\n            <dim>1000</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>10</dim>\n            <dim>1000</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Link ONNX Frontend Library CMake\nDESCRIPTION: Links the 'onnx_fe_standalone_build_test' target against the 'openvino::frontend::onnx' library. This makes the ONNX frontend's functionality available to the static library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/standalone_build/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino::frontend::onnx)\n```\n\n----------------------------------------\n\nTITLE: MVN Kernel Selector Usage in C++\nDESCRIPTION: This code demonstrates how to use the `mvn_kernel_selector` to obtain the best kernel for a given MVN operation. It retrieves the singleton instance of the kernel selector and calls the `GetBestKernels` method, passing in the operation parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_kernels.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// Get static instance of the kernel_selector\nauto& kernel_selector = kernel_selector::mvn_kernel_selector::Instance();\n// Run some heuristics to pick the best mvn kernel for given `mvn_params`\nauto best_kernels = kernel_selector.GetBestKernels(mvn_params);\n```\n\n----------------------------------------\n\nTITLE: BitwiseOr Layer Configuration Example (Numpy Broadcast) in XML\nDESCRIPTION: This XML example demonstrates the configuration of a BitwiseOr layer with numpy broadcasting enabled. The input tensors have different dimensions, and the output dimension reflects the result of the numpy broadcast operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-or-13.rst#_snippet_3\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"BitwiseOr\">\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Add Clang Format Target\nDESCRIPTION: This command adds a clang-format target for the specified target, in this case 'pybind_mock_frontend'. It allows for code formatting using clang-format directly from CMake.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/pyngraph_fe_mock_api/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${PYBIND_FE_NAME}_clang FOR_TARGETS ${PYBIND_FE_NAME})\n```\n\n----------------------------------------\n\nTITLE: BitwiseNot layer configuration in XML\nDESCRIPTION: Shows an example of configuring a BitwiseNot layer in XML. The input and output ports are defined with specific dimensions, illustrating how the layer is connected within a model graph. The input is a tensor of shape (256, 56), and the output is also a tensor of the same shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-not-13.rst#_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"BitwiseNot\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: RandomUniform IR Example\nDESCRIPTION: IR example for the RandomUniform layer in XML format, showing the data, input and output ports, and dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_21\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... name=\"RandomUniform\" type=\"RandomUniform\">\n    <data output_type=\"f32\" global_seed=\"234\" op_seed=\"148\"/>\n    <input>\n        <port id=\"0\" precision=\"I32\">  <!-- shape value: [2, 3, 10] -->\n            <dim>3</dim>\n        </port>\n        <port id=\"1\" precision=\"FP32\"/> <!-- min value -->\n        <port id=\"2\" precision=\"FP32\"/> <!-- max value -->\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP32\" names=\"RandomUniform:0\">\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>10</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Add Operation without Broadcasting in OpenVINO (XML)\nDESCRIPTION: Illustrates the Add operation in OpenVINO with no broadcasting, where input tensors have matching shapes. The auto_broadcast attribute is set to \"none\".\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/add-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Add\">\n    <data auto_broadcast=\"none\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions for Static Library in CMake\nDESCRIPTION: This snippet sets the `OPENVINO_STATIC_LIBRARY` compile definition if the library is being built as a static library. This allows for conditional compilation based on the build type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME}_obj PUBLIC OPENVINO_STATIC_LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target and Source Files\nDESCRIPTION: Defines the target name, finds source and header files in the current directory, and uses the `ov_add_sample` macro to configure the build. It specifies dependencies such as GFlags, format_reader, and ie_samples_utils.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/benchmark_app/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"benchmark_app\")\n\nfile (GLOB SRC ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp)\nfile (GLOB HDR ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp)\n\nov_add_sample(NAME ${TARGET_NAME}\n              SOURCES ${SRC}\n              HEADERS ${HDR}\n              DEPENDENCIES ${GFLAGS_TARGET} format_reader ie_samples_utils)\n```\n\n----------------------------------------\n\nTITLE: Starting Guest VM for Ubuntu Installation (sh)\nDESCRIPTION: Starts a QEMU Guest VM with KVM enabled, allocating memory and CPU, attaching a virtual disk and an Ubuntu ISO image for installation, and configuring network interfaces. It also sets up a VNC server for remote access during installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_22\n\nLANGUAGE: sh\nCODE:\n```\nsudo qemu-system-x86_64 -m 8192 -enable-kvm \\\n -cpu host \\\n -drive if=virtio,file=<path-to-disk-image>/ovsa_ovsa_runtime_vm_disk.qcow2,cache=none \\\n -cdrom <path-to-iso-image>/ubuntu-18.04.5-live-server-amd64.iso \\\n -device e1000,netdev=hostnet1,mac=52:54:00:d1:66:5f \\\n -netdev tap,id=hostnet1,script=<path-to-scripts>/virbr0-qemu-ifup,   downscript=<path-to-scripts>/virbr0-qemu-ifdown \\\n -vnc :2\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO Plugs in Application Snap\nDESCRIPTION: This snippet shows how to configure plugs in the application's snapcraft.yaml to consume the OpenVINO slots. Plugs define the interfaces the application uses, targeting the content provided by the OpenVINO snap.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/integrate-openvino-with-ubuntu-snap.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nplugs:\n  openvino-libs:\n    interface: content\n    content: openvino-libs\n    target: $SNAP/openvino-libs\n    default-provider: openvino-libs-test\n\n  openvino-3rdparty-libs:\n    interface: content\n    content: openvino-extra-libs\n    target: $SNAP/openvino-extra-libs\n    default-provider: openvino-libs-test\n```\n\n----------------------------------------\n\nTITLE: Enabling Snippets Segfault Detector (Bash)\nDESCRIPTION: This snippet shows how to enable the Snippets Segfault Detector by setting the `OV_CPU_SNIPPETS_SEGFAULT_DETECTOR` environment variable.  Any digit can be used as the level to activate the detector. This is currently only effective for x86 or x86-64 CPU backends.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/docs/debug_capabilities/snippets_segfault_detector.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_SNIPPETS_SEGFAULT_DETECTOR=<level> binary ...\n```\n\n----------------------------------------\n\nTITLE: Generate Static Registration File\nDESCRIPTION: This snippet generates a static registration file (static_reg.hpp) containing declarations and registrations for ONNX operators. It parses the source files, extracts relevant information, and writes the generated code to the static_reg.hpp file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/frontend/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n    file(GLOB_RECURSE op_list \"src/op/*.cpp\")\n    set(static_reg_file ${CMAKE_CURRENT_BINARY_DIR}/static_reg.hpp)\n    file(WRITE ${static_reg_file} \"// Copyright (C) 2018-2025 Intel Corporation\\n// SPDX-License-Identifier: Apache-2.0\\n// Auto generated file, DO NOT EDIT INLINE\\n\\n\")\n    file(APPEND ${static_reg_file} \"#include \\\"core/operator_set.hpp\\\"\\n\\n\")\n    file(APPEND ${static_reg_file} \"#define ONNX_DECL_OP(op) extern ov::OutputVector op(const Node&)\\n\\n\")\n    file(APPEND ${static_reg_file} \"namespace ov {\\nnamespace frontend {\\nnamespace onnx {\\n\")\n    foreach(src ${op_list})\n        file(READ ${src} source_code)\n        string(REGEX MATCHALL \"ONNX_OP([^;]+);\" matches \"${source_code}\")\n        foreach(match ${matches})\n            if(${match} MATCHES \"([a-z0-9_]+)::([a-z0-9_]+)::([a-z0-9_]+)\")\n                list(APPEND declarations ${CMAKE_MATCH_0})\n            endif()\n            list(APPEND registrations ${match})\n        endforeach()\n    endforeach()\n    list(APPEND declarations \"com_microsoft::opset_1::register_multiple_translators\")\n    list(APPEND registrations \"com_microsoft::opset_1::register_multiple_translators()\")\n    list(SORT declarations)\n    set(domain \"\")\n    set(opset \"\")\n    set(op_name, \"\")\n    foreach(decl ${declarations})\n        string(REGEX MATCH \"([a-z0-9_]+)::([a-z0-9_]+)::([a-z0-9_]+)\" matches ${decl})\n        if(NOT domain STREQUAL CMAKE_MATCH_1)\n            if(NOT opset STREQUAL \"\")\n                file(APPEND ${static_reg_file} \"}  // namespace ${opset}\\n\")\n            endif()\n            if(NOT domain STREQUAL \"\")\n                file(APPEND ${static_reg_file} \"}  // namespace ${domain}\\n\")\n            endif()\n            set(domain ${CMAKE_MATCH_1})\n            set(opset \"\")\n            file(APPEND ${static_reg_file} \"namespace ${domain} {\\n\")\n        endif()\n        if(NOT opset STREQUAL CMAKE_MATCH_2)\n            if(NOT opset STREQUAL \"\")\n                file(APPEND ${static_reg_file} \"}  // namespace ${opset}\\n\")\n            endif()\n            set(opset ${CMAKE_MATCH_2})\n            file(APPEND ${static_reg_file} \"namespace ${opset} {\\n\")\n        endif()\n        if(NOT op_name STREQUAL CMAKE_MATCH_3)\n            set(op_name ${CMAKE_MATCH_3})\n            if(NOT op_name STREQUAL \"register_multiple_translators\")\n                file(APPEND ${static_reg_file} \"ONNX_DECL_OP(${CMAKE_MATCH_3});\\n\")\n            else()\n                file(APPEND ${static_reg_file} \"extern bool ${CMAKE_MATCH_3}(void);\\n\")\n            endif()\n        endif()\n    endforeach()\n    if(NOT opset STREQUAL \"\")\n        file(APPEND ${static_reg_file} \"}  // namespace ${opset}\\n\")\n    endif()\n    if(NOT domain STREQUAL \"\")\n        file(APPEND ${static_reg_file} \"}  // namespace ${domain}\\n\")\n    endif()\n    file(APPEND ${static_reg_file} \"\\nvoid static_lib_registration(void) {\\n\")\n    foreach(reg ${registrations})\n        string(REPLACE \"ONNX_OP(\" \"ONNX_OP_M(\" reg ${reg})\n        file(APPEND ${static_reg_file} \"    ${reg};\\n\")\n    endforeach()\n    file(APPEND ${static_reg_file} \"}\\n\")\n    file(APPEND ${static_reg_file} \"}  // namespace onnx\\n}  // namespace frontend\\n}  // namespace ov\\n#undef ONNX_DECL_OP\\n\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Extract and Move OpenVINO\nDESCRIPTION: Extracts the downloaded OpenVINO archive, renames the extracted folder, and moves it to the Intel directory in Program Files (x86) using command-line tools.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-windows.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ntar -xf openvino_2025.1.0.zip\nren openvino_toolkit_windows_2025.1.0.18503.6fec06580ab_x86_64 openvino_2025.1.0\nmove openvino_2025.1.0 \"C:\\Program Files (x86)\\Intel\"\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags Based on Compiler ID\nDESCRIPTION: This snippet sets compiler flags based on the compiler being used. If the compiler is MSVC, it disables specific warnings. Otherwise, it disables all warnings. For Clang, it checks and disables deprecated non-prototype warnings and also disables unused variable and deprecated declarations warnings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/zlib/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_C_COMPILER_ID STREQUAL \"MSVC\")\n    set (CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} /wd4996 /wd4244 /W3\")\n    set (CMAKE_C_FLAGS_RELEASE \"${CMAKE_C_FLAGS_RELEASE} /wd4995 /wd4244 /wd4996\")\nelse()\n    set (CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Wno-all\")\n    if(CMAKE_CXX_COMPILER_ID MATCHES \"^(Apple)?Clang$\")\n        include(CheckCCompilerFlag)\n        check_c_compiler_flag(\"-Wdeprecated-non-prototype\" DEPRECATED_NO_PROTOTYPE)\n        if(DEPRECATED_NO_PROTOTYPE)\n            set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Wno-deprecated-non-prototype\")\n        endif()\n        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Wno-unused-variable\")\n        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Wno-deprecated-declarations\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Launching VTune Profiler GUI (Shell)\nDESCRIPTION: Commands to launch the Intel VTune Profiler GUI from the command line on Linux after setting the environment variables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection/debugging-auto-device.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncd /vtune install dir/intel/oneapi/vtune/2021.6.0/env\nsource vars.sh\nvtune-gui\n```\n\n----------------------------------------\n\nTITLE: Slice: Basic Slicing with Default Axes in OpenVINO XML\nDESCRIPTION: This example demonstrates basic slicing of a 1D tensor where the `axes` input is omitted. When `axes` are not explicitly specified, the operation defaults to slicing along the first dimension (axis 0).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-8.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Slice\" ...>\n       <input>\n           <port id=\"0\">       <!-- data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -->\n             <dim>10</dim>\n           </port>\n           <port id=\"1\">       <!-- start: [1] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"2\">       <!-- stop: [8] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"3\">       <!-- step: [1] -->\n             <dim>1</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"4\">       <!-- output: [1, 2, 3, 4, 5, 6, 7] -->\n               <dim>7</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX Build Options in CMake\nDESCRIPTION: This snippet sets various ONNX build options, including the Python executable path, the usage of shared protobuf libraries, the ONNX namespace, the usage of lite protobuf, whether to use ONNX ML, and the custom protobuf executable. These options configure how ONNX is built and linked.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/onnx/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(PYTHON_EXECUTABLE \"${Python3_EXECUTABLE}\")\nset(ONNX_USE_PROTOBUF_SHARED_LIBS OFF CACHE BOOL \"Use dynamic protobuf by ONNX library\" FORCE)\nset(ONNX_NAMESPACE ${OV_ONNX_NAMESPACE})\nset(ONNX_USE_LITE_PROTO ${ONNX_USE_LITE_PROTO_DEFAULT} CACHE BOOL \"Use protobuf lite for ONNX library\" FORCE)\nset(ONNX_ML ON CACHE BOOL \"Use ONNX ML\" FORCE)\nset(ONNX_CUSTOM_PROTOC_EXECUTABLE \"${PROTOC_EXECUTABLE}\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Inputs property of CompiledModel (TypeScript)\nDESCRIPTION: This code shows the declaration of the `inputs` property within the `CompiledModel` interface. This property is an array of `Output` objects representing the input tensors of the compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ninputs: Output []\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files\nDESCRIPTION: This snippet uses the file(GLOB_RECURSE) command to recursively find all C++ source files (.cpp) in the 'src' directory and header files (.hpp) in the 'include' directory. These files are then stored in the LIBRARY_SRC and LIBRARY_HEADERS variables, respectively, to be used in the library build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/mock_py_frontend/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE LIBRARY_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/include/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files\nDESCRIPTION: This snippet uses `file(GLOB_RECURSE)` to collect all source (`.cpp`) and header (`.h`, `.hpp`) files within the `src` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/src/*.h\n                          ${CMAKE_CURRENT_SOURCE_DIR}/src/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Conditional Object Library Definition\nDESCRIPTION: This snippet defines an object library `OBJ_LIB` if `BUILD_SHARED_LIBS` is enabled. It sets `OBJ_LIB` to the object files of the `openvino_hetero_plugin_obj` target. This is relevant for building shared libraries versus static libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/tests/unit/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_SHARED_LIBS)\n    set (OBJ_LIB $<TARGET_OBJECTS:openvino_hetero_plugin_obj>)\nendif()\n```\n\n----------------------------------------\n\nTITLE: GatherND Batch Dimensions Element Example\nDESCRIPTION: Demonstrates GatherND with a non-default batch_dims value (batch_dims = 1), focusing on element extraction. The indices are applied to each batch independently. This highlights the effect of batch_dims on the GatherND operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-8.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 1\nindices = [[1],    <--- this is applied to the first batch\n              [0]]    <--- this is applied to the second batch, shape = (2, 1)\ndata    = [[1, 2], <--- the first batch\n              [3, 4]] <--- the second batch, shape = (2, 2)\noutput  = [2, 3], shape = (2)\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for MSVC\nDESCRIPTION: This snippet sets the linker flags to ignore warning 4217 when using the MSVC compiler.  It prevents linker errors related to locally defined symbols being imported.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/test_builtin_extensions/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} /ignore:4217\")\n    set(CMAKE_MODULE_LINKER_FLAGS \"${CMAKE_MODULE_LINKER_FLAGS} /ignore:4217\")\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} /ignore:4217\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enabling Performance Summary\nDESCRIPTION: This environment variable enables the display of a performance summary when the model is destructed. Internal performance counters are automatically enabled when this variable is set.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nOV_CPU_SUMMARY_PERF=1\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for OpenVINO Runtime in CMake\nDESCRIPTION: This snippet sets the include directories for the OpenVINO runtime object library. It includes system and private directories, utilizing target properties from dependencies like pugixml and xbyak. It also handles multi-configuration generators by setting different binary directories based on the build configuration and CMake version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}_obj SYSTEM PRIVATE\n    $<TARGET_PROPERTY:openvino::pugixml,INTERFACE_INCLUDE_DIRECTORIES>\n    $<$<TARGET_EXISTS:xbyak::xbyak>:$<TARGET_PROPERTY:xbyak::xbyak,INTERFACE_INCLUDE_DIRECTORIES>>)\n\ntarget_include_directories(${TARGET_NAME}_obj PRIVATE\n    \"${CMAKE_CURRENT_SOURCE_DIR}/src\"\n    $<TARGET_PROPERTY:openvino::runtime::dev,INTERFACE_INCLUDE_DIRECTORIES>\n    $<$<TARGET_EXISTS:openvino_proxy_plugin_obj>:$<TARGET_PROPERTY:openvino_proxy_plugin_obj,INTERFACE_INCLUDE_DIRECTORIES>>\n    # for ov_plugins.hpp\n    $<IF:$<AND:$<BOOL:${OV_GENERATOR_MULTI_CONFIG}>,$<VERSION_GREATER_EQUAL:${CMAKE_VERSION},3.20>>,$<CONFIG:debug>:${CMAKE_CURRENT_BINARY_DIR}/Debug,$<CONFIG:release>:${CMAKE_CURRENT_BINARY_DIR}/Release,${CMAKE_CURRENT_BINARY_DIR}>)\n```\n\n----------------------------------------\n\nTITLE: Building Target Faster\nDESCRIPTION: This snippet utilizes `ov_build_target_faster`, likely a custom function, to optimize the build process, probably using unity builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME} UNITY)\n```\n\n----------------------------------------\n\nTITLE: Tile Example 3: Repeats smaller than data shape in XML\nDESCRIPTION: Example demonstrating the Tile operation where the number of elements in the 'repeats' input is less than the shape of the 'data' input. The 'repeats' input is promoted before tiling. The XML configuration illustrates the input and output tensor dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/tile-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Tile\">\n    <input>\n        <port id=\"0\">\n            <dim>5</dim>\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>  <!-- [1, 2, 3] will be promoted to [1, 1, 2, 3] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>5</dim>\n            <dim>2</dim>\n            <dim>6</dim>\n            <dim>12</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Test Target\nDESCRIPTION: Configures and adds a test target named `${TARGET_NAME}` using the `ov_add_test_target` macro. It specifies source directories, include directories, defines, dependencies, link libraries, and labels for the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/functional/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n        NAME\n            ${TARGET_NAME}\n        ROOT\n            ${CMAKE_CURRENT_SOURCE_DIR}\n        ADDITIONAL_SOURCE_DIRS\n            ${TEST_COMMON_SOURCE_DIR}\n        INCLUDES\n            ${CMAKE_CURRENT_SOURCE_DIR}\n            $<TARGET_PROPERTY:openvino_intel_gpu_plugin,SOURCE_DIR>/include/\n            ${TEST_COMMON_INCLUDE_DIR}\n        DEFINES\n            ${DEFINES}\n        DEPENDENCIES\n            openvino_intel_gpu_plugin\n        LINK_LIBRARIES\n            openvino::reference\n            funcSharedTests\n            OpenCL::NewHeaders # should come before OpenCL::OpenCL\n            OpenCL::OpenCL\n        ADD_CPPLINT\n        LABELS\n            OV GPU\n)\n```\n\n----------------------------------------\n\nTITLE: Collecting statistics with CMake\nDESCRIPTION: This snippet shows how to use CMake to configure the build process to collect statistics for conditional compilation. It enables profiling with ITT and sets the SELECTIVE_BUILD option to COLLECT.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/conditional_compilation.md#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nmkdir build\ncd build\ncmake -DENABLE_PROFILING_ITT=ON -DSELECTIVE_BUILD=COLLECT ..\n```\n\n----------------------------------------\n\nTITLE: Set Target Properties\nDESCRIPTION: Sets target properties, including the folder in the IDE and the C++ standard to use (C++17).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/functional/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(\n  ${TARGET_NAME} PROPERTIES FOLDER ${CMAKE_CURRENT_SOURCE_DIR} CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: EmbeddingBagOffsetsSum NumPy Implementation\nDESCRIPTION: This NumPy snippet demonstrates the functionality of the EmbeddingBagOffsetsSum operation. It takes an embedding table, indices, offsets, optional default index, and optional per-sample weights as input. It calculates the embedding bags by summing the embeddings based on the indices and offsets, and handles empty bags using the default index or fills them with zeros.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sparse/embedding-bag-offsets-sum-3.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef embedding_bag_offsets(\n        emb_table: np.ndarray,\n        indices: np.ndarray,\n        offsets: np.ndarray,\n        default_index: Optional[int] = None,\n        per_sample_weights: Optional[np.ndarray] = None,\n    ):\n        if per_sample_weights is None:\n            per_sample_weights = np.ones_like(indices)\n        embeddings = []\n        for emb_idx, emb_weight in zip(indices, per_sample_weights):\n            embeddings.append(emb_table[emb_idx] * emb_weight)\n        previous_offset = offsets[0]\n        bags = []\n        offsets = np.append(offsets, len(indices))\n        for bag_offset in offsets[1:]:\n            bag_size = bag_offset - previous_offset\n            if bag_size != 0:\n                embedding_bag = embeddings[previous_offset:bag_offset]\n                reduced_bag = np.add.reduce(embedding_bag)\n                bags.append(reduced_bag)\n            else:\n                # Empty bag case\n                if default_index is not None and default_index != -1:\n                    bags.append(emb_table[default_index])\n                else:\n                    bags.append(np.zeros(emb_table.shape[1:]))\n            previous_offset = bag_offset\n        return np.stack(bags, axis=0)\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX Static Libraries\nDESCRIPTION: This snippet calls a custom function `ov_install_static_lib` to install the `onnx` and `onnx_proto` static libraries. The libraries are installed to the location specified by `OV_CPACK_ARCHIVEDIR` and assigned to the `OV_CPACK_COMP_CORE` component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/onnx/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(onnx ${OV_CPACK_COMP_CORE})\nov_install_static_lib(onnx_proto ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: Benchmark App Execution with AUTO:GNA\nDESCRIPTION: This snippet executes the OpenVINO benchmark_app with the AUTO plugin, prioritizing GNA. It loads the 'add_abc.xml' model and runs for 10 seconds. The output shows that GNA is selected as the execution device, as it's the only specified device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/tests.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nopenvino/bin/intel64/Release$ ./benchmark_app -m openvino/src/core/tests/models/ir/add_abc.xml -d AUTO:GNA -t 10\n[Step 1/11] Parsing and validating input arguments\n[ INFO ] Parsing input parameters\n[Step 2/11] Loading OpenVINO Runtime\n[ INFO ] OpenVINO:\n[ INFO ] Build ................................. <OpenVINO version>-<Branch name>\n[ INFO ] \n[ INFO ] Device info:\n[ INFO ] AUTO\n[ INFO ] Build ................................. <OpenVINO version>-<Branch name>\n[ INFO ] \n[ INFO ] GNA\n[ INFO ] Build ................................. <OpenVINO version>-<Branch name>\n...\n[ INFO ]   GNA: \n[ INFO ]     OPTIMIZATION_CAPABILITIES: INT16 INT8 EXPORT_IMPORT\n...\n[Step 11/11] Dumping statistics report\n[ INFO ] Execution Devices: [ GNA ]\n...\n[ INFO ] Latency:\n[ INFO ]    Median:           0.01 ms\n[ INFO ]    Average:          0.01 ms\n[ INFO ]    Min:              0.01 ms\n[ INFO ]    Max:              0.20 ms\n[ INFO ] Throughput:          69131.99 FPS\n```\n\n----------------------------------------\n\nTITLE: NV12toRGB XML Layer Configuration Example 2\nDESCRIPTION: This XML snippet demonstrates the configuration of an NV12toRGB layer within an OpenVINO model, taking two separate planes (Y and UV) as input and producing an RGB output. The Y plane tensor has dimensions 1x480x640x1, the UV plane has dimensions 1x240x320x2, and the output tensor has dimensions 1x480x640x3.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/nv12-to-rgb-8.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"NV12toRGB\">\n    <input>\n        <port id=\"0\">  <!-- Y plane -->\n            <dim>1</dim>\n            <dim>480</dim>\n            <dim>640</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">  <!-- UV plane -->\n            <dim>1</dim>\n            <dim>240</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>480</dim>\n            <dim>640</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating Executable in CMake\nDESCRIPTION: This snippet creates the executable target named ov_appverifier_tests using the add_executable command. It includes all source files listed in the SRC and header files in HDR variables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/appverifier_tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(${TARGET_NAME} ${HDR} ${SRC})\n```\n\n----------------------------------------\n\nTITLE: PriorBox Layer Configuration XML\nDESCRIPTION: This XML snippet demonstrates a configuration example for the PriorBox layer in OpenVINO. It showcases how to specify attributes such as aspect ratios, minimum and maximum sizes, step, offset, and variance, along with input and output port dimensions, to generate prior boxes for object detection.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/prior-box-8.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"PriorBox\" ...>\n    <data aspect_ratio=\"2.0\" clip=\"false\" density=\"\" fixed_ratio=\"\" fixed_size=\"\" flip=\"true\" max_size=\"38.46\" min_size=\"16.0\" offset=\"0.5\" step=\"16.0\" variance=\"0.1,0.1,0.2,0.2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>        <!-- values: [24, 42] -->\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>        <!-- values: [384, 672] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>2</dim>\n            <dim>16128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Require OpenVINO Node Module (JavaScript)\nDESCRIPTION: This JavaScript snippet shows how to require the `openvino-node` module and access its addon. This allows using the compiled C++ functionalities that implement OpenVINO functionality by using the `ov` object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/README.md#_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst { addon: ov } = require('openvino-node');\n```\n\n----------------------------------------\n\nTITLE: Promote U64 and Signed Integer using ConvertPromoteTypes in OpenVINO\nDESCRIPTION: This example illustrates the ConvertPromoteTypes operation promoting a U64 and a signed integer (I16) to FP32 because the promotion would normally result in an unsupported I128.  'promote_unsafe' is true and 'pytorch_scalar_promotion' is false.  The 'u64_integer_promotion_target' attribute is set to \"f32\", dictating the resulting type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/convert-promote-types-14.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ConvertPromoteTypes\">\n    <data promote_unsafe=\"true\" pytorch_scalar_promotion=\"false\" u64_integer_promotion_target=\"f32\"/>\n    <input>\n        <port id=\"0\" precision=\"I16\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\" precision=\"U64\">\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\", names=\"ConvertPromoteTypes:0\">  < !-- type provided by u64_integer_promotion_target -->\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"3\" precision=\"FP32\", names=\"ConvertPromoteTypes:1\">  < !-- type provided by u64_integer_promotion_target -->\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Environment Variables (Windows Command Prompt)\nDESCRIPTION: This command executes the `setupvars.bat` batch file, which sets the necessary environment variables for using OpenVINO on Windows. `<INSTALLDIR>` represents OpenVINO's installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n<INSTALLDIR>\\setupvars.bat\n```\n\n----------------------------------------\n\nTITLE: Enabling ITT Profiling (CMake)\nDESCRIPTION: Conditionally enables ITT profiling based on the ENABLE_PROFILING_ITT variable. It links the appropriate ITT API library (ittnotify or ittapi) and defines compilation flags depending on the ENABLE_PROFILING_FILTER setting. If ENABLE_PROFILING_FILTER is not ALL or FIRST_INFERENCE, a fatal error is raised.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/itt/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_PROFILING_ITT)\n    if(TARGET ittapi::ittnotify)\n        set(itt_dependency ittapi::ittnotify)\n    else()\n        set(itt_dependency ittapi::ittapi)\n    endif()\n    target_link_libraries(${TARGET_NAME} PUBLIC ${itt_dependency})\n\n    if(ENABLE_PROFILING_FILTER STREQUAL \"ALL\")\n        target_compile_definitions(${TARGET_NAME} PUBLIC\n            ENABLE_PROFILING_ALL\n            ENABLE_PROFILING_FIRST_INFERENCE)\n    elseif(ENABLE_PROFILING_FILTER STREQUAL \"FIRST_INFERENCE\")\n        target_compile_definitions(${TARGET_NAME} PUBLIC\n            ENABLE_PROFILING_FIRST_INFERENCE)\n    else()\n        message(FATAL_ERROR \"The ${ENABLE_PROFILING_FILTER} profiling filter isn't supported\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Initialize NV12 single-plane preprocessing (C++)\nDESCRIPTION: This C++ code snippet demonstrates how to initialize preprocessing for NV12 single-plane video input with OpenVINO. This configures the model to accept single-plane NV12 video data by setting appropriate color format and memory type. Requires OpenVINO library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_32\n\nLANGUAGE: cpp\nCODE:\n```\nusing namespace ov::preprocess;\nauto p = PrePostProcessor(model);\np.input().tensor().set_element_type(ov::element::u8)\n                  .set_color_format(ov::preprocess::ColorFormat::NV12_SINGLE_PLANE)\n                  .set_memory_type(ov::intel_gpu::memory_type::surface);\np.input().preprocess().convert_color(ov::preprocess::ColorFormat::BGR);\np.input().model().set_layout(\"NCHW\");\nmodel = p.build();\n```\n\n----------------------------------------\n\nTITLE: Create Python Virtual Environment (Windows)\nDESCRIPTION: This command creates a Python virtual environment named 'openvino_env' on Windows. Using a virtual environment isolates project dependencies and avoids conflicts with other Python projects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/pypi_publish/pypi-openvino-rt.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython -m venv openvino_env\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for Protobuf\nDESCRIPTION: This snippet sets compiler flags for Protobuf based on the compiler being used (GCC, Clang, Intel LLVM, or MSVC). It suppresses specific warnings to avoid build errors or improve compatibility.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/protobuf/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG OR (OV_COMPILER_IS_INTEL_LLVM AND UNIX))\n    ov_add_compiler_flags(-Wno-all)\n    if(CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 11 AND CMAKE_COMPILER_IS_GNUCXX)\n        ov_add_compiler_flags(-Wno-stringop-overflow)\n    endif()\nelseif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    # protobuf\\src\\google\\protobuf\\descriptor.cc(822) : error C4703: potentially uninitialized local pointer variable 'to_use' used\n    ov_add_compiler_flags(/wd4703)\nendif()\n```\n\n----------------------------------------\n\nTITLE: IRDFT Layer Definition (3D Input, Signal Size) XML\nDESCRIPTION: Defines an IRDFT layer in XML for OpenVINO with a 3D input tensor and the signal_size input. The input tensor has dimensions 161x161x2, the axes tensor is [0, 1], and the signal_size tensor is [512, 100]. The output tensor has dimensions 512x100.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/irdft-9.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IRDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>161</dim>\n            <dim>161</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- [0, 1] -->\n        </port>\n        <port id=\"2\">\n            <dim>2</dim> <!-- [512, 100] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>512</dim>\n            <dim>100</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Install Python Converter Files\nDESCRIPTION: Installs the Python files for the OpenVINO Converter to a specific destination directory. It uses variables like OV_CPACK_PYTHONDIR and OV_CPACK_COMP_OVC to determine the installation location and component. It also excludes all files by default unless they are explicitly included.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/ovc/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY ${OpenVINOConverter_SOURCE_DIR}/openvino\n        DESTINATION ${OV_CPACK_PYTHONDIR}\n        COMPONENT ${OV_CPACK_COMP_OVC}\n        ${OV_CPACK_COMP_OVC_EXCLUDE_ALL}\n        USE_SOURCE_PERMISSIONS)\n```\n\n----------------------------------------\n\nTITLE: Example Component Definition with Revalidation and Build (YAML)\nDESCRIPTION: This YAML snippet provides an example of defining a component in components.yml with both `revalidate` and `build` dependencies. `component_1` and `component_2` require full revalidation (build and test), while `component_3` only needs to be built when `your_component` is changed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nyour_component:\n  revalidate:\n    - component_1\n    - component_2\n  build:\n    - component_3\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name CMake\nDESCRIPTION: This snippet sets the target name for the functional tests to `ov_auto_func_tests`. This name is used in subsequent commands to configure the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/functional/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_auto_func_tests)\n```\n\n----------------------------------------\n\nTITLE: Smart CI Job Definition for Workflow (YAML)\nDESCRIPTION: This YAML snippet shows the complete Smart CI job definition, including runs-on, outputs, and steps for checking out the action and getting affected components. It defines the structure and necessary configurations to get component dependencies and skip workflows.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_13\n\nLANGUAGE: YAML\nCODE:\n```\njobs:\n  Smart_CI:\n    runs-on: ubuntu-latest\n    outputs:\n      affected_components: \"${{ steps.smart_ci.outputs.affected_components }}\"\n      skip_workflow: \"${{ steps.smart_ci.outputs.skip_workflow }}\"\n    steps:\n      - name: checkout action\n        uses: actions/checkout@v4\n        with:\n          sparse-checkout: .github/actions/smart-ci\n\n      - name: Get affected components\n        id: smart_ci\n        uses: ./.github/actions/smart-ci\n        with:\n          repository: ${{ github.repository }}\n          pr: ${{ github.event.number }}\n          commit_sha: ${{ github.sha }}\n          component_pattern: \"category: (.*)\"\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO with CMake for CPU and IR Frontend (sh)\nDESCRIPTION: This snippet configures OpenVINO's CMake build to enable only the CPU inference backend and the IR frontend, while disabling other backends and frontends. This reduces the size of the final binary when building static libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/static_libaries.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DENABLE_INTEL_GPU=OFF \\\n      -DENABLE_INTEL_NPU=OFF \\\n      -DENABLE_TEMPLATE=OFF \\\n      -DENABLE_HETERO=OFF \\\n      -DENABLE_MULTI=OFF \\\n      -DENABLE_AUTO=OFF \\\n      -DENABLE_AUTO_BATCH=OFF \\\n      -DENABLE_OV_ONNX_FRONTEND=OFF \\\n      -DENABLE_OV_PADDLE_FRONTEND=OFF \\\n      -DENABLE_OV_TF_FRONTEND=OFF \\\n      -DENABLE_OV_TF_LITE_FRONTEND=OFF \\\n      -DENABLE_OV_JAX_FRONTEND=OFF \\\n      -DENABLE_OV_PYTORCH_FRONTEND=OFF \\\n      -DENABLE_OV_JAX_FRONTEND=OFF \\\n      -DENABLE_INTEL_CPU=ON \\\n      -DENABLE_OV_IR_FRONTEND=ON\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License 2.0\nDESCRIPTION: This snippet shows the boilerplate notice required to apply the Apache License 2.0 to a project. It includes placeholders for the copyright year and owner's name, which should be replaced with actual values. This notice must be enclosed in the appropriate comment syntax for the file format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/runtime-third-party-programs.txt#_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Interpolate Layer XML Example\nDESCRIPTION: This XML snippet demonstrates the configuration of an Interpolate layer with specific attributes such as axes, align_corners, pads_begin, pads_end, and mode. It also defines the input and output ports with their corresponding dimensions. The target spatial shape is provided as an input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Interpolate\" ...>\n    <data axes=\"2,3\" align_corners=\"0\" pads_begin=\"0\" pads_end=\"0\" mode=\"linear\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>2</dim>\n            <dim>48</dim>\n            <dim>80</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> \t<!--The values in this input are [50, 60] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>2</dim>\n            <dim>50</dim>\n            <dim>60</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO with Conan and Build\nDESCRIPTION: This command instructs Conan to install the dependencies specified in `conanfile.txt` and build any missing packages.  The `--build=missing` flag ensures that packages are built if pre-built binaries are not available. It also includes options to customize the build, such as disabling Intel GPU support and the ONNX frontend, and enabling shared libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conan.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nconan install conanfile.txt --build=missing -o:h 'openvino/*:enable_intel_gpu=False' -o:h 'openvino/*:enable_onnx_frontend=False' -o:h 'openvino/*:shared=True'\n```\n\n----------------------------------------\n\nTITLE: Enabling Cache Encryption (CPU)\nDESCRIPTION: This snippet demonstrates how to enable cache encryption for the CPU plugin in OpenVINO using Python or C++. It shows how to use the `cache_encryption_callbacks` config option with `compile_model` to encrypt the model while caching it and decrypt it when loading it from the cache.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n/// [ov:caching:part5]\n#include <openvino/openvino.hpp>\n\n#include <vector>\n\nint main() {\n    ov::Core core;\n\n    auto encryption_callback = [](const std::vector<uint8_t>& data) {\n        return data;\n    };\n\n    std::string model_path = \"path_to_model.xml\";\n    std::string cache_dir = \"/path/to/cache/dir\";\n\n    ov::CompiledModel compiled_model = core.compile_model(\n        model_path,\n        \"CPU\",\n        {{\n         ov::cache_dir(),\n         cache_dir,\n         ov::cache_encryption_callbacks(\n             {\n                 encryption_callback,\n                 encryption_callback,\n             }),\n     }});\n\n    return 0;\n}\n/// [ov:caching:part5]\n```\n\n----------------------------------------\n\nTITLE: Add Unit Test Utilities Target CMake\nDESCRIPTION: This CMake snippet defines the 'unit_test_utils' library target, its type (STATIC), source directory, format settings, link libraries (common_test_utils_s, openvino_runtime_s, gmock), dependencies (mock_engine), and public include directories for the build interface. The `ov_add_target` macro is responsible for handling the build configurations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/unit_test_utils/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME unit_test_utils)\n\nadd_subdirectory(mocks/mock_engine)\n\nov_add_target(\n        NAME ${TARGET_NAME}\n        TYPE STATIC\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        ADD_CLANG_FORMAT\n        LINK_LIBRARIES\n            PUBLIC\n                common_test_utils_s\n                openvino_runtime_s\n                gmock\n        DEPENDENCIES\n            mock_engine\n        INCLUDES\n            PUBLIC\n                \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/..>\"\n)\n```\n\n----------------------------------------\n\nTITLE: Prepare Calibration and Validation Datasets - ONNX\nDESCRIPTION: This code snippet shows how to create calibration and validation datasets when using the ONNX framework. These datasets are essential for evaluating and calibrating the quantized model using NNCF. The `dataset_fn` function generates random input data for the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/quantizing-with-accuracy-control.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# [dataset]\nimport numpy as np\nimport onnx\nfrom onnx.checker import check_model\n\nmodel = onnx.load_model(\"model.onnx\")\ncheck_model(model)\n\n\ndef dataset_fn(model):\n    input_name = model.graph.input[0].name\n    shape = [d.dim_value for d in model.graph.input[0].type.tensor_type.shape.dim]\n    calibration_size = 100\n    for _ in range(calibration_size):\n        yield {input_name: np.random.normal(0, 1, shape).astype(np.float32)}\n\n\ncalibration_dataset = dataset_fn(model)\nvalidation_dataset = dataset_fn(model)\n# [dataset]\n```\n\n----------------------------------------\n\nTITLE: Setting AArch64 Multi-ISA Option\nDESCRIPTION: This snippet conditionally sets the `OV_CPU_AARCH64_USE_MULTI_ISA` option based on the compiler (GCC version) and platform (AARCH64, excluding Apple). It enables/disables FP32, FP16 and SVE/SVE2 kernels based on the compiler and platform support.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (AARCH64 AND NOT APPLE AND CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 10.2)\n    # according to https://github.com/ARM-software/ComputeLibrary/issues/1053#issuecomment-1846903707 comment\n    # the 'multi_isa=1' below enables FP32, FP16 and SVE / SVE2 kernels\n    # But: arm_sve.h header is not available on gcc older 10.2 (let's test it), so we have to check it\n    set(OV_CPU_AARCH64_USE_MULTI_ISA_DEFAULT ON)\nelse()\n    set(OV_CPU_AARCH64_USE_MULTI_ISA_DEFAULT OFF)\nendif()\nset(OV_CPU_AARCH64_USE_MULTI_ISA ${OV_CPU_AARCH64_USE_MULTI_ISA_DEFAULT} CACHE BOOL \"Build multi-ISA ACL\")\n```\n\n----------------------------------------\n\nTITLE: ScatterNDUpdate Example 5 (XML) - Generic Dimensions\nDESCRIPTION: This XML snippet shows a ScatterNDUpdate layer configuration with the reduction set to 'none' and provides example dimensions for the input and output ports. This represents a case with higher dimensional inputs to demonstrate flexibility.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-nd-update-15.rst#_snippet_7\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... reduction=\"none\" type=\"ScatterNDUpdate\">\n    <input>\n        <port id=\"0\">\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>10</dim>\n            <dim>15</dim>\n        </port>\n        <port id=\"1\">\n            <dim>25</dim>\n            <dim>125</dim>\n            <dim>3</dim>\n        </port>\n        <port id=\"2\">\n            <dim>25</dim>\n            <dim>125</dim>\n            <dim>15</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>10</dim>\n            <dim>15</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: LRN Square Sum Calculation (4D Data, Axes=[1])\nDESCRIPTION: Calculates the squared sum for local response normalization with a 4D input tensor and normalization along the first axis (channels). It demonstrates how the normalization region is defined and summed, including boundary handling with max/min operations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/lrn-1.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nsqr_sum[a, b, c, d] =\n    sum(data[a, max(0, b - size / 2) : min(data.shape[1], b + size / 2 + 1), c, d] ** 2)\noutput = data / (bias + (alpha / size ** len(axes)) * sqr_sum) ** beta\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Target for Code Generation Test\nDESCRIPTION: Adds a custom target named 'run_codegen_test' that executes the code generation test script using the Python interpreter. This target depends on the code generation script.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(run_codegen_test\n    COMMAND ${Python3_EXECUTABLE} -B ${CODEGEN_TEST_SCRIPT}\n    DEPENDS \"${CODEGEN_SCRIPT}\"\n    WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}\n)\n```\n\n----------------------------------------\n\nTITLE: Range Operation Example (Negative Step) - XML\nDESCRIPTION: This XML snippet demonstrates the Range operation with a negative step. It shows how to define the input ports for start, stop, and step values, and the expected output dimension. The start value is 23, the stop value is 2, and the step value is -3, resulting in the sequence [23, 20, 17, 14, 11, 8, 5].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/range-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Range\">\n    <input>\n        <port id=\"0\">  <!-- start value: 23 -->\n        </port>\n        <port id=\"1\">  <!-- stop value: 2 -->\n        </port>\n        <port id=\"2\">  <!-- step value: -3 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>7</dim> <!-- [23, 20, 17, 14, 11, 8, 5] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Install Targets and Directories\nDESCRIPTION: This snippet defines installation rules for the executable target and relevant directories. The executable is installed to the tests directory and excluded from the ALL target. Additionally, relevant Python files, ONNX models, and data files are installed to the tests/onnx directory, also excluded from the ALL target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ov_onnx_frontend_tests\n        RUNTIME DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n\ninstall(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}\n        DESTINATION tests/onnx\n        COMPONENT tests EXCLUDE_FROM_ALL\n        FILES_MATCHING\n        PATTERN \"*.py\"\n        PATTERN \"*.onnx\"\n        PATTERN \"*.data\")\n```\n\n----------------------------------------\n\nTITLE: Uninstall OpenVINO Runtime with Homebrew (Shell)\nDESCRIPTION: Uninstalls the OpenVINO Runtime using the Homebrew package manager. This command removes OpenVINO and its associated files from the system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-brew.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nbrew uninstall openvino\n```\n\n----------------------------------------\n\nTITLE: Setting Gflags Build Options\nDESCRIPTION: This snippet sets various options for the gflags build. It disables building tests and shared libraries and sets flags to indicate the presence of header files and C99 format for integer types.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/gflags/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(GFLAGS_IS_SUBPROJECT TRUE)\nset(HAVE_SYS_STAT_H 1)\nset(HAVE_INTTYPES_H 1)\nset(INTTYPES_FORMAT C99)\nset(BUILD_TESTING OFF)\nset(BUILD_SHARED_LIBS OFF)\n```\n\n----------------------------------------\n\nTITLE: StringTensorPack Example 1 (1D)\nDESCRIPTION: This example demonstrates the use of StringTensorPack with 1D tensors for 'begins' and 'ends'. It shows how to pack the string \"IntelOpenVINO\" into a string tensor containing [\"Intel\", \"OpenVINO\"] based on the provided begin and end indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/string-tensor-pack-15.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StringTensorPack\" ... >\n    <input>\n        <port id=\"0\" precision=\"I64\">\n            <dim>2</dim>     <!-- begins = [0, 5] -->\n        </port>\n        <port id=\"1\" precision=\"I64\">\n            <dim>2</dim>     <!-- ends = [5, 13] -->\n        </port>\n        <port id=\"2\" precision=\"U8\">\n            <dim>13</dim>    <!-- symbols = \"IntelOpenVINO\" encoded in an utf-8 array -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"STRING\">\n            <dim>2</dim>     <!-- output = [\"Intel\", \"OpenVINO\"] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Define Shared Library Build\nDESCRIPTION: This snippet defines a compile definition when building a shared library. It sets the `ONNX_BUILD_SHARED` macro to 1, indicating that the ONNX frontend is being built as a shared library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/frontend/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME} PRIVATE ONNX_BUILD_SHARED=1)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Concat Layer Configuration XML Example (Negative Axis)\nDESCRIPTION: This XML snippet shows a Concat layer configuration in OpenVINO using a negative axis. It concatenates three input tensors along axis -3, which is equivalent to counting from the end of the dimensions. The 'axis' attribute uses a negative value to specify the concatenation dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/concat-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Concat\">\n    <data axis=\"-3\" />\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>8</dim>  <!-- axis for concatenation -->\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>16</dim>  <!-- axis for concatenation -->\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>32</dim>  <!-- axis for concatenation -->\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>56</dim>  <!-- concatenated axis: 8 + 16 + 32 = 48 -->\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating a simple model and pattern\nDESCRIPTION: This C++ snippet creates a simple OpenVINO model with Add, Relu, and Result operations and a corresponding pattern using the same operations. It initializes nodes and defines the model structure, enabling pattern matching within the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nshared_ptr<ov::Model> create_simple_model_and_pattern() {\n    // Model creation\n    ov::Shape shape = {1, 3, 224, 224};\n    auto parameter_node = opset13::Parameter::create(ov::element::f32, shape);\n    parameter_node->set_friendly_name(\"input\");\n\n    auto add_node = make_shared<opset13::Add>(parameter_node, opset13::Constant::create(ov::element::f32, ov::Shape{}, {1.0f}));\n    auto relu_node = make_shared<opset13::Relu>(add_node);\n    auto result_node = make_shared<opset13::Result>(relu_node);\n    result_node->set_friendly_name(\"result\");\n\n    auto model = make_shared<ov::Model>(ov::ResultVector{result_node}, ov::ParameterVector{parameter_node});\n\n    return model;\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: Defines the target name for the executable as `ov_capi_test`. This name is used throughout the CMake configuration to refer to the executable being built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"ov_capi_test\")\n```\n\n----------------------------------------\n\nTITLE: Installing Python Components for OpenVINOBenchmarkTool\nDESCRIPTION: This snippet uses the `install` command to copy the Python source files of the OpenVINO Benchmark Tool to the designated Python directory within the installation. It specifies the component to be installed, excludes all other components, and uses source permissions to ensure correct file permissions after installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/benchmark_tool/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY ${OpenVINOBenchmarkTool_SOURCE_DIR}/openvino\n        DESTINATION ${OV_CPACK_PYTHONDIR}\n        COMPONENT ${OV_CPACK_COMP_BENCHMARK_APP}\n        ${OV_CPACK_COMP_BENCHMARK_APP_EXCLUDE_ALL}\n        USE_SOURCE_PERMISSIONS)\n```\n\n----------------------------------------\n\nTITLE: Define Model Clone Method (TypeScript)\nDESCRIPTION: This code defines the `clone` method of the `Model` interface. It returns a new `Model` object that is a clone of the original.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nclone(): Model;\n```\n\n----------------------------------------\n\nTITLE: Setting Interprocedural Optimization Property (CMake)\nDESCRIPTION: This snippet sets the `INTERPROCEDURAL_OPTIMIZATION_RELEASE` property for the target based on the value of `ENABLE_LTO`. This enables link-time optimization for release builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/src/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES\n    INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Defining OpenVINO Operation Macro in C++\nDESCRIPTION: This macro registers the new operation with OpenVINO.  It specifies the operation's name, the opset it belongs to, and the parent operation class. This allows OpenVINO to recognize and utilize the custom operation within its computational graph.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/docs/operation_enabling_flow.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nOPENVINO_OP(\"<Operation_name>\", \"opset_name\", <Parent_op> /* Not needed if operation is inherited from ov::Op */);\n```\n\n----------------------------------------\n\nTITLE: Enabling Cache Encryption (GPU)\nDESCRIPTION: This snippet shows how to enable cache encryption for the GPU plugin in OpenVINO using Python and C++. It sets the CacheMode property to OPTIMIZE_SIZE to encrypt the model topology when saving to cache, and decrypts when loading. It's important to use OPTIMIZE_SIZE mode for full encryption.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\n/// [ov:caching:part6]\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n\n    std::string model_path = \"path_to_model.xml\";\n    std::string cache_dir = \"/path/to/cache/dir\";\n\n    ov::CompiledModel compiled_model = core.compile_model(model_path, \"GPU\", {\n        {ov::cache_dir(), cache_dir},\n        {ov::cache_mode.name(), ov::cache_mode(ov::CacheMode::OPTIMIZE_SIZE)}\n    });\n\n    return 0;\n}\n/// [ov:caching:part6]\n```\n\n----------------------------------------\n\nTITLE: Downloading and Installing OpenVINO (Ubuntu 24.04)\nDESCRIPTION: These commands download the OpenVINO Runtime archive for Ubuntu 24.04, extract it, and move the extracted directory to `/opt/intel`. It uses `curl` to download the archive, `tar` to extract it, and `sudo mv` to move the extracted folder with root privileges.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino/packages/2025.1/linux/openvino_toolkit_ubuntu24_2025.1.0.18503.6fec06580ab_x86_64.tgz --output openvino_2025.1.0.tgz\ntar -xf openvino_2025.1.0.tgz\nsudo mv openvino_toolkit_ubuntu24_2025.1.0.18503.6fec06580ab_x86_64 /opt/intel/openvino_2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Accessing Input by Index/Name\nDESCRIPTION: Demonstrates how to access a model's input using the `ov::preprocess::PrePostProcessor` class when the model has only one input, or when there are multiple inputs, by name or index. This is useful for configuring preprocessing steps for specific inputs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n// 1) Set input tensor information. We apply preprocessing\n//    demanding input tensor to have 'u8' type and 'NCHW' layout.\nppp.input().tensor().set_element_type(ov::element::u8).set_layout(\"NCHW\");\n// 2) Adding explicit preprocessing steps\nppp.input().preprocess().convert_element_type(ov::element::f32).convert_layout(\"NHWC\");\n// 3) Set input model information.\nppp.input().model().set_layout(\"NHWC\");\n// 4) Apply preprocessing modifying the original model\nmodel = ppp.build();\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Converting Precision Python\nDESCRIPTION: Shows how to integrate precision conversion into an execution graph as a preprocessing step using the OpenVINO preprocessing API in Python. This is useful when the input data type (e.g., unsigned 8-bit integer) differs from what the model expects (e.g., floating point).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n# 1) Set input tensor information. We apply preprocessing\n#    demanding input tensor to have 'u8' type and 'NCHW' layout.\nppp.input().tensor().set_element_type(ov.Type.u8).set_layout(ov.Layout('NCHW'))\n# 2) Adding explicit preprocessing steps\nppp.input().preprocess().convert_element_type(ov.Type.f32)\n# 3) Set input model information.\nppp.input().model().set_layout(ov.Layout('NCHW'))\n# 4) Apply preprocessing modifying the original model\nmodel = ppp.build()\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Converting Precision C++\nDESCRIPTION: Shows how to integrate precision conversion into an execution graph as a preprocessing step using the OpenVINO preprocessing API in C++. This is useful when the input data type (e.g., unsigned 8-bit integer) differs from what the model expects (e.g., floating point).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n// 1) Set input tensor information. We apply preprocessing\n//    demanding input tensor to have 'u8' type and 'NCHW' layout.\nppp.input().tensor().set_element_type(ov::element::u8).set_layout(\"NCHW\");\n// 2) Adding explicit preprocessing steps\nppp.input().preprocess().convert_element_type(ov::element::f32);\n// 3) Set input model information.\nppp.input().model().set_layout(\"NCHW\");\n// 4) Apply preprocessing modifying the original model\nmodel = ppp.build();\n```\n\n----------------------------------------\n\nTITLE: Create RemoteContext from ID3D11Device (C++)\nDESCRIPTION: This snippet demonstrates how to create an `ov::RemoteContext` from an existing Direct3D 11 device (`ID3D11Device`) using the GPU plugin's C++ API.  It initializes the OpenVINO core and retrieves the extension. Requires the OpenVINO runtime and intel_gpu ocl headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n//! [context_from_d3d_device]\n#include <openvino/runtime/core.hpp>\n#include <openvino/runtime/intel_gpu/ocl/dx.hpp>\n\n#if defined(_WIN32)\n#include <d3d11.h>\n#endif\n\nvoid create_context_from_d3d_device(ID3D11Device* device) {\n    // context from user-defined d3d device\n    ov::Core core;\n    auto remote_context = core.get_extension<ov::intel_gpu::ocl::dx_context>(device);\n    (void)remote_context;\n}\n//! [context_from_d3d_device]\n```\n\n----------------------------------------\n\nTITLE: Squeeze 4D Tensor to 2D Tensor using OpenVINO XML\nDESCRIPTION: This example demonstrates how to use the Squeeze operation to reduce a 4D tensor to a 2D tensor by removing dimensions with a size of 1. The 'allow_axis_skip' attribute is set to false, and the input tensor's dimensions are explicitly defined, with axis '2' being squeezed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/squeeze-15.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Squeeze\" version=\"opset15\">\n    <data allow_axis_skip=\"false\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>1</dim>\n            <dim>2</dim>\n        </port>\n    </input>\n    <input>\n        <port id=\"1\">\n            <dim>2</dim>  <!-- value [0, 2] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ReduceProd Layer with keep_dims=false (XML) - Example 3\nDESCRIPTION: This XML snippet showcases a ReduceProd layer with 'keep_dims' as 'false' and reduction along axis -2.  The input is 6x12x10x24, leading to an output shape of 6x12x24 after removing the second to last dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-prod-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceProd\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [-2] that means independent reduction in each channel, batch and second spatial dimension -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Loading Compiled Encrypted Model from Blob C++\nDESCRIPTION: This C++ snippet shows how to load a compiled encrypted model from a binary blob file using `ov::Core::import_model`. The model, having already been compiled, is read from an `std::ifstream` and imported into the OpenVINO runtime for execution on the specified device (in this case, \"CPU\"). The `blob` variable represents the path to the compiled model file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-security.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\n// Import a model from a blob.\nstd::ifstream compiled_blob(blob, std::ios_base::in | std::ios_base::binary);\nauto compiled_model = core.import_model(compiled_blob, \"CPU\");\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions in CMake\nDESCRIPTION: This command defines compile-time definitions for the target library.  `PRIVATE` visibility restricts these definitions to the target itself. These definitions are used to set the shared library prefix and suffix based on CMake and OpenVINO build variables. This configures the shared library name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME}\n    PRIVATE\n        SHARED_LIB_PREFIX=\"${CMAKE_SHARED_LIBRARY_PREFIX}\"\n        SHARED_LIB_SUFFIX=\"${OV_BUILD_POSTFIX}${CMAKE_SHARED_LIBRARY_SUFFIX}\")\n```\n\n----------------------------------------\n\nTITLE: Setting target properties\nDESCRIPTION: Sets properties for the target, including interprocedural optimization for release builds and the `EXPORT_NAME` which defines the name used when this library is exported for use by other projects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/logger/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME npu_logger_utils)\n```\n\n----------------------------------------\n\nTITLE: Einsum Implicit Mode Example C++\nDESCRIPTION: This example shows how Einsum operates with an equation containing both capital and lowercase letters in implicit mode.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/einsum-7.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nA = [[[1.0, 2.0, 3.0],\n```\n\n----------------------------------------\n\nTITLE: Enabling Link-Time Optimization (LTO)\nDESCRIPTION: This snippet enables Link-Time Optimization (LTO) if the `ENABLE_LTO` variable is set. LTO can improve performance by optimizing across multiple compilation units during the linking phase.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/thirdparty/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_LTO)\n    set(CMAKE_INTERPROCEDURAL_OPTIMIZATION_RELEASE ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Set Threading Interface\nDESCRIPTION: This snippet configures the threading interface for the test target using the 'ov_set_threading_interface_for' function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/functional/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Defining Supported Typed Array Alias in TypeScript\nDESCRIPTION: This code snippet defines a type alias called `SupportedTypedArray` using TypeScript syntax. This alias represents a union of different typed array constructors, including Int8Array, Uint8Array, Int16Array, Uint16Array, Int32Array, Uint32Array, Float32Array, and Float64Array. It allows the code to refer to any of these typed arrays using a single type name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/types/SupportedTypedArray.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nSupportedTypedArray: Int8Array | Uint8Array | Int16Array | Uint16Array | Int32Array | Uint32Array | Float32Array | Float64Array\n```\n\n----------------------------------------\n\nTITLE: Installing Tuning Cache File (CMake)\nDESCRIPTION: This snippet installs the tuning cache JSON file to the specified installation directory and component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_18\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES ${CMAKE_CURRENT_SOURCE_DIR}/cache/cache.json\n        DESTINATION ${CACHE_JSON_INSTALL_DIR}\n        COMPONENT ${CACHE_JSON_COMPONENT})\n```\n\n----------------------------------------\n\nTITLE: Running StressMemLeaksTests (Bash)\nDESCRIPTION: This command executes the `StressMemLeaksTests` binary using `gtest-parallel`. It assumes that the `openvino_bin` directory contains the compiled test executable.  It relies on the googletest framework for test execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngtest-parallel <openvino_bin>/StressMemLeaksTests\n```\n\n----------------------------------------\n\nTITLE: RandomUniform Output Example 2 (C++)\nDESCRIPTION: This code block provides an example of the output from the RandomUniform operation using a double output type, setting global_seed to 80, op_seed to 100, alignment to TENSORFLOW, minval to 2 and maxval to 10.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\ninput_shape  = [2, 2]\nminval = 2\nmaxval = 10\noutput  = [[5.65927959 4.23122376]\\\n         [2.67008206 2.36423758]]\n```\n\n----------------------------------------\n\nTITLE: Configuring Network Sequence with YAML\nDESCRIPTION: This code snippet demonstrates how to define a network sequence in Protopipe using YAML. It shows how to specify a list of models or lists of models to be executed in a chain-like structure, along with an optional delay between them.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ninput_stream_list:\n- network:\n  - { name: A.xml, ip: FP16, il: NCHW, device: CPU }\n  - [{ name: B.xml, ip: FP16, op: FP16 }, { name: C.xml, ip: FP16, op: FP16 }]\n  - { name: D.xml, ip: FP16, op: FP16, config: { PEROFMRANCE_HINT: LATENCY } }\n  delay_in_us: 5000\n```\n\n----------------------------------------\n\nTITLE: Compile Model Configuration Type Definition\nDESCRIPTION: Defines the type for the config parameter used in compileModel, compileModelSync, importModel, and importModelSync methods. It is an object with string keys and values of type OVAny.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconfig: Record<string, OVAny>,\n```\n\n----------------------------------------\n\nTITLE: Initialize NV12 to Grey preprocessing (C++)\nDESCRIPTION: This C++ code snippet shows how to initialize preprocessing for converting NV12 video input to grayscale in OpenVINO. This sets up the model to process only the Y plane of the NV12 data, effectively converting it to grayscale. Requires OpenVINO library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_33\n\nLANGUAGE: cpp\nCODE:\n```\nusing namespace ov::preprocess;\nauto p = PrePostProcessor(model);\np.input().tensor().set_element_type(ov::element::u8)\n                  .set_color_format(ov::preprocess::ColorFormat::NV12_SINGLE_PLANE)\n                  .set_memory_type(ov::intel_gpu::memory_type::surface);\np.input().preprocess().convert_color(ov::preprocess::ColorFormat::BGR);\np.input().model().set_layout(\"NCHW\");\nmodel = p.build();\n```\n\n----------------------------------------\n\nTITLE: Fetching External Dependencies\nDESCRIPTION: This snippet uses `FetchContent` to declare and make available the `node-api-headers` and `node-addon-api` dependencies from GitHub. It specifies the URL and SHA256 hash for each dependency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(FetchContent)\n\nFetchContent_Declare(\n    node-api-headers\n    URL      https://github.com/nodejs/node-api-headers/archive/refs/tags/v1.1.0.tar.gz\n    URL_HASH SHA256=70608bc1e6dddce280285f3462f18a106f687c0720a4b90893e1ecd86e5a8bbf\n)\nFetchContent_MakeAvailable(node-api-headers)\n\nFetchContent_Declare(\n    node-addon-api\n    URL      https://github.com/nodejs/node-addon-api/archive/refs/tags/v8.0.0.tar.gz\n    URL_HASH SHA256=42424c5206b9d67b41af4fcff5d6e3cb22074168035a03b8467852938a281d47\n)\nFetchContent_MakeAvailable(node-addon-api)\n```\n\n----------------------------------------\n\nTITLE: Configuring ONNX Frontend Module with CMake\nDESCRIPTION: This CMake snippet includes a frontend module configuration and then calls the `frontend_module` function to set up the ONNX frontend. It specifies the module name (`py_onnx_frontend`), the frontend type (`onnx`), and the target package version based on variables `OV_CPACK_COMP_PYTHON_OPENVINO` and `pyversion`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/frontend/onnx/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(${pyopenvino_SOURCE_DIR}/frontend/frontend_module.cmake)\nfrontend_module(py_onnx_frontend onnx ${OV_CPACK_COMP_PYTHON_OPENVINO}_${pyversion})\n```\n\n----------------------------------------\n\nTITLE: Range Generation with Positive Step in OpenVINO (XML)\nDESCRIPTION: This XML snippet demonstrates the Range operation with a positive step, generating a sequence from start (2) to stop (23) with a step of 3. The output is an integer tensor of dimension 7.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/range-4.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Range\">\n    <data output_type=\"i32\">\n    <input>\n        <port id=\"0\">  <!-- start value: 2 -->\n        </port>\n        <port id=\"1\">  <!-- stop value: 23 -->\n        </port>\n        <port id=\"2\">  <!-- step value: 3 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>7</dim> <!-- [ 2,  5,  8, 11, 14, 17, 20] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Tools Subdirectory CMake\nDESCRIPTION: This snippet adds the tools subdirectory to the build process if the ENABLE_INTEL_NPU_INTERNAL flag is enabled. It also adds the NPU_INTERNAL_COMPONENT to the CPack configuration as a hidden component, meaning it won't be visible in the default installation package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_INTEL_NPU_INTERNAL)\n    add_subdirectory(tools)\n\n    ov_cpack_add_component(${NPU_INTERNAL_COMPONENT} HIDDEN)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Col2Im Example - Unbatched Input (XML)\nDESCRIPTION: This example demonstrates the Col2Im operation with unbatched input and default optional parameters. It highlights the input and output port configurations for the unbatched case, focusing on the dimensions for channels and output size.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/col2im-15.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Col2Im\" ... >\n    <input>\n        <port id=\"0\" precision=\"I32\">\n            <dim>12</dim>    <!-- C * Product(kernel_size) -->\n            <dim>225</dim>   <!-- L -->\n        </port>\n        <port id=\"1\" precision=\"I32\">\n            <dim>2</dim>     <!-- output_size -->\n        </port>\n        <port id=\"2\" precision=\"I32\">\n            <dim>2</dim>     <!-- kernel_size -->\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"I32\">\n            <dim>3</dim>     <!-- C -->\n            <dim>16</dim>    <!-- output_size[0] -->\n            <dim>16</dim>    <!-- output_size[1] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Create RemoteContext from cl_queue (C++)\nDESCRIPTION: This snippet demonstrates how to create an `ov::RemoteContext` from an existing OpenCL command queue (`cl_command_queue`) using the GPU plugin's C++ API. It initializes the OpenVINO core and retrieves the extension. Requires the OpenVINO runtime and intel_gpu ocl headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n//! [context_from_cl_queue]\n#include <openvino/runtime/core.hpp>\n#include <openvino/runtime/intel_gpu/ocl/ocl.hpp>\n\n#if defined(OPENVINO_USE_OPENCL)\n#include <CL/cl.hpp>\n#endif\n\nvoid create_context_from_cl_queue(cl_command_queue queue) {\n    // context from user-defined OpenCL queue\n    ov::Core core;\n    auto remote_context = core.get_extension<ov::intel_gpu::ocl::ocl_context>(queue);\n    (void)remote_context;\n}\n//! [context_from_cl_queue]\n```\n\n----------------------------------------\n\nTITLE: Concat Layer Configuration XML Example (Positive Axis)\nDESCRIPTION: This XML snippet demonstrates the configuration of a Concat layer in OpenVINO with a positive axis. It concatenates three input tensors along axis 1, resulting in an output tensor with a modified dimension along that axis. The 'axis' attribute specifies the dimension to concatenate along.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/concat-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Concat\">\n    <data axis=\"1\" />\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>8</dim>  <!-- axis for concatenation -->\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>16</dim>  <!-- axis for concatenation -->\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>32</dim>  <!-- axis for concatenation -->\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>56</dim>  <!-- concatenated axis: 8 + 16 + 32 = 48 -->\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Set Threading Interface (CMake)\nDESCRIPTION: This snippet uses ov_set_threading_interface_for to configure the threading interface for the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: MaxPool Layer Definition with 'explicit' Auto Pad (XML)\nDESCRIPTION: This example demonstrates the XML definition of a MaxPool layer in OpenVINO using the 'explicit' auto_pad attribute. The kernel size, padding, and strides are specified. The input and output ports define the dimensions of the tensors, resulting in different output dimensions than the input due to explicit padding.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-8.rst#_snippet_9\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MaxPool\" ... >\n       <data auto_pad=\"explicit\" kernel=\"2,2\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n       <input>\n           <port id=\"0\">\n               <dim>1</dim>\n               <dim>3</dim>\n               <dim>32</dim>\n               <dim>32</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"1\">\n               <dim>1</dim>\n               <dim>3</dim>\n               <dim>17</dim>\n               <dim>17</dim>\n           </port>\n           <port id=\"2\">\n               <dim>1</dim>\n               <dim>3</dim>\n               <dim>17</dim>\n               <dim>17</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Verify Network Bridge (br0)\nDESCRIPTION: This command checks the status of the network bridge `br0` using the `ip` command. It filters the output to show information about the `br0` interface, including its IP address and status.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nip a | grep br0\n```\n\n----------------------------------------\n\nTITLE: Verify Installation\nDESCRIPTION: This snippet shows how to verify the installation of the Compile Tool. It involves sourcing the OpenVINO setupvars.sh script and then running the compile_tool executable with the -h option to display the help message. This ensures that the tool is correctly installed and accessible.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/compile_tool/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsource <openvino_install_dir>/setupvars.sh\n<compile_tool_install_dir>/tools/compile_tool/compile_tool -h\n```\n\n----------------------------------------\n\nTITLE: Set Target Names\nDESCRIPTION: Sets the original and new target names for the common test utils library.  This allows for backward compatibility while introducing a more descriptive name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/common_test_utils/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# Keep old name so that library can be used from NPU repo\nset(TARGET_NAME commonTestUtils)\nset(NEW_TARGET_NAME common_test_utils)\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library\nDESCRIPTION: Installs the static library to the specified installation directory using the `ov_install_static_lib` macro. The library is installed as part of the `NPU_PLUGIN_COMPONENT` component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/common/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${NPU_PLUGIN_COMPONENT})\n```\n\n----------------------------------------\n\nTITLE: Disable OpenVINO Telemetry (opt-out)\nDESCRIPTION: This command disables telemetry reporting in OpenVINO. Running this command prevents the collection of anonymous usage data. This is a global setting and affects all OpenVINO tools that participate in telemetry.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/additional-resources/telemetry.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nopt_in_out --opt_out\n```\n\n----------------------------------------\n\nTITLE: Create Build Directory\nDESCRIPTION: Creates a build directory inside the OpenVINO repository and changes the current directory to it.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_raspbian.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build && cd build/\n```\n\n----------------------------------------\n\nTITLE: Gather Operation Example 6\nDESCRIPTION: Illustrates the Gather operation with negative indices. Negative indices indicate reverse indexing from data.shape[axis].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-8.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 0\naxis = 0\n\nindices = [0, -2, -1]\ndata    = [1, 2, 3, 4, 5]\noutput  = [1, 4, 5]\n```\n\n----------------------------------------\n\nTITLE: Adding Compile Definitions\nDESCRIPTION: Adds a private compile definition to the target, which enables the implementation of the OpenVINO Runtime plugin. This definition is only visible within the library's compilation units.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/backend/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME}\n    PRIVATE\n        IMPLEMENT_OPENVINO_RUNTIME_PLUGIN)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties for LTO in CMake\nDESCRIPTION: This snippet sets the `INTERPROCEDURAL_OPTIMIZATION_RELEASE` property of the target `${TARGET_NAME}` to the value of `${ENABLE_LTO}`. This fixes potential issues related to Link Time Optimization (LTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/tests/unit/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name CMake\nDESCRIPTION: Sets the target name for the functional tests to `ov_auto_batch_func_tests`. This name is used to reference the target in other CMake commands.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/tests/functional/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME ov_auto_batch_func_tests)\n```\n\n----------------------------------------\n\nTITLE: StridedSlice Ellipsis Mask Example in XML\nDESCRIPTION: Demonstrates the use of `ellipsis_mask` in the StridedSlice layer.  This mask allows replacing multiple dimensions with a single ellipsis (...), which expands to include all remaining dimensions not explicitly sliced. It is equivalent to array[0:4, ..., 0:5].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/strided-slice-1.rst#_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StridedSlice\" ...>\n    <data begin_mask=\"0,0,0\" end_mask=\"0,0,0\" new_axis_mask=\"0,0,0\" shrink_axis_mask=\"0,0,0\" ellipsis_mask=\"0,1,0\"/>\n    <input>\n        <port id=\"0\">\n            <dim>10</dim> <!-- first dim -->\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim> <!-- last dim -->\n        </port>\n        <port id=\"1\">\n            <dim>3</dim> <!-- begin: [0, 0, 0] - with second dimension marked as ellipsis -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim> <!-- end: [4, 0, 5] -->\n        </port>\n        <port id=\"3\">\n            <dim>3</dim> <!-- stride: [1, -1, 1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>4</dim> <!-- first dim modified -->\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim> <!-- ellipsis skipped over 8 dimensions to match pattern -->\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>5</dim> <!-- last dim modified -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet adds the 'functional' subdirectory to the current CMake project.  The add_subdirectory command instructs CMake to process the CMakeLists.txt file located in the 'functional' directory, effectively incorporating its build instructions into the current project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(functional)\n```\n\n----------------------------------------\n\nTITLE: Activate Virtual Environment (Windows)\nDESCRIPTION: This command activates the previously created virtual environment `openvino_env` on Windows. Activating the environment ensures that subsequent commands use the environment's Python interpreter and installed packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-pip.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nopenvino_env\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties (CMake)\nDESCRIPTION: This code snippet configures target properties like interprocedural optimization and clang format. It also creates an alias for the target library and sets its export name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/zero/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(\n  ${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n\nadd_library(openvino::npu_zero_utils ALIAS ${TARGET_NAME})\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME npu_zero_utils)\n```\n\n----------------------------------------\n\nTITLE: Defining Conformance Shared Target CMake\nDESCRIPTION: Defines the `conformance_shared` target as a static library, specifying source directories, include paths, and linked libraries. This ensures the library has access to required headers and dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/conformance_infra/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME conformance_shared)\n\nov_add_target(\n        NAME ${TARGET_NAME}\n        TYPE STATIC\n        ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/include\"\n        ADDITIONAL_SOURCE_DIRS\n                ${OpenVINO_SOURCE_DIR}/src/tests/functional/plugin/conformance/subgraphs_dumper_new/include/cache/meta/\n                ${CMAKE_CURRENT_SOURCE_DIR}/src\n        ADD_CPPLINT\n        INCLUDES\n            PUBLIC\n                \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\"\n        LINK_LIBRARIES\n            PUBLIC\n                gflags\n                funcSharedTests\n)\n```\n\n----------------------------------------\n\nTITLE: Installing build dependencies\nDESCRIPTION: This script uses the `install_build_dependencies.sh` script located in the project root directory to install all the necessary build dependencies for OpenVINO. It requires sudo privileges to install system-level packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_linux.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nsudo ./install_build_dependencies.sh\n```\n\n----------------------------------------\n\nTITLE: Uninstall OpenVINO (Latest Version)\nDESCRIPTION: This command removes all packages with 'openvino' in their name using the ZYPPER package manager. This uninstalls the latest installed version of OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-zypper.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nsudo zypper remove *openvino*\n```\n\n----------------------------------------\n\nTITLE: ReduceL1 with keep_dims=true in OpenVINO XML\nDESCRIPTION: Example of ReduceL1 operation in OpenVINO XML format, demonstrating the use of keep_dims attribute set to true.  The input has dimensions 6x12x10x24, and the reduction is performed along axes [2, 3]. The output retains the reduced dimensions with size 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-l1-4.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceL1\" ...>\n    <data keep_dims=\"true\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Generating HTML and CSV Reports using summarize.py for API conformance (Python)\nDESCRIPTION: This snippet demonstrates how to use the `summarize.py` script to generate HTML and CSV reports from an XML report file for API conformance. It requires specifying the XML report file path, the output directory, and the type of report (-t API).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/README.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\npython3 summarize.py --xml /opt/repo/infrastructure-master/thirdparty/gtest-parallel/report_api.xml --out /opt/repo/infrastructure-master/thirdparty/gtest-parallel/ -t API\n```\n\n----------------------------------------\n\nTITLE: Adding Gflags Subdirectory and Properties\nDESCRIPTION: This snippet adds the gflags directory as a subdirectory to the current build and sets the folder property for the `gflags_nothreads_static` target to `thirdparty`.  This organizes the project structure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/gflags/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(gflags EXCLUDE_FROM_ALL)\nset_target_properties(gflags_nothreads_static PROPERTIES FOLDER thirdparty)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties and Variables (CMake)\nDESCRIPTION: Defines the target name, include directories, and searches for source and header files. The REF_IMPL_INCLUDE_DIR variable is set to the include directory, and file globbing is used to find source and header files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_reference\")\n\nset(REF_IMPL_INCLUDE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\n\nfile(GLOB_RECURSE LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE PUBLIC_HEADERS ${REF_IMPL_INCLUDE_DIR}/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Elu Layer Definition XML - OpenVINO\nDESCRIPTION: This XML snippet defines an Elu layer in OpenVINO. It specifies the 'alpha' attribute, input and output ports with their dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/elu-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Elu\">\n    <data alpha=\"1.0\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Interprocedural Optimization (LTO)\nDESCRIPTION: Sets the `INTERPROCEDURAL_OPTIMIZATION_RELEASE` property for the specified target to the value of `ENABLE_LTO`. This enables or disables Link Time Optimization (LTO) for release builds, which can improve performance but may increase build times.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_43\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Full TensorIterator Layer Example (XML)\nDESCRIPTION: This XML example represents a complete TensorIterator layer definition, including detailed configurations for input and output ports with dimension specifications, port mappings that specify connections between the TensorIterator's external ports and the internal layers within its body, and back edges establishing recurrent connections between Result and Parameter layers inside the TensorIterator. The example also shows a snippet of the 'layers' section in the body.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/tensor-iterator-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"TensorIterator\" ...>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>25</dim>\n            <dim>512</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>256</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>256</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>25</dim>\n            <dim>256</dim>\n        </port>\n    </output>\n    <port_map>\n        <input axis=\"1\" external_port_id=\"0\" internal_layer_id=\"0\" start=\"0\"/>\n        <input external_port_id=\"1\" internal_layer_id=\"3\"/>\n        <input external_port_id=\"2\" internal_layer_id=\"4\"/>\n        <output axis=\"1\" external_port_id=\"3\" internal_layer_id=\"12\"/>\n    </port_map>\n    <back_edges>\n        <edge from-layer=\"8\" to-layer=\"4\"/>\n        <edge from-layer=\"9\" to-layer=\"3\"/>\n    </back_edges>\n    <body>\n        <layers>\n            <layer id=\"0\" type=\"Parameter\" ...>\n                <output>\n                    <port id=\"0\" precision=\"FP32\">\n                        <dim>1</dim>\n                        <dim>1</dim>\n                        <dim>512</dim>\n                    </port>\n                </output>\n            </layer>\n            <layer id=\"1\" type=\"Const\" ...>\n                <data offset=\"0\" size=\"16\"/>\n                <output>\n                    <port id=\"1\" precision=\"I64\">\n                        <dim>2</dim>\n                    </port>\n                </output>\n            </layer>\n            <layer id=\"2\" type=\"Reshape\" ...>\n                <input>\n                    <port id=\"0\">\n                        <dim>1</dim>\n                        <dim>1</dim>\n                        <dim>512</dim>\n                    </port>\n                    <port id=\"1\">\n                        <dim>2</dim>\n                    </port>\n                </input>\n                <output>\n                    <port id=\"2\" precision=\"FP32\">\n                        <dim>1</dim>\n\n```\n\n----------------------------------------\n\nTITLE: ScatterNDUpdate Example 1 (XML)\nDESCRIPTION: This XML snippet demonstrates a ScatterNdUpdate layer configuration with reduction set to 'none'. It specifies input and output ports with their precisions and dimensions, illustrating how to define the data flow for the operation within an OpenVINO model. The update occurs at the indices [0, 2, -3, -3, 0] with respective updates values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-nd-update-15.rst#_snippet_3\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... reduction=\"none\" type=\"ScatterNdUpdate\">\n    <input>\n        <port id=\"0\" precision=\"FP32\">  <!-- data -->\n            <dim>4</dim>  <!-- values: [1, 2, 3, 4] -->\n        </port>\n        <port id=\"1\" precision=\"I32\">  <!-- indices -->\n            <dim>5</dim>  <!-- values: [0, 2, -3, -3, 0] -->\n        </port>\n        <port id=\"2\" precision=\"FP32\">  <!-- updates -->\n            <dim>5</dim>  <!-- values: [10, 20, 30, 40, 50] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP32\">\n            <dim>4</dim>  <!-- values: [50, 20, 20, 4] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Shape Parameter Definition in TypeScript\nDESCRIPTION: This TypeScript snippet defines the optional shape parameter for the `PartialShapeConstructor`. This shape parameter allows the user to create partial shapes with a predefined shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PartialShapeConstructor.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nshape: string\n```\n\n----------------------------------------\n\nTITLE: Triangle Filter Coefficients Calculation (Python)\nDESCRIPTION: This Python code defines a function `triangle_coeffs` that calculates the coefficients for a triangle filter.  It takes `dz` (distance) as input and returns a coefficient based on the distance from the center. Coefficients represent weights for a linear interpolation. Values close to 0 have higher weights and values far away have coefficients of 0. Dependencies: numpy.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef triangle_coeffs(dz):\n    return np.maximum(0.0, 1.0 - np.abs(dz))\n```\n\n----------------------------------------\n\nTITLE: Adding a Step to a GitHub Actions Job (YAML)\nDESCRIPTION: This example demonstrates how to add a step to an existing GitHub Actions job. It shows a typical step configuration with a name, a conditional execution clause using the Smart CI system, and a run section containing the commands to be executed. This allows you to integrate new tests into an existing CI workflow.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/adding_tests.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nsteps:\n...\n  - name: OVC unit tests\n    if: fromJSON(inputs.affected-components).OVC.test\n    run: python3 -m pytest -s ${INSTALL_TEST_DIR}/ovc/unit_tests --junitxml=${INSTALL_TEST_DIR}/TEST-OpenVinoConversion.xml\n...\n```\n\n----------------------------------------\n\nTITLE: NPU Compiler Configuration\nDESCRIPTION: This snippet demonstrates how to create a configuration file to specify properties for the NPU compiler.  In this example, the NPU_COMPILER_TYPE is set to MLIR, potentially using a custom build of the NPU Compiler instead of the one distributed within the NPU driver.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/compile_tool/README.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nNPU_COMPILER_TYPE MLIR\n```\n\n----------------------------------------\n\nTITLE: Adding DNNL Include Directories\nDESCRIPTION: This snippet adds the DNNL include directories to the target. These include directories are crucial for compiling code that uses the DNNL library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/vectorized/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} SYSTEM PRIVATE\n    $<TARGET_PROPERTY:dnnl,INCLUDE_DIRECTORIES>)\n\ntarget_include_directories(${TARGET_NAME} SYSTEM PRIVATE\n    $<TARGET_PROPERTY:dnnl,SOURCE_DIR>/src/common\n    $<TARGET_PROPERTY:dnnl,SOURCE_DIR>/src/cpu\n    $<TARGET_PROPERTY:dnnl,SOURCE_DIR>/include)\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Provider Action Usage Example in YAML\nDESCRIPTION: This YAML snippet demonstrates a basic usage of the OpenVINO provider GitHub action within a workflow. It defines a job that downloads pre-built OpenVINO artifacts, sets outputs based on the action's result, and configures the environment for subsequent steps. Key inputs are the `platform` and `revision` to specify which OpenVINO version to download.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/openvino_provider.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nopenvino_download:\n  name: Download prebuilt OpenVINO\n  outputs:\n    status: ${{ steps.openvino_download.outcome }}\n    ov_wheel_source: ${{ steps.openvino_download.outputs.ov_wheel_source }}\n    ov_version: ${{ steps.openvino_download.outputs.ov_version }}\n    docker_tag: ${{ steps.get_docker_tag.outputs.docker_tag }}\n  timeout-minutes: 10\n  defaults:\n    run:\n      shell: bash\n  runs-on: aks-linux-medium\n  container:\n    image: 'openvinogithubactions.azurecr.io/openvino_provider:0.1.0'\n    volumes:\n      - /mount:/mount\n      - ${{ github.workspace }}:${{ github.workspace }}\n\n  steps:\n  - uses: openvinotoolkit/openvino/.github/actions/openvino_provider@master\n    id: openvino_download\n    with:\n      platform: 'ubuntu22'\n      revision: latest_available_commit\n```\n\n----------------------------------------\n\nTITLE: Running the OpenVINO Test Drive Application\nDESCRIPTION: This command is used to start the OpenVINO Test Drive application. It requires Flutter SDK and platform-specific dependencies to be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-test-drive.rst#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nflutter run\n```\n\n----------------------------------------\n\nTITLE: Example: Dump Output Blobs\nDESCRIPTION: Sets the environment variable to dump only the output blobs from nodes during OpenVINO CPU execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_NODE_PORTS=OUT binary ...\n```\n\n----------------------------------------\n\nTITLE: Program Node Structure Definition in C++\nDESCRIPTION: This C++ code provides the structure of a `program_node`, which represents a node in the execution program. A `program_node` holds information related to transformation, optimization, kernel selection, layout, dependencies, and fused operations.  The `selected_impl` field contains the selected kernel information required to run it.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/basic_data_structures.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nstruct program_node {\n...\n    program& myprog;\n    std::unique_ptr<primitive_impl> selected_impl;\n    layout output_layout;\n    std::vector<program_node*> dependencies;\n    std::list<program_node*> users;\n    std::set<primitive_id> memory_dependencies;\n    std::vector<fused_activation_params> fused_activations;\n    std::vector<fused_primitive_desc> fused_prims;\n...;\n}\n```\n\n----------------------------------------\n\nTITLE: Set Target Name (CMake)\nDESCRIPTION: This sets the name of the target to build to openvino_auto_batch_plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_auto_batch_plugin\")\n```\n\n----------------------------------------\n\nTITLE: Exposing pybind11 Module in OpenVINO Python Package\nDESCRIPTION: Demonstrates how to make a pybind11-based module (containing the MyTensor class) visible in the OpenVINO Python package by adding an import statement to the `openvino/runtime/__init__.py` file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino._pyopenvino.mymodule import MyTensor\n```\n\n----------------------------------------\n\nTITLE: Enabling Transformation Serialization in OpenVINO\nDESCRIPTION: This snippet shows how to enable serialization of the model to .xml/.bin format after each transformation pass in OpenVINO, using the `OV_ENABLE_SERIALIZE_TRACING` environment variable.  Setting the variable to \"true\", \"on\" or \"1\" enables serialization for all transformations. A filter string can be provided to serialize only specific passes. No additional dependencies are required, aside from the OpenVINO environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/docs/debug_capabilities/transformation_statistics_collection.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OV_ENABLE_SERIALIZE_TRACING=true\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport OV_ENABLE_SERIALIZE_TRACING=\"Pass1,Pass2,Pass3\"\n```\n\n----------------------------------------\n\nTITLE: Configure Tests\nDESCRIPTION: This snippet configures unit tests for the plugin. It creates an object library if BUILD_SHARED_LIBS is enabled and links it with necessary libraries. It also adds include directories and compile definitions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n    if(BUILD_SHARED_LIBS)\n        set(OBJ_NAME ${TARGET_NAME}_obj)\n\n        add_library(${OBJ_NAME} OBJECT ${SOURCES} ${HEADERS})\n        ov_link_system_libraries(${OBJ_NAME} PUBLIC openvino::pugixml)\n\n        ov_add_version_defines(src/version.cpp ${OBJ_NAME})\n\n        target_include_directories(${OBJ_NAME}\n            PRIVATE\n                $<TARGET_PROPERTY:openvino::runtime::dev,INTERFACE_INCLUDE_DIRECTORIES>\n                $<TARGET_PROPERTY:openvino::itt,INTERFACE_INCLUDE_DIRECTORIES>\n            PUBLIC\n                ${CMAKE_CURRENT_SOURCE_DIR}/src\n                $<TARGET_PROPERTY:openvino::conditional_compilation,INTERFACE_INCLUDE_DIRECTORIES>)\n\n        ov_set_threading_interface_for(${OBJ_NAME})\n\n        target_compile_definitions(${OBJ_NAME} PRIVATE USE_STATIC_IE IMPLEMENT_OPENVINO_RUNTIME_PLUGIN)\n    endif()\n\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Convert OCR Model with Bounded Dynamic Dimensions in Python\nDESCRIPTION: This Python code snippet converts an ONNX OCR model using `openvino.convert_model` and specifies a boundary (1..3) for the batch dimension of inputs `data` and `seq_len` using `openvino.Dimension`.  The `batch_dim` variable defines a dimension with minimum value 1 and maximum value 3, which is then used in the input shapes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/setting-input-shapes.rst#_snippet_8\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\nbatch_dim = ov.Dimension(1, 3)\nov_model = ov.convert_model(\"ocr.onnx\", input=[(\"data\", [batch_dim, 150, 200, 1]), (\"seq_len\", [batch_dim])])\n```\n\n----------------------------------------\n\nTITLE: 1D Convolution Example in OpenVINO XML\nDESCRIPTION: This XML snippet shows an example configuration for a 1D convolution layer in OpenVINO. It defines the layer type, data attributes (dilations, pads_begin, pads_end, strides, auto_pad), and input/output port dimensions.  The 'valid' auto_pad means no padding is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/convolution-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"Convolution\" ...>\n    <data dilations=\"1\" pads_begin=\"0\" pads_end=\"0\" strides=\"2\" auto_pad=\"valid\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>5</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"1\">\n            <dim>16</dim>\n            <dim>5</dim>\n            <dim>4</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>16</dim>\n            <dim>63</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Git Hook for Automatic Sign-Off\nDESCRIPTION: This script is a Git hook that automatically adds the 'Signed-off-by' line to commit messages. It retrieves the user's name and email from Git configuration and appends the sign-off message to the commit message file. This ensures that all commits are signed off automatically.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/commit_signoff_policy.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/sh\n\nCOMMIT_MSG_FILE=$1\nCOMMIT_SOURCE=$2\nSHA1=$3\n\nNAME=$(git config user.name)\nEMAIL=$(git config user.email)\n\nif [ -z \"$NAME\" ]; then\n    echo \"empty git config user.name\"\n    exit 1\nfi\n\nif [ -z \"$EMAIL\" ]; then\n    echo \"empty git config user.email\"\n    exit 1\nfi\n\ngit interpret-trailers --if-exists doNothing --trailer \\\n    \"Signed-off-by: $NAME <$EMAIL>\" \\\n    --in-place \"$1\"\n```\n\n----------------------------------------\n\nTITLE: ReduceSum XML Example (axes=[1], keep_dims=false)\nDESCRIPTION: Illustrates the ReduceSum operation with `keep_dims` set to `false` and axes set to `[1]`. The input tensor is 6x12x10x24, and the reduction happens along the second dimension. The output shape is 6x10x24, as the second dimension is removed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-sum-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceSum\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [1] that means independent reduction in each channel and spatial dimensions -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Custom Operation Declaration in C++ Header\nDESCRIPTION: This code snippet defines the declaration of a custom operation `custom_add` within the `org.openvinotoolkit` domain for the ONNX Frontend.  It includes necessary headers and defines the function signature that takes an `ov::frontend::onnx::Node` as input and returns an `ov::OutputVector`.  This declaration is placed in a `.hpp` file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/how_to_add_op.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#pragma once\n\n#include \"core/node.hpp\"\n\nnamespace ov {\nnamespace frontend {\nnamespace onnx {\nnamespace op {\nnamespace set_1 {\n\nov::OutputVector custom_add(const ov::frontend::onnx::Node& node);\n\n}  // namespace set_1\n}  // namespace op\n}  // namespace onnx\n}  // namespace frontend\n}  // namespace ov\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Keyword Spacing\nDESCRIPTION: This rule enforces consistent spacing around keywords in JavaScript and TypeScript code. It requires specific overrides for the `catch` keyword, ensuring no space after it. Configured as `keyword-spacing: ['error', { overrides: { catch: { after: false } } }]`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_18\n\nLANGUAGE: JavaScript\nCODE:\n```\nkeyword-spacing: ['error', { overrides: { catch: { after: false } } }]\n```\n\n----------------------------------------\n\nTITLE: Defining Dependency Graph with YAML\nDESCRIPTION: This code snippet illustrates how to define a dependency graph in Protopipe using YAML, specifying operations (op_desc) and their connections. It demonstrates how to define different operation types like Infer and CPU, and how to connect them to form a pipeline.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nop_desc:\n  - { tag: A, path: Model-A.xml, ip: FP16, op: FP16 }\n  - { tag: B, path: Model-B.onnx, framework: onnxrt, ep: { name: OV, device_type: CPU_FP32 } }\n  - { tag: C, type: CPU, time_in_us: 5000 }\n  - { tag: D, path: Model-D.onnx, framework: onnxrt }\n  - { tag: E, path: Model-E.xml, il: NCHW, device: NPU, config: { PEFORMANCE_HINT: LATENCY } }\n  - { tag: F, path: Model-F.xml }\nconnections:\n  - [A, C, E, F]\n  - [A, B, D, F]\n  - [B, F]\n```\n\n----------------------------------------\n\nTITLE: Run test setup\nDESCRIPTION: Executes the test setup script. This is a one-time setup required before running individual test files for the first time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/test_examples.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnpm run test_setup\n```\n\n----------------------------------------\n\nTITLE: Globbing Public Headers in CMake\nDESCRIPTION: This snippet finds all header files (`*.hpp`) within the `UTIL_INCLUDE_DIR` and its subdirectories using `file(GLOB_RECURSE)`. The results are stored in `PUBLIC_HEADERS` for later use in building and installing the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE PUBLIC_HEADERS ${UTIL_INCLUDE_DIR}/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Google Test macro TEST usage\nDESCRIPTION: This code snippet shows how to define a simple test case using the TEST macro in GoogleTest.  It defines a test named `TestName` within the `TestSuiteName` test suite. Assertions like `EXPECT_EQ` or `ASSERT_NE` are used within the test body to determine the test outcome.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_unit_test.md#_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\nTEST(TestSuiteName, TestName) {\n  ... test body ...\n}\n```\n\n----------------------------------------\n\nTITLE: Create RemoteContext from cl_context (C++)\nDESCRIPTION: This snippet demonstrates how to create an `ov::RemoteContext` from an existing OpenCL context (`cl_context`) using the GPU plugin's C++ API. It initializes the OpenVINO core and retrieves the extension. Requires the OpenVINO runtime and intel_gpu ocl headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n//! [context_from_cl_context]\n#include <openvino/runtime/core.hpp>\n#include <openvino/runtime/intel_gpu/ocl/ocl.hpp>\n\n#if defined(OPENVINO_USE_OPENCL)\n#include <CL/cl.hpp>\n#endif\n\nvoid create_context_from_cl_context(cl_context context) {\n    // context from user-defined OpenCL context\n    ov::Core core;\n    auto remote_context = core.get_extension<ov::intel_gpu::ocl::ocl_context>(context);\n    (void)remote_context;\n}\n//! [context_from_cl_context]\n```\n\n----------------------------------------\n\nTITLE: Cloning Yocto Repositories\nDESCRIPTION: Clones the necessary repositories (poky, meta-intel, meta-openembedded, and meta-clang) required for building a Yocto image. These repositories provide the base system, Intel-specific configurations, open-embedded packages and clang compiler support, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yocto.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://git.yoctoproject.org/git/poky\ngit clone https://git.yoctoproject.org/meta-intel\ngit clone https://git.openembedded.org/meta-openembedded\ngit clone https://github.com/kraj/meta-clang.git\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: Sets the target name for the library to 'format_reader'. This name is used throughout the CMake configuration to refer to the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/format_reader/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset (TARGET_NAME \"format_reader\")\n```\n\n----------------------------------------\n\nTITLE: Define RESIZE_CUBIC Constant - Typescript\nDESCRIPTION: Defines the RESIZE_CUBIC constant used as an enumeration member within the OpenVINO library. This constant represents the cubic interpolation resize algorithm. It's located in the addon.ts file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/resizeAlgorithm.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nRESIZE_CUBIC: number\n```\n\n----------------------------------------\n\nTITLE: Build with Custom TBB - CMake\nDESCRIPTION: This example demonstrates how to use a custom TBB installation with OpenVINO.  It sets the TBBROOT environment variable to point to the TBB installation directory before running CMake. This is necessary when using a non-default compiler or a custom-built TBB library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/cmake_options_for_custom_compilation.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport TBBROOT=<path to TBB install root>\ncmake ...\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Converting Layout (Transpose) Python\nDESCRIPTION: Shows how to manually transpose axes without using a layout definition via preprocessing steps in the OpenVINO API in Python. This performs the same transpose operation as using source and destination layouts, but can be less readable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n# 1) Set input tensor information. We apply preprocessing\n#    demanding input tensor to have 'u8' type and 'NHWC' layout.\nppp.input().tensor().set_element_type(ov.Type.u8).set_layout(ov.Layout('NHWC'))\n# 2) Adding explicit preprocessing steps\nppp.input().preprocess().convert_layout([0, 3, 1, 2])\n# 3) Set input model information. We assume that the original model has 'NCHW' layout\nppp.input().model().set_layout(ov.Layout('NCHW'))\n# 4) Apply preprocessing modifying the original model\nmodel = ppp.build()\n```\n\n----------------------------------------\n\nTITLE: Excluding Snippets LIBXSMM TPP Paths\nDESCRIPTION: This snippet excludes Snippets TPP-related paths if `ENABLE_SNIPPETS_LIBXSMM_TPP` is not enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_25\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT ENABLE_SNIPPETS_LIBXSMM_TPP)\n    list(APPEND EXCLUDE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/emitters/tpp/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/transformations/tpp/*)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Generate Test Models (If TensorFlow Found)\nDESCRIPTION: If TensorFlow is found, this block generates TensorFlow Lite test models using Python scripts. It creates custom commands to execute the generation scripts and defines a custom target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (tensorflow_FOUND)\n    set(TEST_TENSORFLOW_LITE_MODELS \"${TEST_MODEL_ZOO_OUTPUT_DIR}/tensorflow_lite_test_models\")\n\n    file(GLOB_RECURSE TENSORFLOW_GEN_SCRIPTS ${CMAKE_CURRENT_SOURCE_DIR}/test_models/gen_scripts/generate_*.py)\n    file(GLOB_RECURSE TENSORFLOW_ALL_SCRIPTS ${CMAKE_CURRENT_SOURCE_DIR}/*.py)\n    set(OUT_FILES \"\")\n    foreach(GEN_SCRIPT ${TENSORFLOW_GEN_SCRIPTS})\n        get_filename_component(FILE_WE ${GEN_SCRIPT} NAME_WE)\n        set(OUT_DONE_FILE ${TEST_TENSORFLOW_LITE_MODELS}/${FILE_WE}_done.txt)\n        set(OUT_FILES ${OUT_DONE_FILE} ${OUT_FILES})\n        add_custom_command(OUTPUT ${OUT_DONE_FILE}\n                COMMAND ${Python3_EXECUTABLE}\n                    ${CMAKE_CURRENT_SOURCE_DIR}/test_models/gen_wrapper.py\n                    ${GEN_SCRIPT}\n                    ${TEST_TENSORFLOW_LITE_MODELS}\n                    ${OUT_DONE_FILE}\n                JOB_POOL four_jobs\n                DEPENDS ${TENSORFLOW_ALL_SCRIPTS})\n    endforeach()\n    add_custom_target(tensorflow_lite_test_models DEPENDS ${OUT_FILES})\n\n    install(DIRECTORY ${TEST_TENSORFLOW_LITE_MODELS}\n            DESTINATION tests/${TEST_TENSORFLOW_LITE_MODELS_DIRNAME}\n            COMPONENT tests\n            EXCLUDE_FROM_ALL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Define Model setFriendlyName Method (TypeScript)\nDESCRIPTION: This code defines the `setFriendlyName` method of the `Model` interface.  It sets a friendly name for the model, which is mainly used for debugging.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\nsetFriendlyName(name): void\n```\n\n----------------------------------------\n\nTITLE: Deactivate a Python virtual environment\nDESCRIPTION: This command deactivates the currently active Python virtual environment, restoring the shell's environment variables to their previous state. This effectively returns the shell to using the system's default Python interpreter and packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/README.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\ndeactivate\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries\nDESCRIPTION: This snippet links the target `${PROJECT_NAME}` with the necessary OpenVINO libraries (core::dev, runtime, util) and Windows-specific libraries (DELAYIMP_LIB, CMAKE_JS_LIB).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${PROJECT_NAME} PRIVATE openvino::core::dev openvino::runtime openvino::util ${DELAYIMP_LIB} ${CMAKE_JS_LIB})\n```\n\n----------------------------------------\n\nTITLE: LogSoftmax Layer XML Configuration in OpenVINO\nDESCRIPTION: This XML snippet demonstrates how to configure a LogSoftmax layer in OpenVINO. It includes the layer type, data attributes (axis), input port dimensions, and output port dimensions. The `axis` attribute specifies the dimension along which the LogSoftmax function is applied. The input and output ports define the shape of the input and output tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/log-soft-max-5.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"LogSoftmax\" ... >\n    <data axis=\"1\" />\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Linker Errors (arm64)\nDESCRIPTION: This snippet showcases an example of linker errors encountered when linking arm64 libraries with x86_64 binaries during cross-compilation. The suggested solutions are to disable the usage of such libraries in CMake or completely remove them from the system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\nld: warning: ignoring file /opt/homebrew/lib/libopencv_imgproc.4.6.0.dylib, building for macOS-x86_64 but attempting to link with file built for macOS-arm64\nUndefined symbols for architecture x86_64:\n  \"cv::Mat::Mat(cv::Size_<int>, int, void*, unsigned long)\", referenced from:\n      _image_resize in opencv_c_wrapper.cpp.o\n      _image_save in opencv_c_wrapper.cpp.o\n....\nld: symbol(s) not found for architecture x86_64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\n```\n\n----------------------------------------\n\nTITLE: Building Executable Tests from Source Files in CMake\nDESCRIPTION: This snippet finds all `.cpp` files in the 'tests' directory and compiles them into executable targets. It links the targets with `tests_shared_lib` and `memory_tests_helper`, and adds them as dependencies to the `memory_tests` aggregate target. The compiled executables are then set up for installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/src/memory_tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nFILE(GLOB tests \"*.cpp\")\n\nforeach(test_source ${tests})\n    get_filename_component(test_name ${test_source} NAME_WE)\n    add_executable(${test_name} ${test_source})\n\n    target_link_libraries(${test_name} PRIVATE tests_shared_lib memory_tests_helper)\n\n    add_dependencies(memory_tests ${test_name})\n\n    install(TARGETS ${test_name}\n            RUNTIME DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Converting Layout (Transpose) C++\nDESCRIPTION: Shows how to manually transpose axes without using a layout definition via preprocessing steps in the OpenVINO API in C++. This performs the same transpose operation as using source and destination layouts, but can be less readable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n// 1) Set input tensor information. We apply preprocessing\n//    demanding input tensor to have 'u8' type and 'NHWC' layout.\nppp.input().tensor().set_element_type(ov::element::u8).set_layout(\"NHWC\");\n// 2) Adding explicit preprocessing steps\nppp.input().preprocess().convert_layout({0, 3, 1, 2});\n// 3) Set input model information. We assume that the original model has 'NCHW' layout\nppp.input().model().set_layout(\"NCHW\");\n// 4) Apply preprocessing modifying the original model\nmodel = ppp.build();\n```\n\n----------------------------------------\n\nTITLE: Conditional Functional Tests Configuration (CMake)\nDESCRIPTION: Configures functional tests for the CPU execution provider based on the `ENABLE_FUNCTIONAL_TESTS` CMake option. It defines a function `ov_cpu_func_tests` to manage compiler flags and include the 'functional' subdirectory, thus building the functional tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_FUNCTIONAL_TESTS)\n    function(ov_cpu_func_tests)\n        if(CMAKE_COMPILER_IS_GNUCXX)\n            ov_add_compiler_flags(-Wno-unused-function)\n            ov_add_compiler_flags(-Wno-parentheses)\n            ov_add_compiler_flags(-Wno-unused-local-typedefs)\n            ov_add_compiler_flags(-Wno-reorder)\n            ov_add_compiler_flags(-Wno-comment)\n            ov_add_compiler_flags(-Wno-unused-local-typedefs)\n        endif()\n        if(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 13.0)\n            ov_add_compiler_flags(-Wno-unused-but-set-variable)\n        endif()\n\n        add_subdirectory(functional)\n    endfunction()\n\n    ov_cpu_func_tests()\nendif()\n```\n\n----------------------------------------\n\nTITLE: ScatterNDUpdate Implementation in NumPy\nDESCRIPTION: This Python snippet demonstrates the equivalent functionality of the ScatterNDUpdate-15 operation using NumPy. It showcases how the `data` tensor is updated based on the `indices`, `updates`, and the `reduction` attribute, which determines the operation performed on elements.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-nd-update-15.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef scatter_nd_update_15(data, indices, updates, reduction=None):\n    func = lambda x, y: y\n    if reduction == \"sum\":\n        func = lambda x, y: x + y\n    elif reduction == \"sub\":\n        func = lambda x, y: x - y\n    elif reduction == \"prod\":\n        func = lambda x, y: x * y\n    elif reduction == \"max\":\n        func = max\n    elif reduction == \"min\":\n        func = min\n    out = np.copy(data)\n    # Order of loop iteration is undefined.\n    for ndidx in np.ndindex(indices.shape[:-1]):\n        out[tuple(indices[ndidx])] = func(tuple(out[indices[ndidx]]), updates[ndidx])\n    return out\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to Fuzz Test Target in CMake\nDESCRIPTION: This snippet links the `openvino::runtime`, `openvino::cnpy`, and `openvino::zlib` libraries to the fuzz test target. These libraries provide the necessary functionality for the fuzz tests to interact with the OpenVINO runtime and handle data processing using CNPY and ZLIB. This step is crucial for ensuring that the fuzz tests can exercise the relevant code paths in the OpenVINO project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/src/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${test_name} PRIVATE\n        openvino::runtime openvino::cnpy openvino::zlib)\n```\n\n----------------------------------------\n\nTITLE: Loading Encrypted Model from Memory Buffer C++\nDESCRIPTION: This C++ snippet demonstrates how to load an encrypted model from a memory buffer into the OpenVINO Runtime using the `ov::Core::read_model` method. The model must be decrypted before loading, and this example assumes that the decrypted model is stored in a memory buffer. It requires including the necessary OpenVINO headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-security.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n//! [part0]\n#include <openvino/runtime/core.hpp>\n\n// Assume that model data is read to `model_data` buffer and its size is `model_data_size`\n// Decrypt `model_data` to `decrypted_model_data` using crypto tools\n// Create an OpenVINO runtime Core\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model({decrypted_model_data, decrypted_model_data + model_data_size});\n//! [part0]\n```\n\n----------------------------------------\n\nTITLE: Create RemoteContext from cl_context (C)\nDESCRIPTION: This snippet demonstrates how to create an `ov::RemoteContext` from an existing OpenCL context (`cl_context`) using the GPU plugin's C API. It initializes the OpenVINO core and retrieves the extension. Requires the OpenVINO runtime and intel_gpu ocl headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_3\n\nLANGUAGE: c\nCODE:\n```\n//! [context_from_cl_context]\n#include <openvino/runtime.h>\n#include <openvino/runtime/intel_gpu/ocl/ocl.h>\n\n#if defined(OPENVINO_USE_OPENCL)\n#include <CL/cl.h>\n#endif\n\nvoid create_context_from_cl_context(cl_context context) {\n    // context from user-defined OpenCL context\n    ov_core_t* core = ov_core_create();\n    ov_remote_context remote_context = ov_core_get_extension_ocl(core, context);\n    ov_remote_context_free(remote_context);\n    ov_core_free(core);\n}\n//! [context_from_cl_context]\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Accessing Input by Name Python\nDESCRIPTION: Illustrates how to access a specific input of a model by its name using the `ov::preprocess::PrePostProcessor` in Python. This is essential when dealing with models that have multiple inputs with distinct names.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n# Get input Tensor by name and set input tensor information.\n# We apply preprocessing demanding input tensor to have 'u8' type and 'NCHW' layout.\nppp.input(\"tensor_input_name\").tensor().set_element_type(ov.Type.u8).set_layout(ov.Layout('NCHW'))\n# 2) Adding explicit preprocessing steps\nppp.input(\"tensor_input_name\").preprocess().convert_element_type(ov.Type.f32).convert_layout(ov.Layout('NHWC'))\n# 3) Set input model information.\nppp.input(\"tensor_input_name\").model().set_layout(ov.Layout('NHWC'))\n# 4) Apply preprocessing modifying the original model\nmodel = ppp.build()\n```\n\n----------------------------------------\n\nTITLE: SquaredDifference XML Layer Definition (No Broadcasting)\nDESCRIPTION: This XML snippet defines a SquaredDifference layer in OpenVINO without broadcasting. The `auto_broadcast` attribute is set to \"none\", requiring the input tensors to have matching dimensions. The example shows two input ports and one output port, all with the same dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/squared-difference-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"SquaredDifference\">\n    <data auto_broadcast=\"none\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Mean/Scale Normalization Python\nDESCRIPTION: Shows how to perform mean/scale normalization using the OpenVINO preprocessing API in Python. This involves subtracting the mean and dividing by the standard deviation for each data item. The example integrates these normalization steps into the execution graph.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n# 1) Set input tensor information. We apply preprocessing\n#    demanding input tensor to have 'f32' type and 'NCHW' layout.\nppp.input().tensor().set_element_type(ov.Type.f32).set_layout(ov.Layout('NCHW'))\n# 2) Adding explicit preprocessing steps\nppp.input().preprocess().mean(127.5).scale(127.5)\n# 3) Set input model information.\nppp.input().model().set_layout(ov.Layout('NCHW'))\n# 4) Apply preprocessing modifying the original model\nmodel = ppp.build()\n```\n\n----------------------------------------\n\nTITLE: Build OpenVINO with Selective Build (COLLECT)\nDESCRIPTION: This snippet shows how to initialize git submodules, create a build directory, and then run CMake to configure the build with the `SELECTIVE_BUILD=COLLECT` option, which enables the analysis mode for conditional compilation. The `DENABLE_PROFILING_ITT=ON` flag enables ITT profiling for code usage analysis.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit submodule init\ngit submodule update\nmkdir build && cd build\ncmake -DENABLE_PROFILING_ITT=ON -DSELECTIVE_BUILD=COLLECT ..\ncmake --build .\n```\n\n----------------------------------------\n\nTITLE: Adding ICD Loader Subdirectory CMake\nDESCRIPTION: This snippet adds the icd_loader subdirectory to the build process, excluding it from the ALL target. This means it's built only when explicitly requested.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ocl/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(icd_loader EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Topology Creation Example with Poolings and Concatenation in C++\nDESCRIPTION: This C++ code demonstrates the creation of a topology that consists of two pooling operations, a concatenation operation, and a reorder operation.  It showcases how input layouts and primitive connections are defined using `primitive_id` strings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/basic_data_structures.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nauto input0 = engine.allocate_memory({data_types::i8, format::bfyx, {1, 1, 8, 3}});\nauto input1 = engine.allocate_memory({data_types::i8, format::bfyx, {1, 1, 8, 3}});\nlayout reorder_layout(data_types::i8, format::yxfb, {7, 2, 2, 1});\ntopology topology(input_layout(\"input0\", input0->get_layout()),\n                  input_layout(\"input1\", input1->get_layout()),\n                  pooling(\"pool0 /*primitive_id of this pooling*/\", \"input0 /*primitive_id of input primitive for pool0*/\", pooling_mode::max, {1, 1, 2, 2}, {1, 1, 1, 1}),\n                  pooling(\"pool1\", \"input1\", pooling_mode::max, {1, 1, 2, 2}, {1, 1, 1, 1}),\n                  concatenation(\"concat\",\n                                {\"pool0\", \"pool1\"},\n                                concatenation::concatenation_axis::along_f,\n                                data_types::i8,\n                                \"\",\n                                padding{{0, 0, 0, 0}, 0}),\n                  reorder(\"reorder\", \"concat\", reorder_layout));\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Target to Run Code Generation\nDESCRIPTION: Creates a custom target named `run_ocl_codegen` that depends on the generated kernel source file in the include directory. This target is added to the `ALL` target, meaning it will be executed every time the project is built.  It ensures that the OpenCL kernel sources are generated before other parts of the project are built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/ocl_v2/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(run_ocl_codegen ALL DEPENDS \"${CODEGEN_INCDIR}/${KERNEL_SOURCES}\")\n```\n\n----------------------------------------\n\nTITLE: Compress Model to INT8 with NNCF (OpenVINO IR)\nDESCRIPTION: Performs asymmetrical 8-bit weight quantization of a model in the OpenVINO IR format using NNCF. This snippet represents an external reference to a code example demonstrating the workflow.  The precise code is not provided here.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/weight-compression.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# The following code is located in:\n# docs/optimization_guide/nncf/code/weight_compression_openvino.py\n# with fragment [compression_8bit]\n# Note: Actual code content is not provided here.  Refer to the source file.\n```\n\n----------------------------------------\n\nTITLE: Multinomial Layer Configuration (No Replacement)\nDESCRIPTION: This XML snippet demonstrates the configuration of a Multinomial layer in OpenVINO when sampling without replacement (`with_replacement=\"false\"`).  It showcases the use of attributes like `convert_type`, `log_probs`, `global_seed`, and `op_seed`.  The output contains unique samples from the multinomial distribution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/multinomial-13.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... name=\"Multinomial\" type=\"Multinomial\">\n    <data convert_type=\"f32\", with_replacement=\"false\", log_probs=\"false\", global_seed=\"234\", op_seed=\"148\"/>\n    <input>\n        <port id=\"0\" precision=\"FP32\">  <!-- probs value: [[0.1, 0.5, 0.4]] -->\n            <dim>2</dim> <!-- batch size of 2 -->\n            <dim>3</dim>\n        </port>\n        <port id=\"1\" precision=\"I32\"/> <!-- num_samples value: 2 -->\n    </input>\n    <output>\n        <port id=\"3\" precision=\"I32\" names=\"Multinomial:0\">\n            <dim>2</dim> <!-- batch size of 2 -->\n            <dim>2</dim> <!-- 2 unique samples of classes -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Backend Target with Generated Code\nDESCRIPTION: This CMake snippet uses the `ov_gpu_add_backend_target` function to create a target for the CM kernels.  It specifies the target name, include directory (where the generated code resides), and dependencies (the `run_cm_codegen` target). The function configures the build system to compile the generated code into a library. `ADD_CLANG_FORMAT` indicates to apply clang-format to new generated source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/cm/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_gpu_add_backend_target(\n    NAME ${TARGET_NAME}\n    INCLUDES $<BUILD_INTERFACE:${CODEGEN_INCDIR}>\n    BYPASS\n        ADDITIONAL_SOURCE_DIRS ${CODEGEN_INCDIR}\n        EXCLUDED_SOURCE_PATHS \"${CMAKE_CURRENT_SOURCE_DIR}/include\"\n        DEPENDENCIES run_cm_codegen\n        ADD_CLANG_FORMAT\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for Tests in CMake\nDESCRIPTION: This snippet conditionally adds the `tests` subdirectory to the build process if the `ENABLE_TESTS` variable is set. This integrates the testing infrastructure with the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Define ov_dimension struct in C\nDESCRIPTION: This struct represents the minimum and maximum values for a dimension. It is used to define the range of possible sizes for a tensor dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_4\n\nLANGUAGE: C\nCODE:\n```\ntypedef struct ov_dimension {\n\n    int64_t min;\n\n    int64_t max;\n\n} ov_dimension_t;\n```\n\n----------------------------------------\n\nTITLE: Adding Static Library in CMake\nDESCRIPTION: This command creates a static library named `${TARGET_NAME}` from the source and header files found in the previous steps. `EXCLUDE_FROM_ALL` ensures it isn't built by default, and the source and header files are included. This is the core command to create a library from specified sources.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC EXCLUDE_FROM_ALL ${LIBRARY_SRC} ${LIBRARY_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example with Same Upper Padding (Shell)\nDESCRIPTION: This example illustrates the MaxPool operation with a 4D input, employing a 2D kernel and 'same_upper' padding. The input tensor, strides, kernel size, rounding type, auto_pad setting, and the output tensor resulting from the MaxPool operation are provided.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-1.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[-1, 2, 3],\n                 [4, 5, -6],\n                 [-7, 8, 9]],\n                [[2, -1, 5],\n                 [6, -7, 1],\n                 [8, 2, -3]]]]\nstrides = [1, 1]\nkernel = [2, 2]\nrounding_type = \"floor\"\nauto_pad = \"same_upper\"\noutput = [[[[5, 5, 3],\n                  [8, 9, 9]\n                  [8, 9, 9]],\n                 [[6, 5, 5],\n                  [8, 2, 1],\n                  [8, 2, -3]]]]\n```\n\n----------------------------------------\n\nTITLE: Protopipe Execute Filter Command (Regex)\nDESCRIPTION: This command shows how to execute a filtered set of scenarios from a configuration file using regular expression matching. The `-exec_filter` argument targets scenarios with numbers 0 or 1 in their default names.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n./protopipe -cfg scenarios.yaml -niter 100 -exec_filter=\".*[0-1]\"\n```\n\n----------------------------------------\n\nTITLE: Constraints File Example\nDESCRIPTION: This constraint file example shows how to specify package versions. These versions are enforced when the constraint file is used with `-c` flag in requirements files. The file defines the versions for coverage, astroid, pylint, pyenchant and test-generator.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/requirements_management.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n# main/constraints.txt\n\ncoverage>=4.4.2\nastroid>=2.9.0\npylint>=2.7.0\npyenchant>=3.0.0\ntest-generator==0.1.1\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name and Root Directory (CMake)\nDESCRIPTION: This snippet sets the target name for the library and determines the root directory based on the current CMake source directory. This root directory is later used for finding source and header files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_tensorflow_common\")\nget_filename_component(root_dir \"${CMAKE_CURRENT_SOURCE_DIR}\" DIRECTORY)\n```\n\n----------------------------------------\n\nTITLE: Installing Target\nDESCRIPTION: This snippet installs the target `${PROJECT_NAME}` to the specified runtime directory `${OV_CPACK_RUNTIMEDIR}`. It specifies that both the library and runtime components should be installed, and excludes them from the NPM package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${PROJECT_NAME}\n    LIBRARY DESTINATION ${OV_CPACK_RUNTIMEDIR} COMPONENT ${PROJECT_NAME} ${OV_CPACK_COMP_NPM_EXCLUDE_ALL}\n    RUNTIME DESTINATION ${OV_CPACK_RUNTIMEDIR} COMPONENT ${PROJECT_NAME} ${OV_CPACK_COMP_NPM_EXCLUDE_ALL}\n)\n```\n\n----------------------------------------\n\nTITLE: Project Definition and C++ Standard\nDESCRIPTION: This snippet sets the minimum required CMake version to 3.14, defines the project name as `ov_node_addon`, and sets the C++ standard to 17.  It also adds a preprocessor definition for the NAPI version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.14)\n\nproject(ov_node_addon)\n\nset(CMAKE_CXX_STANDARD 17)\nadd_definitions(-DNAPI_VERSION=8)\n```\n\n----------------------------------------\n\nTITLE: PrePostProcessor input method TypeScript\nDESCRIPTION: Defines the `input` method for the PrePostProcessor interface. This method configures the input parameters. It accepts an optional index or tensor name (`idxOrTensorName`) as input, which can be either a string or a number, and returns an InputInfo object for further configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PrePostProcessor.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ninput(idxOrTensorName?): InputInfo\n```\n\nLANGUAGE: typescript\nCODE:\n```\nidxOrTensorName: string|number\n```\n\n----------------------------------------\n\nTITLE: PriorBoxClustered Layer Configuration XML Example\nDESCRIPTION: This XML snippet demonstrates the configuration of a PriorBoxClustered layer within an OpenVINO model. It shows how to specify attributes like clip, height, offset, step, and variance, along with the input and output port dimensions. The inputs are the output size and image size, while the output is a 2D tensor containing the box coordinates.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/prior-box-clustered-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"PriorBoxClustered\" ... >\n    <data clip=\"false\" height=\"44.0,10.0,30.0,19.0,94.0,32.0,61.0,53.0,17.0\" offset=\"0.5\" step=\"16.0\" variance=\"0.1,0.1,0.2,0.2\" width=\"86.0,13.0,57.0,39.0,68.0,34.0,142.0,50.0,23.0\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>        <!-- [10, 19] -->\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>        <!-- [180, 320] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>2</dim>\n            <dim>6840</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing Multiple OpenVINO Components\nDESCRIPTION: This command installs multiple OpenVINO components (CPU, ARM CPU, NPU, and GPU plugins) from the `conda-forge` channel in a single command. This can be more efficient than installing each component individually.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conda.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nconda install conda-forge::libopenvino-intel-cpu-plugin conda-forge::libopenvino-arm-cpu-plugin conda-forge::libopenvino-intel-npu-plugin conda-forge::libopenvino-intel-gpu-plugin\n```\n\n----------------------------------------\n\nTITLE: Operation Precision Restrictions in OpenVINO C++\nDESCRIPTION: This C++ code snippet demonstrates setting operation precision restrictions for Low Precision Transformations (LPT) in OpenVINO, specifying the allowed precisions for input ports.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/advanced-guides/low-precision-transformations.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nauto& transformation = *transformation;\n\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories (CMake)\nDESCRIPTION: Sets the include directories for the target. The directories are specified using build interface paths.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC\n    $<BUILD_INTERFACE:${REF_IMPL_INCLUDE_DIR}>\n    $<BUILD_INTERFACE:${OV_CORE_DEV_API_PATH}>\n    $<BUILD_INTERFACE:${OV_CORE_INCLUDE_PATH}>)\n```\n\n----------------------------------------\n\nTITLE: Creating Interface Library and Setting Include Directories in CMake\nDESCRIPTION: This code creates an interface library representing the OpenVINO runtime, links it to the `openvino::runtime` target, and sets the interface include directory to the public headers directory. This ensures that dependent projects can access the necessary headers for using the OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} INTERFACE)\ntarget_link_libraries(${TARGET_NAME} INTERFACE openvino::runtime)\ntarget_include_directories(${TARGET_NAME} INTERFACE $<BUILD_INTERFACE:${PUBLIC_HEADERS_DIR}>)\n```\n\n----------------------------------------\n\nTITLE: BitwiseNot output for boolean tensor in Python\nDESCRIPTION: Demonstrates the BitwiseNot operation on a boolean tensor, showing how it performs logical negation equivalent to the LogicalNot operator. The input is a boolean array, and the output is the negation of each element.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-not-13.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# For given boolean input:\ninput = [True, False]\n# Perform logical negation operation same as in LogicalNot operator:\noutput = [False, True]\n```\n\n----------------------------------------\n\nTITLE: Setting Build Options (CMake)\nDESCRIPTION: This snippet sets various build options using CMake's `set` command. These options control whether specific components like the C/C++ API, Python API, GenAI API, notebooks, and OVMS are included in the build. The `CACHE BOOL` argument allows these options to be configured via the CMake GUI or command line.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(ENABLE_CPP_API OFF CACHE BOOL \"Build with C/C++ API.\")\nset(ENABLE_PYTHON_API OFF CACHE BOOL \"Build with Python API.\")\nset(ENABLE_GENAI_API OFF CACHE BOOL \"Build with GenAI API.\")\nset(ENABLE_NOTEBOOKS OFF CACHE BOOL \"Build with openvino notebooks.\")\nset(ENABLE_OVMS OFF CACHE BOOL \"Build with ovms.\")\nset(OVMS_DOCS_DIR \"\" CACHE PATH \"Path to model server documentation dir.\")\n```\n\n----------------------------------------\n\nTITLE: Copy Blobs Post-Build\nDESCRIPTION: Copies `.blob` files to the runtime output directory after the build. This ensures that the test executable can access the necessary blob files during execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/functional/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE BLOBS \"*.blob\")\nfile(MAKE_DIRECTORY ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/intel_npu_blobs)\nadd_custom_command(TARGET ${TARGET_NAME} POST_BUILD\n                  COMMAND ${CMAKE_COMMAND} -E copy ${BLOBS}\n                  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/intel_npu_blobs)\n```\n\n----------------------------------------\n\nTITLE: Adding RISC-V repositories\nDESCRIPTION: This snippet adds RISC-V architecture-specific repositories to the apt sources list. These repositories are used to download RISC-V-specific packages for cross-compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\necho deb [arch=riscv64] http://ports.ubuntu.com/ubuntu-ports/ jammy main >> riscv64-sources.list\necho deb [arch=riscv64] http://ports.ubuntu.com/ubuntu-ports/ jammy universe >> riscv64-sources.list\necho deb [arch=riscv64] http://ports.ubuntu.com/ubuntu-ports/ jammy-updates main >> riscv64-sources.list\necho deb [arch=riscv64] http://ports.ubuntu.com/ubuntu-ports/ jammy-security main >> riscv64-sources.list\nmv riscv64-sources.list /etc/apt/sources.list.d/\ndpkg --add-architecture riscv64\napt-get update -o Dir::Etc::sourcelist=/etc/apt/sources.list.d/riscv64-sources.list\n```\n\n----------------------------------------\n\nTITLE: ReduceL1 along one axis in OpenVINO XML\nDESCRIPTION: Example of ReduceL1 operation in OpenVINO XML where keep_dims is false and reduction occurs along axis 1.  The input tensor shape is 6x12x10x24 and the output tensor shape becomes 6x10x24.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-l1-4.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceL1\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [1] that means independent reduction in each channel and spatial dimensions -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Component Definition Using 'all' Notation (YAML)\nDESCRIPTION: This YAML snippet illustrates how to use the 'all' notation in components.yml. Using 'all' under `build` means all other components will be built but not tested if `your_component_1` is changed. Using 'all' under `revalidate` triggers a full pipeline run (build and test for all components) when `your_component_2` is changed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nyour_component_1:\n  build: 'all'\n\nyour_component_2:\n  revalidate: 'all'\n```\n\n----------------------------------------\n\nTITLE: ShuffleChannels Operation in XML\nDESCRIPTION: Defines a ShuffleChannels layer in OpenVINO's XML format. It specifies the 'group' and 'axis' attributes, along with the input and output port dimensions. This example demonstrates how to configure the ShuffleChannels operation within an OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/shuffle-channels-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ShuffleChannels\" ...>\n        <data group=\"3\" axis=\"1\"/>\n        <input>\n            <port id=\"0\">\n                <dim>5</dim>\n                <dim>12</dim>\n                <dim>200</dim>\n                <dim>400</dim>\n            </port>\n        </input>\n        <output>\n            <port id=\"1\">\n                <dim>5</dim>\n                <dim>12</dim>\n                <dim>200</dim>\n                <dim>400</dim>\n            </port>\n        </output>\n    </layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Static Library in CMake\nDESCRIPTION: This CMake snippet adds a static library target named `openvino_npu_driver_compiler_adapter`. It specifies the sources to be used for building the library using the `SOURCES` variable which was defined in the previous step.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/compiler_adapter/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Custom Operation Definition in C++ Source\nDESCRIPTION: This code snippet provides the C++ implementation of the `custom_add` operation, which takes two inputs, adds them together, and multiplies the result by a scalar `alpha`. It utilizes OpenVINO operations like `Add`, `Multiply`, `Constant`, and `Convert`.  The code also performs input validation to ensure the correct number of inputs and a valid range for the `alpha` attribute. It returns an `ov::OutputVector` containing the result of the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/how_to_add_op.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"op/org.openvinotoolkit/custom_add.hpp\"\n\n#include \"exceptions.hpp\"\n#include \"openvino/op/add.hpp\"\n#include \"openvino/op/constant.hpp\"\n#include \"openvino/op/convert.hpp\"\n#include \"openvino/op/multiply.hpp\"\n#include \"utils/common.hpp\"\n\nnamespace ov {\nnamespace frontend {\nnamespace onnx {\nnamespace op {\nnamespace set_1 {\n\nov::OutputVector custom_add(const ov::frontend::onnx::Node& node) {\n    const auto& inputs = node.get_ov_inputs();\n    CHECK_VALID_NODE(node,\n                     inputs.size() == 2,\n                     \"CustomAdd should have exactly 2 inputs, got: \",\n                     inputs.size());\n    const auto in1 = inputs[0];\n    const auto in2 = inputs[1];\n    const auto alpha = node.get_attribute_value<float>(\"alpha\", 1);\n\n    CHECK_VALID_NODE(node,\n                     alpha >= 1 && alpha < 100,\n                     \"CustomAdd accepts alpha in a range [1, 100), got: \",\n                     alpha);\n\n    const auto alpha_node =\n        std::make_shared<v0::Convert>(v0::Constant::create(ov::element::f32, {}, {alpha}), in1.get_element_type());\n\n    const auto add = std::make_shared<v1::Add>(in1, in2);\n    return {std::make_shared<v1::Multiply>(add, alpha_node)};\n}\n\n}  // namespace set_1\n}  // namespace op\n}  // namespace onnx\n}  // namespace frontend\n}  // namespace ov\n```\n\n----------------------------------------\n\nTITLE: IDFT Layer XML Configuration (3D input, no signal_size)\nDESCRIPTION: This XML configuration shows an IDFT layer with a 3D input tensor and omits the optional signal_size input. The snippet specifies the dimensions of the input and output ports for performing the inverse discrete Fourier transform operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/idft-7.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>\t<!-- [0, 1] -->\n        </port>\n    <output>\n        <port id=\"2\">\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Build Python\nDESCRIPTION: Builds Python from the configured source code using the `make` command. The `-j 8` option specifies the number of parallel jobs to use during the build process, which can significantly reduce the build time. Adjust the number (8 in this example) based on your system's CPU core count.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/python_version_upgrade.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake -j 8\n```\n\n----------------------------------------\n\nTITLE: Gather Operation Example\nDESCRIPTION: Illustrates the Gather operation using shell code. This example provides a general representation of how the output is derived from the inputs and axis.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-1.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\noutput[p_0, p_1, ..., p_{axis-1}, i, ..., j, ...] =\n  input1[p_0, p_1, ..., p_{axis-1}, input2[i, ..., j], ...]\n```\n\n----------------------------------------\n\nTITLE: Defining OPENVINO_STATIC_LIBRARY Definition for Static Builds\nDESCRIPTION: This snippet defines the `OPENVINO_STATIC_LIBRARY` compile definition when `BUILD_SHARED_LIBS` is not enabled. This definition is used to conditionally compile code that is specific to static libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(openvino_core_obj PUBLIC OPENVINO_STATIC_LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Link Libraries CMake\nDESCRIPTION: Links the target against the `openvino::runtime` and `openvino::util` libraries. The libraries are linked privately meaning that they are required for the library to build but they are not required for any target that links against this library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/onnx_common/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino::runtime openvino::util)\n```\n\n----------------------------------------\n\nTITLE: Defining Target Name and Postfixes with CMake\nDESCRIPTION: This snippet defines the target name for the library as `sea_itt_lib` and sets empty postfixes for debug and release builds. It also disables interprocedural optimization for release builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/itt_collector/sea_itt_lib/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME sea_itt_lib)\n\nset(CMAKE_DEBUG_POSTFIX \"\")\nset(CMAKE_RELEASE_POSTFIX \"\")\nset(CMAKE_INTERPROCEDURAL_OPTIMIZATION_RELEASE OFF)\n```\n\n----------------------------------------\n\nTITLE: RDFT Layer XML Configuration (With signal_size, 4D Input, unsorted axes)\nDESCRIPTION: Configures an RDFT layer in XML with a signal_size input and unsorted axes, processing a 4D input tensor.  The signal_size contains a -1, indicating that the full size of the corresponding axis should be used. The example demonstrates flexibility in handling different dimension arrangements and signal sizes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/rdft-9.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"RDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>580</dim>\n            <dim>320</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim> <!-- axes input contains  [3, 1, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim> <!-- signal_size input contains [170, -1, 1024] -->\n        </port>\n    <output>\n        <port id=\"3\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>513</dim>\n            <dim>170</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Uninstalling OpenVINO - Deleting Files\nDESCRIPTION: This command removes the extracted OpenVINO folder and the archive file, completing the uninstallation process. Replace `<extracted_folder>` and `<path_to_archive>` with the actual paths.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-macos.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nrm -r <extracted_folder> && rm <path_to_archive>\n```\n\n----------------------------------------\n\nTITLE: Enabling devtoolset-8 and Checking GCC Version\nDESCRIPTION: This script enables the `devtoolset-8` environment and checks the currently active GCC version. This is necessary after installing GCC via Software Collections (SCL) to ensure that the correct version is being used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/release-notes-openvino/system-requirements.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsource /opt/rh/devtoolset-8/enable\ngcc -v\n```\n\n----------------------------------------\n\nTITLE: Initializing CMake project for OpenVINOBenchmarkTool\nDESCRIPTION: This snippet initializes the CMake project with a minimum required version and sets the project name. It also defines the OpenVINO source directory if it's not already defined, ensuring the project can locate necessary files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/benchmark_tool/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required (VERSION 3.13)\n\nproject(OpenVINOBenchmarkTool)\n\nif(NOT DEFINED OpenVINO_SOURCE_DIR)\n    get_filename_component(OpenVINO_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../..\" REALPATH)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name and Libraries in CMake\nDESCRIPTION: This snippet sets the target name to `op_conformance_utils` and defines a list of libraries that the target depends on: `openvino::runtime`, `openvino::pugixml`, and `openvino::util`. This list will be used later for linking the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/op_conformance_utils/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME op_conformance_utils)\n\nlist(APPEND LIBRARIES\n        openvino::runtime\n        openvino::pugixml\n        openvino::util\n)\n```\n\n----------------------------------------\n\nTITLE: LogicalOr Layer Configuration - Numpy Broadcast XML\nDESCRIPTION: This XML snippet illustrates the configuration of a LogicalOr layer in OpenVINO with numpy broadcasting enabled.  The input tensors have different dimensions, and the output dimensions reflect the result of numpy broadcasting rules applied to the inputs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/logical/logical-or-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"LogicalOr\">\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Slice: Slicing Backwards with Step -1 and Stop 0 in OpenVINO XML\nDESCRIPTION: This example demonstrates slicing a tensor backwards from the end up to index 0 (exclusive). This uses a negative step and a stop index of 0.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-8.rst#_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Slice\" ...>\n       <input>\n           <port id=\"0\">       <!-- data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -->\n             <dim>10</dim>\n           </port>\n           <port id=\"1\">       <!-- start: [9] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"2\">       <!-- stop: [0] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"3\">       <!-- step: [-1] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"4\">       <!-- axes: [0] -->\n             <dim>1</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">       <!-- output: [9, 8, 7, 6, 5, 4, 3, 2, 1] -->\n               <dim>9</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Power Layer Configuration (Numpy Broadcasting) - XML\nDESCRIPTION: This XML snippet configures a Power layer in OpenVINO using numpy broadcasting rules. The input tensors have different dimensions (8x1x6x1 and 7x1x5), but they are broadcastable according to numpy rules. The output tensor has dimensions 8x7x6x5, which is the result of the broadcasting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/power-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Power\">\n    <data auto_broadcast=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Using WrapType with a predicate\nDESCRIPTION: This C++ snippet demonstrates how to use WrapType with a predicate (lambda function) to add an additional check. The predicate validates that the number of consumers for the matched node is exactly 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nshared_ptr<ov::Node> wrap_type_predicate() {\n    auto pattern_sig = make_shared<WrapType<opset13::Relu>>([](const Output<Node>& output) {\n        return output.get_target_inputs().size() == 1;\n    });\n\n    return pattern_sig;\n}\n```\n\n----------------------------------------\n\nTITLE: Define ov_partial_shape struct in C\nDESCRIPTION: This struct represents a partial shape of a tensor, allowing for dynamic dimensions. It contains the rank of the tensor and an array of `ov_dimension_t` structs defining the range of each dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_6\n\nLANGUAGE: C\nCODE:\n```\ntypedef struct ov_partial_shape {\n\n    ov_rank_t rank;\n\n    ov_dimension_t* dims;\n\n} ov_partial_shape_t;\n```\n\n----------------------------------------\n\nTITLE: Running Python ONNX Frontend Tests (Installation Layout)\nDESCRIPTION: This command runs the Python ONNX Frontend tests using pytest in the installation layout. It executes all test files starting with 'test_frontend_onnx' within the specified directory. Replace <OV_INSTALL_DIR> with the OpenVINO installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npytest <OV_INSTALL_DIR>/tests/pyopenvino/tests/test_frontend/test_frontend_onnx*\n```\n\n----------------------------------------\n\nTITLE: Configuring Faster Build for OpenVINO Core Object Library\nDESCRIPTION: This snippet configures a faster build process for the `openvino_core_obj` target. It enables unity builds (`UNITY`) and precompiled headers (`PCH`) to reduce compilation time. The `ov_build_target_faster` function is a custom function that encapsulates these optimizations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(openvino_core_obj\n    UNITY\n    PCH PRIVATE \"src/precomp.hpp\")\n```\n\n----------------------------------------\n\nTITLE: Adding a Subdirectory in CMake\nDESCRIPTION: This CMake command adds a subdirectory to the current build. It instructs CMake to process the CMakeLists.txt file found within the specified subdirectory (`opencv_c_wrapper`) and include its defined targets in the overall build process.  This is essential for modularizing the build and incorporating external components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/c/common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(opencv_c_wrapper)\n```\n\n----------------------------------------\n\nTITLE: Gather-7 Example with negative batch_dims in shell\nDESCRIPTION: Demonstrates Gather-7 with a negative batch_dims value (-1) and axis to 1.  Shows how negative batch_dims is normalized and applied.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-7.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = -1  <-- normalized value will be indices.rank + batch_dims = 2 - 1 = 1\naxis = 1\n\nindices = [[0, 0, 4], <-- this is applied to the first batch\n           [4, 0, 0]]  <-- this is applied to the second batch\nindices_shape = (2, 3)\n\ndata    = [[1, 2, 3, 4, 5],  <-- the first batch\n           [6, 7, 8, 9, 10]]  <-- the second batch\ndata_shape = (2, 5)\n\noutput  = [[ 1, 1, 5],\n           [10, 6, 6]]\noutput_shape = (2, 3)\n```\n\n----------------------------------------\n\nTITLE: Installing missing Yocto tools\nDESCRIPTION: This command installs the chrpath, diffstat, and zstd tools, which are required when adding the \"meta-intel\" layer in a Yocto image build.  These tools are necessary for specific build tasks within the Yocto environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/troubleshooting-install-config.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt-get install chrpath diffstat zstd\n```\n\n----------------------------------------\n\nTITLE: ReduceMean Example with keep_dims=true\nDESCRIPTION: Demonstrates the ReduceMean operation in XML format, where `keep_dims` is set to `true`. This configuration retains the reduced axes in the output tensor with a dimension of 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-mean-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceMean\" ...>\n    <data keep_dims=\"true\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: EmbeddingSegmentsSum Example Configuration in CPP\nDESCRIPTION: This code snippet provides an example configuration for the EmbeddingSegmentsSum operation within an OpenVINO model. It defines the input and output ports, including their dimensions and sample values, demonstrating how the operation is structured within the model's XML representation. The example showcases the usage of emb_table, indices, segment_ids, num_segments, default_index, and per_sample_weights inputs to compute the output embeddings for each bag.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sparse/embedding-segments-sum-3.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"EmbeddingSegmentsSum\" ... >\n       <input>\n           <port id=\"0\">     <!-- emb_table value is: [[-0.2, -0.6], [-0.1, -0.4], [-1.9, -1.8], [-1.,  1.5], [ 0.8, -0.7]] -->\n               <dim>5</dim>\n               <dim>2</dim>\n           </port>\n           <port id=\"1\">     <!-- indices value is: [0, 2, 3, 4] -->\n               <dim>4</dim>\n           </port>\n           <port id=\"2\"/>    <!-- segment_ids value is: [0, 0, 2, 2] - second segment is empty -->\n               <dim>4</dim>\n           </port>\n           <port id=\"3\"/>    <!-- num_segments value is: 3 -->\n           <port id=\"4\"/>    <!-- default_index value is: 0 -->\n           <port id=\"5\"/>    <!-- per_sample_weigths value is: [0.5, 0.5, 0.5, 0.5] -->\n               <dim>4</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"6\">     <!-- output value is: [[-1.05, -1.2], [-0.2, -0.6], [-0.1, 0.4]] -->\n               <dim>3</dim>\n               <dim>2</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Google Test macro TEST_P usage\nDESCRIPTION: This code snippet demonstrates how to create a value-parameterized test using the TEST_P macro in GoogleTest. It defines a test named `TestName` that uses the test fixture class `TestFixtureName`. The test suite is then instantiated using `INSTANTIATE_TEST_SUITE_P` with a specified parameter generator.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_unit_test.md#_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\nTEST_P(TestFixtureName, TestName) {\n  ... statements ...\n}\n\nINSTANTIATE_TEST_SUITE_P(InstantiationName,TestSuiteName,param_generator)\n```\n\n----------------------------------------\n\nTITLE: CMake: Executable Creation\nDESCRIPTION: This snippet creates an executable named 'StressUnitTests' using the provided header and source files. It also links the 'StressTestsCommon' library to the executable, providing necessary dependencies for stress testing.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/unittests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset (TARGET_NAME \"StressUnitTests\")\n\nfile (GLOB_RECURSE SRC *.cpp)\nfile (GLOB_RECURSE HDR *.h)\n\n# Create library file from sources.\nadd_executable(${TARGET_NAME} ${HDR} ${SRC})\n\ntarget_link_libraries(${TARGET_NAME} PRIVATE StressTestsCommon)\n\ninstall(TARGETS ${TARGET_NAME}\n        RUNTIME DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Setting Default Build Type\nDESCRIPTION: This snippet determines and sets the build type for the project. If using a Ninja Multi-Config generator, it defaults to \"Release\". Otherwise, it sets CMAKE_BUILD_TYPE to \"Release\" and allows the user to choose from \"Release\", \"Debug\", \"RelWithDebInfo\", and \"MinSizeRel\".\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nget_property(OV_GENERATOR_MULTI_CONFIG GLOBAL PROPERTY GENERATOR_IS_MULTI_CONFIG)\nif(CMAKE_GENERATOR STREQUAL \"Ninja Multi-Config\")\n    # Ninja-Multi specific, see:\n    # https://cmake.org/cmake/help/latest/variable/CMAKE_DEFAULT_BUILD_TYPE.html\n    set(CMAKE_DEFAULT_BUILD_TYPE \"Release\" CACHE STRING \"CMake default build type\")\nelif(NOT OV_GENERATOR_MULTI_CONFIG)\n    set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"CMake build type\")\n    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Release;Debug;RelWithDebInfo;MinSizeRel\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: ParamsKey Support Check in C++\nDESCRIPTION: This code snippet shows how the `Support()` method is used to check if a kernel implementation (`implKey`) supports the parameters specified by `paramsKey`. It performs a bitwise comparison to ensure that all required features are supported by the implementation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_kernels.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nconst ParamsKey implKey = some_implementation->GetSupportedKey();\nif (!implKey.Support(paramsKey))\n    // Do something\n\n// Support() method do something like follows for each internal bit mask:\nif (!((implKey.mask & paramsKey.mask) == paramsKey.mask))\n    return false;\n```\n\n----------------------------------------\n\nTITLE: NotEqual Layer Definition Example 2 with Broadcasting in C++\nDESCRIPTION: This C++ code snippet shows an example of a NotEqual layer definition in OpenVINO demonstrating broadcasting. The input tensors have different shapes, and the output tensor reflects the broadcasted shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/notequal-1.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"NotEqual\">\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ONNX Test Model Dependencies\nDESCRIPTION: Specifies the Python package dependencies for generating and testing ONNX models. It includes numpy for numerical computation, protobuf for handling ONNX model serialization, docopt for command-line argument parsing, the onnx package itself, and pytest for running tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-c ../../../bindings/python/constraints.txt\nnumpy\nprotobuf\ndocopt\nonnx\npytest\n```\n\n----------------------------------------\n\nTITLE: Importing Custom Module into Helpers\nDESCRIPTION: This snippet shows how to import the `top1_index` from the `custom_module` into the `helpers` module to make it directly accessible from the `openvino.helpers` namespace.  This simplifies the usage of the custom helper function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.helpers.custom_module import top1_index\n```\n\n----------------------------------------\n\nTITLE: List Installed OpenVINO Packages\nDESCRIPTION: This command lists all installed OpenVINO packages on the system using the YUM package manager. It uses a wildcard to find packages starting with 'openvino'. This command is used to confirm that the OpenVINO runtime has been installed and to see the installed version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nyum list installed 'openvino*'\n```\n\n----------------------------------------\n\nTITLE: Exporting Target CMake\nDESCRIPTION: This snippet exports the target for use in other CMake projects. It sets the export name to `runtime::c` and exports the target to the `OpenVINOTargets.cmake` file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/src/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME runtime::c)\n\nov_add_library_version(${TARGET_NAME})\n\nexport(TARGETS ${TARGET_NAME} NAMESPACE openvino::\n       APPEND FILE \"${CMAKE_BINARY_DIR}/OpenVINOTargets.cmake\")\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for Unit Tests (CMake)\nDESCRIPTION: Adds the 'unit' subdirectory to the build process, allowing CMake to process the CMakeLists.txt file located within that directory. This step is essential for building the unit tests associated with the CPU execution provider.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(unit)\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINODeveloperScripts Package\nDESCRIPTION: This snippet uses the `find_package` command to locate the required OpenVINODeveloperScripts package.  It specifies the path to the package and uses the `NO_CMAKE_FIND_ROOT_PATH` and `NO_DEFAULT_PATH` options to ensure the correct package is found within the OpenVINO source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/benchmark_tool/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT OpenVINODeveloperScripts_FOUND)\n    find_package(OpenVINODeveloperScripts REQUIRED\n                 PATHS \"${OpenVINO_SOURCE_DIR}/cmake/developer_package\"\n                 NO_CMAKE_FIND_ROOT_PATH\n                 NO_DEFAULT_PATH)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Mean/Scale RGB Normalization C++\nDESCRIPTION: Demonstrates how to perform mean/scale normalization separately for R, G, B values using the OpenVINO preprocessing API in C++. This is typically done in Computer Vision and requires defining the layout with the 'C' dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n// 1) Set input tensor information. We apply preprocessing\n//    demanding input tensor to have 'f32' type and 'NCHW' layout.\nppp.input().tensor().set_element_type(ov::element::f32).set_layout(\"NCHW\");\n// 2) Adding explicit preprocessing steps\nppp.input().preprocess().mean({127.5, 127.5, 127.5}).scale({127.5, 127.5, 127.5});\n// 3) Set input model information.\nppp.input().model().set_layout(\"NCHW\");\n// 4) Apply preprocessing modifying the original model\nmodel = ppp.build();\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Color Conversion NV12/I420 C++\nDESCRIPTION: Demonstrates how to handle YUV-family source color formats (NV12 and I420) with separate planes using the OpenVINO preprocessing API in C++. It splits the original input into 2 or 3 inputs for NV12 and I420 formats respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n// 1) Set input tensor information. We apply preprocessing\n//    demanding input tensor to have 'nv12_two_planes' color format\nppp.input().tensor().set_element_type(ov::element::u8).set_color_format(ov::preprocess::ColorFormat::NV12_TWO_PLANES);\n// 2) Adding explicit preprocessing steps. Model expects 'rgb' format\nppp.input().preprocess().convert_color(ov::preprocess::ColorFormat::RGB);\n// 3) Apply preprocessing modifying the original model\nmodel = ppp.build();\n```\n\n----------------------------------------\n\nTITLE: Preloading ASan Library in CMake\nDESCRIPTION: This conditional block checks for the `OV_LIBASAN_FILEPATH` variable, which specifies the path to the ASan (AddressSanitizer) library. If the variable is defined, a message is printed indicating that the ASan library will be preloaded when running the fuzzer executables. This is important for detecting memory-related errors during fuzzing.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/src/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(OV_LIBASAN_FILEPATH)\n    message(STATUS \"For tests pre-load asan library used for OpenVINO build:\")\n    message(STATUS \"    LD_PRELOAD=${OV_LIBASAN_FILEPATH} [fuzzer-executable]\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Azure Blob Storage: Passing Credentials to Container\nDESCRIPTION: This YAML snippet details passing Azure Blob Storage credentials to a Docker container for use with `sccache`. The `SCCACHE_AZURE_BLOB_CONTAINER` and `SCCACHE_AZURE_CONNECTION_STRING` environment variables are passed to the container using the `options` key. This allows `sccache` running inside the container to authenticate with Azure Blob Storage.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/caches.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nBuild:\n  ...\n  runs-on: aks-linux-16-cores-32gb\n  container:\n    image: openvinogithubactions.azurecr.io/dockerhub/ubuntu:20.04\n    volumes:\n      - /mount:/mount\n    options: -e SCCACHE_AZURE_BLOB_CONTAINER -e SCCACHE_AZURE_CONNECTION_STRING\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags for GNU C++ (CMake)\nDESCRIPTION: Conditionally adds compiler flags specifically for the GNU C++ compiler to suppress warnings. These flags help reduce noise during the build process by disabling specific warning types related to unused functions, parentheses, reordering, comments and unused typedefs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX)\n    ov_add_compiler_flags(-Wno-unused-function)\n    ov_add_compiler_flags(-Wno-parentheses)\n    ov_add_compiler_flags(-Wno-unused-local-typedefs)\n    ov_add_compiler_flags(-Wno-reorder)\n    ov_add_compiler_flags(-Wno-comment)\n    ov_add_compiler_flags(-Wno-unused-local-typedefs)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Mod Operation with NumPy Broadcasting XML - OpenVINO\nDESCRIPTION: This XML snippet demonstrates the Mod operation in OpenVINO with auto_broadcast set to \"numpy\". It showcases the layer configuration, input ports with different dimensions that are compatible for NumPy broadcasting, and the resulting output port with the broadcasted dimensions. The 'data' tag sets the broadcasting rule, and the 'input' and 'output' tags define the tensor shapes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/mod-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Mod\">\n    <data auto_broadcast=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License Copyright Notice\nDESCRIPTION: This snippet shows the boilerplate copyright notice that should be attached to your work when applying the Apache License 2.0. The fields enclosed by brackets [] should be replaced with your own identifying information, and the text should be enclosed in the appropriate comment syntax for the file format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/third-party-programs.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: ReduceMin Layer Configuration (XML, axes=[1], keep_dims=false)\nDESCRIPTION: This XML snippet demonstrates the ReduceMin layer with `keep_dims` set to false and reduction along axis 1.  The input tensor is 6x12x10x24, and the output after reduction is 6x10x24.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-min-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceMin\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [1] that means independent reduction in each channel and spatial dimensions -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Architecture Diagram using Mermaid\nDESCRIPTION: This code snippet represents the architecture of the OpenVINO AUTO plugin using a Mermaid flowchart. It shows the interaction between the Application, OpenVINO Runtime, AUTO Plugin, CPU Plugin, and GPU Plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/README.md#_snippet_0\n\nLANGUAGE: Mermaid\nCODE:\n```\nflowchart TD\n\n    subgraph Application[\"Application\"]\n    end\n\n    subgraph OpenVINO Runtime[\"OpenVINO Runtime\"]\n        AUTO[\"AUTO Plugin\"] --> CPU[\"CPU Plugin\"]\n        AUTO[\"AUTO Plugin\"] --> GPU[\"GPU Plugin\"]\n    end\n\n    Application --> AUTO\n\n    style Application fill:#6c9f7f\n```\n\n----------------------------------------\n\nTITLE: Prepare Validation Function - OpenVINO\nDESCRIPTION: This snippet provides an example of a validation function for OpenVINO models. The function takes an OpenVINO compiled model and a validation dataset as input, performs inference on the dataset, and returns an accuracy metric (in this case, a dummy value of 0.5).  It showcases the structure needed for accuracy evaluation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/quantizing-with-accuracy-control.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# [validation]\nfrom openvino.runtime import Core\n\nie = Core()\n\n\ndef validate(compiled_model, validation_dataset):\n    # actual validation\n    return 0.5\n\n# [validation]\n```\n\n----------------------------------------\n\nTITLE: Remote Context Class Header Definition C++\nDESCRIPTION: Defines the header of a custom remote context class derived from ov::IRemoteContext. It declares member variables for the device name and device-specific properties. This header should be header-only and not depend on the plugin library directly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/remote-context.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nclass TemplateRemoteContext : public ov::IRemoteContext {\npublic:\n    TemplateRemoteContext(const std::string& device_name, const RemoteContextProperties& properties) : \n        m_name(device_name), m_property(properties) {}\n\n    std::string get_device_name() const override;\n    ov::Any get_property(const std::string& name) const override;\n    ov::SoPtr<ov::ITensor> create_tensor(const ov::element::Type& type, const ov::Shape& shape) const override;\nprivate:\n    std::string m_name;\n    RemoteContextProperties m_property;\n};\n```\n\n----------------------------------------\n\nTITLE: Adding Static Library Target in CMake\nDESCRIPTION: This code snippet creates the static library target named by `TARGET_NAME` (openvino_offline_transformations). It specifies that the library should be built as a static library and includes the source files listed in `LIBRARY_SRC` and header files in `PUBLIC_HEADERS` as part of the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/offline_transformations/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${LIBRARY_SRC} ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Adding Post-Build Commands in CMake\nDESCRIPTION: This CMake snippet adds post-build commands to the pyopenvino target. It copies the `openvino` directory from the Python source directory and the `requirements.txt` file to the library output directory. This ensures that the necessary Python files and dependencies are included with the built module.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(TARGET ${PROJECT_NAME}\n        POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy_directory ${OpenVINOPython_SOURCE_DIR}/src/openvino ${CMAKE_LIBRARY_OUTPUT_DIRECTORY}\n        COMMAND ${CMAKE_COMMAND} -E copy ${OpenVINOPython_SOURCE_DIR}/requirements.txt ${CMAKE_LIBRARY_OUTPUT_DIRECTORY}/../requirements.txt\n        )\n```\n\n----------------------------------------\n\nTITLE: Enabling Transformation Visualization in OpenVINO\nDESCRIPTION: This snippet demonstrates how to enable visualization of the model after each transformation pass in OpenVINO, using the `OV_ENABLE_VISUALIZE_TRACING` environment variable.  Setting the variable to \"true\", \"on\" or \"1\" enables visualization for all transformations. A filter string can be provided to visualize only specific passes. No specific dependencies are needed beyond the OpenVINO environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/docs/debug_capabilities/transformation_statistics_collection.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OV_ENABLE_VISUALIZE_TRACING=true\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport OV_ENABLE_VISUALIZE_TRACING=\"Pass1,Pass2,Pass3\"\n```\n\n----------------------------------------\n\nTITLE: Enabling LTO for Release Builds in CMake\nDESCRIPTION: This CMake snippet enables Link Time Optimization (LTO) for release builds of the `ov_snippets_func_tests` target.  The `set_target_properties` command configures the `INTERPROCEDURAL_OPTIMIZATION_RELEASE` property to the value of `ENABLE_LTO`, potentially improving the runtime performance of the compiled tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# LTO\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Creating Mock Frontend Library with CMake\nDESCRIPTION: This CMake code defines a shared library named `openvino_mock1_frontend` from the source file `mock_frontend.cpp`. It sets compile definitions, include directories, and links it to the `openvino::frontend::common` library. Additionally, it adds dependencies for unit tests and specifies installation instructions for the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/tests/frontend/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(SRC ${CMAKE_CURRENT_SOURCE_DIR}/mock_frontend.cpp)\nset(MOCK1_FE_NAME openvino_mock1_frontend)\nadd_library(${MOCK1_FE_NAME} SHARED ${SRC})\n\nov_add_library_version(${MOCK1_FE_NAME})\n\ntarget_compile_definitions(${MOCK1_FE_NAME} PRIVATE \"-DMOCK_VARIANT=\\\"1\\\"\")\n\ntarget_include_directories(${MOCK1_FE_NAME} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})\n\ntarget_link_libraries(${MOCK1_FE_NAME} PRIVATE openvino::frontend::common)\nadd_dependencies(ov_core_unit_tests ${MOCK1_FE_NAME})\n\nov_add_clang_format_target(${MOCK1_FE_NAME}_clang FOR_TARGETS ${MOCK1_FE_NAME})\n\ninstall(TARGETS ${MOCK1_FE_NAME}\n        RUNTIME DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL\n        LIBRARY DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Enabling Cache Encryption (GPU)\nDESCRIPTION: This snippet shows how to enable cache encryption for the GPU plugin in OpenVINO using Python and C++. It sets the CacheMode property to OPTIMIZE_SIZE to encrypt the model topology when saving to cache, and decrypts when loading. It's important to use OPTIMIZE_SIZE mode for full encryption.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n### [ov:caching:part6]\nimport openvino.runtime as ov\n\ncore = ov.Core()\n\nmodel_path = \"path_to_model.xml\"\ncache_dir = \"/path/to/cache/dir\"\n\ncompiled_model = core.compile_model(model_path, \"GPU\", config={\n    \"CACHE_DIR\": cache_dir,\n    \"CACHE_MODE\": \"OPTIMIZE_SIZE\"\n})\n### [ov:caching:part6]\n```\n\n----------------------------------------\n\nTITLE: Defining INTERFACE Library in CMake\nDESCRIPTION: This CMake command defines an INTERFACE library named 'gtest_main_manifest'. INTERFACE libraries are used to provide headers and link dependencies without producing an actual library file. This makes it suitable for providing common test entry points and configurations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/gtest_main_manifest/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${TARGET_NAME} INTERFACE)\n```\n\n----------------------------------------\n\nTITLE: Enabling ITT Profiling during Compilation (Shell)\nDESCRIPTION: Enables Instrumentation and Tracing Technology (ITT) APIs in OpenVINO during compilation. This allows for performance analysis using tools like Intel VTune Profiler.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection/debugging-auto-device.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n-DENABLE_PROFILING_ITT=ON\n```\n\n----------------------------------------\n\nTITLE: Install Build Dependencies\nDESCRIPTION: Installs the build dependencies for OpenVINO using the `install_build_dependencies.sh` script.  This script handles the installation of necessary system-level packages. The script needs execute permissions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/documentation_build_instructions.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n$ chmod +x install_build_dependencies.sh\n$ ./install_build_dependencies.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring JAX Frontend Module with CMake\nDESCRIPTION: This snippet uses the `frontend_module` function to configure and build the `py_jax_frontend` module for OpenVINO. It specifies the module name, framework (jax), and the component for packaging, ensuring that the necessary dependencies and configurations are applied correctly. `pyopenvino_SOURCE_DIR` and `OV_CPACK_COMP_PYTHON_OPENVINO` are expected to be defined beforehand.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/frontend/jax/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(${pyopenvino_SOURCE_DIR}/frontend/frontend_module.cmake)\nfrontend_module(py_jax_frontend jax ${OV_CPACK_COMP_PYTHON_OPENVINO}_${pyversion})\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library (CMake)\nDESCRIPTION: This snippet installs the static library `openvino_tensorflow_common` as part of the `OV_CPACK_COMP_CORE` component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/src/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: Creating Source Group\nDESCRIPTION: Organizes the source files in the IDE's project view by creating a source group corresponding to the directory structure. This helps to improve project organization and maintainability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/backend/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(TREE ${CMAKE_CURRENT_SOURCE_DIR} FILES ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Add Dynamic Impl to Implementation Map\nDESCRIPTION: This snippet shows how to add a dynamic implementation for a primitive to the implementation map, specifying the implementation type (ocl), shape type (dynamic_shape), and the corresponding kernel creation function. It also adds a static shape implementation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/dynamic_shape/dynamic_impl.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nimplementation_map<activation>::add(impl_types::ocl,\n                                    shape_types::dynamic_shape,\n                                    typed_primitive_impl_ocl<activation>::create<activation_impl>,\n                                    types,\n                                    dyn_formats);\n\nimplementation_map<activation>::add(impl_types::ocl,\n                                    shape_types::static_shape,\n                                    typed_primitive_impl_ocl<activation>::create<activation_impl>,\n                                    keys);\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Color Conversion NV12/I420 Python\nDESCRIPTION: Demonstrates how to handle YUV-family source color formats (NV12 and I420) with separate planes using the OpenVINO preprocessing API in Python. It splits the original input into 2 or 3 inputs for NV12 and I420 formats respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n# 1) Set input tensor information. We apply preprocessing\n#    demanding input tensor to have 'nv12_two_planes' color format\nppp.input().tensor().set_element_type(ov.Type.u8).set_color_format(ov.ColorFormat.NV12_TWO_PLANES)\n# 2) Adding explicit preprocessing steps. Model expects 'rgb' format\nppp.input().preprocess().convert_color(ov.ColorFormat.RGB)\n# 3) Apply preprocessing modifying the original model\nmodel = ppp.build()\n```\n\n----------------------------------------\n\nTITLE: Adding Jit Constant for Sub-group Size in OpenVINO\nDESCRIPTION: This code snippet demonstrates how to add a constant to the Jitter object, specifically for defining the sub-group size in OpenCL kernels. It creates a JitConstants object, adds a constant named \"SUB_GROUP_SIZE\" with the specified size, and is called within GetJitConstants method of the kernel.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_ops_enabling.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n  // GetJitConstants method of the kernel\n  const size_t sub_group_size = 16;\n  JitConstants jit = MakeBaseParamsJitConstants(params);\n  jit.AddConstant(MakeJitConstant(\"SUB_GROUP_SIZE\", sub_group_size ));\n```\n\n----------------------------------------\n\nTITLE: MaxPool Layer with Explicit Padding (XML)\nDESCRIPTION: This XML example defines a MaxPool layer with explicit padding. The configuration specifies attributes like auto_pad, kernel, pads_begin, pads_end, and strides, along with the input and output port dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-1.rst#_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MaxPool\" ... >\n    <data auto_pad=\"explicit\" kernel=\"2,2\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>17</dim>\n            <dim>17</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Color Conversion C++\nDESCRIPTION: Shows how to reverse color channels from RGB to BGR (or vice versa) using the OpenVINO preprocessing API in C++. It requires specifying the source color format in the tensor section and performing a `convert_color` pre-processing operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n// 1) Set input tensor information. We apply preprocessing\n//    demanding input tensor to have 'bgr' color format\nppp.input().tensor().set_element_type(ov::element::u8).set_color_format(ov::preprocess::ColorFormat::BGR);\n// 2) Adding explicit preprocessing steps. Model expects 'rgb' format\nppp.input().preprocess().convert_color(ov::preprocess::ColorFormat::RGB);\n// 3) Apply preprocessing modifying the original model\nmodel = ppp.build();\n```\n\n----------------------------------------\n\nTITLE: Replacing a Node using ov::replace_node (C++)\nDESCRIPTION: This code snippet demonstrates how to replace a node in an OpenVINO model using the `ov::replace_node` function. It specifically shows how to replace a Negative operation with a Multiply operation. This function requires the number of output ports for both nodes to be the same.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov:replace_node]\nauto neg = std::make_shared<op::v0::Negative>(input);\nauto mul_constant = ov::op::v0::Constant::create(input->get_element_type(), input->get_shape(), { -1 });\nauto mul = std::make_shared<op::v1::Multiply>(neg, mul_constant);\nov::replace_node(neg, mul);\n//! [ov:replace_node]\n```\n\n----------------------------------------\n\nTITLE: Squeeze 1D Tensor to 0D Tensor XML - OpenVINO\nDESCRIPTION: This XML snippet demonstrates the Squeeze operation applied to a 1D tensor with a single element (dimension 1), effectively converting it into a 0D tensor (constant). The second input provides the index [0] to squeeze the dimension of size 1. The resulting output has no dimensions, representing a scalar value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/squeeze-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Squeeze\">\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n        </port>\n    </input>\n    <input>\n        <port id=\"1\">\n            <dim>1</dim>  <!-- value is [0] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: AdaptiveMaxPool XML Layer Definition\nDESCRIPTION: This XML snippet defines an AdaptiveMaxPool layer with specific input and output dimensions. It demonstrates how to configure the layer within an OpenVINO model using XML.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/adaptive-max-pool-8.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"AdaptiveMaxPool\" ... >\n    <data output_type=\"i64\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <input>\n        <port id=\"1\">\n            <dim>2</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>16</dim>\n            <dim>16</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>16</dim>\n            <dim>16</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Merging XML Reports using merge_xmls.py (Python)\nDESCRIPTION: This snippet illustrates how to use the `merge_xmls.py` script to aggregate multiple XML reports generated from parallel test runs into a single report file. It requires specifying the input folders containing the XML reports, the output folder for the merged report, and the desired filename.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/README.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\npython3 merge_xmls.py --input_folders=/path/to/temp_output_report_folder --output_folder=/path/to/output_report_folder --output_filename=report_aggregated\n```\n\n----------------------------------------\n\nTITLE: Unsqueeze 2D to 4D Tensor XML Configuration\nDESCRIPTION: This XML configuration demonstrates the use of the Unsqueeze operation to transform a 2D tensor into a 4D tensor by inserting dimensions of size 1 at the specified indices. The input tensor has dimensions 2x3, and the indices [0, 3] are used to insert the new dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/unsqueeze-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Unsqueeze\">\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <input>\n        <port id=\"1\">\n            <dim>2</dim>  <!-- value is [0, 3] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Disabling System Include Paths for PugiXML in CMake\nDESCRIPTION: This snippet disables the system include paths for the PugiXML library. This is done to prevent potential compilation errors that can occur when the system PugiXML's include directory conflicts with the project's own PugiXML include directory. It's particularly important when using a system-installed PugiXML.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_SYSTEM_PUGIXML)\n    # system pugixml has /usr/include as include directories\n    # we cannot use them as system ones, leads to compilation errors\n    set_target_properties(openvino_core_obj PROPERTIES NO_SYSTEM_FROM_IMPORTED ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Build ITT Collector for Code Usage Analysis\nDESCRIPTION: This snippet shows how to build the `sea_itt_lib` target, which is an ITT collector used for code usage analysis. This collector generates statistics about which parts of the OpenVINO code are used during inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncmake --build . --target sea_itt_lib\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Options in CMake\nDESCRIPTION: This snippet conditionally adds the -Wall compile option when using the GNU C++ compiler. This enables all warnings, which can help identify potential issues in the code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/common/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif (CMAKE_COMPILER_IS_GNUCXX)\n    target_compile_options(${TARGET_NAME} PRIVATE -Wall)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Partially Defined Layout in C++\nDESCRIPTION: This snippet shows how to define a partially defined layout in C++ using `ov::Layout`, where some dimensions are marked as unimportant using `?`. This is useful when you have dimensions that don't require specific labeling.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nov::Layout layout(\"[?, C, ?, W]\");\n```\n\n----------------------------------------\n\nTITLE: Exporting Target for Developer Package\nDESCRIPTION: This snippet uses the 'ov_developer_package_export_targets' function to export the target for inclusion in a developer package. It also specifies the install include directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/shared/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET ${TARGET_NAME}\n                                    INSTALL_INCLUDE_DIRECTORIES \"${PUBLIC_HEADERS_DIR}/\")\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Build Pipeline Gantt Chart\nDESCRIPTION: This Gantt chart visualizes the OpenVINO build pipeline, outlining the key stages of setting up the environment, building OpenVINO, and running tests. It provides a timeline and dependencies between the different phases of the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build.md#_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngantt \n    %% Use a hack for centry as a persantage\n    dateFormat YYYY\n    axisFormat %y\n    todayMarker off\n    title       OpenVINO getting started pipeline\n    Setup environment :env, 2000, 1716w\n    Build openvino :crit, build, after env, 1716w\n    Run tests :active, run, after build, 1716w\n```\n\n----------------------------------------\n\nTITLE: Setting Positive Regex Pattern for multiple layers - Bash\nDESCRIPTION: This example shows how to set `OV_CPU_INFER_PRC_POS_PATTERN` to selectively enable precision enforcement on multiple specific layers, such as `input_layernorm` and `self_attn.q_proj`.  The regular expression uses the `|` operator to specify multiple alternatives. It is a bash code snippet.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/infer_prc.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n\"__module\\.model\\.layers\\.(\\d+)\\.input_layernorm/|__module\\.model\\.layers\\.(\\d+)\\.self_attn\\.q_proj/\"\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX Namespace in CMake\nDESCRIPTION: This snippet sets the ONNX namespace to `openvino_onnx`, which helps prevent naming conflicts within the OpenVINO project. It also defines options for using static runtime libraries in MSVC and enables or disables lite protobuf based on the `FORCE_FRONTENDS_USE_PROTOBUF` flag.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/onnx/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(OV_ONNX_NAMESPACE openvino_onnx)\n\nif(NOT DEFINED ONNX_USE_MSVC_STATIC_RUNTIME)\n    set(ONNX_USE_MSVC_STATIC_RUNTIME OFF)\nendif()\n\nif(FORCE_FRONTENDS_USE_PROTOBUF)\n    set(ONNX_USE_LITE_PROTO_DEFAULT OFF)\nelse()\n    set(ONNX_USE_LITE_PROTO_DEFAULT ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: MaxPool Layer with Valid Padding (XML)\nDESCRIPTION: This XML example defines a MaxPool layer with valid padding. The configuration details the attributes like auto_pad, kernel, pads_begin, pads_end, and strides, in addition to the input and output port dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-1.rst#_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MaxPool\" ... >\n    <data auto_pad=\"valid\" kernel=\"2,2\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>16</dim>\n            <dim>16</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags CMake\nDESCRIPTION: This snippet adds compiler flags to suppress specific warnings when using the Intel LLVM compiler on Windows. The flags suppress warnings about macro redefinitions, non-portable include paths, and comments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ocl/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(OV_COMPILER_IS_INTEL_LLVM AND WIN32)\n    ov_add_compiler_flags(\"/Wno-macro-redefined\")\n    ov_add_compiler_flags(\"/Wno-nonportable-include-path\")\n    ov_add_compiler_flags(\"/Wno-comment\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Exporting Developer Package Targets in CMake\nDESCRIPTION: This snippet exports the targets for the developer package, ensuring that the include directories are correctly set when the library is used by other projects. It installs the include directories specified by INSTALL_INCLUDE_DIRECTORIES.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_21\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET ${TARGET_NAME}\n                                    INSTALL_INCLUDE_DIRECTORIES \"${UTIL_INCLUDE_DIR}/\")\n```\n\n----------------------------------------\n\nTITLE: BitwiseXor Boolean Example in Python\nDESCRIPTION: Demonstrates the BitwiseXor operation on boolean tensors.  The input tensors `a` and `b` are boolean arrays, and the output shows the result of the element-wise XOR operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-xor-13.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\n# For given boolean inputs:\na = [True, False, False]\nb = [True, True, False]\n# Perform logical XOR operation same as in LogicalXor operator:\noutput = [False, True, False]\n```\n\n----------------------------------------\n\nTITLE: Protopipe Reference Mode with Random Input\nDESCRIPTION: This YAML shows how to generate random input data within Protopipe's reference mode. It demonstrates specifying a uniform distribution with low and high bounds for the random data generation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nrandom: { dist: uniform, low: -1.0, high: 1.0 } # specified globally for all models\nmulti_inference:\n- input_stream_list:\n  - network:\n    - { name: A.xml, ip: FP16, input_data: A-inputs/, output_data: B-inputs/ }\n      # overwrites global initializer for the model B.xml\n    - { name: B.xml, ip: FP16, input_data: B-inputs/, output_data: B-outptus/, random: { dist: uniform, low: 0, high: 255.0 }}\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries\nDESCRIPTION: This snippet links the target library against 'openvino::core::dev', making its symbols available to the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common_translators/src/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino::core::dev)\n```\n\n----------------------------------------\n\nTITLE: Squeeze 4D Tensor to 2D Tensor XML - OpenVINO\nDESCRIPTION: This XML snippet demonstrates the Squeeze operation applied to a 4D tensor, reducing it to a 2D tensor by removing dimensions at specified indices where the size is equal to 1. The input tensor has dimensions [1, 3, 1, 2], and the second input specifies the indices [0, 2] to be squeezed. The output tensor will have dimensions [3, 2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/squeeze-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Squeeze\">\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>1</dim>\n            <dim>2</dim>\n        </port>\n    </input>\n    <input>\n        <port id=\"1\">\n            <dim>2</dim>  <!-- value [0, 2] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ReduceLogicalAnd XML Example with keep_dims=true\nDESCRIPTION: XML example demonstrating the ReduceLogicalAnd operation with the 'keep_dims' attribute set to 'true'. This configuration maintains the reduced axes in the output tensor, setting their dimensions to 1. The example showcases a reduction along axes 2 and 3 of the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-logical-and-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceLogicalAnd\" ...>\n    <data keep_dims=\"true\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files in CMake\nDESCRIPTION: These snippets use `file(GLOB_RECURSE)` to recursively find all C++ source files and header files within the specified directories.  The results are stored in `LIBRARY_SRC` and `PUBLIC_HEADERS` variables, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE PUBLIC_HEADERS ${PUBLIC_HEADERS_DIR}/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: CTCLoss XML Layer Configuration\nDESCRIPTION: This XML snippet demonstrates a configuration for the CTCLoss layer, specifying input and output port dimensions.  It includes input ports for logits, logit_length, labels, label_length, and blank_index.  The output port defines the shape of the resulting loss tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/ctc-loss-4.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"CTCLoss\" ...>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>20</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"1\">\n            <dim>8</dim>\n        </port>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>20</dim>\n        </port>\n        <port id=\"3\">\n            <dim>8</dim>\n        </port>\n        <port id=\"4\">  <!-- blank_index value is: 120 -->\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>8</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: DetectionOutput Layer Configuration in XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a DetectionOutput layer within an OpenVINO model. It showcases various attributes like `background_label_id`, `code_type`, `nms_threshold`, and input/output port dimensions. The configuration includes settings for non-maximum suppression, input normalization, and bounding box handling.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/detectionoutput-8.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"DetectionOutput\" version=\"opset8\">\n    <data background_label_id=\"1\" code_type=\"caffe.PriorBoxParameter.CENTER_SIZE\" confidence_threshold=\"0.019999999552965164\" input_height=\"1\" input_width=\"1\" keep_top_k=\"200\" nms_threshold=\"0.44999998807907104\" normalized=\"true\" share_location=\"true\" top_k=\"200\" variance_encoded_in_target=\"false\" clip_after_nms=\"false\" clip_before_nms=\"false\" objectness_score=\"0\" decrease_label_id=\"false\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>5376</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>2688</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>2</dim>\n            <dim>5376</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>200</dim>\n            <dim>7</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This snippet sets the target name for the executable to ov_appverifier_tests. This name is then used in subsequent CMake commands to define the executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/appverifier_tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_appverifier_tests)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties in CMake\nDESCRIPTION: This snippet sets the `EXPORT_NAME` property for the `TARGET_NAME` library to `util`. This controls the name used when the library is exported for use by other projects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME util)\n```\n\n----------------------------------------\n\nTITLE: Configuration for Enabling CPU Fallback in AUTO plugin (JSON)\nDESCRIPTION: This JSON snippet shows the configuration to enable CPU fallback at startup for the AUTO plugin using the `ENABLE_STARTUP_FALLBACK` property set to `YES`. This will use CPU as a helper device at the beginning.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/tests.md#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"AUTO\": {\n            \"ENABLE_STARTUP_FALLBACK\": \"YES\"\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Excluding SHL Paths\nDESCRIPTION: This snippet excludes SHL-related paths if `OV_CPU_WITH_SHL` is not enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_18\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT OV_CPU_WITH_SHL)\n    list(APPEND EXCLUDE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/nodes/executors/shl/*)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Include Directories (CMake)\nDESCRIPTION: This snippet sets the include directories for the target library. It specifies the public include directories that are required to compile against the library. It uses generator expressions to handle build and install interfaces.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/shape_inference/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC\n    $<BUILD_INTERFACE:${SHAPE_INFER_INCLUDE_DIR}>\n    $<BUILD_INTERFACE:${OV_CORE_INCLUDE_PATH}>\n    $<TARGET_PROPERTY:openvino::core::dev,INTERFACE_INCLUDE_DIRECTORIES>)\n```\n\n----------------------------------------\n\nTITLE: Installing GCC 8.3.1 on CentOS 7\nDESCRIPTION: These commands install GCC 8.3.1 on CentOS 7 using the `devtoolset-8` package.  It first updates the system, installs the necessary repositories (`centos-release-scl` and `epel-release`), and then installs `devtoolset-8`.  After installation, it enables the devtoolset to be used and checks the gcc version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/release-notes-openvino/system-requirements.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nsudo yum update -y && sudo yum install -y centos-release-scl epel-release\nsudo yum install -y devtoolset-8\n```\n\n----------------------------------------\n\nTITLE: Run Benchmark App Test Sample in OpenVINO\nDESCRIPTION: Executes the benchmark_app tool with a specified model and device. This command runs a test sample and checks for any errors during execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/docs/develop_cc_for_new_component.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./benchmark_app -m <your_model.xml> -d CPU\n```\n\n----------------------------------------\n\nTITLE: Slice: Basic Slicing in OpenVINO XML\nDESCRIPTION: This example demonstrates basic slicing of a 1D tensor using the Slice operation in OpenVINO. It shows how to specify the start, stop, step, and axes parameters to extract a sub-tensor from the input data. The `axes` input specifies the dimension to slice.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-8.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Slice\" ...>\n       <input>\n           <port id=\"0\">       <!-- data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -->\n             <dim>10</dim>\n           </port>\n           <port id=\"1\">       <!-- start: [1] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"2\">       <!-- stop: [8] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"3\">       <!-- step: [1] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"4\">       <!-- axes: [0] -->\n             <dim>1</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">       <!-- output: [1, 2, 3, 4, 5, 6, 7] -->\n               <dim>7</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: CumSum Example 3: Inclusive Reverse Summation in OpenVINO (XML)\nDESCRIPTION: This XML example demonstrates the CumSum operation with inclusive summation (exclusive=\"0\") and reverse direction (reverse=\"1\").  It calculates the reverse cumulative sum of the input tensor [1., 2., 3., 4., 5.] along axis 0, resulting in the output tensor [15., 14., 12., 9., 5.].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/cumsum-3.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"CumSum\" exclusive=\"0\" reverse=\"1\">\n    <input>\n        <port id=\"0\">     <!-- input value is: [1., 2., 3., 4., 5.] -->\n            <dim>5</dim>\n        </port>\n        <port id=\"1\"/>     <!-- axis value is: 0 -->\n    </input>\n    <output>\n        <port id=\"2\">     <!-- output value is: [15., 14., 12., 9., 5.] -->\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: BinaryConvolution XML Layer Definition\nDESCRIPTION: This XML snippet defines a BinaryConvolution layer in OpenVINO. It specifies attributes such as dilations, pads, strides, mode, pad_value and auto_pad, along with input and output port dimensions. The mode attribute determines how the input and weight tensors are interpreted, and the XML describes explicit padding.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/binary-convolution-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"BinaryConvolution\" ...>\n    <data dilations=\"1,1\" pads_begin=\"2,2\" pads_end=\"2,2\" strides=\"1,1\" mode=\"xnor-popcount\" pad_value=\"0\" auto_pad=\"explicit\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>64</dim>\n            <dim>3</dim>\n            <dim>5</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Col2Im Example with Non-Default Dilations and Padding (XML)\nDESCRIPTION: This example demonstrates the Col2Im operation with non-default dilations and padding.  It illustrates configuring the data layer to specify the dilations and padding to be applied.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/col2im-15.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Col2Im\" ... >\n    <data dilations=\"2,2\" pads_begin=\"3,3\" pads_end=\"3,3\"/>\n    <input>\n        <port id=\"0\" precision=\"I32\">\n            <dim>12</dim>    <!-- batch_axis -->\n            <dim>12/dim</dim>     <!-- C * Product(kernel_size) -->\n            <dim>324</dim>   <!-- L -->\n        </port>\n        <port id=\"1\" precision=\"I32\">\n            <dim>2</dim>     <!-- output_size -->\n        </port>\n        <port id=\"2\" precision=\"I32\">\n            <dim>2</dim>     <!-- kernel_size -->\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"I32\">\n            <dim>12</dim>    <!-- batch_axis -->\n            <dim>3</dim>     <!-- C -->\n            <dim>32</dim>    <!-- output_size[0] -->\n            <dim>32</dim>    <!-- output_size[1] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: STFT Configuration for 2D Signal, transpose_frames=false (XML)\nDESCRIPTION: This XML snippet demonstrates the configuration of the STFT operation for a 2D signal input where the transpose_frames attribute is set to false. The input signal has dimensions [3, 56], the window has a dimension of 7, frame_size is 11 and frame_step is 3. The output shape is [3, 16, 6, 2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/stft-15.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"STFT\" ... >\n    <data transpose_frames=\"false\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n        </port>\n        <port id=\"2\"></port> <!-- value: 11 -->\n        <port id=\"3\"></port> <!-- value: 3 -->\n    <output>\n        <port id=\"4\">\n            <dim>3</dim>\n            <dim>16</dim>\n            <dim>6</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Slicing 3D Tensor with All Axes Specified XML\nDESCRIPTION: This XML snippet configures a Slice layer for a 3D tensor, specifying all axes. The start, stop, step, and axes are defined for each of the three dimensions. The input tensor is 20x10x5, and after slicing, the output tensor becomes 4x10x5.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-8.rst#_snippet_10\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Slice\" ...>\n       <input>\n           <port id=\"0\">       <!-- data -->\n             <dim>20</dim>\n             <dim>10</dim>\n             <dim>5</dim>\n           </port>\n           <port id=\"1\">       <!-- start: [0, 0, 0] -->\n             <dim>2</dim>\n           </port>\n           <port id=\"2\">       <!-- stop: [4, 10, 5] -->\n             <dim>2</dim>\n           </port>\n           <port id=\"3\">       <!-- step: [1, 1, 1] -->\n             <dim>2</dim>\n           </port>\n           <port id=\"4\">       <!-- axes: [0, 1, 2] -->\n             <dim>2</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">       <!-- output -->\n               <dim>4</dim>\n               <dim>10</dim>\n               <dim>5</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: NormalizeL2 Example (XML) - Channel & Spatial Normalization\nDESCRIPTION: This XML snippet demonstrates the usage of the NormalizeL2 operation for normalizing over the channel and spatial dimensions in an NCHW layout. It defines a layer with input and output ports, and specifies the 'eps' and 'eps_mode' attributes. The axes input indicates that normalization is performed along the channel and spatial dimensions (axes 1, 2, and 3).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/normalize-l2-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"NormalizeL2\" ...>\n    <data eps=\"1e-8\" eps_mode=\"add\"/>\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>         <!-- axes list [1, 2, 3] means normalization over channel and spatial dimensions -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Running a Specific OpenVINO AUTO Plugin Unit Test\nDESCRIPTION: This snippet shows how to execute a specific unit test by using the `--gtest_filter` option with the `ov_auto_unit_tests` executable. The filter name specifies which tests to include in the test run using wildcards.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/tests.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./ov_auto_unit_tests --gtest_filter='*filter_name*'\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries with CMake\nDESCRIPTION: This CMake snippet links the 'timetests_helper' library with gflags and tests_shared_lib. The `target_link_libraries` command is used to specify the dependencies that the library needs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/src/timetests_helper/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} gflags tests_shared_lib)\n```\n\n----------------------------------------\n\nTITLE: Installing Specific OpenVINO Components\nDESCRIPTION: This command installs a specific OpenVINO component from the `conda-forge` channel. Replacing `<component_name>` with the desired component's name allows you to install only the parts of OpenVINO that you need.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conda.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nconda install conda-forge::<component_name>\n```\n\n----------------------------------------\n\nTITLE: Change Directory (Azure ML)\nDESCRIPTION: This command changes the current directory to `openvino_notebooks`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_21\n\nLANGUAGE: console\nCODE:\n```\ncd openvino_notebooks\n```\n\n----------------------------------------\n\nTITLE: Make Stateful Model Using Tensor Names in C++\nDESCRIPTION: This snippet demonstrates how to apply the MakeStateful transformation to an OpenVINO model using tensor names in C++. It involves creating a transformation and passing the parameter/result tensor name pairs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nov::pass::MakeStateful::ParamResMap param_res_map = {{\n    \"tensor_name_1\",\n    \"tensor_name_4\",\n}};\n\nmodel->transform(std::make_shared<ov::pass::MakeStateful>(param_res_map));\n```\n\n----------------------------------------\n\nTITLE: Modifying Requirements File\nDESCRIPTION: This code block reads the contents of 'smoke_tests/requirements.txt', replaces a specific string within it, and writes the modified content back to a file in the build directory before installation. This ensures that the correct relative path is used in the installed requirements file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/samples_tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nfile(READ smoke_tests/requirements.txt REQUIREMENTS_REPO)\nstring(REPLACE \"-c ../../constraints.txt\" \"-c ../constraints.txt\" REQUIREMENTS_TMP ${REQUIREMENTS_REPO})\nfile(WRITE ${CMAKE_CURRENT_BINARY_DIR}/smoke_tests/requirements.txt ${REQUIREMENTS_TMP})\ninstall(FILES ${CMAKE_CURRENT_BINARY_DIR}/smoke_tests/requirements.txt DESTINATION tests/smoke_tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Conditional OpenCV Integration with CMake\nDESCRIPTION: This CMake snippet checks for OpenCV and enables its support for reference preprocessing if found. It links against OpenCV libraries and defines a compile definition to activate OpenCV-specific tests. If OpenCV is not found, a warning message is displayed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/tests/functional/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenCV QUIET COMPONENTS core imgproc)\n\nif(OpenCV_FOUND AND OpenCV_VERSION VERSION_GREATER_EQUAL 3.4)\n    message(STATUS \"Reference preprocessing: OpenCV tests are enabled\")\n    target_compile_definitions(${TARGET_NAME} PRIVATE OPENCV_TEMPLATE_TESTS)\n    target_link_libraries(${TARGET_NAME} PRIVATE opencv_imgproc opencv_core)\nelse()\n    message(WARNING \"Reference preprocessing: OpenCV tests are disabled, because OpenCV ver. 3.4+ is not found\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Define Model input Method (TypeScript) - Name Parameter\nDESCRIPTION: This code defines the `input` method of the `Model` interface with a name parameter. It gets the input of the model identified by the tensor name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\ninput(name: string): Output\n```\n\n----------------------------------------\n\nTITLE: Creating ARM Compute Library Version File\nDESCRIPTION: This CMake function, `ov_create_acl_version_file`, extracts the version of the ARM Compute Library from the `SConscript` file and writes it to a file named `arm_compute_version.embed`. This file is used by oneDNN to detect the ACL version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/thirdparty/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ov_create_acl_version_file)\n    set(ACL_ROOT_DIR \"${intel_cpu_thirdparty_SOURCE_DIR}/ComputeLibrary\")\n    set(ACL_SCONSCRIPT_FILE_PATH \"${ACL_ROOT_DIR}/SConscript\")\n    file(READ ${ACL_SCONSCRIPT_FILE_PATH} ACL_SCONSCRIPT_FILE_CONTENT)\n    string(REGEX MATCH \"v([0-9]+\\.[0-9]+\\.?[0-9]*)\" ACL_VERSION \"${ACL_SCONSCRIPT_FILE_CONTENT}\")\n    set(ACL_VERSION \"${CMAKE_MATCH_1}\")\n\n    set(ACL_VERSION_FILE_PATH \"${ACL_ROOT_DIR}/build/${OV_CPU_ARM_TARGET_ARCH}/src/core/arm_compute_version.embed\")\n    file(WRITE ${ACL_VERSION_FILE_PATH} \"\\\"arm_compute_version=v${ACL_VERSION}\\\"\")\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Defining Test Target with CMake\nDESCRIPTION: This snippet defines a test target named `ov_op_conformance_tests` using the `ov_add_test_target` CMake macro. It specifies the source directory, include directories, linked libraries (`conformance_shared` and `op_conformance_utils`), and labels the target as `OV OP_CONFORMANCE`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/op_conformance_runner/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME ov_op_conformance_tests)\n\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/src\"\n        INCLUDES\n            PRIVATE\n                \"${CMAKE_CURRENT_SOURCE_DIR}/include\"\n        ADD_CPPLINT\n        LINK_LIBRARIES\n            PUBLIC\n                conformance_shared\n                op_conformance_utils\n        LABELS\n            OV OP_CONFORMANCE\n)\n```\n\n----------------------------------------\n\nTITLE: 2D ConvolutionBackpropData Example\nDESCRIPTION: Example configuration of a 2D ConvolutionBackpropData layer with explicit padding. The layer performs upsampling with specified dilations, padding, strides and output_padding. The input and output dimensions are explicitly defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/convolution-backprop-data-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"5\" name=\"upsampling_node\" type=\"ConvolutionBackpropData\">\n    <data dilations=\"1,1\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\" output_padding=\"0,0\" auto_pad=\"explicit\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>20</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>20</dim>\n            <dim>10</dim>\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>10</dim>\n            <dim>447</dim>\n            <dim>447</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Running Python ONNX Operators Tests (Installation Layout)\nDESCRIPTION: This command runs the Python ONNX operators tests using pytest in the installation layout.  It excludes the 'test_zoo_models.py' and 'test_backend.py' files from the test execution.  Replace <OV_INSTALL_DIR> with the OpenVINO installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\npytest <OV_INSTALL_DIR>/tests/pyopenvino/tests/test_onnx \\\n    --ignore=<OV_INSTALL_DIR>/tests/pyopenvino/tests/test_onnx/test_zoo_models.py \\\n    --ignore=<OV_INSTALL_DIR>/tests/pyopenvino/tests/test_onnx/test_backend.py\n```\n\n----------------------------------------\n\nTITLE: Emulating RVV 0.7.1\nDESCRIPTION: This snippet shows how to emulate RVV 0.7.1 using QEMU with the Xuantie toolchain.  It specifies the path to the QEMU emulator, the target CPU with RVV 0.7.1 extensions enabled, and the path to the executable file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\n<xuantie_install_path>/bin/qemu-riscv64 -cpu rv64,x-v=true,vext_spec=v0.7.1 <executable_file_path>\n```\n\n----------------------------------------\n\nTITLE: CMake Flag for Python Executable\nDESCRIPTION: This cmake flag specifies the path to the Python executable within a virtual environment. It ensures that CMake uses the correct Python interpreter for building the OpenVINO™ Python API. Requires cmake 3.16 or higher.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/build.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\n-DPython3_EXECUTABLE=/home/user/.pyenv/versions/3.10.7/bin/python\n```\n\n----------------------------------------\n\nTITLE: EmbeddingBagOffsets Example 2 (XML)\nDESCRIPTION: This XML snippet provides an example configuration for the EmbeddingBagOffsets operation with the reduction attribute set to \"sum\", per_sample_weights provided, and default_index set to -1 to fill the empty bag with 0. It showcases a different configuration to handle the case of empty bags.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sparse/embedding-bag-offsets-15.rst#_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"EmbeddingBagOffsets\" ... >\n       <data reduction=\"sum\"/>\n       <input>\n           <port id=\"0\">     <!-- emb_table value is: [[-0.2, -0.6], [-0.1, -0.4], [-1.9, -1.8], [-1.,  1.5], [ 0.8, -0.7]] -->\n               <dim>5</dim>\n               <dim>2</dim>\n           </port>\n           <port id=\"1\">     <!-- indices value is: [0, 2, 3, 4] -->\n               <dim>4</dim>\n           </port>\n           <port id=\"2\">     <!-- offsets value is: [0, 2, 2] - 3 \"bags\" containing [2,0,4-2] elements, second \"bag\" is empty -->\n               <dim>3</dim>\n           </port>\n           <port id=\"3\"/>    <!-- default_index value is: -1 - fill empty bag with 0-->\n           <port id=\"4\"/>    <!-- per_sample_weights value is: [0.5, 0.2, -2, 1] -->\n               <dim>4</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">     <!-- output value is: [[-0.48, -0.66], [0., 0.], [2.8, -3.7]] -->\n               <dim>3</dim>\n               <dim>2</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Roll Operation Example with Multiple Axes and Shifts\nDESCRIPTION: This example shows the Roll operation with different shift values for different axes. The shifts are -1 along axis 0 and 2 along axis 1, demonstrating how elements are shifted in multiple dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/roll-7.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ndata    = [[ 1,  2,  3],\n            [ 4,  5,  6],\n            [ 7,  8,  9],\n            [10, 11, 12]]\n    output  = [[ 5,  6,  4],\n            [ 8,  9,  7],\n            [11, 12, 10],\n            [ 2,  3,  1]]\n```\n\n----------------------------------------\n\nTITLE: Adding Dependency on IR Frontend in CMake\nDESCRIPTION: This snippet adds a dependency to the unit test target if the OV_IR_FRONTEND is enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/tests/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_OV_IR_FRONTEND)\n    add_dependencies(${TARGET_NAME} openvino_ir_frontend)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Append Includes and Libraries\nDESCRIPTION: Appends include directories and libraries to the `OPTIONAL_FUNC_TESTS_INCLUDES` and `OPTIONAL_FUNC_TESTS_LIBS` lists. These are specific to the NPU compiler adapter and related components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/functional/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND OPTIONAL_FUNC_TESTS_INCLUDES\n      \"${OpenVINO_SOURCE_DIR}/src/plugins/intel_npu/src/compiler_adapter/include\")\n\nlist(APPEND OPTIONAL_FUNC_TESTS_LIBS openvino_npu_driver_compiler_adapter\n      openvino_npu_level_zero_backend openvino_npu_zero_utils)\n```\n\n----------------------------------------\n\nTITLE: Setting Positive Regex Pattern for specific layer - Bash\nDESCRIPTION: This example demonstrates how to set `OV_CPU_INFER_PRC_POS_PATTERN` to selectively enable precision enforcement on specific layers, such as `input_layernorm`, within a model. The regular expression includes escaping special characters and using sub-patterns to match layer indices. It is a bash code snippet.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/infer_prc.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"__module\\.model\\.layers\\.(\\d+)\\.input_layernorm/\"\n```\n\n----------------------------------------\n\nTITLE: Check QEMU Version\nDESCRIPTION: This command checks the installed QEMU version. The expected output is the version number. A version of 2.12.0 or higher is recommended.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nqemu-system-x86_64 --version\n```\n\n----------------------------------------\n\nTITLE: Emulate CPU Core Number with numactl\nDESCRIPTION: This snippet shows how to use the `numactl` tool to control the number of CPU cores available to the `benchmark_app`. This allows emulating different CPU configurations for collecting statistics. `${OPENVINO_LIBRARY_DIR}` should point to the location of OpenVINO libraries, `${MY_MODEL_RESULT}` is the directory to store the results, and `${MY_MODEL}.xml` is the model file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\npython thirdparty/itt_collector/runtool/sea_runtool.py --bindir ${OPENVINO_LIBRARY_DIR} -o ${MY_MODEL_RESULT} ! numactl -C 0-$core_num ./benchmark_app -niter 1 -nireq 1 -m ${MY_MODEL}.xml\n```\n\n----------------------------------------\n\nTITLE: MaxPool Operation Example with Explicit Padding (Shell)\nDESCRIPTION: This example demonstrates the MaxPool operation on a 4D input with a 2D kernel and explicit padding. It defines the input tensor, kernel size, strides, dilations, padding parameters, and expected output. The rounding type is set to \"floor\".\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-8.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]],\n                [[10, 11, 12],\n                 [13, 14, 15],\n                 [16, 17, 18]],\n                [[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]],\n                [[10, 11, 12],\n                 [13, 14, 15],\n                 [16, 17, 18]]\n                 ]]\n      strides = [1, 1]\n      kernel = [2, 2]\n      dilations = [2, 2]\n      rounding_type = \"floor\"\n      auto_pad = \"explicit\"\n      pads_begin = [1, 1]\n      pads_end = [1, 1]\n      output0 = [[[[5, 6, 5],\n                   [8, 9, 8],\n                   [5, 6, 5]]]]\n      output1 = [[[[4, 5, 4],\n                   [7, 8, 7],\n                   [4, 5, 4]]]]\n```\n\n----------------------------------------\n\nTITLE: Sample Conditional Compilation Test Usage\nDESCRIPTION: This command shows a sample usage of the pytest command to run the conditional compilation tests.  It sets the required parameters for the tests, including paths to sea_runtool, collector_dir, artifacts, openvino_root_dir and omz_repo.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/conditional_compilation/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest test_cc.py --sea_runtool=./thirdparty/itt_collector/runtool/sea_runtool.py --collector_dir=./bin/intel64/Release --artifacts=../artifacts --openvino_root_dir=. --omz_repo=../_open_model_zoo\n```\n\n----------------------------------------\n\nTITLE: Uninstall Latest OpenVINO Runtime\nDESCRIPTION: This command uninstalls the latest version of the OpenVINO Runtime using the YUM package manager. It requires sudo privileges to remove software packages and their dependencies. It removes the most recently installed OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nsudo yum autoremove openvino\n```\n\n----------------------------------------\n\nTITLE: Add Code Style Check CMake\nDESCRIPTION: Adds a code style check target using `ov_add_clang_format_target` for the 'onnx_fe_standalone_build_test' target.  This ensures that the code adheres to the project's coding style guidelines.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/standalone_build/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name CMake\nDESCRIPTION: This snippet sets the target name for the `ov_subgraphs_dumper` tool. This name is later used in other CMake commands to define build targets and dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/subgraphs_dumper/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_subgraphs_dumper)\n```\n\n----------------------------------------\n\nTITLE: compile_model() Implementation C++\nDESCRIPTION: This snippet presents the `compile_model()` method, which is responsible for applying OpenVINO passes to the input `ov::Model`, potentially including low-precision transformations.  The transform_model() function is assumed to define a plugin-specific conversion pipeline.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/compiled-model.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nvoid CompiledModel::compile_model() {\n    transform_model(m_model, m_cfg);\n\n    // apply Low precision transformations\n    if (m_cfg.count(ov::hint::inference_precision.name())) {\n        auto precision = m_cfg[ov::hint::inference_precision.name()].as<ov::element::Type>();\n        if (precision == ov::element::i8 || precision == ov::element::u8) {\n            ov::pass::enable_fake_quantize(m_model);\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Conditional Fuzzing Enablement\nDESCRIPTION: This snippet conditionally enables fuzzing based on the `ENABLE_FUZZING` flag. If `ENABLE_FUZZING` is not set, a message is displayed indicating that fuzz tests will be built without fuzzer support. Otherwise, the `enable_fuzzing()` function is called to configure the project for fuzzing.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT ENABLE_FUZZING)\n    message(STATUS\n        \"Fuzz tests will be built without fuzzer support. You can use those to\\n\"\n        \"run crash reproducers and corpus inputs. Configure ENABLE_FUZZING=ON\\n\"\n        \"to built with a fuzzer.\")\nelse()\n    enable_fuzzing()\nendif()\n```\n\n----------------------------------------\n\nTITLE: NV12toBGR Layer Configuration (Single Input)\nDESCRIPTION: This XML snippet configures an NV12toBGR layer with a single input port representing the NV12 image data. The input has dimensions of 1x720x640x1 and the output, which is the converted BGR image, has dimensions of 1x480x640x3.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/nv12-to-bgr-8.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"NV12toBGR\">\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>720</dim>\n            <dim>640</dim>\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>480</dim>\n            <dim>640</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License Notice\nDESCRIPTION: This code snippet represents the boilerplate notice for applying the Apache License 2.0 to a work. It includes placeholders for the copyright year and the name of the copyright owner, which should be replaced with the actual values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/third-party-programs.txt#_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Compile Model with Remote Context C++\nDESCRIPTION: This snippet shows the `compile_model` method, which compiles the model with remote context. It supports the heterogeneous execution and allows to execute the model on different devices. It uses the configuration to tailor the compilation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<ov::ICompiledModel> Plugin::compile_model(const std::shared_ptr<const ov::Model>& model, const ov::RemoteContext& context, const ov::AnyMap& properties) {\n    OPENVINO_ASSERT(context, \"Remote context doesn't exist\");\n    // 1. Update plugin config with compile config\n    auto cfg = Configuration{properties, get_stream_executor(properties)};\n    auto compiled_model = std::make_shared<CompiledModel>(model, cfg, shared_from_this(), context);\n\n    return compiled_model;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Synchronous Inference in OpenVINO with C++\nDESCRIPTION: This C++ snippet showcases synchronous inference using `ov::InferRequest::infer`. It sets input tensor data, calls infer, and then accesses the output tensor. Requires OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"CPU\");\nov::InferRequest infer_request = compiled_model.create_infer_request();\n\nov::Tensor input_tensor = infer_request.get_input_tensor();\nstd::vector<float> input_data(input_tensor.get_size());\nstd::generate(input_data.begin(), input_data.end(), []() { return static_cast<float>(rand()) / RAND_MAX; });\nstd::memcpy(input_tensor.data(), input_data.data(), input_data.size() * sizeof(float));\n\ninfer_request.infer();\n\nov::Tensor output_tensor = infer_request.get_output_tensor();\nconst float* output_data = output_tensor.data<const float>();\n```\n\n----------------------------------------\n\nTITLE: Tokenize and Prepare Inputs for Inference\nDESCRIPTION: Tokenizes input text using the compiled OpenVINO tokenizer and prepares it for inference.  It includes creating position IDs and beam indices and reading the EOS token.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ntext_input = [\"Quick brown fox jumped\"]\n\nmodel_input = {name.any_name: output for name, output in tokenizer(text_input).items()}\n\nif \"position_ids\" in (input.any_name for input in infer_request.model_inputs):\n   model_input[\"position_ids\"] = np.arange(model_input[\"input_ids\"].shape[1], dtype=np.int64)[np.newaxis, :]\n\n# no beam search, set idx to 0\nmodel_input[\"beam_idx\"] = np.array([0], dtype=np.int32)\n# end of sentence token is where the model signifies the end of text generation\n# read EOS token ID from rt_info of tokenizer/detokenizer ov.Model object\neos_token = ov_tokenizer.get_rt_info(EOS_TOKEN_ID_NAME).value\n```\n\n----------------------------------------\n\nTITLE: Transpose Layer Example 2 XML\nDESCRIPTION: This example demonstrates a Transpose layer in XML format where the input_order is an empty 1D tensor. When input_order is empty, the axes of the input tensor are inverted. The input tensor with shape [2, 3, 4] becomes [4, 3, 2] after the transpose operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/transpose-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Transpose\">\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>0</dim> <!-- input_order is an empty 1D tensor -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>4</dim>\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Custom Operation Class Definition (C++)\nDESCRIPTION: Defines a CustomOperation class with two attributes, `attr1` and `attr2`. This class is used to demonstrate attribute mapping in OpenVINO frontend extensions, where attributes from a framework operation are mapped to corresponding attributes in the OpenVINO operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/frontend-extensions.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nclass CustomOperation : public ov::op::Op {\npublic:\n    OPENVINO_OP(\"CustomOperation\");\n    CustomOperation() = default;\n    CustomOperation(const OutputVector& args, int64_t attr1, float attr2);\n\n    void validate_and_infer_types() override;\n    std::shared_ptr<Node> clone_with_new_inputs(const OutputVector& new_args) const override;\n    bool visit_attributes(AttributeVisitor& visitor) override;\n\nprivate:\n    int64_t m_attr1;\n    float m_attr2;\n};\n```\n\n----------------------------------------\n\nTITLE: CTCGreedyDecoder XML Layer Definition\nDESCRIPTION: This XML snippet defines a CTCGreedyDecoder layer with specific input and output dimensions, as well as the ctc_merge_repeated attribute set to true. It shows how to configure the layer within an OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/ctc-greedy-decoder-1.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"CTCGreedyDecoder\" ...>\n    <data ctc_merge_repeated=\"true\" />\n    <input>\n        <port id=\"0\">\n            <dim>20</dim>\n            <dim>8</dim>\n            <dim>128</dim>\n       </port>\n        <port id=\"1\">\n            <dim>20</dim>\n            <dim>8</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>20</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n       </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Find IntelSYCL and add Compiler Flags (Intel LLVM)\nDESCRIPTION: This snippet finds the IntelSYCL package when the compiler is Intel LLVM.  It adds compiler flags to disable warnings as errors on Windows during the find_package operation, and then re-enables them. It also disables ignored-attributes warnings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(OV_COMPILER_IS_INTEL_LLVM)\n    # For windows we need to disable warning as error option to make FindSYCL.cmake work\n    if (WIN32)\n        ov_add_compiler_flags(/WX-)\n    endif()\n\n    find_package(IntelSYCL REQUIRED)\n\n    if (WIN32)\n        ov_add_compiler_flags(/WX)\n        ov_add_compiler_flags(/Wno-ignored-attributes)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: OVMS Documentation Directory Check (CMake)\nDESCRIPTION: This snippet checks if the `ENABLE_OVMS` option is enabled and if the `OVMS_DOCS_DIR` variable is empty. If both conditions are true, it issues a fatal error message, indicating that the user must provide a path to the model server documentation directory when building OVMS.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(${ENABLE_OVMS} AND (OVMS_DOCS_DIR STREQUAL \"\"))\n    message( FATAL_ERROR \"You want to build OVMS, but OVMS_DOCS_DIR variable is empty.\" )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Color Conversion Python\nDESCRIPTION: Shows how to reverse color channels from RGB to BGR (or vice versa) using the OpenVINO preprocessing API in Python. It requires specifying the source color format in the tensor section and performing a `convert_color` pre-processing operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n# 1) Set input tensor information. We apply preprocessing\n#    demanding input tensor to have 'bgr' color format\nppp.input().tensor().set_element_type(ov.Type.u8).set_color_format(ov.ColorFormat.BGR)\n# 2) Adding explicit preprocessing steps. Model expects 'rgb' format\nppp.input().preprocess().convert_color(ov.ColorFormat.RGB)\n# 3) Apply preprocessing modifying the original model\nmodel = ppp.build()\n```\n\n----------------------------------------\n\nTITLE: Replacing a Node Manually (C++)\nDESCRIPTION: This code snippet demonstrates an alternative method for replacing a node using port methods. It performs the same replacement as the previous example, replacing a Negative operation with a Multiply operation, but without using the `ov::replace_node` helper function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov:manual_replace]\nauto neg = std::make_shared<op::v0::Negative>(input);\nauto mul_constant = ov::op::v0::Constant::create(input->get_element_type(), input->get_shape(), { -1 });\nauto mul = std::make_shared<op::v1::Multiply>(neg, mul_constant);\nneg->output(0).replace_destination(mul->input_value(0));\n//! [ov:manual_replace]\n```\n\n----------------------------------------\n\nTITLE: Get default RemoteContext from CompiledModel (C++)\nDESCRIPTION: This snippet shows how to retrieve the default `ov::RemoteContext` from a compiled OpenVINO model (`ov::CompiledModel`) using the GPU plugin's C++ API. Requires the OpenVINO runtime and intel_gpu ocl headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n//! [default_context_from_model]\n#include <openvino/runtime/core.hpp>\n#include <openvino/runtime/intel_gpu/ocl/ocl.hpp>\n\nvoid get_default_context_from_model(const ov::CompiledModel& model) {\n    // default context from compiled model\n    auto remote_context = model.get_context().as<ov::intel_gpu::ocl::ocl_context>();\n    (void)remote_context;\n}\n//! [default_context_from_model]\n```\n\n----------------------------------------\n\nTITLE: Patching OpenVINO Version for ONNX Runtime Build (Optional)\nDESCRIPTION: This command is an optional step to patch the OpenVINO version for building ONNX Runtime with a non-release OpenVINO version. It creates or modifies the 'version.txt' file in the OpenVINO deployment directory to contain the version number.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_13\n\nLANGUAGE: Shell\nCODE:\n```\ncd <OV_INSTALL_DIR>/deployment_tools/openvino && touch version.txt && echo \"2021.4\" > version.txt\n```\n\n----------------------------------------\n\nTITLE: Upgrade PIP (Windows)\nDESCRIPTION: This command upgrades pip, wheel, and setuptools to the latest versions within the activated virtual environment. Upgrading pip ensures that the latest features and bug fixes are available for package management.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\npython -m pip install --upgrade pip wheel setuptools\n```\n\n----------------------------------------\n\nTITLE: Adding Additional DNNL Include Directories\nDESCRIPTION: This adds more DNNL include directories, specifically those containing common, cpu, and general include files, to the target's include path.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} SYSTEM PRIVATE\n    $<TARGET_PROPERTY:dnnl,SOURCE_DIR>/src/common\n    $<TARGET_PROPERTY:dnnl,SOURCE_DIR>/src/cpu\n    $<TARGET_PROPERTY:dnnl,SOURCE_DIR>/include)\n```\n\n----------------------------------------\n\nTITLE: Pull Request Description Template (External Contributors)\nDESCRIPTION: This is a template for external contributors to use when creating a pull request for the OpenVINO™ Python API.  It includes sections for details of the changes and requirements that have been introduced or changed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/contributing.md#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nDetails:\n...\n\nRequirements introduced/changed:       <-- only if applicable\n...\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO GenAI from Archive - Ubuntu 20.04\nDESCRIPTION: These commands download and extract the OpenVINO GenAI archive for Ubuntu 20.04. The `curl` command downloads the tarball, and the `tar` command extracts its contents.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-genai.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino_genai/packages/2025.1/linux/openvino_genai_ubuntu20_2025.1.0.0_x86_64.tar.gz  --output openvino_genai_2025.1.0.0.tgz\ntar -xf openvino_genai_2025.1.0.0.tgz\n```\n\n----------------------------------------\n\nTITLE: Installing Directories\nDESCRIPTION: This snippet installs several directories to specific locations within the install prefix for the 'tests' component. It excludes these directories from the 'ALL' target, meaning they are only installed when the 'tests' component is explicitly installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY test_runner/ DESTINATION tests/time_tests/test_runner COMPONENT tests EXCLUDE_FROM_ALL)\ninstall(DIRECTORY .automation/ DESTINATION tests/time_tests/test_runner/.automation COMPONENT tests EXCLUDE_FROM_ALL)\ninstall(DIRECTORY scripts/ DESTINATION tests/time_tests/scripts COMPONENT tests EXCLUDE_FROM_ALL)\ninstall(DIRECTORY ../utils/ DESTINATION tests/utils COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Set Include Directories for OpenVINO Proxy Plugin\nDESCRIPTION: This snippet sets the include directories for the OpenVINO proxy plugin. It adds the `PUBLIC_HEADERS_DIR` as a public interface directory, making the headers available to other targets that link to this library. It also adds the `src` directory and the interface include directories of the `openvino::runtime::dev` target as private include directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}\n    PUBLIC\n        $<BUILD_INTERFACE:${PUBLIC_HEADERS_DIR}>\n    PRIVATE\n        ${CMAKE_CURRENT_SOURCE_DIR}/src\n        $<BUILD_INTERFACE:$<TARGET_PROPERTY:openvino::runtime::dev,INTERFACE_INCLUDE_DIRECTORIES>>)\n```\n\n----------------------------------------\n\nTITLE: Checking Protobuf Version\nDESCRIPTION: This snippet checks the Protobuf version.  If the version is less than 3.9, it raises a fatal error.  This ensures that the minimum required version of Protobuf is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/protobuf/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(protobuf_VERSION VERSION_LESS 3.9)\n    message(FATAL_ERROR \"Minimum supported version of protobuf-lite library is 3.9.0 (provided ${protobuf_VERSION})\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: DFT Layer XML Definition (signal_size, 4D input)\nDESCRIPTION: Defines a DFT layer in XML format with the signal_size input for a 4-dimensional input tensor. The example illustrates how to specify the signal size along the specified axes to control the output shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/dft-7.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"DFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- axes input contains [1, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>2</dim> <!-- signal_size input contains [512, 100] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>512</dim>\n            <dim>100</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining dGPU Runner for Tests in YAML\nDESCRIPTION: This YAML snippet demonstrates how to specify a self-hosted runner with a dedicated GPU (dGPU) using the `dgpu` label. Jobs using this configuration will execute on runners equipped with dGPUs. This selection ensures that the job has access to a discrete GPU for computationally intensive tasks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/runners.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nGPU:\n    name: GPU Tests\n    needs: [ Build, Smart_CI ]\n    runs-on: [ self-hosted, dgpu ]\n  ...\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files CMake\nDESCRIPTION: Uses `file(GLOB_RECURSE)` to collect all `.cpp`, `.hpp`, and `.h` files in the current source directory and its subdirectories. The collected files are stored in the `SOURCES` variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/utils/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES \"*.cpp\" \"*.hpp\" \"*.h\")\n```\n\n----------------------------------------\n\nTITLE: GRN Layer XML Configuration in OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of a GRN layer within an OpenVINO model. It showcases the specification of the 'bias' attribute and the input/output port definitions, including dimensions and precision.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/grn-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"GRN\">\n    <data bias=\"1e-4\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>20</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"f32\">\n            <dim>1</dim>\n            <dim>20</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Compiling Model Directly\nDESCRIPTION: This snippet shows how to compile a model directly from a model path, skipping the explicit `read_model` step in Python or C++. It initializes the OpenVINO Core object and compiles the model using the specified device and configuration, including the cache directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n/// [ov:caching:part1]\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n\n    std::string model_path = \"path_to_model.xml\";\n    std::string cache_dir = \"/path/to/cache/dir\";\n\n    ov::CompiledModel compiled_model = core.compile_model(model_path, \"CPU\", {\n        {ov::cache_dir(), cache_dir}\n    });\n\n    return 0;\n}\n/// [ov:caching:part1]\n```\n\n----------------------------------------\n\nTITLE: Running Specific Python OpenModelZoo Test (Build Layout)\nDESCRIPTION: This command runs a specific Python OpenModelZoo test using pytest in the build layout.  It targets a particular AlexNet model.  Replace <OV_REPO_DIR> with the OpenVINO repository directory and <ONNX_MODELS_DIR> with the directory containing the ONNX models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\npytest --backend=CPU <OV_REPO_DIR>/src/bindings/python/tests/test_onnx/test_zoo_models.py -v -n 4 --forked -k 'not _cuda' --model_zoo_dir=<ONNX_MODELS_DIR> -k test_onnx_model_zoo_vision_classification_alexnet_model_bvlcalexnet_9_bvlc_alexnet_model_cpu\n```\n\n----------------------------------------\n\nTITLE: Including Subdirectories in CMake\nDESCRIPTION: This snippet uses the `add_subdirectory` command in CMake to include the 'unit' and 'functional' directories into the project's build process. This will process the CMakeLists.txt file in each subdirectory and incorporate them into the current build context. No specific dependencies are mentioned, but 'unit' and 'functional' directories must exist and contain valid CMakeLists.txt files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(unit)\nadd_subdirectory(functional)\n```\n\n----------------------------------------\n\nTITLE: Compiler Options for Selective Build\nDESCRIPTION: This snippet adds compiler options to suppress warnings related to unused functions, variables, parameters, and local typedefs when using GCC or Clang. Also adds option to suppress unused-but-set-variable warnings if supported.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG)\n        # After disabling a block of code, some variables might be unused.\n        target_compile_options(${TARGET_NAME} INTERFACE\n                                -Wno-unused-function\n                                -Wno-unused-variable\n                                -Wno-unused-parameter\n                                -Wno-unused-local-typedefs)\n    endif()\n    if(UNUSED_BUT_SET_VARIABLE_SUPPORTED)\n        target_compile_options(${TARGET_NAME} INTERFACE -Wno-unused-but-set-variable)\n    endif()\n```\n\n----------------------------------------\n\nTITLE: Setting Execution Mode in OpenVINO (Python)\nDESCRIPTION: This snippet demonstrates how to set the `execution_mode` hint in Python to control the trade-off between accuracy and performance during inference.  It configures the OpenVINO Core object to use either `ACCURACY` or `PERFORMANCE` mode depending on the desired outcome.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/precision-control.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.runtime import Core, properties, Hint\n\ncore = Core()\n\n#  Get supported properties\nprint(core.get_property(\"CPU\", properties.supported_properties))\n\n#  Set execution mode to ACCURACY\ncore.set_property(\"CPU\", {Hint.execution_mode(): Hint.ExecutionMode.ACCURACY})\n\n#  Set execution mode to PERFORMANCE\ncore.set_property(\"CPU\", {Hint.execution_mode(): Hint.ExecutionMode.PERFORMANCE})\n```\n\n----------------------------------------\n\nTITLE: Conditional Dependencies in CMake\nDESCRIPTION: This snippet conditionally appends dependencies and compile definitions based on whether certain features are enabled (ENABLE_AUTO_BATCH, ENABLE_HETERO, ENABLE_OV_IR_FRONTEND).  It uses `list(APPEND)` to add to the `DEPENDENCIES` and `COMPILE_DEFINITIONS` variables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_AUTO_BATCH)\n    list(APPEND DEPENDENCIES openvino_auto_batch_plugin)\n    list(APPEND COMPILE_DEFINITIONS ENABLE_AUTO_BATCH)\nendif()\n\nif(ENABLE_HETERO)\n    list(APPEND DEPENDENCIES openvino_hetero_plugin)\n    list(APPEND COMPILE_DEFINITIONS HETERO_ENABLED)\nendif()\n\nif(ENABLE_OV_IR_FRONTEND)\n    list(APPEND DEPENDENCIES openvino_ir_frontend)\n    list(APPEND COMPILE_DEFINITIONS IR_FRONTEND_ENABLED)\nendif()\n```\n\n----------------------------------------\n\nTITLE: RMS Normalization Formula in Python\nDESCRIPTION: This Python code snippet represents the mathematical formula for RMS normalization. It calculates the normalized output by dividing the input tensor 'x' by the square root of the mean of squared values along the last dimension, adding a small epsilon value for numerical stability, and then multiplying by a scaling factor 'scale'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/internal/rms.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\n(x / Sqrt(ReduceMean(x^2, -1) + eps)) * scale\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties and Include Directories (CMake)\nDESCRIPTION: This snippet sets target properties such as include directories (using the `INTERFACE_INCLUDE_DIRECTORIES` from `protobuf::libprotobuf`), interprocedural optimization, and compile features (using `INTERFACE_COMPILE_FEATURES` from `protobuf::libprotobuf`). It configures the library to work correctly with the protobuf library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common/shutdown_protobuf/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} SYSTEM PRIVATE\n    $<BUILD_INTERFACE:$<TARGET_PROPERTY:protobuf::libprotobuf,INTERFACE_INCLUDE_DIRECTORIES>>)\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\ntarget_compile_features(${TARGET_NAME} PRIVATE $<TARGET_PROPERTY:protobuf::libprotobuf,INTERFACE_COMPILE_FEATURES>)\n```\n\n----------------------------------------\n\nTITLE: JSON Configuration Example - Meta Device Properties\nDESCRIPTION: This JSON snippet demonstrates how to configure properties for a meta-device (AUTO/MULTI) using a JSON file. It includes setting the `PERFORMANCE_HINT`, `PERF_COUNT`, and `DEVICE_PROPERTIES` for CPU and GPU, specifying inference precision and number of streams for each.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n                                                     \"AUTO\": {\n                                                             \"PERFORMANCE_HINT\": \"THROUGHPUT\",\n                                                             \"PERF_COUNT\": \"NO\",\n                                                             \"DEVICE_PROPERTIES\": \"{CPU:{INFERENCE_PRECISION_HINT:f32,NUM_STREAMS:3},GPU:{INFERENCE_PRECISION_HINT:f32,NUM_STREAMS:5}}\"\n                                                     }\n                                             }\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINO with PkgConfig\nDESCRIPTION: This snippet attempts to locate the OpenVINO installation using PkgConfig, importing the target if found. It's conditional on PkgConfig being available, not cross-compiling, and the build type being Release. If PkgConfig is not used it falls back to find_package(OpenVINO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/lib/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(PkgConfig_FOUND AND NOT CMAKE_CROSSCOMPILING AND CMAKE_BUILD_TYPE STREQUAL \"Release\")\n    pkg_search_module(openvino REQUIRED\n                      IMPORTED_TARGET\n                      openvino)\n    set(ov_link_libraries PkgConfig::openvino)\nelse()\n    find_package(OpenVINO REQUIRED COMPONENTS Runtime)\n    set(ov_link_libraries openvino::runtime)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Kernel Source and Header File Names\nDESCRIPTION: Sets the names for the generated OpenCL kernel source and header files, which are then used to define the complete paths to these files within the code generation include directory and the cache directory. These files will contain the generated OpenCL kernel code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/ocl_v2/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(KERNEL_SOURCES \"gpu_ocl_kernel_sources.inc\")\nset(KERNEL_HEADERS \"gpu_ocl_kernel_headers.inc\")\n\nset(CODEGEN_CACHE_SOURCES \"${CODEGEN_INCDIR}/${KERNEL_SOURCES}\"\n                          \"${CODEGEN_INCDIR}/${KERNEL_HEADERS}\")\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories\nDESCRIPTION: These snippets add the specified third-party subdirectories to the build. gflags, gtest, and pugixml are added as subprojects. The EXCLUDE_FROM_ALL option prevents these subprojects from being built by default.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/thread_local/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(${OpenVINO_SOURCE_DIR}/thirdparty/gflags\n                 ${CMAKE_CURRENT_BINARY_DIR}/gflags_build\n                 EXCLUDE_FROM_ALL)\nadd_subdirectory(${OpenVINO_SOURCE_DIR}/thirdparty/gtest\n                 ${CMAKE_CURRENT_BINARY_DIR}/gtest_build\n                 EXCLUDE_FROM_ALL)\nadd_subdirectory(${OpenVINO_SOURCE_DIR}/thirdparty/pugixml\n                 ${CMAKE_CURRENT_BINARY_DIR}/pugixml_build\n                 EXCLUDE_FROM_ALL)\nadd_subdirectory(\"${OpenVINO_SOURCE_DIR}/tests/lib\" tests_shared_lib)\n```\n\n----------------------------------------\n\nTITLE: IsFinite Layer with Sample Input/Output\nDESCRIPTION: This XML snippet shows another IsFinite layer example, with a single-dimension input tensor. The comment clarifies the input and expected output values, illustrating the mapping of NaN/Inf to False and finite numbers to True.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/isfinite-10.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IsFinite\">\n    <input>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>4</dim> <!-- Input value is: [NaN, 2.1, 3.7, Inf] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"BOOL\">\n            <dim>4</dim> <!-- Output value is: [False, True, True, False] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: MatMul Vector-Matrix Multiplication with Transpose XML\nDESCRIPTION: Configuration example demonstrating MatMul operation for vector-matrix multiplication with transposition of the second matrix. The `transpose_b` attribute is set to true. The input ports define the dimensions of the input vector (1024) and the transposed input matrix (1000x1024). The output port defines the resulting vector (1000).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/matmul-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MatMul\">\n    <data transpose_b=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1024</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1000</dim>\n            <dim>1024</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1000</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Tokenizers Wheel Package (sh)\nDESCRIPTION: This command installs the OpenVINO Tokenizers wheel package. It's optional and is needed only when testing operations that use conversion and operation extensions from OpenVINO Tokenizers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/layer_tests/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install openvino_tokenizers.whl\n```\n\n----------------------------------------\n\nTITLE: Auto Batch Plugin Activation (CMake)\nDESCRIPTION: This snippet checks if auto-batching is enabled and exits if not.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT ENABLE_AUTO_BATCH)\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Exclude Paths Based on Architecture\nDESCRIPTION: This snippet conditionally excludes source paths based on the target architecture (RISCV64, X86, X86_64, ARM, AARCH64) and enabled features (OV_CPU_WITH_SHL). This allows disabling certain tests based on the target architecture or when specific features are not enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/functional/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT RISCV64)\n    list(APPEND EXCLUDED_SOURCE_PATHS\n         ${CMAKE_CURRENT_SOURCE_DIR}/custom/single_layer_tests/instances/riscv64\n         ${CMAKE_CURRENT_SOURCE_DIR}/utils/riscv64)\nelif(NOT OV_CPU_WITH_SHL)\n    list(APPEND EXCLUDED_SOURCE_PATHS\n         ${CMAKE_CURRENT_SOURCE_DIR}/custom/single_layer_tests/instances/riscv64/shl)\nendif()\n\nif(NOT (X86 OR X86_64))\n    list(APPEND EXCLUDED_SOURCE_PATHS\n         ${CMAKE_CURRENT_SOURCE_DIR}/custom/single_layer_tests/instances/x64\n         ${CMAKE_CURRENT_SOURCE_DIR}/custom/subgraph_tests/src/x64\n         ${CMAKE_CURRENT_SOURCE_DIR}/shared_tests_instances/snippets/x64\n         ${CMAKE_CURRENT_SOURCE_DIR}/utils/x64)\nendif()\n\nif(NOT (ARM OR AARCH64))\n    list(APPEND EXCLUDED_SOURCE_PATHS\n         ${CMAKE_CURRENT_SOURCE_DIR}/custom/single_layer_tests/instances/arm\n         ${CMAKE_CURRENT_SOURCE_DIR}/custom/subgraph_tests/src/arm\n         ${CMAKE_CURRENT_SOURCE_DIR}/shared_tests_instances/snippets/arm\n         ${CMAKE_CURRENT_SOURCE_DIR}/shared_tests_instances/low_precision_transformations/aarch64\n         ${CMAKE_CURRENT_SOURCE_DIR}/utils/arm)\nelse()\n    # temporary disable all custom tests for ARM\n    list(APPEND EXCLUDED_SOURCE_PATHS\n        ${CMAKE_CURRENT_SOURCE_DIR}/custom/single_layer_tests\n        ${CMAKE_CURRENT_SOURCE_DIR}/custom/subgraph_tests)\n    # except ones which already enabled\n    file(GLOB_RECURSE TMP_LIST_OF_TEST_CLASSES          ${CMAKE_CURRENT_SOURCE_DIR}/custom/single_layer_tests/classes/*.cpp)\n    file(GLOB_RECURSE TMP_LIST_OF_COMMON_TEST_INSTANCES ${CMAKE_CURRENT_SOURCE_DIR}/custom/single_layer_tests/instances/common/*.cpp)\n    file(GLOB_RECURSE TMP_LIST_OF_ARM_TEST_INSTANCES    ${CMAKE_CURRENT_SOURCE_DIR}/custom/single_layer_tests/instances/arm/*.cpp)\n    file(GLOB_RECURSE TMP_LIST_OF_ARM_SUBGRAPH_TESTS    ${CMAKE_CURRENT_SOURCE_DIR}/custom/subgraph_tests/src/arm/*.cpp)\n    file(GLOB_RECURSE TMP_LIST_OF_COMMON_SUBGRAPH_TESTS ${CMAKE_CURRENT_SOURCE_DIR}/custom/subgraph_tests/src/common/*.cpp)\n    file(GLOB_RECURSE TMP_LIST_OF_SUBGRAPH_TEST_CLASSES ${CMAKE_CURRENT_SOURCE_DIR}/custom/subgraph_tests/src/classes/*.*)\n\n    list(APPEND TMP_LIST_OF_EXPLICITLY_ENABLED_TESTS\n        ${TMP_LIST_OF_TEST_CLASSES} ${TMP_LIST_OF_COMMON_TEST_INSTANCES} ${TMP_LIST_OF_ARM_TEST_INSTANCES} ${TMP_LIST_OF_ARM_SUBGRAPH_TESTS} ${TMP_LIST_OF_COMMON_SUBGRAPH_TESTS} ${TMP_LIST_OF_SUBGRAPH_TEST_CLASSES})\n    set(TMP_EXPLICITLY_ENABLED_TESTS \"${TMP_LIST_OF_EXPLICITLY_ENABLED_TESTS}\")\nendif()\n\nif(NOT X86_64)\n    list(APPEND EXCLUDED_SOURCE_PATHS\n         ${CMAKE_CURRENT_SOURCE_DIR}/custom/single_layer_tests/instances/x64\n         ${CMAKE_CURRENT_SOURCE_DIR}/custom/subgraph_tests/src/x64\n         ${CMAKE_CURRENT_SOURCE_DIR}/shared_tests_instances/low_precision_transformations/x64)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties (IPO)\nDESCRIPTION: Sets the Interprocedural Optimization (IPO) property for the release build of the target library, enabling link-time optimization.  It uses the `ENABLE_LTO` variable to determine if IPO should be enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/common/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: setLayout Method Definition TypeScript\nDESCRIPTION: Defines the setLayout method, part of the InputModelInfo interface, accepting a layout string as input.  This method returns an instance of InputModelInfo, allowing for method chaining. It's used for configuring input model layouts.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InputModelInfo.rst#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nsetLayout(layout): InputModelInfo\n```\n\n----------------------------------------\n\nTITLE: AVX2/AVX512F Optimization Flags\nDESCRIPTION: This snippet conditionally adds AVX2 and AVX512F optimization flags and definitions based on the `ENABLE_AVX2` and `ENABLE_AVX512F` flags. If enabled, it calls `ov_avx2_optimization_flags` and `ov_avx512_optimization_flags` to get the flags, and appends them to the `compile_flags` and `definitions` lists.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/vectorized/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(X86_64)\n    if(ENABLE_AVX2)\n        ov_avx2_optimization_flags(avx2_flags)\n        list(APPEND compile_flags ${avx2_flags})\n        list(APPEND definitions HAVE_AVX2)\n    endif()\n    if(ENABLE_AVX512F)\n        ov_avx512_optimization_flags(avx512_flags)\n        list(APPEND compile_flags ${avx512_flags})\n        list(APPEND definitions HAVE_AVX512F)\n    endif()\n\n    target_compile_options(${TARGET_NAME} PRIVATE \"${compile_flags}\")\n    target_compile_definitions(${TARGET_NAME} PRIVATE ${definitions})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Threading Interface in CMake\nDESCRIPTION: This line sets the threading interface for the specified target, `ov_auto_unit_tests`. The threading interface likely influences how the application handles concurrent execution and parallel processing.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/unit/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Shape with Defined Rank and Range Dimensions C++\nDESCRIPTION: These code snippets illustrate how to create PartialShape objects with defined rank and dimensions having a specified range in OpenVINO using C++.\nThey initialize `ov::PartialShape` objects where some dimensions are dynamic within a given interval.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/docs/shape_propagation.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nov::PartialShape({{1, 8}, 3, 400, 400}); // rank == 4, first dimension is dynamic and can be any number from 1 to 8 inclusive, all the other dimensions are static\nov::PartialShape({{2, -1}, 3, 400, 400}); // rank == 4, first dimension is dynamic and can be any number larger or equal 2, all the other dimensions are static\nov::PartialShape({{-1, 8}, 3, 400, 400}); // rank == 4, first dimension is dynamic and will not be larger than 8, all the other dimensions are static\n```\n\n----------------------------------------\n\nTITLE: Adding MSVC Compiler Flags\nDESCRIPTION: This snippet conditionally adds the compiler flag '/wd4305' for MSVC compilers to suppress specific conversion warnings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    ov_add_compiler_flags(/wd4305)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Library CMake\nDESCRIPTION: Creates a static library named `${TARGET_NAME}` (which is `ie_samples_utils`) from the collected source files. The `EXCLUDE_FROM_ALL` option prevents it from being built by default when building the entire project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/utils/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC EXCLUDE_FROM_ALL ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions in CMake\nDESCRIPTION: This snippet defines compile definitions for the proxy plugin tests. These definitions are used to pass information to the compiler during the build process, such as the CI build number.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(COMPILE_DEFINITIONS \"CI_BUILD_NUMBER=\\\"mock_version\\\"\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Static Library with CMake\nDESCRIPTION: This snippet defines a CMake target to build a static library named `fuzz-testhelper`. It sets the source files, include directories, and compiler definitions. The `ENABLE_FUZZING` variable controls whether the `WITH_LIBFUZZER` definition is added.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/fuzz-testhelper/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME fuzz-testhelper)\n\nfile(\n    GLOB SRC_FILES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cc)\n\nadd_library(\n    ${TARGET_NAME} STATIC\n    ${SRC_FILES})\n\ntarget_include_directories(${TARGET_NAME} PUBLIC \"${CMAKE_CURRENT_SOURCE_DIR}\")\n\nif(ENABLE_FUZZING)\n    target_compile_definitions(${TARGET_NAME} PRIVATE WITH_LIBFUZZER)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Show Help for Aggregate Average Counters\nDESCRIPTION: This command displays the help message for the `aggregate-average-counters.py` script, providing information on available options and usage instructions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/aggregate-average-counters/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\naggregate-average-counters.py --help\n```\n\n----------------------------------------\n\nTITLE: Interpolate Calculation Class Initialization\nDESCRIPTION: Initializes the `InterpolateCalculation` class with attributes from a dictionary. It sets the interpolation mode, function, pads, coordinate transformation mode, nearest mode, cube coefficient, antialias flag, shape calculation mode, and functions for coordinate transformation and nearest pixel calculation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass InterpolateCalculation:\n    def __init__(self, attrs: dict):\n        self.mode = attrs['mode']\n        self.func = {\n            'nearest': self.nearest_interpolation,\n            'linear': self.linear_interpolation,\n            'cubic': self.cubic_interpolation,\n            'linear_onnx': self.onnx_linear_interpolation\n        }[self.mode]\n        self.attrs = attrs\n\n        self.pads_begin = attrs.get('pads_begin', [0])\n        self.pads_end = attrs.get('pads_end', [0])\n        self.coordinate_transformation_mode = attrs.get('coordinate_transformation_mode', 'half_pixel')\n        self.nearest_mode = attrs.get('nearest_mode', 'round_prefer_floor')\n        self.cube_coeff = attrs.get('cube_coeff', -0.75)\n        self.antialias = attrs.get('antialias', False)\n\n        self.shape_calculation_mode = {\n            'sizes': ShapeCalculationMode.SIZES,\n            'scales': ShapeCalculationMode.SCALES\n        }[attrs['shape_calculation_mode']]\n\n        self.get_original_coordinate = self.get_coordinate_transformation_mode()\n        self.get_nearest_pixel = GetNearestPixel(self.nearest_mode)\n```\n\n----------------------------------------\n\nTITLE: SegmentMax with 2D input data and no num_segments in OpenVINO XML\nDESCRIPTION: This example demonstrates the SegmentMax operation with a 2D input `data` tensor and without specifying the optional `num_segments` input.  The number of segments is implicitly determined from the `segment_ids` tensor.  The `fill_mode` is set to `LOWEST`, so empty segments are filled with the lowest possible value for the data type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/segment-max-16.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"SegmentMax\" ... >\n    <data empty_segment_value=\"LOWEST\">\n        <input>\n            <port id=\"0\" precision=\"I32\">   <!-- data -->\n                <dim>3</dim>\n                <dim>4</dim>\n            </port>\n            <port id=\"1\" precision=\"I64\">   <!-- segment_ids with 2 segments: [0, 1, 1] -->\n                <dim>3</dim>\n            </port>\n        </input>\n        <output>\n            <port id=\"2\" precision=\"I32\">\n                <dim>2</dim>\n                <dim>4</dim>\n            </port>\n        </output>\n    </layer>\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example 2 (valid padding)\nDESCRIPTION: This example demonstrates how MaxPool operates with a 3D input, a 1D kernel, and `auto_pad` set to 'valid'. It showcases the input data, strides, kernel size, and the resulting output shapes and values after the MaxPool operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/pooling_shape_rules.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[-1, 2, 3, 5, -7, 9, 1]]]   # shape: (1, 1, 7)\n      strides = [1]\n      kernel = [3]\n      rounding_type = \"floor\"\n      auto_pad = \"valid\"\n      output0 = [[[3, 5, 5, 9, 9]]]   # shape: (1, 1, 5)\n      output1 = [[[2, 3, 3, 5, 5]]]   # shape: (1, 1, 5)\n```\n\n----------------------------------------\n\nTITLE: PaddleDetection Version Detection and Download\nDESCRIPTION: This snippet determines the PaddleDetection version based on the installed PaddlePaddle version and downloads the appropriate `ops.py` file. It first fetches the PaddlePaddle version using Python. Then, based on the version, it sets the URL and SHA256 checksum for the PaddleDetection `ops.py` file and downloads it using the `DownloadAndCheck` function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(paddlepaddle_FOUND)\n    execute_process(\n        COMMAND ${Python3_EXECUTABLE} -c \"import paddle; print(paddle.__version__)\"\n        OUTPUT_VARIABLE PADDLE_VERSION\n        OUTPUT_STRIP_TRAILING_WHITESPACE\n    )\n    message(STATUS \"PaddlePaddle version: ${PADDLE_VERSION}\")\n    \n    if(PADDLE_VERSION VERSION_GREATER_EQUAL \"2.6.0\" OR PADDLE_VERSION VERSION_EQUAL \"0.0.0\")\n        set(PADDLEDET_OPS_URL \"https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.5/ppdet/modeling/ops.py\")\n        set(PADDLEDET_OPS_SHA256 \"e3da816421698ee97bb272c4410a03c300ab92045b7c87cccb9e52a8c18bc088\")\n        set(PADDLEDET_DIRNAME ${CMAKE_CURRENT_BINARY_DIR}/thirdparty/PaddleDetection/release25/ppdet/modeling/)\n    else()\n        set(PADDLEDET_OPS_URL \"https://raw.githubusercontent.com/PaddlePaddle/PaddleDetection/release/2.1/ppdet/modeling/ops.py\")\n        set(PADDLEDET_OPS_SHA256 \"5cc079eda295ed78b58fba8223c51d85a931a7069ecad51c6af5c2fd26b7a8cb\")\n        set(PADDLEDET_DIRNAME ${CMAKE_CURRENT_BINARY_DIR}/thirdparty/PaddleDetection/release21/ppdet/modeling/)\n    endif()\n    \n    DownloadAndCheck(${PADDLEDET_OPS_URL} ${PADDLEDET_DIRNAME}/ops.py PADDLEDET_FATAL PADDLEDET_RESULT ${PADDLEDET_OPS_SHA256})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for Documentation Build\nDESCRIPTION: This command configures the CMake build with the necessary paths to LaTeX, Graphviz, and Doxygen executables on Windows.  It also enables the documentation build using the ENABLE_DOCS flag. The paths need to match the installation locations of these dependencies. This step is essential for building the 'openvino_docs' target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/building_documentation.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DLATEX_COMPILER=\"C:/Program Files/MiKTeX/miktex/bin/x64/latex.exe\" \\\n      -DDOXYGEN_DOT_EXECUTABLE=\"C:/Program Files (x86)/Graphviz2.38/bin/dot.exe\" \\\n      -DDOXYGEN_EXECUTABLE=\"C:/Program Files/doxygen/bin/doxygen.exe\" \\\n      -DENABLE_DOCS=ON \\\n```\n\n----------------------------------------\n\nTITLE: Adding cpplint Target\nDESCRIPTION: This snippet adds a cpplint target for the `openvino_intel_gpu_runtime` library, enabling code style checking using cpplint.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nadd_cpplint_target(${TARGET_NAME}_cpplint FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Minimum-1 XML example with numpy broadcasting\nDESCRIPTION: This XML snippet shows the Minimum-1 operation using 'numpy' broadcasting. It demonstrates how input tensors with different shapes are automatically broadcasted to a common shape before the minimum operation is performed. The example uses input tensors of shapes 8x1x6x1 and 7x1x5, resulting in an output tensor of shape 8x7x6x5.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/minimum-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Minimum\">\n    <data auto_broadcast=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting environment variables on Windows (PowerShell)\nDESCRIPTION: This snippet shows how to set environment variables in Windows PowerShell before running `benchmark_app`. This allows enabling debug options similar to the Linux example.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_debug_utils.md#_snippet_1\n\nLANGUAGE: PowerShell\nCODE:\n```\n$env:OV_VERBOSE=1\n.\\benchmark_app.exe ...      # Run benchmark_app with OV_VERBOSE option\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: This snippet uses target_include_directories to add an include directory to the target library. The PUBLIC keyword makes the include directory available to other targets that link against this library. The expression \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\" ensures that the include directory is only used when building the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/common/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\")\n```\n\n----------------------------------------\n\nTITLE: LogicalAnd XML Example - No Broadcast\nDESCRIPTION: This XML snippet demonstrates the LogicalAnd operation with two input tensors of the same shape, meaning no broadcasting is required.  The inputs and output ports are defined with their dimensions. The type attribute in the layer indicates the kind of operation being performed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/logical/logical-and-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"LogicalAnd\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Gather-7 Example with batch_dims=2 in shell\nDESCRIPTION: Shows Gather-7 with batch_dims set to 2 and axis to 2. This example demonstrates how indices are applied to multiple batches, resulting in the corresponding output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-7.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 2\naxis = 2\n\nindices = [[[0, 0, 4],  <-- this is applied to the first batch, index = (0, 0)\n            [4, 0, 0]],  <-- this is applied to the second batch, index = (0, 1)\n\n           [[1, 2, 4],  <-- this is applied to the third batch, index = (1, 0)\n            [4, 3, 2]]]  <-- this is applied to the fourth batch, index = (1, 1)\nindices_shape = (2, 2, 3)\n\ndata    = [[[1, 2, 3, 4, 5],  <-- the first batch, index = (0, 0)\n            [6, 7, 8, 9, 10]],  <-- the second batch, index = (0, 1)\n\n           [[11, 12, 13, 14, 15],  <-- the third batch, index = (1, 0)\n            [16, 17, 18, 19, 20]]]  <-- the fourth batch, index = (1, 1)\ndata_shape = (2, 2, 5)\n\noutput  = [[[ 1, 1, 5],\n            [10, 6, 6]],\n\n           [[12, 13, 15],\n            [20, 19, 18]]]\noutput_shape = (2, 2, 3)\n```\n\n----------------------------------------\n\nTITLE: SliceScatter Example 3: Update every second value on axes\nDESCRIPTION: This XML code demonstrates an example of using SliceScatter to update every second value over both axes with different slice starts. The data tensor has shape (3, 5), the updates tensor has shape (2, 2). The slice is defined by start=[0, 1], stop=[3, 5], step=[2, 2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-scatter-15.rst#_snippet_3\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"SliceScatter\">\n    <input>\n        <port id=\"0\" precision=\"FP32\">  <!-- data -->\n            <dim>3</dim>\n            <dim>5</dim>  <!-- values: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14]] -->\n        </port>\n        <port id=\"1\" precision=\"FP32\">  <!-- updates -->\n            <dim>2</dim>\n            <dim>2</dim>  <!-- values: [[50, 60], [70, 80]] -->\n        </port>\n        <port id=\"2\" precision=\"I32\">  <!-- start -->\n            <dim>1</dim>  <!-- values: [0, 1] -->\n        </port>\n        <port id=\"3\" precision=\"I32\">  <!-- stop -->\n            <dim>1</dim>  <!-- values: [3, 5] -->\n        </port>\n        <port id=\"4\" precision=\"I32\">  <!-- step -->\n            <dim>1</dim>  <!-- values: [2, 2] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"5\" precision=\"FP32\">\n            <dim>3</dim>\n            <dim>5</dim>  <!-- values: [[0, 50, 2, 60, 4], [5, 6, 7, 8, 9], [10, 70, 12, 80, 14]] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Building with static MSVC Runtime with CMake (sh)\nDESCRIPTION: This snippet shows how to build OpenVINO with a static MSVC runtime using a special CMake toolchain file.  The toolchain file ensures that all dependent libraries are built with a compatible MSVC runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/static_libaries.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_TOOLCHAIN_FILE=<openvino source dir>/cmake/toolchains/mt.runtime.win32.toolchain.cmake <other options>\n```\n\n----------------------------------------\n\nTITLE: Cosh Layer XML Configuration in OpenVINO\nDESCRIPTION: Defines a Cosh layer configuration using XML in OpenVINO. It specifies the input and output tensor dimensions for the Cosh operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/cosh-1.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"Cosh\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Globbing Library Sources in CMake\nDESCRIPTION: This snippet uses the `file(GLOB_RECURSE)` command to find all C++ source files (`*.cpp`) within the `src` directory and its subdirectories. The results are stored in the `LIBRARY_SRC` variable for later use in creating the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific File Handling (Windows) in CMake\nDESCRIPTION: This conditional block handles platform-specific file inclusion. On Windows, it removes Linux-specific files from the `LIBRARY_SRC` variable. It uses `file(GLOB_RECURSE)` to find Linux-specific files and then `list(REMOVE_ITEM)` to exclude them.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (WIN32)\n    # Remove linux specific files\n    file(GLOB_RECURSE LIN_FILES ${CMAKE_CURRENT_SOURCE_DIR}/src/os/lin/*.cpp\n                                ${CMAKE_CURRENT_SOURCE_DIR}/src/os/lin/*.hpp)\n    list(REMOVE_ITEM LIBRARY_SRC ${LIN_FILES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: EmbeddingBagPackedSum XML Example\nDESCRIPTION: This XML snippet demonstrates the structure and usage of the EmbeddingBagPackedSum operation within an OpenVINO model. It defines the input ports for the embedding table, indices, and per-sample weights, along with the output port for the resulting embeddings. The example showcases the shapes and example values for each tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sparse/embedding-bag-packed-sum-3.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"EmbeddingBagPackedSum\" ... >\n    <input>\n        <port id=\"0\">     <!-- emb_table value is: [[-0.2, -0.6], [-0.1, -0.4], [-1.9, -1.8], [-1.,  1.5], [ 0.8, -0.7]] -->\n            <dim>5</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">     <!-- indices value is: [[0, 2], [1, 2], [3, 4]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"2\"/>    <!-- per_sample_weights value is: [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">     <!-- output value is: [[-1.05, -1.2], [-1., -1.1], [-0.1, 0.4]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Disable FP16 Compression with OVC CLI\nDESCRIPTION: This command demonstrates how to disable FP16 compression when converting a model to the OpenVINO Intermediate Representation (IR) format using the `ovc` command-line tool. The `--compress_to_fp16=False` argument prevents the weights from being compressed to FP16 during the conversion process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/conversion-parameters.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\novc path_to_your_model --compress_to_fp16=False\n```\n\n----------------------------------------\n\nTITLE: Adding Library and Alias (CMake)\nDESCRIPTION: This snippet creates a static library named `openvino_tensorflow_common` from the source and header files found in the previous steps. It also creates an alias `openvino::frontend::tensorflow_common` for easier referencing of the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${LIBRARY_SRC} ${LIBRARY_HEADERS})\nadd_library(openvino::frontend::tensorflow_common ALIAS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Install Time Tests\nDESCRIPTION: This snippet installs the built time tests to a specified installation path. It requires the `<build_dir>` and `<install_path>` to be replaced with actual directory paths.  This command uses cmake's install functionality.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nсmake --install <build_dir> --prefix <install_path>\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files in CMake\nDESCRIPTION: Uses the `file(GLOB)` command to collect all `.cpp`, `.h`, and `.hpp` files in the current source directory.  This dynamically includes all source and header files into the build process.  This approach can be problematic as new files added won't automatically be included unless CMake is rerun.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/unit_test_utils/mocks/mock_engine/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile (GLOB LIBRARY_SRC\n       ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n      )\n\nfile (GLOB LIBRARY_HEADERS\n       ${CMAKE_CURRENT_SOURCE_DIR}/*.h\n       ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n      )\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for OpenVINO Documentation Snippets (C++)\nDESCRIPTION: This snippet configures compiler flags for the OpenVINO documentation snippets library. It adds -Wall and -Wno-unused-variable flags for GCC, Clang, and Intel LLVM compilers. If supported, it also adds -Wno-unused-but-set-variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/snippets/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG OR OV_COMPILER_IS_INTEL_LLVM)\n    ov_add_compiler_flags(-Wall)\n    ov_add_compiler_flags(-Wno-unused-variable)\nendif()\nif(UNUSED_BUT_SET_VARIABLE_SUPPORTED)\n    ov_add_compiler_flags(-Wno-unused-but-set-variable)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Reshape model input with dynamic batch size - C\nDESCRIPTION: This C snippet illustrates reshaping a model input to enable a dynamic batch size.  It leverages the `ov_dimension` structure to denote the undefined dimension.  The initial shape is assumed to be [1, 3, 224, 224].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov_dynamic_shapes:reshape_undefined]\nov_core_t* core = NULL;\nov_model_t* model = NULL;\nov_dimension_t dimension = { .min = -1, .max = -1 };\nov_shape_t shape = { .dims = {dimension.min, 3, 224, 224}, .rank = 4 };\nconst char* input_name = ov_model_get_input_name(model, 0, &rc);\nov_reshape_model(model, &input_name, &shape, 1, &rc);\n//! [ov_dynamic_shapes:reshape_undefined]\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to the CPU Plugin Target\nDESCRIPTION: Links the `dnnl`, `openvino::shape_inference`, and `openvino::snippets` libraries to the specified target privately. This means that the libraries are only visible to the target and not to any other targets that link to it.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_30\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE dnnl\n                                             openvino::shape_inference\n                                             openvino::snippets)\n```\n\n----------------------------------------\n\nTITLE: Defining Compile Definitions\nDESCRIPTION: This snippet defines compile definitions for the target ${TARGET_NAME} based on whether specific frontend support is enabled (ONNX, TensorFlow, Paddle). It conditionally defines ENABLE_OV_ONNX_FRONTEND, ENABLE_OV_TF_FRONTEND, and ENABLE_OV_PADDLE_FRONTEND based on the corresponding CMake variables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/mock_py_frontend/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME} PUBLIC\n    $<$<BOOL:${ENABLE_OV_ONNX_FRONTEND}>:ENABLE_OV_ONNX_FRONTEND>\n    $<$<BOOL:${ENABLE_OV_TF_FRONTEND}>:ENABLE_OV_TF_FRONTEND>\n    $<$<BOOL:${ENABLE_OV_PADDLE_FRONTEND}>:ENABLE_OV_PADDLE_FRONTEND>)\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library (CMake)\nDESCRIPTION: This snippet installs the static library using the `ov_install_static_lib` macro. The component is set to `OV_CPACK_COMP_CORE`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: Checking Python __init__.py files alignment\nDESCRIPTION: This function `ov_check_init_files_alignment` checks the alignment of `__init__.py` files in pairs. It compares the files using `CMAKE_COMMAND -E compare_files` and issues a fatal error if they are misaligned.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ov_check_init_files_alignment init_files)\n    # check the files in pairs\n    list(LENGTH init_files init_files_count)\n    math(EXPR file_loop_range \"${init_files_count}-2\")\n    foreach(init_file_idx RANGE 0 ${file_loop_range})\n        math(EXPR init_file_idx_next \"${init_file_idx}+1\")\n        list(GET init_files ${init_file_idx} file1)\n        list(GET init_files ${init_file_idx_next} file2)\n\n        execute_process(COMMAND ${CMAKE_COMMAND} -E compare_files ${file1} ${file2}\n            RESULT_VARIABLE compare_result\n        )\n        if(compare_result EQUAL 1)\n            message(FATAL_ERROR \"The runtime __init__.py files are misaligned: ${file1} and ${file2}\")\n        endif()\n    endforeach()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Running Python ONNX Frontend Tests (Build Layout)\nDESCRIPTION: This command runs the Python ONNX Frontend tests using pytest in the build layout. It executes all test files starting with 'test_frontend_onnx' within the specified directory. Replace <OV_REPO_DIR> with the OpenVINO repository directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npytest <OV_REPO_DIR>/src/bindings/python/tests/test_frontend/test_frontend_onnx*\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags in CMake\nDESCRIPTION: This snippet adds compiler flags to suppress missing declarations warnings for GCC and Clang compilers. This is done as a temporary workaround until the warnings can be properly fixed. The flags will only be added if using either GNU or Clang.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/compile_tool/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG)\n    ov_add_compiler_flags(-Wno-missing-declarations)\nendif()\n```\n\n----------------------------------------\n\nTITLE: BatchToSpace XML Example (2D input)\nDESCRIPTION: This example demonstrates the BatchToSpace operation with a 2D input tensor. It shows the input and output port configurations, including the data dimensions, block shape, crops begin, and crops end values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/batch-to-space-2.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"BatchToSpace\" ...>\n    <input>\n        <port id=\"0\">       <!-- data -->\n            <dim>10</dim>   <!-- batch -->\n            <dim>2</dim>    <!-- spatial dimension 1 -->\n        </port>\n        <port id=\"1\">       <!-- block_shape value: [1, 5] -->\n            <dim>2</dim>\n        </port>\n        <port id=\"2\">       <!-- crops_begin value: [0, 2] -->\n            <dim>2</dim>\n        </port>\n        <port id=\"3\">       <!-- crops_end value: [0, 0] -->\n            <dim>2</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>2</dim>    <!-- data.shape[0] / (block_shape.shape[0] * block_shape.shape[1]) -->\n            <dim>8</dim>    <!-- data.shape[1] * block_shape.shape[1] - crops_begin[1] - crops_end[1]-->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: MaxPool Operation Example with Axis Parameter (Shell)\nDESCRIPTION: This example shows how MaxPool operates on 4D input using a 2D kernel with a non-default axis value. The code defines the input tensor, kernel size, strides, dilations, padding parameters, the axis, and the expected output. The rounding type is \"floor\" and auto_pad is set to \"explicit\".\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-8.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]],\n                [[10, 11, 12],\n                 [13, 14, 15],\n                 [16, 17, 18]]\n                 ]]\n      strides = [1, 1]\n      kernel = [2, 2]\n      dilations = [1, 1]\n      rounding_type = \"floor\"\n      auto_pad = \"explicit\"\n      pads_begin = [0, 0]\n      pads_end = [0, 0]\n      axis = 2\n      output0 = [[[[5, 6],\n                   [8, 9]],\n                  [[14, 15],\n                   [17, 18]]]]\n      output1 = [[[[4, 5],\n                   [7, 8]],\n                  [[4, 5],\n                   [7, 8]]]]\n```\n\n----------------------------------------\n\nTITLE: Wrapping by Rewrite in C\nDESCRIPTION: This code snippet demonstrates how to wrap a C++ class in C by rewriting the class's information into a new C struct. This is useful when the object needs to be created, operated, and read by the user. The struct `ov_class_name_t` contains a member of the C++ class type `ov::ClassName`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/how_to_wrap_openvino_objects_with_c.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ntypedef struct {\n    ov::ClassName object;\n} ov_class_name_t;\n```\n\n----------------------------------------\n\nTITLE: Declaring Plugin-Specific Properties in C++\nDESCRIPTION: This C++ code snippet demonstrates how to declare plugin-specific properties using the `ov::Property` interface in OpenVINO. It shows the declaration of a `Properties` class, inheriting from `ov::properties`, and defining custom properties as public members. This allows for device-specific configurations and settings to be exposed by the plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin-properties.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nclass Properties : public ov::properties {\npublic:\n    // Define custom properties here\n    // Example:\n    // ov::Property<int> my_property = ...;\n};\n\n```\n\n----------------------------------------\n\nTITLE: ReduceL2 reducing second spatial dimension in OpenVINO XML\nDESCRIPTION: This example shows ReduceL2 reducing across the second spatial dimension (`axes` = [-2]) and `keep_dims` is false, so that spatial dimension is removed from the output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-l2-4.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceL2\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [-2] that means independent reduction in each channel, batch and second spatial dimension -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding a Subdirectory in CMake\nDESCRIPTION: The `add_subdirectory` command instructs CMake to process the CMakeLists.txt file located in the specified subdirectory ('src' in this case). This allows the project to be organized into multiple directories, each with its own CMake configuration. The command ensures that targets defined within the 'src' directory are available for use in the main project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/pytorch/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Add Clang Format Target CMake\nDESCRIPTION: Adds a custom target for running clang-format on the library's source code. This helps maintain consistent code formatting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/onnx_common/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Applying CC Macros to Code Region\nDESCRIPTION: This code snippet demonstrates how to apply the conditional compilation macros to a code region within the `clone_with_new_inputs` function. It utilizes the `OV_CPU_SCOPE` macro to conditionally include or exclude the code based on the compilation mode.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/docs/develop_cc_for_new_component.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nOutputVector Abs::clone_with_new_inputs(const OutputVector& new_args) const {\n    OV_CPU_SCOPE(Abs_clone_with_new_inputs);\n    if (new_args.size() != 1) {\n        OPENVINO_THROW(\"Incorrect number of new arguments\");\n    }\n\n    return {std::make_shared<Abs>(new_args.at(0))};\n```\n\n----------------------------------------\n\nTITLE: Setting Project Properties\nDESCRIPTION: This snippet sets the project name, description, homepage URL, and supported languages (C and CXX).  It is a standard CMake command to configure the project metadata.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nproject(Samples\n        DESCRIPTION \"OpenVINO Samples\"\n        HOMEPAGE_URL \"https://docs.openvino.ai/2025/get-started/learn-openvino/openvino-samples.html\"\n        LANGUAGES C CXX)\n```\n\n----------------------------------------\n\nTITLE: Cached Path Configuration Example\nDESCRIPTION: Demonstrates how to configure the tool to use pre-built applications based on commit hashes. This speeds up the process by avoiding rebuilds for commits with known good builds. Supports 'mandatory' and 'optional' schemes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/commit_slider/README.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n\"cachedPathConfig\": {\n    \"enable\" : true,\n    \"scheme\" : \"mandatory\",\n    \"cashMap\" : {\n        \"hash_1_\": \"app/path/hash_1_\",\n        \"hash_2_\": \"app/path/hash_2_\",\n        ..............................\n        \"hash_N_\": \"app/path/hash_N_\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: I420toBGR Layer Configuration (Single Input)\nDESCRIPTION: This XML snippet defines an I420toBGR layer configuration with a single input port. It specifies the dimensions for input (I420) and output (BGR) tensors, defining the image size and channel configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/i420-to-bgr-8.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"I420toBGR\">\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>720</dim>\n            <dim>640</dim>\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>480</dim>\n            <dim>640</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Resizing Image (Auto Dimensions) C++\nDESCRIPTION: Demonstrates how to resize an image using the OpenVINO preprocessing API in C++, omitting target width/height when the original model has known spatial dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n// 1) Set input tensor information. We apply preprocessing\n//    demanding input tensor to have 'u8' type and 'NHWC' layout.\nppp.input().tensor().set_element_type(ov::element::u8).set_layout(\"NHWC\");\n// 2) Adding explicit preprocessing steps\nppp.input().preprocess().resize(ov::preprocess::ResizeAlgorithm::RESIZE_LINEAR);\n// 3) Set input model information. We assume that the original model has 'NCHW' layout\nppp.input().model().set_layout(\"NCHW\");\n// 4) Apply preprocessing modifying the original model\nmodel = ppp.build();\n```\n\n----------------------------------------\n\nTITLE: Compiling Model C\nDESCRIPTION: This code snippet compiles a model for a specific device, allowing for optional property configurations. It requires the core instance, model instance, device name, property arguments size and a pointer to store the compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_20\n\nLANGUAGE: C\nCODE:\n```\nov_core_t* core = nullptr;\nov_core_create(&core);\nov_model_t* model = nullptr;\nov_core_read_model(core, xml_file_name.c_str(), bin_file_name.c_str(), &model);\nconst char* key = ov_property_key_hint_performance_mode;\nconst char* num = \"LATENCY\";\nov_compiled_model_t* compiled_model = nullptr;\nov_core_compile_model(core, model, \"CPU\", 2, &compiled_model, key, num);\n...\nov_compiled_model_free(compiled_model);\nov_model_free(model);\nov_core_free(core);\n```\n\n----------------------------------------\n\nTITLE: Setting Positive Regex Pattern for Inference Precision - Bash\nDESCRIPTION: This example demonstrates setting the `OV_CPU_INFER_PRC_POS_PATTERN` environment variable to match all nodes. This allows you to see the full list of nodes that could have their precision enforced before adding more specific filters. It leverages bash to set the environment variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/infer_prc.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOV_CPU_INFER_PRC_POS_PATTERN=\".*\"\n```\n\n----------------------------------------\n\nTITLE: Download example model\nDESCRIPTION: Downloads the `mobelinet-v3-tf` example model's XML and BIN files from the specified URLs and saves them to a designated directory. This model is used for testing the OpenVINO build on Android.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_android.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nmkdir $OPV_HOME_DIR/mobelinet-v3-tf\nwget https://storage.openvinotoolkit.org/repositories/openvino_notebooks/models/mobelinet-v3-tf/FP32/v3-small_224_1.0_float.xml -P $OPV_HOME_DIR/mobelinet-v3-tf/\nwget https://storage.openvinotoolkit.org/repositories/openvino_notebooks/models/mobelinet-v3-tf/FP32/v3-small_224_1.0_float.bin -P $OPV_HOME_DIR/mobelinet-v3-tf/\n```\n\n----------------------------------------\n\nTITLE: Defining Executable Target with CMake\nDESCRIPTION: This snippet defines the executable target for the compile_tool. It sets the target name, type, root directory, and links the necessary libraries. The libraries include openvino::runtime, gflags, Threads::Threads and npu_tools_utils which are all linked privately to the compile_tool executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/compile_tool/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_target(ADD_CPPLINT\n              TYPE EXECUTABLE\n              NAME ${TARGET_NAME}\n              ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n              LINK_LIBRARIES\n                  PRIVATE\n                      openvino::runtime\n                      gflags\n                      Threads::Threads\n                      npu_tools_utils)\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINODeveloperScripts Package\nDESCRIPTION: This snippet uses the `find_package` command to locate the `OpenVINODeveloperScripts` package. It specifies the required version and provides a path to search for the package, which is relative to the `OpenVINO_SOURCE_DIR`.  It also prevents searching in default locations, ensuring the correct package version is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(OpenVINODeveloperScripts REQUIRED\n             PATHS \"${OpenVINO_SOURCE_DIR}/cmake/developer_package\"\n             NO_CMAKE_FIND_ROOT_PATH\n             NO_DEFAULT_PATH)\n```\n\n----------------------------------------\n\nTITLE: Process Inference Results in C\nDESCRIPTION: This C code snippet demonstrates retrieving inference results with the C API. It retrieves the output tensor and reads its data. The example assumes that the inference request is executed already.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_35\n\nLANGUAGE: cpp\nCODE:\n```\n//! [part6]\n// Get output tensor\nov_tensor output_tensor_data = {0};\nOPENVINO_ASSERT(ov_infer_request_get_output_tensor(infer_request, 0, &output_tensor_data));\n\n// Get shape\nsize_t output_size = 0;\nOPENVINO_ASSERT(ov_tensor_get_size(output_tensor_data, &output_size));\n\n// Get data\nfloat* output_data = (float*)malloc(output_size * sizeof(float));\nOPENVINO_ASSERT(output_data != NULL);\nOPENVINO_ASSERT(ov_tensor_get_data(output_tensor_data, OPENVINO_TYPE_FP32, output_data, output_size * sizeof(float)) == 0);\n\n//! [part6]\n```\n\n----------------------------------------\n\nTITLE: Conditional Dependencies and Definitions for ONNX Frontend\nDESCRIPTION: This snippet conditionally appends dependencies 'test_model_zoo' and 'openvino_onnx_frontend' to the 'DEPENDENCIES' list and defines the 'TEST_MODELS' variable if 'ENABLE_OV_ONNX_FRONTEND' is true. Otherwise, it sets 'EXCLUDED_SOURCE_PATHS' to exclude the 'src/onnx' directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/shared/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_OV_ONNX_FRONTEND)\n    list(APPEND DEPENDENCIES test_model_zoo openvino_onnx_frontend)\n    list(APPEND DEFINES TEST_MODELS=\"${TEST_MODEL_ZOO}/func_tests/models/\")\nelse()\n    set(EXCLUDED_SOURCE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/onnx)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enabling Weightless Caching\nDESCRIPTION: This snippet enables weightless caching in OpenVINO using Python or C++.  It initializes an OpenVINO Core object, reads a model, and compiles it, setting the `CacheMode` property to `OPTIMIZE_SIZE` to enable weightless caching, reducing the size of the cache file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n### [ov:caching:part4]\nimport openvino.runtime as ov\n\ncore = ov.Core()\n\nmodel_path = \"path_to_model.xml\"\ncache_dir = \"/path/to/cache/dir\"\n\nmodel = core.read_model(model_path)\ncompiled_model = core.compile_model(model, \"GPU\", config={\n    \"CACHE_DIR\": cache_dir,\n    \"CACHE_MODE\": \"OPTIMIZE_SIZE\"\n})\n### [ov:caching:part4]\n```\n\n----------------------------------------\n\nTITLE: Assign Operation XML Example\nDESCRIPTION: This XML snippet demonstrates the structure of an Assign operation within an OpenVINO model. It specifies the `variable_id` attribute, which identifies the variable to be updated, and defines the input port with its dimensions.  The operation takes a tensor as input and assigns its value to the specified variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/assign-3.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Assign\" ...>\n    <data variable_id=\"lstm_state_1\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </input>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: OVSA Python Dependencies Installation\nDESCRIPTION: Installs the necessary Python dependencies for running inference using the OpenVINO Model Server.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_56\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt install pip3\npip3 install cmake\npip3 install scikit-build\npip3 install opencv-python\npip3 install futures==3.1.1\npip3 install tensorflow-serving-api==1.14.0\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example with Valid Padding (Shell)\nDESCRIPTION: This example demonstrates the MaxPool operation with a 3D input and 1D kernel, using valid padding. It specifies the input tensor, strides, kernel size, rounding type, auto_pad setting, and the resulting output tensor after the MaxPool operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-1.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[-1, 2, 3, 5, -7, 9, 1]]]\nstrides = [1]\nkernel = [3]\nrounding_type = \"floor\"\nauto_pad = \"valid\"\noutput = [[[3, 5, 5, 9, 9]]]\n```\n\n----------------------------------------\n\nTITLE: Linking OpenCL and Setting Compile Definitions (OpenVINO)\nDESCRIPTION: This snippet checks for the OpenCL target and, if found, links the `OpenCL::OpenCL` library to the target. It also checks for libva and, if found, defines ENABLE_LIBVA and links the libva library. If building on Windows, it defines ENABLE_DX11 and links d3d11 and dxgi.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/snippets/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(TARGET OpenCL::OpenCL)\n    target_link_libraries(${TARGET_NAME} PRIVATE OpenCL::OpenCL)\n\n    if(libva_FOUND)\n        target_compile_definitions(${TARGET_NAME} PRIVATE ENABLE_LIBVA)\n        target_link_libraries(${TARGET_NAME} PRIVATE PkgConfig::libva)\n    endif()\n\n    if(WIN32)\n        target_compile_definitions(${TARGET_NAME} PRIVATE ENABLE_DX11)\n        target_link_libraries(${TARGET_NAME} PRIVATE d3d11 dxgi)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: NodeAddon Interface Definition in TypeScript\nDESCRIPTION: Defines the NodeAddon interface, which exposes the Core, Tensor, and PartialShape constructors, as well as a preprocess property (containing resizeAlgorithm and PrePostProcessor), saveModelSync function and element.  This interface is central to using the OpenVINO Node.js API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/addon.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nexport interface NodeAddon {\n       Core: CoreConstructor;\n       Tensor: TensorConstructor;\n       PartialShape: PartialShapeConstructor;\n\n       preprocess: {\n         resizeAlgorithm: typeof resizeAlgorithm;\n         PrePostProcessor: PrePostProcessorConstructor;\n       };\n       saveModelSync(model: Model, path: string, compressToFp16?: boolean): void;\n       element: typeof element;\n     }\n```\n\n----------------------------------------\n\nTITLE: GatherND with Batch Dimensions Example (Elements)\nDESCRIPTION: This example showcases GatherND when the batch_dims attribute is set to a non-default value (1 in this case).  The operation starts gathering from the b+1 dimension, requiring the first b dimensions in data and indices tensors to be equal.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-5.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 1\nindices = [[1],    <--- this is applied to the first batch\n              [0]]    <--- this is applied to the second batch, shape = (2, 1)\ndata    = [[1, 2], <--- the first batch\n              [3, 4]] <--- the second batch, shape = (2, 2)\noutput  = [2, 3], shape = (2)\n```\n\n----------------------------------------\n\nTITLE: Installing Library Target in CMake\nDESCRIPTION: Configures the installation process for the `mock_engine` library.  It specifies that the library should be installed in the 'tests' directory, that it's part of the 'tests' component, and that it should be excluded from the 'all' install target.  This suggests the library is primarily for testing.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/unit_test_utils/mocks/mock_engine/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n        LIBRARY DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: ExtractImagePatches Layer Definition XML\nDESCRIPTION: This XML snippet defines an ExtractImagePatches layer with specific attributes like sizes, strides, rates, and auto_pad. It also specifies the input and output port dimensions and precision.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/extract-image-patches-3.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"ExtractImagePatches\" ...>\n    <data sizes=\"3,3\" strides=\"5,5\" rates=\"1,1\" auto_pad=\"valid\"/>\n    <input>\n        <port id=\"0\">\n            <dim>64</dim>\n            <dim>3</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"f32\">\n            <dim>64</dim>\n            <dim>27</dim>\n            <dim>2</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: MVN Layer with reduction_axes Attribute XML\nDESCRIPTION: This XML snippet defines an MVN layer in OpenVINO with the `reduction_axes` attribute specified.  This configuration calculates normalization based on the provided axes. The example shows the input and output port dimensions and includes the epsilon value for numerical stability during variance normalization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/mvn-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MVN\">\n    <data reduction_axes=\"2,3\" eps=\"1e-9\" normalize_variance=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Model Creation in Python (OpenVINO)\nDESCRIPTION: This Python snippet, referenced by `samples/python/model_creation_sample/model_creation_sample.py`, demonstrates how to create a model from weights and perform inference using the OpenVINO Runtime. It requires the OpenVINO library and a LeNet model weights file (`.bin`). The script takes the path to the weights file and the device name as command-line arguments and outputs inference results to the console.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/model-creation.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. doxygensnippet:: samples/python/model_creation_sample/model_creation_sample.py\n   :language: python\n```\n\n----------------------------------------\n\nTITLE: Setting Interprocedural Optimization Properties in CMake\nDESCRIPTION: This snippet sets the interprocedural optimization (IPO) properties for both the object library and the static library. It controls link-time optimization (LTO) by enabling it based on the value of the `ENABLE_LTO` variable, optimizing the build for performance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME}_s PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n\n# LTO\n\nset_target_properties(${TARGET_NAME}_obj\n                      PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO with Conditional Compilation\nDESCRIPTION: This snippet shows how to configure CMake to build OpenVINO with conditional compilation enabled. It sets the SELECTIVE_BUILD option to ON and specifies the path to the statistics files generated by the ITT collector.  It also disables ITT profiling.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/conditional_compilation.md#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -DSELECTIVE_BUILD=ON \\ \\\n-DSELECTIVE_BUILD_STAT=${ABSOLUTE_PATH_TO_STATISTICS_FILES}/*.csv \\ \\\n -DENABLE_PROFILING_ITT=OFF ..\n```\n\n----------------------------------------\n\nTITLE: Wrap cl::Buffer (C)\nDESCRIPTION: This C snippet shows how to wrap an existing OpenCL buffer (cl::Buffer) into an OpenVINO RemoteTensor using the GPU plugin. This enables using existing OpenCL buffers as inputs to OpenVINO models. Requires OpenCL and OpenVINO libraries. The wrapped cl::Buffer object is then treated as a RemoteTensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_22\n\nLANGUAGE: c\nCODE:\n```\n// example usage\n{\n    cl_mem mem;\n    size_t buffer_size;\n    // fill mem with data\n\n    auto remote_blob = context.create_tensor(buffer_size, mem);\n}\n```\n\n----------------------------------------\n\nTITLE: LogicalOr Layer Configuration - No Broadcast XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a LogicalOr layer in OpenVINO with no broadcasting. The input tensors have the same dimensions (256x56).  The output port also has the same dimensions, reflecting the element-wise logical OR operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/logical/logical-or-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"LogicalOr\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Checking Device Caching Support\nDESCRIPTION: This snippet checks if a device supports model caching in OpenVINO using Python or C++. It queries the device's capabilities to determine if model export/import is supported. The snippet prints whether caching is supported or not for the specified device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n/// [ov:caching:part3]\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n\n    std::string device_name = \"CPU\";\n    if (core.get_property(device_name, ov::model_import_export())) {\n        std::cout << device_name << \" supports model caching\\n\";\n    } else {\n        std::cout << device_name << \" doesn't support model caching\\n\";\n    }\n\n    return 0;\n}\n/// [ov:caching:part3]\n```\n\n----------------------------------------\n\nTITLE: Function Overloading C++\nDESCRIPTION: Demonstrates function overloading in pybind11 by defining two versions of the `say_hello` function: one that takes only the `MyTensor` object and another that takes a `MyTensor` object and a string message.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\ncls.def(\"say_hello\", [](const MyTensor& self) {\n    py::print(\"Hello there!\");\n    for (size_t i = 0; i < self.get_size(); i++) {\n        py::print(self._tensor.data<float>() + i);\n    }\n});\n\ncls.def(\"say_hello\", [](const MyTensor& self, std::string& message) {\n    py::print(message);\n    for (size_t i = 0; i < self.get_size(); i++) {\n        py::print(self._tensor.data<float>() + i);\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Test Target in CMake\nDESCRIPTION: This CMake snippet adds a test target named `${TARGET_NAME}` (which is `ov_transformations_tests`). It specifies the root source directory, dependencies, link libraries, and assigns labels.  It uses the `ov_add_test_target` macro to simplify the process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n    NAME ${TARGET_NAME}\n    ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n    DEPENDENCIES\n    LINK_LIBRARIES\n        gmock\n        func_test_utils\n        openvino::offline_transformations\n        openvino::reference\n        sharedTestClasses\n        ov_lpt_models\n    ADD_CLANG_FORMAT\n    LABELS\n        OV UNIT TRANSFORMATIONS\n)\n```\n\n----------------------------------------\n\nTITLE: ShuffleChannels Tensor Reshape in C++\nDESCRIPTION: Illustrates how the input tensor's shape is interpreted as a 4D tensor before channel shuffling. The dimensions are calculated based on the 'axis' and the input tensor's shape, defining how leading and trailing dimensions are flattened and reshaped.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/shuffle-channels-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ndim 0: data_shape[0] * data_shape[1] * ... * data_shape[axis-1]\n             (or 1 if axis == 0)\ndim 1: group\ndim 2: data_shape[axis] / group\ndim 3: data_shape[axis+1] * data_shape[axis+2] * ... * data_shape[data_shape.size()-1]\n            (or 1 if axis points to last dimension)\n```\n\n----------------------------------------\n\nTITLE: Compiling Model with Performance Hint in C++\nDESCRIPTION: This C++ snippet demonstrates how to compile a model using OpenVINO and specify the `THROUGHPUT` performance hint. This hint optimizes the model for maximum throughput on the target device. The snippet assumes that the `core` object and `model` object are already defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/high-level-performance-hints.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov::CompiledModel compiled_model = core.compile_model(\n    model,\n    \"CPU\",\n    ov::hint::performance_mode(ov::hint::PerformanceMode::THROUGHPUT));\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files in CMake\nDESCRIPTION: Uses `file(GLOB_RECURSE)` to find all C++ source files (*.cpp) and header files (*.h) within the current directory and its subdirectories. These files are then stored in the SRC and HDR variables, respectively, for later use in defining the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/common/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile (GLOB_RECURSE SRC *.cpp)\nfile (GLOB_RECURSE HDR *.h)\n```\n\n----------------------------------------\n\nTITLE: Compiling Model with Read Model and Caching\nDESCRIPTION: This snippet compiles a model using `read_model` and enables caching.  It initializes an OpenVINO Core object, reads a model from the specified path, then compiles the model with the specified device and configuration, including the cache directory, in Python and C++.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n### [ov:caching:part2]\nimport openvino.runtime as ov\n\ncore = ov.Core()\n\nmodel_path = \"path_to_model.xml\"\ncache_dir = \"/path/to/cache/dir\"\n\nmodel = core.read_model(model_path)\ncompiled_model = core.compile_model(model, \"CPU\", config={\n    \"CACHE_DIR\": cache_dir\n})\n### [ov:caching:part2]\n```\n\n----------------------------------------\n\nTITLE: Creating Source Groups in CMake\nDESCRIPTION: These snippets create source groups in the IDE (e.g., Visual Studio). The first creates a group named `src` containing all files in `LIBRARY_SRC`.  The second creates a group named `include` containing all files in `PUBLIC_HEADERS`. This helps organize the project structure in the IDE.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Creating Source Groups (CMake)\nDESCRIPTION: This snippet organizes the source and header files into named groups for better visualization in IDEs like Visual Studio. It creates a \"src\" group for source files and an \"include\" group for header files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/shape_inference/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Add Compiler Flags for GCC/Clang CMake\nDESCRIPTION: This snippet adds the -Wmissing-declarations compiler flag if the compiler is either GCC or Clang. This flag enables warnings for missing declarations, helping to improve code quality and catch potential errors during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG)\n    ov_add_compiler_flags(-Wmissing-declarations)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Starting Guest VM for Runtime Role (sh)\nDESCRIPTION: Starts the Guest VM in QEMU for the Runtime Role, enabling KVM, configuring memory and CPU, setting up network interfaces with TAP devices and associated scripts, and connecting to the virtual TPM device through a socket. It also specifies a VNC server for remote access.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_25\n\nLANGUAGE: sh\nCODE:\n```\nsudo qemu-system-x86_64 \\\n -cpu host \\\n -enable-kvm \\\n -m 8192 \\\n -smp 8,sockets=1,cores=8,threads=1 \\\n -device e1000,netdev=hostnet2,mac=52:54:00:d1:67:6f \\\n -netdev tap,id=hostnet2,script=<path-to-scripts>/br0-qemu-ifup,downscript=<path-to-scripts>/br0-qemu-ifdown \\\n -device e1000,netdev=hostnet3,mac=52:54:00:d1:67:5f \\\n -netdev tap,id=hostnet3,script=<path-to-scripts>/virbr0-qemu-ifup,downscript=<path-to-scripts>/virbr0-qemu-ifdown \\\n -drive if=virtio,file=<path-to-disk-image>/ovsa_runtime_vm_disk.qcow2,cache=none \\\n -chardev socket,id=chrtpm,path=/var/OVSA/vtpm/vtpm_runtime/swtpm-sock \\\n -tpmdev emulator,id=tpm0,chardev=chrtpm \\\n -device tpm-tis,tpmdev=tpm0 \\\n -vnc :2\n```\n\n----------------------------------------\n\nTITLE: Checking OpenVINO™ environment initialization\nDESCRIPTION: This script snippet displays information about OpenVINO environment initialization. This snippet would be expected to output the status of OpenVINO environment setup.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/troubleshooting-install-config.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n[setupvars.sh] OpenVINO™ environment initialized\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library\nDESCRIPTION: This snippet installs the static library using the custom `ov_install_static_lib` function, placing it in the `OV_CPACK_COMP_CORE` component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common_translators/src/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: CMake Build\nDESCRIPTION: This snippet builds the OpenVINO project using CMake.  It specifies the `Release` configuration and uses parallel compilation, utilizing all available CPU cores.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ncmake --build . --config Release --parallel $(sysctl -n hw.ncpu)\n```\n\n----------------------------------------\n\nTITLE: Gathering Reference Values for MemCheckTests (Bash)\nDESCRIPTION: This bash script creates a directory for logs, executes `MemCheckTests` with logging enabled, and then filters the logs to extract reference values for models.  It uses `grep`, `sed`, and `sort` to process the log files and output a sorted list of model reference values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p MemCheckTests-logs && \\\ngtest-parallel -d ./MemCheckTests-logs ./MemCheckTests && \\\ngrep -rh ./MemCheckTests-logs -e \".*<model \" | sed -e \"s/.*<model /<model /\" | sort\n```\n\n----------------------------------------\n\nTITLE: Tensor Constructor with Type, Shape, and Data\nDESCRIPTION: Constructs a tensor with the specified element type, shape, and data. The tensor wraps the provided TypedArray.  The type parameter specifies the element type, the shape parameter specifies the dimensions of the tensor, and the tensorData parameter is a SupportedTypedArray.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/TensorConstructor.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nnew TensorConstructor(type, shape, tensorData): Tensor\n```\n\n----------------------------------------\n\nTITLE: Adding Static Library\nDESCRIPTION: Adds a static library target named `${TARGET_NAME}` using the source files found in the `SOURCES` variable.  An alias is created for easier referencing, and the export name is defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/common/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${SOURCES})\nadd_library(openvino::npu_common ALIAS ${TARGET_NAME})\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME npu_common)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO GenAI from Archive - Ubuntu 24.04\nDESCRIPTION: These commands download and extract the OpenVINO GenAI archive for Ubuntu 24.04. The `curl` command downloads the tarball, and the `tar` command extracts its contents.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-genai.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino_genai/packages/2025.1/linux/openvino_genai_ubuntu24_2025.1.0.0_x86_64.tar.gz --output openvino_genai_2025.1.0.0.tgz\ntar -xf openvino_genai_2025.1.0.0.tgz\n```\n\n----------------------------------------\n\nTITLE: Conditional Faster Build Configuration\nDESCRIPTION: If 'ENABLE_FASTER_BUILD' is enabled, this sets the 'UNITY_BUILD' property to 'ON' and 'UNITY_BUILD_MODE' to 'GROUP' for the target.  Then, it calls the `group_source_file` function to organize the source files into groups, enabling unity builds which can drastically reduce compile times.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_FASTER_BUILD) \n    set_target_properties(${TARGET_NAME} PROPERTIES UNITY_BUILD ON UNITY_BUILD_MODE GROUP)\n    group_source_file(unit_src_nodes ${CMAKE_CURRENT_SOURCE_DIR}/nodes)\n    group_source_file(unit_src_snippets_transformations ${CMAKE_CURRENT_SOURCE_DIR}/snippets_transformations)\n    group_source_file(unit_src_transformations ${CMAKE_CURRENT_SOURCE_DIR}/transformations)\n    group_source_file(unit_src_custom_shape_infer ${CMAKE_CURRENT_SOURCE_DIR}/shape_inference_test/custom_shape_infer)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Suppress MSVC Warnings\nDESCRIPTION: Suppresses specific warnings in the MSVC compiler.  It disables warnings related to data conversion and signed/unsigned mismatch. It also sets the C++ standard flag to ensure compatibility with oneDNN.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    # 4267 4244 conversion from 'XXX' to 'YYY', possible loss of data\n    ov_add_compiler_flags(/wd4244)\n    # '<': signed/unsigned mismatch\n    ov_add_compiler_flags(/wd4018)\n\n    # see https://github.com/oneapi-src/oneDNN/issues/2028\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /Zc:__cplusplus\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Shape with Undefined Rank C++\nDESCRIPTION: This code snippet demonstrates how to create a PartialShape with an undefined rank in OpenVINO using C++.\nIt initializes an `ov::PartialShape` object to represent a shape where the rank is not known.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/docs/shape_propagation.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nov::PartialShape::dynamic();\n```\n\n----------------------------------------\n\nTITLE: Getting Tensor Shape in OpenVINO (C)\nDESCRIPTION: This function retrieves the shape of an OpenVINO tensor. It takes a pointer to the tensor and a pointer to a shape structure as input. The function returns a status code indicating success or failure in retrieving the tensor shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_45\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_tensor_get_shape(const ov_tensor_t* tensor, ov_shape_t* shape)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Development Requirements for Wheel Package\nDESCRIPTION: This snippet installs the necessary Python development requirements for building a wheel package (.whl). It uses pip to install the packages listed in the requirements-dev.txt file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_windows.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npip install -r <openvino source tree>\\src\\bindings\\python\\wheel\\requirements-dev.txt\n```\n\n----------------------------------------\n\nTITLE: Broadcast with explicit mode in OpenVINO (XML)\nDESCRIPTION: This XML snippet shows the Broadcast operation with the 'explicit' mode. It illustrates broadcasting a tensor with shape [16] to [1, 16, 50, 50] based on the target shape [4] and axes_mapping [1] when the third input specifies how the input dimensions map to the output dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/broadcast-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Broadcast\" ...>\n    <data mode=\"explicit\"/>\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n       </port>\n        <port id=\"1\">\n            <dim>4</dim>   <!--The tensor contains 4 elements: [1, 16, 50, 50] -->\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>   <!--The tensor contains 1 elements: [1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>16</dim>\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: MaxPool Layer Definition with 'same_upper' Auto Pad (XML)\nDESCRIPTION: This example demonstrates the XML definition of a MaxPool layer in OpenVINO using the 'same_upper' auto_pad attribute. It specifies the kernel size, padding, and strides. The input and output ports define the dimensions of the tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-8.rst#_snippet_8\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MaxPool\" ... >\n       <data auto_pad=\"same_upper\" kernel=\"2,2\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n       <input>\n           <port id=\"0\">\n               <dim>1</dim>\n               <dim>3</dim>\n               <dim>32</dim>\n               <dim>32</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"1\">\n               <dim>1</dim>\n               <dim>3</dim>\n               <dim>32</dim>\n               <dim>32</dim>\n           </port>\n           <port id=\"2\">\n               <dim>1</dim>\n               <dim>3</dim>\n               <dim>32</dim>\n               <dim>32</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Applying Faster Build Options in CMake\nDESCRIPTION: Applies build settings for faster compilation using the ov_build_target_faster macro, which likely sets compiler flags or other build configurations to optimize build times. UNITY likely enables unity builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME}_obj UNITY)\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO in SELECTIVE_BUILD Mode\nDESCRIPTION: This CMake command configures and builds OpenVINO in SELECTIVE_BUILD mode. It enables LTO, uses the collected statistics, and disables ITT profiling. This mode utilizes the data from the analyzer mode to selectively build the code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/docs/develop_cc_for_new_component.md#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncmake \\\n            -DCMAKE_BUILD_TYPE=Release \\\n            -DENABLE_LTO=ON \\\n            -DSELECTIVE_BUILD=ON \\\n            -DSELECTIVE_BUILD_STAT=$OPENVINO_HOME/cc_data/*.csv \\\n            -DENABLE_PROFILING_ITT=OFF \\\n            -B build \\\n            -S .\n    cmake --build build -j `nproc`\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries in CMake\nDESCRIPTION: This command links the specified libraries to the target library. PUBLIC libraries are linked and their include directories are propagated to dependent targets. PRIVATE libraries are linked only for the target itself. The specified libraries include OpenVINO components, common test utilities, and CNPY.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME}\n        PUBLIC\n                openvino::offline_transformations\n                common_test_utils\n                func_test_utils\n                openvino::util\n                openvino::runtime\n        PRIVATE\n                openvino::cnpy)\n```\n\n----------------------------------------\n\nTITLE: Gather Operation Example 3\nDESCRIPTION: Shows the Gather operation with a batch_dims value of 2. The indices and data tensors are 3D arrays, demonstrating how the operation applies to multiple batches.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-8.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 2\naxis = 2\n\nindices = [[[0, 0, 4],  <-- this is applied to the first batch, index = (0, 0)\n            [4, 0, 0]],  <-- this is applied to the second batch, index = (0, 1)\n\n           [[1, 2, 4],  <-- this is applied to the third batch, index = (1, 0)\n            [4, 3, 2]]]  <-- this is applied to the fourth batch, index = (1, 1)\nindices_shape = (2, 2, 3)\n\ndata    = [[[1, 2, 3, 4, 5],  <-- the first batch, index = (0, 0)\n            [6, 7, 8, 9, 10]],  <-- the second batch, index = (0, 1)\n\n           [[11, 12, 13, 14, 15],  <-- the third batch, index = (1, 0)\n            [16, 17, 18, 19, 20]]]  <-- the fourth batch, index = (1, 1)\ndata_shape = (2, 2, 5)\n\noutput  = [[[ 1, 1, 5],\n            [10, 6, 6]],\n\n           [[12, 13, 15],\n            [20, 19, 18]]]\noutput_shape = (2, 2, 3)\n```\n\n----------------------------------------\n\nTITLE: Multinomial Layer Configuration (Multiple Batches)\nDESCRIPTION: This XML snippet demonstrates the configuration of a Multinomial layer in OpenVINO for multiple batch inputs. It utilizes attributes like `convert_type`, `with_replacement`, `log_probs`, `global_seed`, and `op_seed`. The input provides probabilities for multiple batches, and the output defines the shape and type of the sampled indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/multinomial-13.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... name=\"Multinomial\" type=\"Multinomial\">\n    <data convert_type=\"f32\", with_replacement=\"true\", log_probs=\"true\", global_seed=\"234\", op_seed=\"148\"/>\n    <input>\n        <port id=\"0\" precision=\"FP32\">  <!-- probs value: [[-1, 1, 2], [50, 1, 21]] -->\n            <dim>2</dim> <!-- batch size of 2 -->\n            <dim>3</dim>\n        </port>\n        <port id=\"1\" precision=\"I32\"/> <!-- num_samples value: 10 -->\n    </input>\n    <output>\n        <port id=\"3\" precision=\"I32\" names=\"Multinomial:0\">\n            <dim>2</dim> <!--dimension depends on input batch size -->\n            <dim>10</dim> <!--dimension depends on num_samples -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Enabling Code Style Check in CMake\nDESCRIPTION: This command enables code style checking (clang-format) for the `tensorflow_lite_fe_standalone_build_test` target. It uses a custom CMake function `ov_add_clang_format_target` to add a new target for performing the check.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/tests/standalone_build/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: GRN Normalization Formula\nDESCRIPTION: This is the mathematical formula describing the Global Response Normalization (GRN) operation. It shows how the output is calculated by dividing the input by the square root of the sum of squares across channels, plus a bias.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/grn-1.rst#_snippet_1\n\nLANGUAGE: math\nCODE:\n```\noutput[i0, i1, ..., iN] = x[i0, i1, ..., iN] / sqrt(sum[j = 0..C-1](x[i0, j, ..., iN]**2) + bias)\n```\n\n----------------------------------------\n\nTITLE: Copy OpenVINO Tokenizers Library (Windows)\nDESCRIPTION: Specifies the directory to copy the OpenVINO Tokenizers prebuilt library for Windows systems.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n<openvino_dir>\\runtime\\bin\\intel64\\Release\\\n```\n\n----------------------------------------\n\nTITLE: Setting the GENERATED property for generated sources\nDESCRIPTION: Sets the `GENERATED` property to `TRUE` for the generated source files. This indicates to CMake that these files are automatically generated and should be treated accordingly during the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/ocl_v2/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset_property(SOURCE ${CODEGEN_CACHE_SOURCES} PROPERTY GENERATED TRUE)\n```\n\n----------------------------------------\n\nTITLE: Subtract Layer Definition (No Broadcast) - XML\nDESCRIPTION: This example demonstrates a Subtract layer definition in XML with no auto-broadcasting.  The input tensors must have matching shapes. It shows the layer definition, input ports with dimensions, and the output port with the resulting dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/subtract-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Subtract\">\n    <data auto_broadcast=\"none\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ReduceMin Layer Configuration (XML, keep_dims=false)\nDESCRIPTION: This XML snippet shows a configuration for the ReduceMin layer with `keep_dims` set to false. The input tensor has dimensions 6x12x10x24, and reduction is performed along axes 2 and 3. Since `keep_dims` is false, the output tensor has dimensions 6x12.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-min-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceMin\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Add Common Utils Targets\nDESCRIPTION: Calls the `add_common_utils` function to create two library targets: `commonTestUtils` (and its static variant) and `common_test_utils` (and its static variant). The static variant is configured to link against the static OpenVINO runtime library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/common_test_utils/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_common_utils(${TARGET_NAME})\nadd_common_utils(${TARGET_NAME}_s USE_STATIC_IE)\n```\n\n----------------------------------------\n\nTITLE: Adding Windows-Specific Dependency in CMake\nDESCRIPTION: On Windows, this snippet adds the `winmm.lib` library as a dependency. It first creates an INTERFACE library called `winmm` and links `winmm.lib` to it. Then, it appends `winmm` to the list of dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (WIN32)\n    # WA: add_tool_target expects to have all dependencies as cmake targets.\n    add_library(winmm INTERFACE)\n    target_link_libraries(winmm INTERFACE \"winmm.lib\")\n    list(APPEND DEPENDENCIES winmm)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Constant Attribute Value (C++)\nDESCRIPTION: Shows how to set a constant value for an attribute in the OpenVINO operation. The `default_attributes` parameter of the `OpExtension` constructor allows assigning a fixed value to an attribute, overriding any value from the framework model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/frontend-extensions.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nCore core;\ncore.add_extension(std::make_shared<ov::frontend::OpExtension<CustomOperation>>(\n    std::unordered_map<std::string, std::string>{\n        {\"fw_attr1\", \"attr1\"}},\n    std::unordered_map<std::string, ov::Any>{\n        {\"attr2\", 5}}));\nauto model = core.read_model(\"model.onnx\");\n```\n\n----------------------------------------\n\nTITLE: Conditional Dependencies Based on Features\nDESCRIPTION: These snippets conditionally add dependencies to the 'DEPENDENCIES' list based on the values of CMake variables such as 'ENABLE_HETERO', 'ENABLE_AUTO', 'ENABLE_MULTI', 'ENABLE_AUTO_BATCH', 'ENABLE_TEMPLATE', 'ENABLE_OV_IR_FRONTEND'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/shared/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_HETERO)\n    list(APPEND DEPENDENCIES openvino_hetero_plugin)\nendif()\n\nif(ENABLE_AUTO OR ENABLE_MULTI)\n    list(APPEND DEPENDENCIES openvino_auto_plugin)\nendif()\n\nif(ENABLE_AUTO_BATCH)\n    list(APPEND DEPENDENCIES openvino_auto_batch_plugin)\nendif()\n\nif(ENABLE_TEMPLATE)\n    list(APPEND DEPENDENCIES openvino_template_plugin)\nendif()\n\nif(ENABLE_OV_IR_FRONTEND)\n    list(APPEND DEPENDENCIES openvino_ir_frontend)\nendif()\n```\n\n----------------------------------------\n\nTITLE: 1D GroupConvolutionBackpropData Example (OpenVINO XML)\nDESCRIPTION: Example of a 1D GroupConvolutionBackpropData layer configuration in OpenVINO XML format. This configuration specifies the layer's ID, name, type, attributes (dilations, pads_begin, pads_end, strides), input port dimensions, and output port dimensions with precision.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/group-convolution-backprop-data-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"5\" name=\"upsampling_node\" type=\"GroupConvolutionBackpropData\">\n    <data dilations=\"1\" pads_begin=\"1\" pads_end=\"1\" strides=\"2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>20</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>\n            <dim>5</dim>\n            <dim>2</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>8</dim>\n            <dim>447</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Build OpenVINO Bindings (Bash)\nDESCRIPTION: This command executes the build process using CMake, compiling the OpenVINO Node.js bindings. The `--config Release` flag specifies that the build should be optimized for release, and `-j4` utilizes 4 parallel jobs to speed up the compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncmake --build . --config Release --verbose -j4\n```\n\n----------------------------------------\n\nTITLE: Exporting Target for Developer Package\nDESCRIPTION: This CMake snippet exports the `func_test_utils` target for use in developer packages. It specifies the installation include directories to be included in the exported target, which is `${CMAKE_CURRENT_SOURCE_DIR}/include/`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/functional_test_utils/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nov_developer_package_export_targets(TARGET ${TARGET_NAME}\n                                    INSTALL_INCLUDE_DIRECTORIES \"${CMAKE_CURRENT_SOURCE_DIR}/include/\")\n```\n\n----------------------------------------\n\nTITLE: Registering a Conversion Extension in C++\nDESCRIPTION: This snippet demonstrates how to register a custom operation using `ConversionExtension` in C++. It defines a lambda function that takes an `ov::frontend::NodeContext` and returns the outputs of an `Add` operation. This effectively maps the custom operation to an OpenVINO subgraph.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/how_to_add_op.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\ncore.add_extension(ov::frontend::onnx::ConversionExtension(\"org.openvinotoolkit\", \"CustomAdd\", ov::frontend::CreatorFunction(\n                                                            [](const ov::frontend::NodeContext& context)\n                                                            {\n                                                                const auto add = std::make_shared<ov::opset9::Add>(context.get_input(0), context.get_input(1));\n                                                                return add->outputs();\n                                                            })));\n```\n\n----------------------------------------\n\nTITLE: Setting FlatBuffers Options in CMake\nDESCRIPTION: This snippet configures FlatBuffers build options such as disabling tests and installation. It also sets the C++ standard to be used during the build process. These options are forced to ensure consistency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/flatbuffers/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(FLATBUFFERS_BUILD_TESTS OFF CACHE BOOL \"\" FORCE)\nset(FLATBUFFERS_INSTALL OFF CACHE BOOL \"\" FORCE)\nset(FLATBUFFERS_CPP_STD ${CMAKE_CXX_STANDARD})\n```\n\n----------------------------------------\n\nTITLE: Installing build dependencies using Brew\nDESCRIPTION: This command uses the `brew install` command to install CMake, Scons, fdupes, git-lfs and Ninja, which are necessary build dependencies for compiling OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n% brew install cmake scons fdupes git-lfs ninja\n```\n\n----------------------------------------\n\nTITLE: Cloning Xuantie GNU toolchain and building\nDESCRIPTION: This snippet clones the Xuantie GNU toolchain repository, configures it with a specified installation path, and then builds the toolchain along with the QEMU emulator. This toolchain is tailored for RISC-V with vector extensions from SHL.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/XUANTIE-RV/xuantie-gnu-toolchain.git\ncd xuantie-gnu-toolchain\n./configure --prefix=<xuantie_install_path>\nmake linux build-qemu -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: CMake Test Target Definition\nDESCRIPTION: This CMake snippet defines a test target named 'ov_tensorflow_common_tests' using the custom macro 'ov_add_test_target'. It links the necessary libraries such as 'gtest_main_manifest', 'frontend_shared_test_classes', and 'openvino_tensorflow_common'. The test target includes clang format checks and is labeled with 'OV', 'UNIT', and 'TF_COMMON'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME \"ov_tensorflow_common_tests\")\n\nov_add_test_target(\n    NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        LINK_LIBRARIES\n            gtest_main_manifest \n            frontend_shared_test_classes\n            openvino_tensorflow_common\n        ADD_CLANG_FORMAT\n        LABELS\n            OV UNIT TF_COMMON\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Static Library\nDESCRIPTION: Creates a static library target with the specified target name and source files. The target name is defined earlier using set(). The sources are retrieved using GLOB_RECURSE.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/backend/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Building Target Faster CMake\nDESCRIPTION: Enables a faster build configuration for the `conformance_shared` target using the UNITY build system. This is an optimization for reducing compilation time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/conformance_infra/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME} UNITY)\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Minimum Version\nDESCRIPTION: This snippet sets the minimum required CMake version to 3.13. This ensures that the CMake version used to build the project is at least 3.13, preventing compatibility issues with older CMake versions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\n```\n\n----------------------------------------\n\nTITLE: Compiling Model Directly\nDESCRIPTION: This snippet shows how to compile a model directly from a model path, skipping the explicit `read_model` step in Python or C++. It initializes the OpenVINO Core object and compiles the model using the specified device and configuration, including the cache directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimizing-latency/model-caching-overview.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n### [ov:caching:part1]\nimport openvino.runtime as ov\n\ncore = ov.Core()\n\nmodel_path = \"path_to_model.xml\"\ncache_dir = \"/path/to/cache/dir\"\n\ncompiled_model = core.compile_model(model_path, \"CPU\", config={\n    \"CACHE_DIR\": cache_dir\n})\n### [ov:caching:part1]\n```\n\n----------------------------------------\n\nTITLE: Dependency Requirements\nDESCRIPTION: This snippet specifies various Python package dependencies. Some dependencies have version constraints and platform-specific conditions, such as `onnxruntime` version depending on the Python version and `torchvision` and `sympy` depending on platform architecture and Python version. These ensure that the correct versions of packages are installed for the specific environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/layer_tests/requirements.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nnumpy\nonnxruntime>=1.18.0,<=1.19.2; python_version <= '3.9'\nonnxruntime>=1.18.0; python_version >= '3.10'\nrequests\ntorch\ntorchvision; platform_machine == 'arm64' and python_version >= '3.9'\ntorchvision; platform_machine != 'arm64'\nsympy; platform_machine == 'arm64' and python_version >= '3.9'\nsympy; platform_machine != 'arm64'\ntransformers\npackaging\npillow\npytest\ndefusedxml\ntensorflow\ntensorflow-addons; python_version <= '3.10'\n```\n\n----------------------------------------\n\nTITLE: Optimizing Build Target in CMake\nDESCRIPTION: This snippet optimizes the build process for the specified target using the `ov_build_target_faster` function. It also sets a precompiled header (`src/precomp.hpp`) for faster compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/compiler_adapter/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME}\n    PCH PRIVATE \"src/precomp.hpp\"\n)\n```\n\n----------------------------------------\n\nTITLE: Using Optional in the middle of a pattern\nDESCRIPTION: This Python snippet demonstrates how to use the Optional operator to specify that a node in the middle of a pattern may or may not be present in the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.tools.ovc.composite.pattern import WrapType, Optional\n\ndef pattern_optional_middle():\n    # Creating nodes\n    relu_node = WrapType(\"opset13.Relu\")\n    sigmoid_node = WrapType(\"opset13.Sigmoid\")\n    optional_node = Optional(relu_node)\n\n    return optional_node, sigmoid_node\n```\n\n----------------------------------------\n\nTITLE: Test Model Generation and Installation\nDESCRIPTION: This snippet defines custom commands to generate and install PaddlePaddle test models. If PaddleDetection setup was successful, it generates the test models by executing Python scripts and installs them to the specified directory. If the setup fails, it adds a custom command to display a warning message during the build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\n# If 'paddlepaddle' is not found, code will still be compiled, but models will not be generated and tests will fail\n# This is done this way for 'code style' and check cases - cmake shall pass, but CI machine doesn't need to have\n# 'paddlepaddle' installed to check code style\nif(PADDLEDET_RESULT)\n    set(TEST_PADDLE_MODELS ${TEST_MODEL_ZOO_OUTPUT_DIR}/paddle_test_models/)\n\n    file(GLOB_RECURSE PADDLE_ALL_SCRIPTS ${CMAKE_CURRENT_SOURCE_DIR}/*.py)\n    set(OUT_FILE ${TEST_PADDLE_MODELS}/generate_done.txt)\n    add_custom_command(OUTPUT ${OUT_FILE}\n            COMMAND ${CMAKE_COMMAND} -E env PYTHONPATH=${PADDLEDET_DIRNAME}\n                ${Python3_EXECUTABLE}\n                    ${CMAKE_CURRENT_SOURCE_DIR}/test_models/gen_wrapper.py\n                    ${CMAKE_CURRENT_SOURCE_DIR}/test_models/gen_scripts\n                    ${TEST_PADDLE_MODELS}\n            DEPENDS ${PADDLE_ALL_SCRIPTS})\n    add_custom_target(paddle_test_models DEPENDS ${OUT_FILE})\n\n    install(DIRECTORY ${TEST_PADDLE_MODELS}\n            DESTINATION tests/${TEST_PADDLE_MODELS_DIRNAME}\n            COMPONENT tests\n            EXCLUDE_FROM_ALL)\n\nelse()\n    # Produce warning message at build time as well\n    add_custom_command(OUTPUT unable_build_paddle_models.txt\n            COMMAND ${CMAKE_COMMAND}\n            -E cmake_echo_color --red \"Warning: Unable to generate PaddlePaddle test models. Running '${TARGET_NAME}' will likely fail\"\n            )\n    add_custom_target(paddle_test_models DEPENDS unable_build_paddle_models.txt)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Smoke Tests\nDESCRIPTION: This snippet demonstrates how to install smoke tests for OpenVINO samples by creating a build directory, navigating into it, running CMake, and then installing the tests using the cmake_install.cmake script. The `<INSTALL_DIR>` placeholder needs to be replaced with the actual OpenVINO installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/samples_tests/smoke_tests/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd <working directory>/tests/samples_tests/smoke_tests\nmkdir build && cd build\ncmake ../..\ncmake -DCOMPONENT=tests -DCMAKE_INSTALL_PREFIX=<INSTALL_DIR> -P cmake_install.cmake\n```\n\n----------------------------------------\n\nTITLE: ReduceProd Layer with keep_dims=true (XML)\nDESCRIPTION: This XML snippet defines a ReduceProd layer with the 'keep_dims' attribute set to 'true'. The input tensor has dimensions 6x12x10x24, and the reduction is performed along axes 2 and 3, resulting in an output tensor with dimensions 6x12x1x1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-prod-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceProd\" ...>\n    <data keep_dims=\"true\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINO Developer Package\nDESCRIPTION: This snippet finds the OpenVINO Developer Package and sets relevant variables based on its location. It checks if the package is relocatable and sets the OpenVINO source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED OpenVINO_SOURCE_DIR)\n    find_package(OpenVINODeveloperPackage REQUIRED)\n\n    # we assume that OpenVINODeveloperPackage is generated in OpenVINO build tree\n    set(OpenVINO_BINARY_DIR \"${OpenVINODeveloperPackage_DIR}\")\n    # but this can be invalid for cases of OpenVINODeveloperPackage relocatable installation\n    # so, we need to disable wheen generation for this case\n    if(NOT EXISTS \"${OpenVINO_BINARY_DIR}/cmake_install.cmake\")\n        set(OpenVINODeveloperPackage_RELOCATABLE ON)\n    endif()\n\n    set(OpenVINO_SOURCE_DIR \"${OpenVINOPython_SOURCE_DIR}/../../../\")\nendif()\n\nif(NOT DEFINED OpenVINODeveloperPackage_RELOCATABLE)\n    set(OpenVINODeveloperPackage_RELOCATABLE OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building the Project with CMake\nDESCRIPTION: This command builds the project using CMake with the Release configuration. The `--parallel` flag enables parallel compilation, speeding up the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_with_sycl.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncmake --build . --config Release --parallel\n```\n\n----------------------------------------\n\nTITLE: DeformablePSROIPooling Layer Configuration (Two Inputs)\nDESCRIPTION: This XML snippet shows a configuration for the DeformablePSROIPooling layer with two inputs: position score maps and regions of interest.  It demonstrates the structure of the layer, including the `data` attributes and the input and output port specifications. The `spatial_scale`, `output_dim`, and `group_size` attributes are required.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/deformable-psroi-pooling-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"DeformablePSROIPooling\" ... >\n    <data spatial_scale=\"0.0625\" output_dim=\"882\" group_size=\"3\" mode=\"bilinear_deformable\" spatial_bins_x=\"4\" spatial_bins_y=\"4\" trans_std=\"0.0\" part_size=\"3\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>7938</dim>\n            <dim>63</dim>\n            <dim>38</dim>\n        </port>\n        <port id=\"1\">\n            <dim>300</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>300</dim>\n            <dim>882</dim>\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Enabling LTO Globally in CMake\nDESCRIPTION: This snippet enables Link Time Optimization (LTO) globally for all libraries defined below in the CMake script.  The `ENABLE_LTO` variable is assumed to be defined elsewhere, likely in a parent CMakeLists.txt file. LTO can improve performance by optimizing across different compilation units.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Defining Wheel Package Check Function\nDESCRIPTION: This snippet checks the conditions for building wheel packages. It checks for the `patchelf` tool on Linux and checks the CMake version. If `patchelf` is not found or the CMake version is too low, it disables wheel building.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\n# user explicitly specified ENABLE_WHEEL=ON\nif(ENABLE_WHEEL)\n    set(find_package_mode REQUIRED)\n    set(message_mode FATAL_ERROR)\nelse()\n    set(find_package_mode QUIET)\n    set(message_mode WARNING)\nendif()\n\nset(wheel_reqs \"${OpenVINOPython_SOURCE_DIR}/wheel/requirements-dev.txt\")\nov_check_pip_packages(REQUIREMENTS_FILE \"${OpenVINOPython_SOURCE_DIR}/wheel/requirements-dev.txt\"\n                      RESULT_VAR ENABLE_WHEEL_DEFAULT\n                      MESSAGE_MODE WARNING)\n\nif(LINUX)\n    find_host_program(patchelf_program\n                      NAMES patchelf\n                      DOC \"Path to patchelf tool\")\n    if(NOT patchelf_program)\n        set(ENABLE_WHEEL_DEFAULT OFF)\n        message(${message_mode} \"patchelf is not found. It is required to build OpenVINO Runtime wheel. Install via `pip install patchelf` or `apt install patchelf`.\")\n    endif()\nendif()\n\nif(CMAKE_VERSION VERSION_GREATER_EQUAL 3.15)\n    set(SETUP_PY_REQUIREMENTS_FOUND ON)\nelse()\n    message(${message_mode} \"Cmake version 3.15 and higher is required to build 'openvino' wheel. Provided version ${CMAKE_VERSION}\")\n    set(SETUP_PY_REQUIREMENTS_FOUND OFF)\nendif()\n\nif(NOT SETUP_PY_REQUIREMENTS_FOUND)\n    # setup.py requirements are importnant to build wheel\n    set(ENABLE_WHEEL_DEFAULT OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Run specific test files\nDESCRIPTION: Runs specific OpenVINO JavaScript API test files using node. This allows targeting individual tests or groups of tests using glob patterns.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/test_examples.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnode --test \"tests/unit/core.test.js\" \"tests/unit/*model.test.js\"\n```\n\n----------------------------------------\n\nTITLE: Create Conda Environment (Azure ML)\nDESCRIPTION: This command creates a Conda environment named `openvino_env` with Python 3.9 in the Azure ML environment.  The `-y` flag automatically answers 'yes' to any prompts during the environment creation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_18\n\nLANGUAGE: console\nCODE:\n```\nconda create --name openvino_env python=3.9 -y\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags (GCC)\nDESCRIPTION: This snippet adds a compiler flag to suppress missing declaration warnings when using GCC.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX)\n    ov_add_compiler_flags(-Wno-missing-declarations)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Sphinx Theme using pip\nDESCRIPTION: This snippet shows how to install the `openvino_sphinx_theme` using pip. It first changes the directory to the theme's location and then installs it using `python -m pip install --user .`.  The `--user` flag installs the package in the user's home directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/openvino_sphinx_theme/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd openvino/docs/openvino_sphinx_theme\npython -m pip install --user .\n```\n\n----------------------------------------\n\nTITLE: Adding Source Subdirectory in CMake\nDESCRIPTION: This CMake command adds the 'src' subdirectory to the build process. It includes the CMakeLists.txt file present in the 'src' directory to define how the source code should be compiled and linked.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/ir/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Import Model with Remote Context C++\nDESCRIPTION: This snippet shows how to import compiled model with remote context.  The importing of compiled model mechanism allows to import a previously exported backend specific model and wrap it using an `CompiledModel` object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nvoid Plugin::import_model(std::istream& model, const ov::RemoteContext& context, const ov::AnyMap& properties) {\n    OPENVINO_ASSERT(context, \"Remote context doesn't exist\");\n    // 1. Update plugin config with compile config\n    auto cfg = Configuration{properties, get_stream_executor(properties)};\n    auto compiled_model = std::make_shared<CompiledModel>(model, cfg, shared_from_this(), context);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Handle Missing TensorFlow\nDESCRIPTION: If TensorFlow is not found, this block adds a custom command to display a warning message during build time indicating that the TensorFlow Lite test models cannot be generated and the tests might fail.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nelse()\n    # Produce warning message at build time as well\n    add_custom_command(OUTPUT unable_build_tensorflow_models.txt\n            COMMAND ${CMAKE_COMMAND}\n            -E cmake_echo_color --red \"Warning: Unable to generate tensorflow lite test models. Running '${TARGET_NAME}' will likely fail\"\n            )\n    add_custom_target(tensorflow_lite_test_models DEPENDS unable_build_tensorflow_models.txt)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Sqrt Layer Definition with Integer Input in OpenVINO (XML)\nDESCRIPTION: This XML snippet illustrates the Sqrt layer in OpenVINO applied to integer input values. It showcases the input and output ports, dimensions, and the rounded integer results after applying the square root operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/sqrt-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Sqrt\">\n    <input>\n        <port id=\"0\">\n            <dim>4</dim> <!-- int input values: [4, 7, 9, 10] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>4</dim> <!-- int output values: [2, 3, 3, 3] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Pinning GitHub Actions Version with Commit Hash\nDESCRIPTION: This snippet demonstrates pinning a GitHub Action to a specific commit hash instead of using a tag. This protects the workflow from unexpected changes or supply chain compromises by ensuring that the same version of the action is always used. It shows the recommended approach.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/security.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nuses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Target CMake\nDESCRIPTION: This snippet adds an executable target for the `ov_subgraphs_dumper` tool. It specifies the source directory, includes, linked libraries, dependencies, and compile definitions.  The `ov_add_test_target` macro encapsulates adding an executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/subgraphs_dumper/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        TYPE EXECUTABLE\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}/src\n        INCLUDES\n            PRIVATE\n                \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\"\n        LINK_LIBRARIES\n            PRIVATE\n                ${LIBRARIES}\n        DEPENDENCIES\n            ov_frontends\n            ${LIBRARIES}\n        DEFINES\n            ${COMPILE_DEFINITIONS}\n        ADD_CPPLINT\n)\n```\n\n----------------------------------------\n\nTITLE: Running MemCheckTests (Python)\nDESCRIPTION: This script executes `MemCheckTests` using a Python script that wraps `gtest-parallel`. It requires the paths to `gtest_parallel`, the `MemCheckTests` executable, a test configuration file, and a reference configuration file.  The script likely handles parallel execution and configuration management.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython ./scripts/run_memcheck.py --gtest_parallel <gtest_parallel_py_path> <openvino_bin>/MemCheckTests -- --test_conf=<test_conf_path> --refs_conf=<refs_conf_path>\n```\n\n----------------------------------------\n\nTITLE: ReduceMean Example with keep_dims=false\nDESCRIPTION: Illustrates the ReduceMean operation in XML format, where `keep_dims` is set to `false`.  The reduced axes are removed from the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-mean-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceMean\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating a PrePostProcessor Object in OpenVINO (Python)\nDESCRIPTION: This snippet demonstrates how to create an instance of the `ov::preprocess::PrePostProcessor` class in Python.  This object is used to define preprocessing steps for a model read from disk before loading it onto a device.  No specific dependencies are required beyond the OpenVINO Python API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n```\n\n----------------------------------------\n\nTITLE: Creating a PrePostProcessor Object in OpenVINO (C++)\nDESCRIPTION: This snippet demonstrates how to create an instance of the `ov::preprocess::PrePostProcessor` class in C++.  This object is used to define preprocessing steps for a model read from disk before loading it onto a device. The snippet uses the `ov::preprocess::PrePostProcessor` constructor with the model object as its argument.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n```\n\n----------------------------------------\n\nTITLE: Compiling Model with Performance Hint in Python\nDESCRIPTION: This Python snippet demonstrates how to compile a model using OpenVINO and specify the `THROUGHPUT` performance hint. This hint optimizes the model for maximum throughput on the target device. The snippet assumes that the `core` object and `model` object are already defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/high-level-performance-hints.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncompiled_model = core.compile_model(model=model, device_name=\"CPU\", config={ov.hint.performance_mode: ov.hint.PerformanceMode.THROUGHPUT})\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for ONNX Targets\nDESCRIPTION: This snippet iterates through the `onnx` and `onnx_proto` targets, adding the interface include directories from the protobuf library to their respective include directories. This ensures that the ONNX targets can find the necessary protobuf headers during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/onnx/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(_onnx_target onnx onnx_proto)\n    target_include_directories(${_onnx_target} SYSTEM PRIVATE\n        $<TARGET_PROPERTY:protobuf::libprotobuf,INTERFACE_INCLUDE_DIRECTORIES>)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: I420toRGB Layer Definition (Single Plane) XML\nDESCRIPTION: Defines an I420toRGB layer in XML format, demonstrating the conversion from a single-plane I420 input to an RGB output. The input has dimensions 1x720x640x1, and the output is 1x480x640x3.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/i420-to-rgb-8.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"I420toRGB\">\n       <input>\n           <port id=\"0\">\n               <dim>1</dim>\n               <dim>720</dim>\n               <dim>640</dim>\n               <dim>1</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"1\">\n               <dim>1</dim>\n               <dim>480</dim>\n               <dim>640</dim>\n               <dim>3</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Tan Operation Example 1 XML\nDESCRIPTION: This example demonstrates the Tan operation with a sample input tensor and its corresponding output after applying the element-wise tangent function.  The input is a list of floating-point values, and the output shows the tangent of each value. The configuration is represented in XML format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/tan-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\ninput = [0.0, 0.25, -0.25, 0.5, -0.5]\noutput = [0.0, 0.25534192, -0.25534192, 0.54630249, -0.54630249]\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to Target in CMake\nDESCRIPTION: This snippet links necessary libraries to the ov_appverifier_tests target.  It links gtest, pugixml, and gflags as public libraries, and gtest_main as a private library. These libraries provide functionality for testing, XML parsing, and command-line argument parsing, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/appverifier_tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME}\n    PUBLIC\n        gtest\n        pugixml\n        gflags\n    PRIVATE\n        gtest_main)\n```\n\n----------------------------------------\n\nTITLE: Defining OpenVINO Output Interface in TypeScript\nDESCRIPTION: Defines the `Output` interface, which represents the output of a model. It specifies properties like `anyName` (a string), `shape` (an array of numbers), and methods to retrieve these properties and convert the object to a string. This interface is part of the OpenVINO TypeScript bindings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Output.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface Output {\n    anyName: string;\n    shape: number[];\n    getAnyName(): string;\n    getPartialShape(): PartialShape;\n    getShape(): number[];\n    toString(): string;\n}\n```\n\n----------------------------------------\n\nTITLE: Define workflow triggers in YAML\nDESCRIPTION: This YAML snippet defines the triggers for a GitHub Actions workflow. It includes a schedule trigger (cron job), a pull request trigger (on PRs with path filtering), and a push trigger (on pushes to master or releases branches with path filtering).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/overview.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\non:\n  schedule:\n    # at 00:00 on Wednesday and Saturday\n    - cron: '0 0 * * 3,6'\n  pull_request:\n    paths:\n      - '**'\n      - '!**/docs/**'\n      - '!docs/**'\n      - 'docs/snippets/**'\n      - '!**/**.md'\n      - '!**.md'\n  push:\n    paths:\n      - '**'\n      - '!docs/**'\n      - '!**/docs/**'\n      - 'docs/snippets/**'\n      - '!**/**.md'\n      - '!**.md'\n    branches:\n      - master\n      - 'releases/**'\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO using pip\nDESCRIPTION: This command installs the OpenVINO package using pip, the Python package installer. The -U flag ensures that the package is upgraded to the latest version if it is already installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install -U openvino\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Kernels with Classification Sample - C++\nDESCRIPTION: This C++ command-line example demonstrates how to load custom kernels for the classification sample using the `-c` option.  It specifies the path to the model, input image, device (GPU), and the custom kernel configuration file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-gpu-operations.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n$ ./classification_sample -m <path_to_model>/bvlc_alexnet_fp16.xml -i ./validation_set/daily/227x227/apron.bmp -d GPU\n-c <absolute_path_to_config>/custom_layer_example.xml\n```\n\n----------------------------------------\n\nTITLE: Adding Preprocessor Definition for Unit Tests in CMake\nDESCRIPTION: This adds a preprocessor definition `MULTIUNITTEST`.  This is likely used in the C++ code to conditionally compile code specific to the unit testing environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/unit/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_definitions(-DMULTIUNITTEST)\n```\n\n----------------------------------------\n\nTITLE: BitwiseOr Example with uint8 Tensors in Python\nDESCRIPTION: This example demonstrates the BitwiseOr operation with uint8 tensors. It shows how the operation converts uint8 values to their binary representation, performs a bitwise OR, and then converts the result back to uint8.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-or-13.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# For given uint8 inputs:\na = [21, 120]\nb = [3, 37]\n# Create a binary representation of uint8:\n# binary a: [00010101, 01111000]\n# binary b: [00000011, 00100101]\n# Perform bitwise OR of corresponding elements in a and b:\n# [00010111, 01111101]\n# Convert binary values to uint8:\noutput = [23, 125]\n```\n\n----------------------------------------\n\nTITLE: Starting vTPM for Runtime VM (sh)\nDESCRIPTION: Starts the swtpm socket for the Runtime VM, clears and initializes the TPM, writes the HW quote data into NVRAM, and restarts the vTPM for QEMU. It specifies different ports (8380 and 8381) and a different TPM state directory (/var/OVSA/vtpm/vtpm_runtime).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_24\n\nLANGUAGE: sh\nCODE:\n```\nsudo swtpm socket --tpm2 --server port=8380 \\\n --ctrl type=tcp,port=8381 \\\n --flags not-need-init --tpmstate dir=/var/OVSA/vtpm/vtpm_runtime &\n\nsudo tpm2_startup --clear -T swtpm:port=8380\nsudo tpm2_startup -T swtpm:port=8380\npython3 <path to Security-Addon source>/Scripts/host/OVSA_write_hwquote_swtpm_nvram.py 8380\nsudo pkill -f vtpm_runtime\n\nswtpm socket --tpmstate dir=/var/OVSA/vtpm/vtpm_runtime \\\n --tpm2 \\\n --ctrl type=unixio,path=/var/OVSA/vtpm/vtpm_runtime/swtpm-sock \\\n --log level=20\n```\n\n----------------------------------------\n\nTITLE: Split Layer Configuration in XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a Split layer in OpenVINO. It defines the layer's id, type, number of splits, input ports (data and axis), and output ports with their corresponding dimensions. The 'num_splits' attribute determines how many output tensors are created from the input data tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/split-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Split\" ...>\n    <data num_splits=\"3\" />\n    <input>\n        <port id=\"0\">       <!-- some data -->\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">       <!-- axis: 1 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>4</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"3\">\n            <dim>6</dim>\n            <dim>4</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"4\">\n            <dim>6</dim>\n            <dim>4</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Check Codestyle of Tests with flake8 Python\nDESCRIPTION: Checks the code style of the OpenVINO™ Python API tests using `flake8`. This command runs `flake8` on the specified tests directory (`./tests/`) using a custom configuration file (`setup.cfg`).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npython -m flake8 ./tests/ --config=setup.cfg\n```\n\n----------------------------------------\n\nTITLE: Uninstall Specific OpenVINO Runtime Version Example\nDESCRIPTION: This command uninstalls the OpenVINO Runtime version 2025.1.0 using the YUM package manager. It requires sudo privileges to remove software packages and their dependencies. This is an example demonstrating how to remove a specific version of OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\nsudo yum autoremove openvino-2025.1.0\n```\n\n----------------------------------------\n\nTITLE: OVSA Model Server Configuration\nDESCRIPTION: Example JSON configuration for the OpenVINO Model Server that defines a custom loader for OVSA and specifies the model details, including the keystore and controlled access file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_54\n\nLANGUAGE: sh\nCODE:\n```\n{\n\"custom_loader_config_list\":[\n\t{\n\t\t\"config\":{\n\t\t\t\t\"loader_name\":\"ovsa\",\n\t\t\t\t\"library_path\": \"/ovsa-runtime/lib/libovsaruntime.so\"\n\t\t}\n\t}\n],\n\"model_config_list\":[\n\t{\n\t\"config\":{\n\t   \t\"name\":\"controlled-access-model\",\n\t\t\t\"base_path\":\"/sampleloader/model/fd\",\n\t   \t\"custom_loader_options\": {\"loader_name\":  \"ovsa\", \"keystore\":  \"custkeystore\", \"controlled_access_file\": \"<name-of-the-model>\"}\n\t}\n\t}\n]\n}\n```\n\n----------------------------------------\n\nTITLE: NotEqual Layer Definition Example 1 in C++\nDESCRIPTION: This C++ code snippet shows an example of a NotEqual layer definition in OpenVINO with input tensors of the same shape. It defines the input and output ports with their corresponding dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/notequal-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"NotEqual\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: SoftMax XML Layer Configuration in OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of a SoftMax layer within an OpenVINO model. It showcases the 'type' attribute set to 'SoftMax', the optional 'axis' data attribute, and the input and output layer connections. The 'axis' attribute determines the dimension along which the SoftMax function is applied.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/softmax-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"SoftMax\" ... >\n    <data axis=\"1\" />\n    <input> ... </input>\n    <output> ... </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version Based on Platform\nDESCRIPTION: This snippet sets the minimum required CMake version based on the operating system.  It uses version 3.16 for Windows and 3.13 for other platforms to support features for finding Python.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n    # 3.16: FindPython3.cmake can find Python via -DPython3_EXECUTABLE\n    # 3.18: FindPython3.cmake can find Python automatically from virtualenv\n    cmake_minimum_required(VERSION 3.16)\nelse()\n    # 3.13: default choice\n    cmake_minimum_required(VERSION 3.13)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Test Target CMake\nDESCRIPTION: Creates the test target using `ov_add_test_target`. Defines the test name, root directory, excluded source paths, dependencies, and link libraries.  Also sets labels to categorize the tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n    NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        EXCLUDED_SOURCE_PATHS ${EXCLUDED_TESTS}\n        DEPENDENCIES ${TF_TESTS_DEPENDENCIES}\n        LINK_LIBRARIES\n            gtest_main_manifest \n            frontend_shared_test_classes \n            openvino_tensorflow_frontend \n            openvino_tensorflow_common\n        ADD_CLANG_FORMAT\n        LABELS\n            ${ctest_labels} TF_FE\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Protobuf as Subdirectory\nDESCRIPTION: This snippet adds the Protobuf source directory as a subdirectory in the CMake project. It determines the path to the Protobuf CMakeLists.txt file and adds it using `add_subdirectory`. It also retrieves the Protobuf version from the subdirectory's properties.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/protobuf/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/protobuf/CMakeLists.txt\")\n    set(protobuf_dir protobuf)\nelse()\n    set(protobuf_dir protobuf/cmake)\nendif()\n\nadd_subdirectory(${protobuf_dir} EXCLUDE_FROM_ALL)\nget_directory_property(protobuf_VERSION DIRECTORY ${protobuf_dir} DEFINITION protobuf_VERSION)\n\nset(Protobuf_INCLUDE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}/protobuf/src)\n```\n\n----------------------------------------\n\nTITLE: MatMul Vector-Matrix Multiplication XML Configuration\nDESCRIPTION: Configuration example demonstrating MatMul operation for vector-matrix multiplication. The input ports define the dimensions of the input vector (1024) and the input matrix (1024x1000). The output port defines the resulting vector (1000).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/matmul-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MatMul\">\n    <input>\n        <port id=\"0\">\n            <dim>1024</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1024</dim>\n            <dim>1000</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1000</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Asinh Layer XML Configuration in OpenVINO\nDESCRIPTION: This XML snippet demonstrates how to configure an Asinh layer in OpenVINO. It shows the input and output ports, along with their dimensions. This layer performs an element-wise inverse hyperbolic sine operation on the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/asinh-3.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Asinh\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Finding Subdirectories and Excluding Paths\nDESCRIPTION: Finds all subdirectories within the 'impls' directory and stores them in the `SUBDIRS` variable. Populates `AVAILABLE_IMPL_TYPES` with directory names and `EXCLUDE_PATHS` with full paths of the directories to be excluded.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SUBDIRS \"${CMAKE_CURRENT_SOURCE_DIR}/impls/*\")\n\nforeach(SUBDIR IN LISTS SUBDIRS)\n    if(IS_DIRECTORY ${SUBDIR})\n        get_filename_component(SUBDIR_NAME ${SUBDIR} NAME)\n        list(APPEND AVAILABLE_IMPL_TYPES ${SUBDIR_NAME})\n        list(APPEND EXCLUDE_PATHS ${SUBDIR})\n    endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Less-1 Layer Definition (Same Dimensions) - XML\nDESCRIPTION: Defines a Less-1 layer in OpenVINO for element-wise comparison when input tensors have the same dimensions. It specifies the input and output ports with their respective dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/less-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Less\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO Sample with CMake\nDESCRIPTION: This snippet uses the `ov_add_sample` CMake macro to define the build configuration for the `hello_reshape_ssd` sample. It specifies the source file (`main.cpp`) and the required dependencies: `format_reader` and `ie_samples_utils`. The macro handles linking these dependencies during the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_reshape_ssd/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_sample(NAME hello_reshape_ssd\n              SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/main.cpp\"\n              DEPENDENCIES format_reader ie_samples_utils)\n```\n\n----------------------------------------\n\nTITLE: GatherND Leading Dimensions Example\nDESCRIPTION: Shows how GatherND handles indices with leading dimensions.  The structure of the 'indices' tensor dictates the output shape and how elements are gathered from 'data'. This demonstrates GatherND's ability to work with multi-dimensional index tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-8.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nindices = [[[1]], [[0]]]\ndata    = [[1, 2],\n              [3, 4]]\noutput  = [[[3, 4]],\n              [[1, 2]]]\n```\n\n----------------------------------------\n\nTITLE: Conditional Execution of Parent Job Based on Skip Workflow (YAML)\nDESCRIPTION: This YAML snippet shows how to conditionally execute a parent job (e.g., Build) based on the `skip_workflow` output from the Smart CI job. This allows for skipping the entire workflow if certain conditions are met.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_15\n\nLANGUAGE: YAML\nCODE:\n```\nBuild:\n  needs: Smart_CI\n  ...\n  if: \"!needs.smart_ci.outputs.skip_workflow\"\n  ...\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories\nDESCRIPTION: This snippet adds several subdirectories to the build process.  Each subdirectory contains source code and CMakeLists.txt for different components: `common`, `unittests`, `memleaks_tests`, and `memcheck_tests`. CMake will process the CMakeLists.txt files in each of these subdirectories during the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(common)\nadd_subdirectory(unittests)\nadd_subdirectory(memleaks_tests)\nadd_subdirectory(memcheck_tests)\n```\n\n----------------------------------------\n\nTITLE: Reshape multiple inputs with dynamic dimension - Python\nDESCRIPTION: This Python code snippet demonstrates how to iterate through all input layers of a model and set the second dimension of each input to be dynamic.  This is especially useful for NLP models with multiple input layers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# ! [reshape_multiple_inputs]\nfor input_layer in model.inputs:\n    input_shape = input_layer.shape\n    input_shape[1] = ov.Dimension(-1)\n    model.reshape({input_layer.get_any_name(): input_shape})\n# ! [reshape_multiple_inputs]\n```\n\n----------------------------------------\n\nTITLE: Target Compile Definitions (CMake)\nDESCRIPTION: Adds compile definitions to the target.  `XBYAK_NO_OP_NAMES` and `XBYAK64` are added privately.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME} PRIVATE XBYAK_NO_OP_NAMES XBYAK64)\n```\n\n----------------------------------------\n\nTITLE: Glob Source and Header Files CMake\nDESCRIPTION: Uses `file(GLOB_RECURSE)` to find all C++ source files (`.cpp`) under the `src` directory and header files (`.hpp`) under the `include` directory.  These lists of files are then used to define the library's source code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/onnx_common/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE PUBLIC_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/include/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Primitive Structure Definition in C++\nDESCRIPTION: This C++ code snippet defines the structure of a `primitive`, which represents an operation in the GPU plugin's graph structure (topology).  It contains essential information about the operation, including its unique ID, type, output padding, and input IDs.  The `primitive_id` is a unique string identifier assigned to each primitive.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/basic_data_structures.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nstruct primitive {\n...\n    const primitive_id id;\n    const primitive_type* type;\n    padding output_padding;\n    std::vector<primitive_id> input;\n...;\n}\n```\n\n----------------------------------------\n\nTITLE: Transpose Layer Example 1 XML\nDESCRIPTION: This example shows a Transpose layer configuration in XML format, where the input tensor of shape [2, 3, 4] is transposed according to the order [2, 0, 1], resulting in an output tensor of shape [4, 2, 3]. The `input_order` specifies the permutation to apply to the input tensor's axes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/transpose-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Transpose\">\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>  <!-- [2, 0, 1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>4</dim>\n            <dim>2</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake with vcpkg toolchain file\nDESCRIPTION: This command configures the CMake build system for the OpenVINO C++ samples, specifying the vcpkg toolchain file to use OpenCL. Replace `path/to/vcpkg` with the actual path to your vcpkg installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/use_device_mem.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nopenvino_install\\samples\\cpp> cmake -DCMAKE_BUILD_TYPE=Release -B build -DCMAKE_TOOLCHAIN_FILE=path/to/vcpkg/scripts/buildsystems/vcpkg.cmake\n```\n\n----------------------------------------\n\nTITLE: wait_pipeline() Method C++\nDESCRIPTION: This code snippet shows the implementation of the `wait_pipeline()` method. It waits for the pipeline to complete, which is relevant in cases where the plugin supports asynchronous execution, ensuring all computations have finished before proceeding.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/synch-inference-request.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nvoid InferRequest::wait_pipeline() {\n    // Waits a pipeline in case of plugin asynchronous execution\n}\n```\n\n----------------------------------------\n\nTITLE: Set Compile Definitions for Static Library\nDESCRIPTION: This snippet conditionally sets a compile definition if `BUILD_SHARED_LIBS` is not enabled.  If building a static library, it adds the `OPENVINO_STATIC_LIBRARY` definition.  This is necessary to differentiate between shared and static library builds during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME} PUBLIC OPENVINO_STATIC_LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Update Git Submodules (Bash)\nDESCRIPTION: This command ensures that all Git submodules required for the project are properly initialized and updated to their latest versions. This step is crucial for including external dependencies that are managed as submodules within the main OpenVINO repository.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Core Property Declaration\nDESCRIPTION: Declares the Core property within the NodeAddon interface, which is an instance of the CoreConstructor.  The Core object is essential for loading and managing models within OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/addon.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nCore: CoreConstructor\n```\n\n----------------------------------------\n\nTITLE: ExtractImagePatches Output Example 3 C++\nDESCRIPTION: This C++ snippet illustrates the output of the ExtractImagePatches operation with sizes=\"4,4\", strides=\"9,9\", rates=\"1,1\", and auto_pad=\"same_upper\". The output shape is [1, 16, 2, 2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/extract-image-patches-3.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n[[[[  0   0]\n   [  0  89]]\n\n  [[  0   0]\n   [ 81  90]]\n\n  [[  0   0]\n   [ 82   0]]\n\n  [[  0   0]\n   [ 83   0]]\n\n  [[  0   9]\n   [  0  99]]\n\n  [[  1  10]\n   [ 91 100]]\n\n  [[  2   0]\n   [ 92   0]]\n\n  [[  3   0]\n   [ 93   0]]\n\n  [[  0  19]\n   [  0   0]]\n\n  [[ 11  20]\n   [  0   0]]\n\n  [[ 12   0]\n   [  0   0]]\n\n  [[ 13   0]\n   [  0   0]]\n\n  [[  0  29]\n   [  0   0]]\n\n  [[ 21  30]\n   [  0   0]]\n\n  [[ 22   0]\n   [  0   0]]\n\n  [[ 23   0]\n   [  0   0]]]]\n```\n\n----------------------------------------\n\nTITLE: SquaredDifference XML Layer Definition (Numpy Broadcasting)\nDESCRIPTION: This XML snippet defines a SquaredDifference layer in OpenVINO with numpy broadcasting enabled. The `auto_broadcast` attribute is set to \"numpy\", allowing the input tensors to have different dimensions which are then broadcasted according to numpy rules. The input and output ports demonstrate how the dimensions are adjusted after broadcasting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/squared-difference-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"SquaredDifference\">\n    <data auto_broadcast=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating InferRequest from CompiledModel (TypeScript)\nDESCRIPTION: This code shows the declaration of the `createInferRequest` method within the `CompiledModel` interface. This method creates an `InferRequest` object to perform inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ncreateInferRequest(): InferRequest\n```\n\n----------------------------------------\n\nTITLE: Adding CPack Component\nDESCRIPTION: This CMake command adds a hidden component for licensing using `ov_cpack_add_component`. It takes `${OV_CPACK_COMP_LICENSING}` as an argument, which is assumed to be a variable defining the name of the licensing component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nov_cpack_add_component(${OV_CPACK_COMP_LICENSING} HIDDEN)\n```\n\n----------------------------------------\n\nTITLE: ROIAlign Layer Configuration XML - OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of an ROIAlign layer within the OpenVINO framework. It specifies the layer's attributes such as `pooled_h`, `pooled_w`, `spatial_scale`, `sampling_ratio`, `mode`, and `aligned_mode`. The input and output port dimensions are also defined, showcasing how the layer integrates with other components in a neural network.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/roi-align-9.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ROIAlign\" ... >\n    <data pooled_h=\"6\" pooled_w=\"6\" spatial_scale=\"16.0\" sampling_ratio=\"2\" mode=\"avg\" aligned_mode=\"half_pixel\"/>\n    <input>\n        <port id=\"0\">\n            <dim>7</dim>\n            <dim>256</dim>\n            <dim>200</dim>\n            <dim>200</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1000</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1000</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP32\">\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>6</dim>\n            <dim>6</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Standalone Build Instructions\nDESCRIPTION: These commands detail how to build the Compile Tool as a standalone application using CMake. It includes creating a build directory, configuring the build with the OpenVINO directory, building in Release mode, and installing the tool. Note that the exact command might differ based on platform.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/compile_tool/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmkdir compile_tool_build && cd compile_tool_build\ncmake -DOpenVINO_DIR=<openvino_install_dir>/runtime/cmake <compile_tool_source_dir>\ncmake --build . --config Release\ncmake --install . --prefix <compile_tool_install_dir>\n```\n\n----------------------------------------\n\nTITLE: Install Wheel File - CMake\nDESCRIPTION: This CMake code installs the generated wheel file to the destination directory specified by `OV_CPACK_WHEELSDIR`. It also defines the component and exclusion settings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES ${openvino_wheel_path}\n        DESTINATION ${OV_CPACK_WHEELSDIR}\n        COMPONENT ${OV_CPACK_COMP_PYTHON_WHEELS}\n        ${OV_CPACK_COMP_PYTHON_WHEELS_EXCLUDE_ALL})\n```\n\n----------------------------------------\n\nTITLE: Tan Layer Definition XML\nDESCRIPTION: This code snippet illustrates the XML structure for defining a Tan layer within an OpenVINO model. It includes input and output port configurations, specifying the dimensions of the tensors involved in the operation. This XML defines the connectivity and shape information for the Tan layer within the overall model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/tan-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Tan\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Atanh Layer XML Configuration in OpenVINO\nDESCRIPTION: This XML snippet shows how to configure an Atanh layer in OpenVINO. It defines the input and output ports with their respective dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/atanh-3.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Atanh\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: CTCGreedyDecoderSeqLen XML Example\nDESCRIPTION: This XML snippet demonstrates the configuration of a CTCGreedyDecoderSeqLen layer within an OpenVINO model. It specifies attributes such as `merge_repeated`, `classes_index_type`, and `sequence_length_type`, along with input and output port definitions including their dimensions and precision. The input ports define the shape of the input tensors (data, sequence_length, and blank_index), while the output ports specify the dimensions and precision of the decoded classes and their sequence lengths.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/ctc-greedy-decoder-seq-len-6.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"CTCGreedyDecoderSeqLen\" version=\"opset6\">\n    <data merge_repeated=\"true\" classes_index_type=\"i64\" sequence_length_type=\"i64\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>20</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"1\">\n            <dim>8</dim>\n        </port>\n        <port id=\"2\"/>  <!-- blank_index = 120 -->\n    </input>\n    <output>\n        <port id=\"0\" precision=\"I64\">\n            <dim>8</dim>\n            <dim>20</dim>\n        </port>\n        <port id=\"1\" precision=\"I64\">\n            <dim>8</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for Memory Tests Helper in CMake\nDESCRIPTION: This CMake command adds a subdirectory named 'memory_tests_helper' to the current build process. Similar to 'memory_tests', this directory is expected to have its own CMakeLists.txt file that defines how it should be built and integrated into the project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(memory_tests_helper)\n```\n\n----------------------------------------\n\nTITLE: Running VTune Profiler with OpenVINO\nDESCRIPTION: This command demonstrates how to run the Intel VTune Profiler to collect hotspot statistics for an OpenVINO application. It specifies sampling mode, stack collection, stack size, and sampling interval, along with the path to the benchmark application, number of threads, API type, number of iterations and inference requests, and the path to the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/docs/performance_analysis_ITT_counters.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nvtune -collect hotspots -k sampling-mode=hw -k enable-stack-collection=true -k stack-size=0 -k sampling-interval=0.5 -- ./benchmark_app -nthreads=1 -api sync -niter 1 -nireq 1 -m ./resnet-50-pytorch/resnet-50-pytorch.xml\n```\n\n----------------------------------------\n\nTITLE: Include Install TBB CMake Module\nDESCRIPTION: This snippet includes the cmake/install_tbb.cmake module, which likely handles the installation or configuration of the Intel Threading Building Blocks (TBB) library. TBB is often used for parallel processing and concurrency in OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(cmake/install_tbb.cmake)\n```\n\n----------------------------------------\n\nTITLE: Installing Target\nDESCRIPTION: This snippet uses the `install` command to specify how the library should be installed.  It sets the destination directory to 'tests', and includes it under the 'tests' component.  It is excluded from all install targets by default.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/test_builtin_extensions/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n        LIBRARY DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Build Conformance Tests\nDESCRIPTION: These commands build the necessary targets for running the OpenVINO conformance tests, including the subgraphs dumper, API conformance tests, and opset conformance tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/README.md#_snippet_1\n\nLANGUAGE: makefile\nCODE:\n```\nmake --jobs=$(nproc --all) ov_subgraphs_dumper\nmake --jobs=$(nproc --all) ov_op_conformance_tests\nmake --jobs=$(nproc --all) ov_api_conformance_tests\n```\n\n----------------------------------------\n\nTITLE: ShapeOf-3 XML Layer Definition\nDESCRIPTION: This XML snippet demonstrates the configuration of a ShapeOf-3 layer in OpenVINO. It specifies the input tensor's dimensions and the resulting output tensor's dimension, which represents the rank of the input tensor. The `output_type` attribute determines the data type of the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/shape-of-3.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ShapeOf\">\n    <data output_type=\"i64\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">  <!-- output value is: [2,3,224,224]-->\n            <dim>4</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Enforcing NCC Naming Style (CMake)\nDESCRIPTION: This snippet enforces the NCC naming style for the `openvino_tensorflow_common` library. It specifies the source directories and additional include directories for the naming style check.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/src/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nov_ncc_naming_style(FOR_TARGET ${TARGET_NAME}\n                    SOURCE_DIRECTORIES \"${root_dir}/include\"\n                                       \"${root_dir}/src\"\n                    ADDITIONAL_INCLUDE_DIRECTORIES\n                        $<TARGET_PROPERTY:${TARGET_NAME},INTERFACE_INCLUDE_DIRECTORIES>\n                        $<TARGET_PROPERTY:${TARGET_NAME},INCLUDE_DIRECTORIES>)\n```\n\n----------------------------------------\n\nTITLE: CompiledModel Constructor Implementation C++\nDESCRIPTION: This code snippet shows the constructor implementation for the CompiledModel class. It accepts a shared pointer to an ov::Model, a shared pointer to the Plugin, and an optional configuration map. It initializes the class members and calls the compile_model() method to compile the model for the specific device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/compiled-model.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nCompiledModel::CompiledModel(const std::shared_ptr<ov::Model>& model, const std::shared_ptr<Plugin>& plugin, const ov::AnyMap& cfg) :\n    ov::ICompiledModel{{plugin->get_core(), model, cfg}},\n    m_cfg{cfg},\n    m_model{model} {\n    OPENVINO_ASSERT(model, \"nullptr model is not allowed.\");\n    compile_model();\n}\n```\n\n----------------------------------------\n\nTITLE: Inverse Layer Definition (3D input, adjoint=true)\nDESCRIPTION: This XML code defines an Inverse layer with a 3D input tensor, including a batch dimension of size 2. The `adjoint` attribute is set to `true`, indicating that the adjoint matrix should be computed. The input and output ports are specified with precision FP32 and dimensions representing the batch size, rows, and columns of the square matrices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/inverse-14.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... name=\"Inverse\" type=\"Inverse\">\n    <data adjoint=\"true\"/>\n    <input>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>2</dim> <!-- batch size of 2 -->\n            <dim>4</dim> <!-- 4 rows of square matrix -->\n            <dim>4</dim> <!-- 4 columns of square matrix -->\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"FP32\" names=\"Inverse:0\">\n            <dim>2</dim> <!-- batch size of 2 -->\n            <dim>4</dim> <!-- 4 rows of square matrix -->\n            <dim>4</dim> <!-- 4 columns of square matrix -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ROIAlignRotated XML Example\nDESCRIPTION: This XML snippet demonstrates the configuration of a ROIAlignRotated layer within an OpenVINO model. It defines the layer's type, data attributes (pooled_h, pooled_w, spatial_scale, sampling_ratio, clockwise_mode), input port dimensions, and output port dimensions and precision.  It illustrates how to specify the parameters required for the ROIAlignRotated operation in an OpenVINO XML model file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/roi-align-rotated-15.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ROIAlignRotated\" ... >\n    <data pooled_h=\"6\" pooled_w=\"6\" spatial_scale=\"16.0\" sampling_ratio=\"2\" clockwise_mode=\"True\"/>\n    <input>\n        <port id=\"0\">\n            <dim>7</dim>\n            <dim>256</dim>\n            <dim>200</dim>\n            <dim>200</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1000</dim>\n            <dim>5</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1000</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP32\">\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>6</dim>\n            <dim>6</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Set Target OpenCL Version\nDESCRIPTION: Sets the target version of OpenCL to be used by the GPU plugin. The value is stored in a cache variable and used to define a preprocessor macro.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset(INTEL_GPU_TARGET_OCL_VERSION \"300\"\n  CACHE STRING \"Target version of OpenCL which should be used by GPU plugin\")\n\nadd_definitions(-DCL_TARGET_OPENCL_VERSION=${INTEL_GPU_TARGET_OCL_VERSION})\n```\n\n----------------------------------------\n\nTITLE: Set Target Properties\nDESCRIPTION: This snippet sets the INTERPROCEDURAL_OPTIMIZATION_RELEASE property for the target to the value of ENABLE_LTO, enabling link time optimization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Building the ONNX Static Library in CMake\nDESCRIPTION: This snippet calls the `ov_onnx_build_static` function to initiate the building of the ONNX static library. It's a single-line call that triggers the build process configured within the function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/onnx/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_onnx_build_static()\n```\n\n----------------------------------------\n\nTITLE: Smart CI Job Definition in GitHub Actions Workflow (YAML)\nDESCRIPTION: This YAML snippet defines a Smart CI job within a GitHub Actions workflow. It specifies the job's outputs, which include the list of affected components and whether the workflow should be skipped, based on the results of the smart_ci step. The job uses a custom GitHub Action located at ./.github/actions/smart-ci.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\njobs:\n  Smart_CI:\n    outputs:\n      affected_components: \"${{ steps.smart_ci.outputs.affected_components }}\"\n      skip_workflow: \"${{ steps.smart_ci.outputs.skip_workflow }}\"\n    steps:\n      - name: Get affected components\n        id: smart_ci\n        uses: ./.github/actions/smart-ci\n        ...\n```\n\n----------------------------------------\n\nTITLE: Uninstall OpenVINO\nDESCRIPTION: Removes the extracted OpenVINO folder and the downloaded archive file using command-line tools.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-windows.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nrmdir /s <extracted_folder>\ndel <path_to_archive>\n```\n\n----------------------------------------\n\nTITLE: Link Libraries\nDESCRIPTION: This snippet links the `openvino_hetero_plugin` target with the `openvino::pugixml` library. It makes pugixml a private dependency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino::pugixml)\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Inclusion in CMake\nDESCRIPTION: This CMake code snippet conditionally includes the `sea_itt_lib` subdirectory into the current build process. The inclusion is dependent on the value of the `SELECTIVE_BUILD` variable being equal to the string \"COLLECT\".\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/itt_collector/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(SELECTIVE_BUILD STREQUAL \"COLLECT\")\n    add_subdirectory(sea_itt_lib)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Tanh Layer in OpenVINO XML\nDESCRIPTION: This XML snippet defines a Tanh layer within an OpenVINO model. It specifies the input and output ports along with their dimensions. The layer takes a tensor as input, performs the hyperbolic tangent operation element-wise, and outputs a tensor with the same shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/tanh-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Tanh\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: OVSA Certificate Storage\nDESCRIPTION: Adds a CA-signed certificate to the keystore using the ovsatool. It requires the certificate file and the keystore name as input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_49\n\nLANGUAGE: sh\nCODE:\n```\n/opt/ovsa/bin/ovsatool keygen -storecert -c custkeystore.csr.crt -k custkeystore\n```\n\n----------------------------------------\n\nTITLE: Adding Xbyak-riscv Subdirectory\nDESCRIPTION: This code block adds the xbyak_riscv subdirectory for RISC-V 64-bit architectures. It sets the `XBYAK_RISCV_V` variable to enable RISC-V vector extension support in Xbyak.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/thirdparty/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(RISCV64)\n    set(XBYAK_RISCV_V ON)\n    add_subdirectory(xbyak_riscv)\n    ov_install_static_lib(xbyak_riscv ${OV_CPACK_COMP_CORE})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories (CMake)\nDESCRIPTION: This snippet configures target include directories for the main library, including Level Zero headers, NPUExt interface include directories, and the NPU Utils source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/zero/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}\n  PUBLIC\n    $<TARGET_PROPERTY:LevelZero::Headers,INTERFACE_INCLUDE_DIRECTORIES>\n    $<TARGET_PROPERTY:LevelZero::NPUExt,INTERFACE_INCLUDE_DIRECTORIES>\n    $<BUILD_INTERFACE:${NPU_UTILS_SOURCE_DIR}/include>)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO™ via PIP behind a proxy\nDESCRIPTION: This command shows how to install OpenVINO™ using PIP when behind a proxy server. It specifies the proxy address and port using the `--proxy` flag and also trusts pypi.org using the `--trusted-host` flag. This is essential when the environment requires a proxy to access external resources.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/troubleshooting-install-config.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npip install --proxy http://address:port --trusted-host pypi.org openvino\n```\n\n----------------------------------------\n\nTITLE: Dump All Blobs by Name (Regex Alternative)\nDESCRIPTION: Sets the environment variable to dump all blobs by matching any node name using '.+' regex pattern during OpenVINO CPU execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_NODE_NAME=\".+\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Conditional Job Execution Based on Smart CI Output (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to conditionally execute a job based on the output of the Smart CI job. It uses an 'if' condition to check if a specific component is affected and requires a 'test' scope.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_10\n\nLANGUAGE: YAML\nCODE:\n```\n# The job below will start if YOUR_COMPONENT is affected and \"test\" scope is required\njob_that_validates_your_component:\n  needs: [Build, Smart_CI]\n  ...\n  if: fromJSON(needs.smart_ci.outputs.affected_components).YOUR_COMPONENT.test # or <...>.build, if needed\n  steps:\n    - ...\n```\n\n----------------------------------------\n\nTITLE: Add API Validator Post-Build Step (CMake)\nDESCRIPTION: This snippet adds a post-build step to validate the API of the target using ov_add_api_validator_post_build_step.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_api_validator_post_build_step(TARGET ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Adding Library Target CMake\nDESCRIPTION: This snippet creates the library target using the 'ov_add_target' CMake function. It defines the target's name, type (STATIC), root directory for headers, include directories, source directories, and link libraries. The library links against openvino::runtime::dev and common_test_utils.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/ov_helpers/ov_lpt_models/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_target(\n        NAME ${TARGET_NAME}\n        TYPE STATIC\n        ROOT ${PUBLIC_HEADERS_DIR}\n        INCLUDES\n            PUBLIC\n                \"$<BUILD_INTERFACE:${PUBLIC_HEADERS_DIR}>\"\n        ADDITIONAL_SOURCE_DIRS\n            ${CMAKE_CURRENT_SOURCE_DIR}/src\n        LINK_LIBRARIES\n            PRIVATE\n                openvino::runtime::dev\n                common_test_utils\n        ADD_CPPLINT\n)\n```\n\n----------------------------------------\n\nTITLE: Power Layer Configuration (No Broadcasting) - XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a Power layer in OpenVINO with auto_broadcast set to \"none\". This means that the input tensors must have matching dimensions. The example shows input and output ports with dimensions 256x56.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/power-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Power\">\n    <data auto_broadcast=\"none\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating Static Library\nDESCRIPTION: This snippet creates a static library for the interpreter backend, including specified source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC EXCLUDE_FROM_ALL ${OPS_SRC} ${SRC})\n```\n\n----------------------------------------\n\nTITLE: Adding Test Subdirectory\nDESCRIPTION: Conditionally adds the `tests` subdirectory to the build process if the `ENABLE_TESTS` variable is enabled. This allows for building and running unit tests for the CPU plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_45\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Image Classification Sample (Python, Linux)\nDESCRIPTION: This command runs the `classification_sample_async.py` sample with a specified model (`googlenet-v1.xml`), input image (`dog.bmp`), and target device (`CPU`) on a Linux system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_29\n\nLANGUAGE: sh\nCODE:\n```\npython classification_sample_async.py -m ~/ir/googlenet-v1.xml -i ~/Downloads/dog.bmp -d CPU\n```\n\n----------------------------------------\n\nTITLE: Installing Python Samples Directory in CMake\nDESCRIPTION: Installs the python samples directory to the specified destination.  It installs the directory and uses an exclusion pattern.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(DIRECTORY python/\n        DESTINATION ${OV_CPACK_SAMPLESDIR}/python\n        COMPONENT ${OV_CPACK_COMP_PYTHON_SAMPLES}\n        ${OV_CPACK_COMP_PYTHON_SAMPLES_EXCLUDE_ALL})\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flag for GNU C++ and Clang (CMake)\nDESCRIPTION: Conditionally adds a compiler flag to suppress warnings related to unused but set variables. This flag is applied to both GNU C++ and Clang compilers, specifically Clang versions 13.0 and greater, helping to reduce build noise.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 13.0)\n    ov_add_compiler_flags(-Wno-unused-but-set-variable)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enable OpenVINO Telemetry (opt-in)\nDESCRIPTION: This command enables telemetry reporting in OpenVINO. When enabled, anonymous usage data is collected to help improve OpenVINO tools. This setting can be changed at any time by running the opt-out command.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/additional-resources/telemetry.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nopt_in_out --opt_in\n```\n\n----------------------------------------\n\nTITLE: Find OpenVINODeveloperScripts Package\nDESCRIPTION: Finds the OpenVINODeveloperScripts package, which likely contains helper functions and macros for building OpenVINO components. It specifies a path to search for the package and marks it as required.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/ovc/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT OpenVINODeveloperScripts_FOUND)\n    find_package(OpenVINODeveloperScripts REQUIRED\n                 PATHS \"${OpenVINO_SOURCE_DIR}/cmake/developer_package\"\n                 NO_CMAKE_FIND_ROOT_PATH\n                 NO_DEFAULT_PATH)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Resizing Image C++\nDESCRIPTION: Demonstrates how to resize an input image using the OpenVINO preprocessing API in C++. It defines H and W dimensions of the layout.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n// 1) Set input tensor information. We apply preprocessing\n//    demanding input tensor to have 'u8' type and 'NHWC' layout.\nppp.input().tensor().set_element_type(ov::element::u8).set_layout(\"NHWC\");\n// 2) Adding explicit preprocessing steps\nppp.input().preprocess().resize(ov::preprocess::ResizeAlgorithm::RESIZE_LINEAR, 256, 256);\n// 3) Set input model information. We assume that the original model has 'NCHW' layout\nppp.input().model().set_layout(\"NCHW\");\n// 4) Apply preprocessing modifying the original model\nmodel = ppp.build();\n```\n\n----------------------------------------\n\nTITLE: Installing Public Headers in CMake\nDESCRIPTION: This code installs the public header files from the specified directory to the designated installation directory. It also manages component settings and excludes to ensure only the necessary headers are included in the installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY \"${PUBLIC_HEADERS_DIR}/\"\n        DESTINATION ${OV_CPACK_INCLUDEDIR}\n        COMPONENT ${OV_CPACK_COMP_CORE_DEV}\n        ${OV_CPACK_COMP_CORE_DEV_EXCLUDE_ALL})\n```\n\n----------------------------------------\n\nTITLE: Install Target in CMake\nDESCRIPTION: This CMake code installs the compiled MemCheckTests executable to the 'tests' directory. It also specifies that the installation should be included as part of the 'tests' component and excluded from the 'all' target, meaning it's not built by default.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/memcheck_tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n        RUNTIME DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Compiling Model Asynchronously (Model Object)\nDESCRIPTION: Asynchronously compiles a model from a Model object for a specified device. It allows users to create multiple compiled models simultaneously, limited by hardware resources. The config parameter allows specifying properties relevant only for this load operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ncompileModel(model, deviceName, config?): Promise<CompiledModel>\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO GenAI from Archive - macOS ARM 64-bit\nDESCRIPTION: These commands download and extract the OpenVINO GenAI archive for macOS (ARM 64-bit architecture). The `curl` command downloads the tarball, and the `tar` command extracts its contents.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-genai.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino_genai/packages/2025.1/macos/openvino_genai_macos_12_6_2025.1.0.0_arm64.tar.gz --output openvino_genai_2025.1.0.0.tgz\ntar -xf openvino_genai_2025.1.0.0.tgz\n```\n\n----------------------------------------\n\nTITLE: Visit Attributes in Python\nDESCRIPTION: This snippet demonstrates how to override the `visit_attributes` method in Python for a custom OpenVINO operation. This method enables serialization and deserialization of operation attributes by walking over all the attributes using the type-aware `on_attribute` helper of the `AttributeVisitor`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-openvino-operations.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    def visit_attributes(self, attribute_visitor):\n        \"\"\"Visit attributes.\n\n        :param attribute_visitor: Attribute visitor.\n        \"\"\"\n        # The attribute visitor is used to serialize and deserialize the attributes\n        # of the operation.\n        attribute_visitor.on_attribute('val', self._val)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Resizing Image Python\nDESCRIPTION: Demonstrates how to resize an input image using the OpenVINO preprocessing API in Python.  It defines H and W dimensions of the layout.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nppp = ov.preprocess.PrePostProcessor(model)\n# 1) Set input tensor information. We apply preprocessing\n#    demanding input tensor to have 'u8' type and 'NHWC' layout.\nppp.input().tensor().set_element_type(ov.Type.u8).set_layout(ov.Layout('NHWC'))\n# 2) Adding explicit preprocessing steps\nppp.input().preprocess().resize(ov.ResizeAlgorithm.RESIZE_LINEAR, 256, 256)\n# 3) Set input model information. We assume that the original model has 'NCHW' layout\nppp.input().model().set_layout(ov.Layout('NCHW'))\n# 4) Apply preprocessing modifying the original model\nmodel = ppp.build()\n```\n\n----------------------------------------\n\nTITLE: Creating Source Groups in CMake\nDESCRIPTION: This code snippet creates source groups in Visual Studio project for better organization. It groups source files under \"src\" and header files under \"include\".  This only affects how the source code is organized in the IDE and has no effect on the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/offline_transformations/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Log Layer XML Configuration - OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of a Log layer in OpenVINO. It defines the input and output ports with their respective dimensions. The layer performs an element-wise natural logarithm on the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/log-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Log\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Python Model Conversion\nDESCRIPTION: This Python code snippet demonstrates how to convert a model using the OpenVINO API. It imports the openvino module and utilizes the convert_model function to convert a model specified by its file path. The output is an OpenVINO model object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-reshape-ssd.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openvino as ov\n\nov_model = ov.convert_model('./test_data/models/mobilenet-ssd')\n# or, when model is a Python model object\nov_model = ov.convert_model(mobilenet-ssd)\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files in CMake\nDESCRIPTION: This CMake snippet uses `file(GLOB_RECURSE)` to find all `.cpp`, `.hpp`, and `.h` files in the current source directory and its subdirectories, storing the list in the `SOURCES` variable. This list is then used to define the source files for the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/compiler_adapter/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES *.cpp *.hpp *.h)\nsource_group(TREE ${CMAKE_CURRENT_SOURCE_DIR} FILES ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Group Source Files in Visual Studio Projects\nDESCRIPTION: This snippet creates named folders within the Visual Studio project to organize the source files. `source_group` is used to group `.cpp` files under \"src\" and `.hpp` files under \"include\".  This improves project organization within the IDE.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${LIBRARY_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to the Target\nDESCRIPTION: This snippet links the specified libraries (`openvino_npu_driver_compiler_adapter`, `openvino_npu_level_zero_backend`, `openvino::npu_al`, `openvino::npu_common`) to the target as private dependencies. It ensures that the target can access the symbols defined in these libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/plugin/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME}\n    PRIVATE\n        openvino_npu_driver_compiler_adapter\n        openvino_npu_level_zero_backend\n)\n\ntarget_link_libraries(${TARGET_NAME}\n    PRIVATE\n        openvino::npu_al\n        openvino::npu_common\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Output Directories\nDESCRIPTION: Sets the output directories for libraries, archives, PDB files (program database), and runtime executables. All output artifacts are placed in the `${CMAKE_BINARY_DIR}/test` directory. This helps organize the build output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/test\")\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/test\")\nset(CMAKE_PDB_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/test\")\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/test\")\n```\n\n----------------------------------------\n\nTITLE: Preprocessing: Converting Layout C++\nDESCRIPTION: Demonstrates how to implicitly convert the layout (transposing) of a user's tensor to match the layout expected by the original model using the OpenVINO preprocessing API in C++.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.rst#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nov::preprocess::PrePostProcessor ppp(model);\n// 1) Set input tensor information. We apply preprocessing\n//    demanding input tensor to have 'u8' type and 'NHWC' layout.\nppp.input().tensor().set_element_type(ov::element::u8).set_layout(\"NHWC\");\n// 2) Set input model information. We assume that the original model has 'NCHW' layout\nppp.input().model().set_layout(\"NCHW\");\n// 3) Apply preprocessing modifying the original model\nmodel = ppp.build();\n```\n\n----------------------------------------\n\nTITLE: JSON Configuration Example - Device Properties\nDESCRIPTION: This JSON snippet demonstrates the structure for configuring device properties within a JSON file. It showcases how to set properties for both individual hardware devices (CPU) and meta-devices (AUTO) including performance hints and device-specific settings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n                                                  \"CPU\": {\"NUM_STREAMS\": \"3\", \"PERF_COUNT\": \"NO\"}\n                                             }\n```\n\n----------------------------------------\n\nTITLE: Linking OpenVINO libraries directly with GCC (sh)\nDESCRIPTION: This snippet shows how to link the static OpenVINO libraries using the GCC compiler. It uses the `-Wl,--whole-archive` and `-Wl,--no-whole-archive` linker flags to ensure all symbols from the static libraries are included in the final executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/static_libaries.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ngcc main.cpp -Wl,--whole-archive <all libraries from <root>/runtime/lib> > -Wl,--no-whole-archive -o a.out\n```\n\n----------------------------------------\n\nTITLE: Floor Operation XML Layer Definition\nDESCRIPTION: Defines a Floor layer in OpenVINO's XML format. The layer takes a tensor as input and outputs a tensor containing the floor of each element. The example shows a layer with input and output tensors of shape (256, 56).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/floor-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Floor\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorFlow Frontend Module\nDESCRIPTION: This snippet configures a frontend module named `py_tensorflow_frontend` for TensorFlow. It specifies the frontend as 'tensorflow' and associates it with a version identifier derived from `OV_CPACK_COMP_PYTHON_OPENVINO` and `pyversion`. This sets up the frontend for use with OpenVINO's Python bindings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/frontend/tensorflow/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfrontend_module(py_tensorflow_frontend tensorflow ${OV_CPACK_COMP_PYTHON_OPENVINO}_${pyversion})\n```\n\n----------------------------------------\n\nTITLE: NV12 Image Conversion using FFmpeg\nDESCRIPTION: This command uses FFmpeg to convert an image (e.g., `cat.jpg`) to an uncompressed NV12 format image (`cat.yuv`). The `-pix_fmt nv12` option specifies the pixel format for the output file. This is a prerequisite step before running the classification sample with NV12 images.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/hello-nv12-input-classification.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nffmpeg -i cat.jpg -pix_fmt nv12 cat.yuv\n```\n\n----------------------------------------\n\nTITLE: Including and Configuring PyTorch Frontend Module in CMake\nDESCRIPTION: This CMake snippet includes a pre-defined CMake module and configures it for building the PyTorch frontend. It uses the `frontend_module` macro, which likely handles setting up necessary dependencies, include directories, and build options specific to the PyTorch frontend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/frontend/pytorch/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(${pyopenvino_SOURCE_DIR}/frontend/frontend_module.cmake)\nfrontend_module(py_pytorch_frontend pytorch ${OV_CPACK_COMP_PYTHON_OPENVINO}_${pyversion})\n```\n\n----------------------------------------\n\nTITLE: Inserting a Node Using ov::replace_node (C++)\nDESCRIPTION: This code snippet demonstrates inserting a node using `ov::replace_node`. It creates a copy of the original node and inserts a Relu node between the original node and its copy, effectively inserting the Relu into the graph.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov:insert_node_with_copy]\nauto op_copy = op->clone_with_new_inputs(op->input_values());\nauto relu = std::make_shared<op::v0::Relu>(input);\nov::replace_node(op, relu);\nrelu->output(0).replace_destination(op_copy->input_value(0));\n//! [ov:insert_node_with_copy]\n```\n\n----------------------------------------\n\nTITLE: String Tensor Constructor\nDESCRIPTION: Constructs a string tensor using an array of strings. Each element of the tensor is a string of arbitrary length. The tensorData parameter is a string array that populates the tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/TensorConstructor.rst#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nnew TensorConstructor(tensorData: string[]): Tensor;\n```\n\n----------------------------------------\n\nTITLE: Activating a Conda Environment\nDESCRIPTION: This command activates the Conda environment named `py310`.  Activating the environment ensures that subsequent commands are executed within the isolated environment, using the specified Python version and any installed packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conda.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nconda activate py310\n```\n\n----------------------------------------\n\nTITLE: Import Model C++\nDESCRIPTION: This snippet shows how to import the compiled model. The importing of compiled model mechanism allows to import a previously exported backend specific model and wrap it using an `CompiledModel` object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<ov::ICompiledModel> Plugin::import_model(std::istream& model, const ov::AnyMap& properties) {\n    // 1. Update plugin config with compile config\n    auto cfg = Configuration{properties, get_stream_executor(properties)};\n    auto compiled_model = std::make_shared<CompiledModel>(model, cfg, shared_from_this());\n\n    return compiled_model;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Adding Samples Subdirectories\nDESCRIPTION: This snippet collects all sample subdirectories, removes \"thirdparty\" and \"common\" directories from the list, and then uses the `ov_add_samples_to_build` function to add the remaining subdirectories to the build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\n# collect all samples subdirectories\nfile(GLOB samples_dirs RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} *)\n\n# skip building of unnecessary subdirectories\nif(EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/thirdparty\")\n    list(REMOVE_ITEM samples_dirs thirdparty)\nendif()\nlist(REMOVE_ITEM samples_dirs common)\n\nov_add_samples_to_build(${samples_dirs})\n```\n\n----------------------------------------\n\nTITLE: Installing the Target\nDESCRIPTION: This snippet installs the target '${TARGET_NAME}' to the 'tests' directory under the 'RUNTIME' destination, and it's part of the 'tests' component. 'EXCLUDE_FROM_ALL' means the target will not be built as part of the default 'all' target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n        RUNTIME DESTINATION tests\n        COMPONENT tests\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Configure Network Bridge (br0)\nDESCRIPTION: This configuration updates the /etc/netplan/50-cloud-init.yaml file to enable external network access by configuring a bridge (br0). It sets dhcp4 to 'yes' for the bridge interface, using eno1 as the bridged interface.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nnetwork:\n  ethernets:\n     eno1:\n       dhcp4: false\n  bridges:\n     br0:\n       interfaces: [eno1]\n       dhcp4: yes\n\t   \t  dhcp-identifier: mac\n  version: 2\n```\n\n----------------------------------------\n\nTITLE: Mersenne-Twister Constants (C++)\nDESCRIPTION: This code snippet sets the values of the constants used in the Mersenne-Twister algorithm, specifically for parity with PyTorch. These constants (u, s, b, t, c, l, m) are crucial for the tempering and state generation processes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nu = 11\ns = 7\nb = 0x9d2c5680\nt = 15\nc = 0xefc60000\nl = 18\nm = 397\n```\n\n----------------------------------------\n\nTITLE: Set Sphinx Theme in conf.py\nDESCRIPTION: This snippet demonstrates how to set the `html_theme` variable in the `conf.py` file to use the `openvino_sphinx_theme`.  Setting this variable is necessary to activate the theme for your Sphinx documentation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/openvino_sphinx_theme/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nhtml_theme = 'openvino_sphinx_theme'\n```\n\n----------------------------------------\n\nTITLE: Building Doxygen from Source (Ubuntu)\nDESCRIPTION: These commands clone the Doxygen repository, checkout a specific release, create a build directory, configure the build using CMake, build the project, and install it system-wide. This process ensures that a compatible version of Doxygen (>= 1.8.20) is available for generating the documentation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/building_documentation.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n$ git clone https://github.com/doxygen/doxygen.git\n$ cd doxygen\n$ git checkout Release_1_8_20\n$ mkdir build\n$ cd build\n$ cmake ..\n$ cmake --build . -j8\n$ sudo make install\n```\n\n----------------------------------------\n\nTITLE: Get CMake Path with OpenVINO Utils\nDESCRIPTION: This Python code snippet retrieves the path to the CMake configurations and libraries using the `get_cmake_path()` method from the `openvino.utils` module. This is useful for building and integrating with OpenVINO components in custom applications.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-pip.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.utils import get_cmake_path\ncmake_path = get_cmake_path()\n```\n\n----------------------------------------\n\nTITLE: Verify Hardware Virtualization\nDESCRIPTION: This command verifies that hardware virtualization is enabled in the BIOS. It checks for the existence of /dev/kvm and confirms that KVM acceleration can be used.  If kvm-ok is not found, the cpu-checker package needs to be installed first.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkvm-ok\n```\n\n----------------------------------------\n\nTITLE: Set HuggingFace Endpoint (Windows)\nDESCRIPTION: These commands install or upgrade the `huggingface_hub` package and set the `HF_ENDPOINT` environment variable to `https://hf-mirror.com`.  This is specifically for users in PRC experiencing issues accessing Hugging Face and redirects traffic to a mirror site.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\npip install -U huggingface_hub\nset HF_ENDPOINT = https://hf-mirror.com\n```\n\n----------------------------------------\n\nTITLE: HSigmoid Layer XML Definition in OpenVINO\nDESCRIPTION: This XML snippet defines an HSigmoid layer within an OpenVINO model. It specifies the input and output ports, including their dimensions. This example demonstrates how to represent the HSigmoid operation in the OpenVINO Intermediate Representation (IR).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/hsigmoid-5.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"HSigmoid\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding clang-format Target\nDESCRIPTION: Adds a clang-format target for the specified library. This allows for easy code formatting using clang-format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/thread_local/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: InferRequest Destructor C++\nDESCRIPTION: This code snippet shows the destructor for the InferRequest class. It can contain plugin-specific logic to finish and destroy the infer request, releasing allocated resources and performing any necessary cleanup operations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/synch-inference-request.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nInferRequest::~InferRequest() {\n    // Plugin specific logic to finish and destroy infer request\n}\n```\n\n----------------------------------------\n\nTITLE: virbr0 Interface Down Script\nDESCRIPTION: This script, `virbr0-qemu-ifdown`, brings down the specified network interface (`$nic`) and removes it from the `virbr0` bridge. It reads configuration from `/etc/default/qemu-kvm` if the file exists.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\n#!/bin/sh\nnic=$1\nif [ -f /etc/default/qemu-kvm ]; then\n. /etc/default/qemu-kvm\nfi\nswitch=virbr0\nbrctl delif $switch $nic\nifconfig $nic 0.0.0.0 down\n```\n\n----------------------------------------\n\nTITLE: Inserting a Node into a Model (C++)\nDESCRIPTION: This code snippet demonstrates how to insert a Relu node into an OpenVINO model. It shows the process of creating a Relu node and connecting it between an existing node and its consumer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov:insert_node]\nauto relu = std::make_shared<op::v0::Relu>(input);\nauto consumer_output = op->output(0);\nconsumer_output.replace_destination(relu->input_value(0));\nrelu->output(0).replace_destination(op->input_value(0));\n//! [ov:insert_node]\n```\n\n----------------------------------------\n\nTITLE: Get CPU Device Name (C++)\nDESCRIPTION: This snippet demonstrates how to get the full name of the CPU device using `ov::Core::get_property` in C++. It fetches the device name and prints it to the console. It requires an initialized `ov::Core` object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/query-device-properties.rst#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nov::Core core;\nstd::string device_name = \"CPU\";\nauto device_full_name = core.get_property(device_name, ov::device::full_name.name());\nstd::cout << device_name << \" full name: \" << device_full_name.as<std::string>() << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Querying Optimal Number of Requests in Python\nDESCRIPTION: This Python snippet demonstrates how to query the optimal number of inference requests using `ov::optimal_number_of_infer_requests`. It returns the recommended number of requests that should be run simultaneously for optimal device utilization. It assumes the `compiled_model` object is already defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/high-level-performance-hints.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnireq = compiled_model.get_property(ov.optimal_number_of_infer_requests)\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files in CMake\nDESCRIPTION: This snippet uses the file(GLOB_RECURSE) command to find all C++ source files (*.cpp) and header files (*.h) in the project.  The found files are stored in the SRC and HDR variables respectively, to be used in creating the executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/appverifier_tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile (GLOB_RECURSE SRC *.cpp)\nfile (GLOB_RECURSE HDR *.h)\n```\n\n----------------------------------------\n\nTITLE: Check TPM Availability\nDESCRIPTION: This command checks for Trusted Platform Module (TPM) support by searching the kernel boot logs for TPM-related devices. The expected output indicates the presence of /dev/tpm0 and /dev/tpmrm0, which signify TPM support.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndmesg | grep -i TPM\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO as Execution Provider for ONNX Runtime\nDESCRIPTION: This code snippet shows how to configure ONNX Runtime to use OpenVINO as the execution provider. This allows leveraging Intel hardware for enhanced inference performance with minimal code changes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-integrations.rst#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ndevice = `CPU_FP32`\n# Set OpenVINO as the Execution provider to infer this model\nsess.set_providers([`OpenVINOExecutionProvider`], [{device_type` : device}])\n```\n\n----------------------------------------\n\nTITLE: Export Unit Test Utilities Target CMake\nDESCRIPTION: This snippet uses the `ov_developer_package_export_targets` macro to configure how the 'unit_test_utils' target is exported for the developer package. It specifies the target name, the installation directory for include directories, and the destination directory within the developer package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/unit_test_utils/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET ${TARGET_NAME}\n                                    INSTALL_INCLUDE_DIRECTORIES \"${CMAKE_CURRENT_SOURCE_DIR}/../unit_test_utils\"\n                                    INSTALL_DESTIONATION \"developer_package/include/unit_test_utils/unit_test_utils\")\n```\n\n----------------------------------------\n\nTITLE: OpenVINO C API Architecture Diagram\nDESCRIPTION: This Mermaid diagram illustrates the relationship between a C application, the OpenVINO C API wrapper (openvino::c), and the core OpenVINO library (openvino). It visualizes the data flow and dependencies between these components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/README.md#_snippet_0\n\nLANGUAGE: Mermaid\nCODE:\n```\nflowchart LR\n    c_application[(\"C Application\")]\n    openvino_c{{openvino::c}}\n    openvino{{openvino}}\n\n    style c_application fill:#427cb0\n    style openvino_c fill:#6c9f7f\n    style openvino fill:#6c9f7f\n\n    c_application-->openvino_c-->openvino\n```\n\n----------------------------------------\n\nTITLE: Generating OpenVINO Developer Package using CMake\nDESCRIPTION: This snippet shows how to generate the OpenVINO Developer Package using CMake. This package contains necessary files for plugin development, including CMake scripts, libraries, and headers. It is essential for building plugins against a specific OpenVINO build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/build-plugin-using-cmake.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n$ mkdir openvino-release-build\n$ cd openvino-release-build\n$ cmake -DCMAKE_BUILD_TYPE=Release ../openvino\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Kernel Configuration File - C++\nDESCRIPTION: This C++ snippet shows how to set the custom kernel configuration file path using `ov::Core::set_property()` before loading the network. This enables the OpenVINO plugin to utilize custom operations defined in the specified configuration file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-gpu-operations.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\ncore.set_property(\"GPU\", ov::properties({{ \"CONFIG_FILE\", \"path_to_config_file\" }}));\nauto model = core.read_model(\"path_to_model\");\nauto compiled_model = core.compile_model(model, \"GPU\");\n```\n\n----------------------------------------\n\nTITLE: virbr0 Interface Up Script\nDESCRIPTION: This script, `virbr0-qemu-ifup`, brings up the specified network interface (`$nic`) and adds it to the `virbr0` bridge. It reads configuration from `/etc/default/qemu-kvm` if the file exists.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\n#!/bin/sh\nnic=$1\nif [ -f /etc/default/qemu-kvm ]; then\n\t. /etc/default/qemu-kvm\nfi\nswitch=virbr0\nifconfig $nic 0.0.0.0 up\nbrctl addif ${switch} $nic\n```\n\n----------------------------------------\n\nTITLE: Multiply Layer Definition - No Broadcast XML\nDESCRIPTION: This example shows a Multiply layer definition in OpenVINO's XML format where auto-broadcasting is disabled. The input tensors have the same shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/multiply-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Multiply\">\n    <data auto_broadcast=\"none\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing pre-production dependencies on Windows\nDESCRIPTION: This snippet installs a pre-production version of OpenVINO and related packages on Windows. It uses the `--pre` flag and specifies an extra index URL to access nightly builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_3\n\nLANGUAGE: Console\nCODE:\n```\npip install --pre openvino openvino-tokenizers openvino-genai --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n```\n\n----------------------------------------\n\nTITLE: Automatic Device Selection C++\nDESCRIPTION: This C++ snippet demonstrates how to compile a model for automatic device selection across multiple GPUs using `ov::Core::compile_model()`. It utilizes the `compile_model_auto` fragment from the specified C++ file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device.rst#_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n// [compile_model_auto]\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nov::CompiledModel compiled_model = core.compile_model(model, \"AUTO:GPU.1,GPU.0\", {\"CUMULATIVE_THROUGHPUT\": \"YES\"});\n// [compile_model_auto]\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation Definitions\nDESCRIPTION: This snippet conditionally defines the `ENABLE_IMD_BACKEND` and `OPENVINO_STATIC_LIBRARY` preprocessor macros based on the values of `ENABLE_IMD_BACKEND` and `BUILD_SHARED_LIBS` CMake variables, respectively. This allows conditional compilation of the code depending on specified configurations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/plugin/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_IMD_BACKEND)\n    target_compile_definitions(${TARGET_NAME} PRIVATE ENABLE_IMD_BACKEND)\nendif()\n\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME} PRIVATE OPENVINO_STATIC_LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Set Property C++\nDESCRIPTION: This snippet illustrates the `set_property` method, used to configure plugin properties.  It allows updating the plugin's configuration based on user-provided property keys, which are then used during model compilation and inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nvoid Plugin::set_property(const ov::AnyMap& properties) {\n    m_cfg = Configuration{properties, get_stream_executor(properties)};\n}\n\n```\n\n----------------------------------------\n\nTITLE: Sinh Layer XML Configuration - OpenVINO\nDESCRIPTION: This XML configuration defines a Sinh layer in OpenVINO. It specifies the input and output ports with their dimensions, demonstrating how to configure the hyperbolic sine operation within an OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/sinh-1.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"Sinh\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing Header Files\nDESCRIPTION: This snippet installs the header files into the developer package's include directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY \"${CMAKE_CURRENT_SOURCE_DIR}/\"\n        DESTINATION developer_package/include/${TARGET_NAME}\n        COMPONENT developer_package EXCLUDE_FROM_ALL\n        FILES_MATCHING PATTERN \"*.hpp\")\n```\n\n----------------------------------------\n\nTITLE: HardSigmoid Layer XML Configuration\nDESCRIPTION: This XML snippet demonstrates the configuration of a HardSigmoid layer in OpenVINO. It specifies the input and output ports, along with their dimensions. The input includes the data tensor, alpha scalar, and beta scalar, while the output is the result of the hard sigmoid operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/hard-sigmoid-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"HardSigmoid\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\"/>\n        <port id=\"2\"/>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Create Object Library for OpenVINO Proxy Plugin\nDESCRIPTION: This snippet creates an object library named `openvino_proxy_plugin_obj` from the specified source and header files. The `add_library` command creates the library using the files defined in `LIBRARY_SRC` and `LIBRARY_HEADERS`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} OBJECT ${LIBRARY_SRC} ${LIBRARY_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for Tests - CMake\nDESCRIPTION: This code adds the `functional` and `unit` directories as subdirectories to the CMake build process. This allows the CMake project to include and build the functional and unit tests within these directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(functional)\nadd_subdirectory(unit)\n```\n\n----------------------------------------\n\nTITLE: Conditional Plugin Building with CMake\nDESCRIPTION: This snippet conditionally builds the OpenVINO AUTO and MULTI plugins based on the `ENABLE_AUTO` and `ENABLE_MULTI` flags. It uses `ov_add_plugin` to define the plugin, sets the device name, and specifies source files. If both flags are enabled, both AUTO and MULTI are created. It also includes an else-if structure to handle cases where only one flag is enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT ENABLE_AUTO AND NOT ENABLE_MULTI)\n    return()\nendif()\n\nset (TARGET_NAME \"openvino_auto_plugin\")\n\nfile(GLOB SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp ${CMAKE_CURRENT_SOURCE_DIR}/src/utils/*.cpp)\n\n# WA for issue 141313\nif(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 11)\n    ov_add_compiler_flags(-Wno-stringop-overflow)\nendif()\n\nif(ENABLE_AUTO AND ENABLE_MULTI)\n    ov_add_plugin(NAME ${TARGET_NAME}\n                  DEVICE_NAME \"MULTI\"\n                  PSEUDO_DEVICE\n                  SOURCES ${SOURCES}\n                  VERSION_DEFINES_FOR src/plugin.cpp)\n\n    ov_add_plugin(NAME ${TARGET_NAME}\n                  DEVICE_NAME \"AUTO\"\n                  PSEUDO_DEVICE\n                  PSEUDO_PLUGIN_FOR \"MULTI\")\nelif(ENABLE_AUTO)\n    ov_add_plugin(NAME ${TARGET_NAME}\n                  DEVICE_NAME \"AUTO\"\n                  PSEUDO_DEVICE\n                  SOURCES ${SOURCES}\n                  VERSION_DEFINES_FOR src/plugin.cpp)\nelif(ENABLE_MULTI)\n    ov_add_plugin(NAME ${TARGET_NAME}\n                  DEVICE_NAME \"MULTI\"\n                  PSEUDO_DEVICE\n                  SOURCES ${SOURCES}\n                  VERSION_DEFINES_FOR src/plugin.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: OS Check\nDESCRIPTION: Checks if the operating system is Windows (WIN32). If not, it throws a fatal error, indicating that the test only supports Windows. This ensures that the tests are only run on compatible platforms.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT WIN32)\n    message(FATAL_ERROR \"This test only supports windows OS\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: BitwiseAnd example with numpy broadcast in XML\nDESCRIPTION: Presents a BitwiseAnd layer in XML using numpy broadcasting. The input ports '0' and '1' have different dimensions (8x1x6x1 and 7x1x5 respectively), showcasing how numpy broadcasting rules are applied. The output port '2' reflects the resulting broadcasted shape (8x7x6x5).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-and-13.rst#_snippet_3\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"BitwiseAnd\">\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Set Target Properties and Dependencies\nDESCRIPTION: This code adds dependencies to the executable, sets include directories, defines compilation definitions (including the shared library prefix/suffix and the ONNX opset version), and adds dependencies based on ONNX_TESTS_DEPENDENCIES.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_dependencies(ov_onnx_frontend_tests openvino_template_extension)\n\ntarget_include_directories(ov_onnx_frontend_tests PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}\")\n\ntarget_compile_definitions(ov_onnx_frontend_tests\n    PRIVATE\n        SHARED_LIB_PREFIX=\"${CMAKE_SHARED_LIBRARY_PREFIX}\"\n        SHARED_LIB_SUFFIX=\"${OV_BUILD_POSTFIX}${CMAKE_SHARED_LIBRARY_SUFFIX}\")\n\nset(ONNX_OPSET_VERSION 17 CACHE INTERNAL \"Supported version of ONNX operator set\")\ntarget_compile_definitions(ov_onnx_frontend_tests PRIVATE ONNX_OPSET_VERSION=${ONNX_OPSET_VERSION})\n\nif(ONNX_TESTS_DEPENDENCIES)\n    add_dependencies(ov_onnx_frontend_tests ${ONNX_TESTS_DEPENDENCIES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Compile Definitions in CMake\nDESCRIPTION: This snippet sets the compile definitions for the test target using `target_compile_definitions`.  It uses the `PRIVATE` keyword to ensure the definitions are only applied to the current target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME} PRIVATE ${COMPILE_DEFINITIONS})\n```\n\n----------------------------------------\n\nTITLE: Gather Layer Example in XML\nDESCRIPTION: Example of a Gather layer represented in XML format.  It defines the input and output ports with their corresponding dimensions, illustrating how the Gather operation modifies the shape of the data. Axis is implicitly defined by port '2' input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Gather\" ...>\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>15</dim>\n            <dim>4</dim>\n            <dim>20</dim>\n            <dim>28</dim>\n        </port>\n        <port id=\"2\"/>        <!--  axis = 1  -->\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>      <!-- embedded dimension from the 1st input -->\n            <dim>15</dim>     <!-- embedded dimension from the 2nd input -->\n            <dim>4</dim>      <!-- embedded dimension from the 2nd input -->\n            <dim>20</dim>     <!-- embedded dimension from the 2nd input -->\n            <dim>28</dim>     <!-- embedded dimension from the 2nd input -->\n            <dim>10</dim>     <!-- embedded dimension from the 1st input -->\n            <dim>24</dim>     <!-- embedded dimension from the 1st input -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ExtractImagePatches Output Example 4 C++\nDESCRIPTION: This C++ snippet displays the output of the ExtractImagePatches operation with sizes=\"3,3\", strides=\"5,5\", rates=\"2,2\", and auto_pad=\"valid\". The resulting output_shape is [1, 9, 2, 2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/extract-image-patches-3.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n[[[[  1   6]\n   [ 51  56]]\n\n  [[  3   8]\n   [ 53  58]]\n\n  [[  5  10]\n   [ 55  60]]\n\n  [[ 21  26]\n   [ 71  76]]\n\n  [[ 23  28]\n   [ 73  78]]\n\n  [[ 25  30]\n   [ 75  80]]\n\n  [[ 41  46]\n   [ 91  96]]\n\n  [[ 43  48]\n   [ 93  98]]\n\n  [[ 45  50]\n   [ 95 100]]]]\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License boilerplate\nDESCRIPTION: This snippet provides the boilerplate notice to attach to your work when applying the Apache License. It includes fields for the copyright year and copyright owner's name, which need to be replaced with your own identifying information. The text should be enclosed in the appropriate comment syntax for the file format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/third-party-programs.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Defining InputInfo Interface in TypeScript\nDESCRIPTION: This code snippet defines the `InputInfo` interface in TypeScript.  It includes methods for accessing the model information, preprocessing steps, and tensor information for a given input. Each method returns a corresponding interface or object containing the relevant data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InputInfo.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface InputInfo {\n    model(): InputModelInfo;\n    preprocess(): PreProcessSteps;\n    tensor(): InputTensorInfo;\n}\n```\n\n----------------------------------------\n\nTITLE: ModelPass Example Implementation Python\nDESCRIPTION: This code snippet illustrates a ModelPass implementation in Python using the OpenVINO Python API.  It shows how to create a ModelPass and override the run_on_model function to apply transformations to a given model. The run_on_model function takes the ov.Model as input and returns a boolean indicating if the model was modified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/model-pass.rst#_snippet_2\n\nLANGUAGE: py\nCODE:\n```\nfrom openvino.runtime import Model, pass_manager\n\nclass ModelPass(pass_manager.ModelPass):\n    def __init__(self):\n        super().__init__()\n\n    def run_on_model(self, model: Model) -> bool:\n        # Add transformation code here\n        return False\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories with CMake\nDESCRIPTION: This snippet demonstrates adding several subdirectories to the CMake build process. Each `add_subdirectory` command specifies a directory containing another CMakeLists.txt file that will be processed as part of the larger project. The inclusion of the `snippets` directory depends on the `ENABLE_INTEL_CPU` flag.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(itt)\nadd_subdirectory(conditional_compilation)\nadd_subdirectory(util)\n\nif(ENABLE_INTEL_CPU)\n    add_subdirectory(snippets)\nendif()\n\nadd_subdirectory(transformations)\nadd_subdirectory(offline_transformations)\nadd_subdirectory(low_precision_transformations)\n```\n\n----------------------------------------\n\nTITLE: NonMaxSuppression Layer Configuration in OpenVINO (CPP)\nDESCRIPTION: This snippet shows how to configure a NonMaxSuppression layer in OpenVINO using the C++ API. It defines the layer type, attributes such as box encoding, sorting order, and output type, as well as input and output ports with their dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/non-max-suppression-3.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"NonMaxSuppression\" ... >\n    <data box_encoding=\"corner\" sort_result_descending=\"1\" output_type=\"i64\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>1000</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>1000</dim>\n        </port>\n        <port id=\"2\"/>\n        <port id=\"3\"/>\n        <port id=\"4\"/>\n    </input>\n    <output>\n        <port id=\"5\" precision=\"I64\">\n            <dim>1000</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Reshape with dimension bounds - Python\nDESCRIPTION: This Python code demonstrates setting dimension bounds for a mobilenet-v2 model with an initial shape of [1,3,224,224]. It shows different equivalent ways to pass lower and upper bounds using `reshape` method, including direct passing, using `ov.Dimension`, and using string representation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# ! [reshape_bounds]\nmodel.reshape([(1, 10), 3, 224, 224])\nmodel.reshape([ov.Dimension(1, 10), 3, 224, 224])\nmodel.reshape(\"1..10, 3, 224, 224\")\n# ! [reshape_bounds]\n```\n\n----------------------------------------\n\nTITLE: ReduceMean Example with axis [1]\nDESCRIPTION: Demonstrates the ReduceMean operation with a specific axis ([1]) for reduction, showing the resulting output dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-mean-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceMean\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>  <!-- value is [1] that means independent reduction in each channel and spatial dimensions -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Minimum-1 XML example with no broadcasting\nDESCRIPTION: This XML snippet demonstrates the Minimum-1 operation with the 'auto_broadcast' attribute set to 'none'. This requires both input tensors to have matching shapes. The example shows two input tensors with dimensions 256x56, resulting in an output tensor of the same dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/minimum-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Minimum\">\n    <data auto_broadcast=\"none\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO in SELECTIVE_BUILD_ANALYZER Mode\nDESCRIPTION: This CMake command configures and builds OpenVINO in SELECTIVE_BUILD_ANALYZER mode. It enables ITT profiling and sets the build type to Release. This mode is used to collect data for conditional compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/docs/develop_cc_for_new_component.md#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake \\\n            -DCMAKE_BUILD_TYPE=Release \\\n            -DENABLE_PROFILING_ITT=ON \\\n            -DSELECTIVE_BUILD=COLLECT \\\n            -B build \\\n            -S .\n    cmake --build build -j `nproc`\n```\n\n----------------------------------------\n\nTITLE: Defining a New Macro in itt.hpp\nDESCRIPTION: Example of defining a new macro in the itt.hpp file for conditional compilation. The macro is defined differently for each mode: SELECTIVE_BUILD_ANALYZER, SELECTIVE_BUILD, and when conditional compilation is off. This allows for specific behavior in each compilation stage.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/docs/develop_cc_for_new_component.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n#if defined(SELECTIVE_BUILD_ANALYZER)\n#define OV_CPU_SCOPE(name) OV_ITT_SCOPED_TASK(itt::domains::CPU, name)\n#elif defined(SELECTIVE_BUILD)\n#define OV_CPU_SCOPE(name) OPENVINO_CC_SCOPE_IS_ENABLED(CPU, name)\n#else\n#define OV_CPU_SCOPE(name)\n#endif\n\n#if defined(SELECTIVE_BUILD_ANALYZER)\n#define OV_GPU_SCOPE(name) OV_ITT_SCOPED_TASK(itt::domains::GPU, name)\n#elif defined(SELECTIVE_BUILD)\n#define OV_GPU_SCOPE(name) OPENVINO_CC_SCOPE_IS_ENABLED(GPU, name)\n#else\n#define OV_GPU_SCOPE(name)\n#endif\n```\n\n----------------------------------------\n\nTITLE: Detect Platform Tag with wheel.vendored.packaging.tags - Python\nDESCRIPTION: This snippet executes a Python command to detect the platform tag using the `wheel.vendored.packaging.tags` module. It captures the platform tag and stores it in the `PLATFORM_TAG` variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport wheel.vendored.packaging.tags as tags ; print(f'{next(tags.platform_tags())}')\n```\n\n----------------------------------------\n\nTITLE: Marking Target as C++\nDESCRIPTION: Marks the specified target as a C++ target using the `ov_mark_target_as_cc` macro, indicating that it should be compiled as C++ code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_29\n\nLANGUAGE: cmake\nCODE:\n```\nov_mark_target_as_cc(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: SoftSign Layer Configuration in XML\nDESCRIPTION: This XML snippet demonstrates how to configure a SoftSign layer in OpenVINO. It defines the input and output ports with their respective dimensions.  The layer type is specified as \"SoftSign\".\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/softsign-9.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"SoftSign\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Divide Layer XML Definition (Broadcast)\nDESCRIPTION: This XML snippet defines a Divide layer in OpenVINO with NumPy auto-broadcasting. The inputs are two tensors of different shapes (8x1x6x1 and 7x1x5), and the output is a tensor with the broadcasted shape (8x7x6x5). The `m_pythondiv` attribute is set to \"false\", indicating regular division.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/divide-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Divide\">\n    <data auto_broadcast=\"numpy\" m_pythondiv=\"false\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Disable Suggest Override Warning CMake\nDESCRIPTION: This snippet checks if the SUGGEST_OVERRIDE_SUPPORTED variable is set and then disables the '-Wno-suggest-override' warning in the C++ compiler flags if it is. This is useful to suppress specific compiler warnings during the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/unit_test_utils/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(SUGGEST_OVERRIDE_SUPPORTED)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-suggest-override\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Globbing source files\nDESCRIPTION: Collects all `.cpp` files recursively from the current directory. These files are then used as source files for the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/logger/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES *.cpp)\n```\n\n----------------------------------------\n\nTITLE: Wrapping inference execution in C\nDESCRIPTION: This C code snippet shows how to wrap C++ interfaces for operating on objects, specifically inference execution using `ov::InferRequest`. It calls the C++ API directly, taking a C `struct` pointer for the operation object without needing to save a return value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/how_to_wrap_openvino_interfaces_with_c.md#_snippet_2\n\nLANGUAGE: C\nCODE:\n```\nhttps://github.com/openvinotoolkit/openvino/blob/d96c25844d6cfd5ad131539c8a0928266127b05a/src/bindings/c/src/ov_infer_request.cpp#L236-L247\n```\n\n----------------------------------------\n\nTITLE: Defining and Setting Global Property for Extra Link Libraries\nDESCRIPTION: Defines a global property named `EXTRA_LINK_LIBRARIES_GLOBAL` to accumulate link libraries from all backends. This allows different backends to contribute their required link libraries to the final target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ndefine_property(GLOBAL PROPERTY EXTRA_LINK_LIBRARIES_GLOBAL BRIEF_DOCS \"All link libs\" FULL_DOCS \"Link libraries collection from all backends\")\nset_property(GLOBAL PROPERTY EXTRA_LINK_LIBRARIES_GLOBAL \"\")\n```\n\n----------------------------------------\n\nTITLE: 1D GroupConvolution XML Example\nDESCRIPTION: This XML snippet demonstrates the configuration for a 1D GroupConvolution layer in OpenVINO. It specifies the input and output tensor dimensions, along with attributes like dilations, pads, and strides, showcasing how to define a 1D convolution operation with grouped input channels.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/group-convolution-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"GroupConvolution\" ...>\n    <data dilations=\"1\" pads_begin=\"2\" pads_end=\"2\" strides=\"1\" auto_pad=\"explicit\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>12</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>224</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Samples Release Directory on Linux\nDESCRIPTION: This command navigates to the OpenVINO C++ samples release directory on a Linux system. It assumes the build directory is located in the user's home directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_20\n\nLANGUAGE: sh\nCODE:\n```\ncd ~/openvino_cpp_samples_build/intel64/Release\n```\n\n----------------------------------------\n\nTITLE: Requirements File with Constraints\nDESCRIPTION: This example demonstrates a requirements file using a constraints file. The `-c` flag includes the constraints file, specifying the package versions.  The requirements file lists only package names, relying on the constraints file for version management.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/requirements_management.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n# requirements_pytorch.txt\n\n-c main/constraints.txt  # versions are already defined here!\ncoverage\nastroid\npylint\npyenchant\n```\n\n----------------------------------------\n\nTITLE: Exp Layer Definition in XML - OpenVINO\nDESCRIPTION: This XML snippet defines an Exp layer within an OpenVINO model. It specifies the input and output ports with their corresponding dimensions. The layer takes a tensor of shape (1, 256) as input and produces an output tensor of the same shape after applying the exponential function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/exp-1.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"Exp\">\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>256</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>256</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining Target and Suppressing Warnings CMake\nDESCRIPTION: This snippet defines the target name for the C API library and suppresses deprecated warnings. The target name is set to `openvino_c`, and the `ov_disable_deprecated_warnings()` macro is used to suppress warnings related to legacy exception types.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME openvino_c)\n\n# Suppress warnings due to catch macro with legacy exception types\nov_disable_deprecated_warnings()\n\nadd_definitions(-DIN_OV_COMPONENT)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Requirements for OpenVINO Samples (Linux)\nDESCRIPTION: This code snippet demonstrates how to install the necessary Python dependencies for running OpenVINO samples on Linux. It uses `pip` to install the packages listed in the `requirements.txt` file located in the sample directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncd <INSTALL_DIR>/samples/python/<SAMPLE_DIR>\npython3 -m pip install -r ./requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Exclude Layers by Type\nDESCRIPTION: Quantizes the model, excluding layers of specified types (e.g., Conv2d, Linear) from the quantization process. This allows you to avoid quantizing certain layer types that might degrade accuracy.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_22\n\nLANGUAGE: sh\nCODE:\n```\ntypes = ['Conv2d', 'Linear']\nncf.quantize(model, dataset, ignored_scope=nncf.IgnoredScope(types=types))\n\n```\n\n----------------------------------------\n\nTITLE: set_property() Implementation C++\nDESCRIPTION: This snippet demonstrates how to set compiled model-specific properties. The implementation provided here just copies all input properties into the m_cfg member. Individual properties are not validated or handled separately.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/compiled-model.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nvoid CompiledModel::set_property(const ov::AnyMap& properties) {\n    m_cfg = properties;\n}\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files in CMake\nDESCRIPTION: This snippet uses the file(GLOB_RECURSE) command to find all .cpp and .hpp files in the current source directory and its subdirectories and stores them in the SOURCES variable. These files will be compiled into the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/common/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES \"*.cpp\" \"*.hpp\")\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Internal Components Relationship Diagram\nDESCRIPTION: This diagram shows the internal components of OpenVINO and their relationships.  It illustrates dependencies between modules such as util, itt, pugixml, builders, reference implementations, shape inference, core, inference, frontend_common, transformations, LP transformations, and the openvino::runtime library. It depicts the interaction between public and developer APIs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/docs/architecture.md#_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TB\n    util[(openvino::util)]\n    style util fill:#01429f\n\n    itt[(openvino::itt)]\n    style itt fill:#01429f\n\n    pugixml[(openvino::pugixml)]\n    style pugixml fill:#01429f\n\n    builders[(builders)]\n    style builders fill:#01429f\n\n    reference[(reference implementations)]\n    style reference fill:#01429f\n\n    shape_inference[(shape inference)]\n    style shape_inference fill:#01429f\n\n    subgraph core [core]\n        style core fill:#6e9f01\n        core_api{{Core public API}}\n\n        core_api--->builders\n        core_api--->reference\n        core_api--->shape_inference\n    end\n\n    subgraph core_dev [openvino::core::dev]\n        style core_dev fill:#af8401\n        core_dev_api{{Core developer API}}\n    end\n\n    core ===> core_dev\n\n    core_dev_api-.->openvino_dev_api\n\n    pugixml ===> core\n    builders ===> core\n    reference ===> core\n    shape_inference ===> core\n    util ===> core\n    itt ===> core\n\n    subgraph inference [inference]\n        style inference fill:#6e9f01\n        inference_api{{Inference public API}}\n        inference_dev_api{{Infernce developer API}}\n\n        inference_api-.->inference_dev_api\n    end\n\n    core ===> inference\n    util ===> inference\n    itt ===> inference\n\n    inference_dev_api-.->openvino_dev_api\n\n    subgraph frontend_common [frontend_common]\n        style frontend_common fill:#6e9f01\n        frontend_api{{Frontend API}}\n    end\n\n    core_dev ===> frontend_common\n    util ===> frontend_common\n\n\n    frontend_common ---> core_dev\n\n\n    subgraph transformations [OpenVINO transformations]\n        style transformations fill:#6e9f01\n        transformations_api{{Transformations API}}\n    end\n\n    reference ===> transformations\n    builders ===> transformations\n    shape_inference ===> transformations\n    itt ===> transformations\n    core_dev ===> transformations\n\n    transformations ---> core_dev\n\n    subgraph lp_transformations [OpenVINO LP transformations]\n        style lp_transformations fill:#6e9f01\n        lp_transformations_api{{LP transformations API}}\n    end\n\n    itt ===> lp_transformations\n    transformations ---> lp_transformations\n    core ---> lp_transformations\n\n\n    subgraph openvino [openvino::runtime]\n        style openvino fill:#cb6969\n        openvino_api{{OpenVINO Public API}}\n    end\n\n    transformations ===> openvino\n    lp_transformations ===> openvino\n    frontend_common ===> openvino\n    reference ===> openvino\n    builders ===> openvino\n    shape_inference ===> openvino\n    core ===> openvino\n\n    frontend_api-.->openvino_api\n    inference_api-.->openvino_api\n    core_api-.->openvino_api\n\n    subgraph openvino_dev [openvino::runtime::dev]\n        style openvino_dev fill:#af8401\n        openvino_dev_api{{OpenVINO developer API}}\n    end\n\n    openvino ===> openvino_dev\n    inference_dev_api-.->openvino_dev_api\n    lp_transformations_api-.->openvino_dev_api\n```\n\n----------------------------------------\n\nTITLE: Setting Library Output Path CMake\nDESCRIPTION: This CMake function determines the output directory for the library based on whether a multi-configuration generator is used. If it is, the output directory is configured to include the configuration type (e.g., Debug, Release).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ocl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(get_lib_path OUTPUT_DIR FINAL_OUTPUT_DIR)\n    if(OV_GENERATOR_MULTI_CONFIG)\n        set(OUTPUT_DIR \"${OUTPUT_DIR}/$<CONFIG>\")\n    endif()\n\n    set(\"${FINAL_OUTPUT_DIR}\" \"${OUTPUT_DIR}\" PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Defining Compile Definitions for Static Library (CMake)\nDESCRIPTION: This snippet adds a compile definition `OPENVINO_STATIC_LIBRARY` when `BUILD_SHARED_LIBS` is not enabled, indicating that the library is being built as a static library. This allows for conditional compilation based on the library type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/src/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME} PRIVATE OPENVINO_STATIC_LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for OpenCL CMake\nDESCRIPTION: This snippet sets include directories for the OpenCL target, pointing to the cl_headers and clhpp_headers directories. These headers are used during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ocl/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(OpenCL INTERFACE\n    $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/cl_headers>\n    $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/clhpp_headers/include>)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO sample using CMake\nDESCRIPTION: This CMake snippet uses the `ov_add_sample` macro to configure an OpenVINO sample application. It defines the name of the sample as 'hello_nv12_input_classification_c' and specifies that the source code is located in 'main.c'. The `CMAKE_CURRENT_SOURCE_DIR` variable provides the current directory path to locate the source file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/c/hello_nv12_input_classification/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_sample(NAME hello_nv12_input_classification_c\n              SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/main.c\")\n```\n\n----------------------------------------\n\nTITLE: Defining Level Zero Headers Target (CMake)\nDESCRIPTION: This snippet defines an INTERFACE library for Level Zero headers and appends the interface include directories from LevelZero::LevelZero target to it. It also defines an alias for it.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/zero/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(level-zero-headers INTERFACE)\nset_property(TARGET level-zero-headers APPEND PROPERTY INTERFACE_INCLUDE_DIRECTORIES $<TARGET_PROPERTY:LevelZero::LevelZero,INTERFACE_INCLUDE_DIRECTORIES>)\nadd_library(LevelZero::Headers ALIAS level-zero-headers)\n```\n\n----------------------------------------\n\nTITLE: Verify Python installation\nDESCRIPTION: Verifies the installed Python version using the `python3.8 --version` command. This confirms that the new Python version is installed correctly and accessible.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/python_version_upgrade.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3.8 --version\n```\n\n----------------------------------------\n\nTITLE: Filter Blob Dumps by Execution ID\nDESCRIPTION: Filters the blobs to be dumped based on the execution ID of the nodes for OpenVINO CPU executions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_NODE_EXEC_ID='<space_separated_list_of_ids>' binary ...\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library for OpenVINO Core Dev API\nDESCRIPTION: This snippet installs the static library for the OpenVINO Core Dev API. It uses a custom function `ov_install_static_lib` to handle the installation.  This function likely takes the library name and the component to which it belongs as arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n# Install interface libraries for case BUILD_SHARED_LIBS=OFF\nov_install_static_lib(openvino_core_dev ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: Opt-out of OpenVINO Telemetry (Bash)\nDESCRIPTION: This command allows users to opt-out of the OpenVINO telemetry data collection. It uses the `opt_in_out` utility with the `--opt_out` flag to disable telemetry reporting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nopt_in_out --opt_out\n```\n\n----------------------------------------\n\nTITLE: Skipping Workflow Based on Labels and File Changes (YAML)\nDESCRIPTION: This YAML snippet illustrates how to configure the Smart CI action to skip an entire workflow based on specific labels or file changes. It shows how to define the `skip_when_only_listed_labels_set` and `skip_when_only_listed_files_changed` parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_14\n\nLANGUAGE: YAML\nCODE:\n```\nSmart_CI:\n  runs-on: ubuntu-latest\n  outputs:\n    ...\n    # The output below is set only if the workflow can be completely skipped, and is empty otherwise\n    skip_workflow: \"${{ steps.smart_ci.outputs.skip_workflow }}\"\n  steps:\n    ...\n    - name: Get affected components\n      id: smart_ci\n      uses: ./.github/actions/smart-ci\n      with:\n        ...\n        # Comma-separated rules for skipping the entire workflow\n        skip_when_only_listed_labels_set: 'docs'\n        skip_when_only_listed_files_changed: '*.md,*.rst,*.png,*.jpg,*.svg'\n```\n\n----------------------------------------\n\nTITLE: Setting Threading Interface\nDESCRIPTION: This snippet configures the threading interface for the test target `${TARGET_NAME}` using `ov_set_threading_interface_for`.  This ensures that the tests use the correct threading settings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/functional/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Multiclass Non-Maximum Suppression Layer Configuration (3 Inputs)\nDESCRIPTION: This example illustrates the configuration of a MulticlassNonMaxSuppression layer in OpenVINO using three input tensors: boxes, scores, and roisnum. It showcases how the layer's data attributes, input port dimensions, and output port precisions are defined for processing object detection results with a variable number of regions of interest.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/multiclass-non-max-suppression-9.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"MulticlassNonMaxSuppression\" ... >\n    <data sort_result=\"score\" output_type=\"i64\" sort_result_across_batch=\"false\" iou_threshold=\"0.2\" score_threshold=\"0.5\" nms_top_k=\"-1\" keep_top_k=\"-1\" background_class=\"-1\"    normalized=\"false\" nms_eta=\"0.0\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>100</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>\n            <dim>100</dim>\n        </port>\n        <port id=\"2\">\n            <dim>10</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"5\" precision=\"FP32\">\n            <dim>-1</dim> <!-- \"-1\" means a undefined dimension calculated during the model inference -->\n            <dim>6</dim>\n        </port>\n        <port id=\"6\" precision=\"I64\">\n            <dim>-1</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"7\" precision=\"I64\">\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Test Target in CMake\nDESCRIPTION: This snippet uses the `ov_add_test_target` macro to define a test target named `${TARGET_NAME}`.  It specifies the root directory, dependencies (openvino_template_extension), and link libraries (unit_test_utils). It also enables clang-format and sets labels for the test.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/tests/unit/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        DEPENDENCIES\n            openvino_template_extension\n        LINK_LIBRARIES\n            unit_test_utils\n        ADD_CLANG_FORMAT\n        LABELS\n            OV UNIT RUNTIME\n)\n```\n\n----------------------------------------\n\nTITLE: PReLU layer definition with 1D input in XML\nDESCRIPTION: This example shows the XML representation of a PReLU layer with a 1D input tensor 'data' and a 1D 'slope' tensor. The 'data' tensor has a dimension of 128, while the 'slope' tensor has a dimension of 1. The output tensor has the same shape as the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/prelu-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Prelu\">\n    <input>\n        <port id=\"0\">\n            <dim>128</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Enable GPU Debug Capabilities\nDESCRIPTION: Enables GPU debug configurations and debug capabilities by defining preprocessor macros.  This allows for extra debugging information and functionalities in the GPU plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_GPU_DEBUG_CAPS)\n    add_definitions(-DGPU_DEBUG_CONFIG=1)\n    add_definitions(-DENABLE_DEBUG_CAPS=1)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing C Samples Directory in CMake\nDESCRIPTION: Installs the C samples directory to the specified destination. It excludes CMakeLists.txt and .clang-format files from the installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(DIRECTORY c\n        DESTINATION ${OV_CPACK_SAMPLESDIR}\n        COMPONENT ${OV_CPACK_COMP_C_SAMPLES}\n        ${OV_CPACK_COMP_C_SAMPLES_EXCLUDE_ALL}\n        PATTERN c/CMakeLists.txt EXCLUDE\n        PATTERN c/.clang-format EXCLUDE)\n```\n\n----------------------------------------\n\nTITLE: Setting Source File Properties in CMake\nDESCRIPTION: This snippet sets include directories for threading.cpp based on the openvino::core::dev interface target property. It also disables the -Wno-suggest-override warning for specific files if SUGGEST_OVERRIDE_SUPPORTED is enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset_source_files_properties(${CMAKE_CURRENT_SOURCE_DIR}/threading.cpp\n        PROPERTIES INCLUDE_DIRECTORIES $<TARGET_PROPERTY:openvino::core::dev,INTERFACE_INCLUDE_DIRECTORIES>)\n\nif(SUGGEST_OVERRIDE_SUPPORTED)\n    set_source_files_properties(ov_tensor_test.cpp\n                                type_prop/multiclass_nms.cpp\n                                type_prop/squeeze.cpp\n                                PROPERTIES COMPILE_OPTIONS -Wno-suggest-override)\nendif()\n```\n\n----------------------------------------\n\nTITLE: OpenCV G-API Build Commands\nDESCRIPTION: These commands build the OpenCV G-API module with OpenVINO support. They create a build directory, run CMake with specific build options, and then build the `opencv_gapi` target. Options include enabling OpenVINO and/or ONNX support.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_22\n\nLANGUAGE: cmake\nCODE:\n```\nmkdir \"build\" && cd \"build\"\ncmake ..  -DBUILD_LIST=gapi          ^\n          -DCMAKE_BUILD_TYPE=Release ^\n          -DWITH_OPENVINO=ON\ncmake --build . --config Release --target opencv_gapi --parallel\n```\n\nLANGUAGE: cmake\nCODE:\n```\nmkdir \"build\" && cd \"build\"\ncmake ..  -DBUILD_LIST=gapi                             ^\n          -DCMAKE_BUILD_TYPE=Release                    ^\n          -DWITH_OPENVINO=ON                            ^\n          -DWITH_ONNX=ON                                ^\n          -DORT_INSTALL_DIR=<onnxrt-install-dir>\ncmake --build . --config Release --target opencv_gapi --parallel\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate 3D Tensor Update - C++ (axis=2)\nDESCRIPTION: This code snippet shows how the ScatterElementsUpdate operation updates a 3D tensor when axis is set to 2. It demonstrates the element update logic based on the indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i][j][indices[i][j][k]] = reduction(updates[i][j][k], output[i][j][indices[i][j][k]]) if axis = 2\n```\n\n----------------------------------------\n\nTITLE: Set Output Tensor by Tensor Object TypeScript\nDESCRIPTION: Sets the output tensor for the InferRequest when the model has a single output.  The tensor's type and shape must match the model's output requirements. An exception is thrown if the model has multiple outputs. This method returns void.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nsetOutputTensor(tensor): void\n```\n\n----------------------------------------\n\nTITLE: Force Shared CRT for Google Test\nDESCRIPTION: This snippet forces the use of a shared C Runtime Library (CRT) for Google Test. This is important on Windows when different parts of the project may be compiled with different CRT settings. Using a shared CRT avoids potential conflicts.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/gtest/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(gtest_force_shared_crt ON CACHE BOOL \"disable static CRT for google test\")\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (TensorFlow) with OpenVINO in C++\nDESCRIPTION: This code compiles a model in TensorFlow format using the `core.compile_model()` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\nov::CompiledModel compiled_model_tf = core.compile_model(\"path_to_model.pb\", \"AUTO\");\n```\n\n----------------------------------------\n\nTITLE: Conditional SYCL Source Removal\nDESCRIPTION: This snippet conditionally removes the SYCL source files from `LIBRARY_SOURCES_ALL` if the compiler is not Intel LLVM. This ensures that SYCL code is only compiled when using the appropriate compiler.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT OV_COMPILER_IS_INTEL_LLVM)\n    list(REMOVE_ITEM LIBRARY_SOURCES_ALL ${SYCL_SOURCES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Key Spacing in Object Literals\nDESCRIPTION: This rule enforces consistent spacing between keys and values in object literals in JavaScript and TypeScript code. It ensures no space before the colon and is configured as `key-spacing: ['error', { beforeColon: false }]`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_16\n\nLANGUAGE: JavaScript\nCODE:\n```\nkey-spacing: ['error', { beforeColon: false }]\n```\n\n----------------------------------------\n\nTITLE: Using WrapType with a predicate\nDESCRIPTION: This Python snippet demonstrates how to use WrapType with a predicate (lambda function) to add an additional check. The predicate validates that the number of consumers for the matched node is exactly 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.tools.ovc.composite.pattern import WrapType\n\ndef wrap_type_predicate():\n    pattern_sig = WrapType(\"opset13.Relu\", lambda node: len(node.get_output_target_inputs(0)) == 1)\n\n    return pattern_sig\n```\n\n----------------------------------------\n\nTITLE: Adding Dependency to 'fuzz' Target in CMake\nDESCRIPTION: This snippet adds a dependency from the 'fuzz' custom target to the individual fuzz test target. This ensures that all individual fuzz test targets are built before the 'fuzz' target is considered complete. This makes building all fuzz tests a simple process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/src/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_dependencies(fuzz ${test_name})\n```\n\n----------------------------------------\n\nTITLE: Defining Complex Layout with ov::Layout in C++\nDESCRIPTION: This snippet shows how to define a complex layout using `ov::Layout` in C++ with square brackets, assigning descriptive names to each dimension (N, C, H, W). This syntax offers enhanced readability and descriptive dimension labeling.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nov::Layout layout(\"[N, C, H, W]\");\n```\n\n----------------------------------------\n\nTITLE: Setting Properties in OpenVINO (TypeScript)\nDESCRIPTION: These snippets demonstrate the `setProperty` function, which allows setting properties for the OpenVINO runtime. It can set global properties or properties specific to a device. It takes a Record<string, OVAny> object as input, representing property name-value pairs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nsetProperty(properties: Record<string, OVAny>): void\n```\n\nLANGUAGE: typescript\nCODE:\n```\nproperties: Record<string, OVAny>,\n```\n\nLANGUAGE: typescript\nCODE:\n```\nsetProperty(deviceName, properties: Record<string, OVAny>): void\n```\n\n----------------------------------------\n\nTITLE: Setting Linker Flags for Intel LLVM on UNIX\nDESCRIPTION: This snippet sets linker flags for the Intel LLVM compiler on UNIX systems, using 'lld' as the linker and removing '-pie' from release flags.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(OV_COMPILER_IS_INTEL_LLVM AND UNIX)\n  set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -fuse-ld=lld\")\n  string(REPLACE \"-pie\" \"\" CMAKE_EXE_LINKER_FLAGS_RELEASE \"${CMAKE_EXE_LINKER_FLAGS_RELEASE}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enabling Faster Build for Target in CMake\nDESCRIPTION: This CMake snippet uses the `ov_build_target_faster` macro to enable faster build options for the specified target `ov_subgraphs_dumper_tests`. The `UNITY` option likely groups multiple source files into a single compilation unit to speed up the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/subgraphs_dumper/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME} UNITY)\n```\n\n----------------------------------------\n\nTITLE: Running All Tests with Pytest\nDESCRIPTION: This snippet demonstrates how to run all available tests using the `pytest` framework. It executes `pytest` through the Python interpreter, initiating the test discovery and execution process within the current working directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/samples_tests/smoke_tests/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest\n```\n\n----------------------------------------\n\nTITLE: Benchmark App Command - Limited Device Choice\nDESCRIPTION: This command executes the OpenVINO Benchmark Application, using AUTO plugin with a specified device list(CPU, GPU).  It runs inference with a given model and input for 1000 iterations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app –d AUTO:CPU,GPU –m <model> -i <input> -niter 1000\n```\n\n----------------------------------------\n\nTITLE: Transform Model C++\nDESCRIPTION: This snippet shows how to apply the transformations to the model.  It accepts a shared pointer to an `ov::Model` object and applies transformations to make it more hardware-friendly, improving performance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<ov::Model> Plugin::transform_model(const std::shared_ptr<ov::Model>& model, const ov::AnyMap& properties) const {\n    auto transformed_model = model->clone();\n\n    return transformed_model;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Filter Format Specification (Bash)\nDESCRIPTION: This code snippet shows the generic format for defining filters in OpenVINO. The filter name is followed by a comma-separated list of tokens. Tokens are processed from left to right, with a minus sign prefix indicating exclusion.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/debug_caps_filters.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nfilter_name=<comma_separated_tokens>\n```\n\n----------------------------------------\n\nTITLE: Defining Simple Layout with ov::Layout in C++\nDESCRIPTION: This snippet demonstrates how to define a simple layout using the `ov::Layout` class in C++, assigning letters to each dimension (N, C, H, W). It provides a basic way to specify the meaning of each dimension in a tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov::Layout layout(\"NCHW\");\n```\n\n----------------------------------------\n\nTITLE: Set Tensor by Name and Tensor Object TypeScript\nDESCRIPTION: Sets an input or output tensor for the InferRequest by name. The `name` parameter specifies the name of the tensor. The tensor's type and shape must match the model's input/output requirements. This method returns void.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nsetTensor(name, tensor): void\n```\n\n----------------------------------------\n\nTITLE: set_tensors_impl() Method C++\nDESCRIPTION: This code snippet shows the implementation of the `set_tensors_impl()` method, which allows setting batched tensors if the plugin supports it. This method is crucial for handling input data in batch inference scenarios.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/synch-inference-request.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nvoid InferRequest::set_tensors_impl() {\n    // Implementation for setting batched tensors\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up OVSA artefacts directory\nDESCRIPTION: Creates and navigates into the `artefacts` directory, sets the OVSA_DEV_ARTEFACTS environment variable to the directory path, and sources the setupvars.sh script, initializing the environment for OVSA development tasks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_38\n\nLANGUAGE: sh\nCODE:\n```\nmkdir -p ~/OVSA/artefacts\ncd ~/OVSA/artefacts\nexport OVSA_DEV_ARTEFACTS=$PWD\nsource /opt/ovsa/scripts/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: Benchmark Application Command in Shell\nDESCRIPTION: This shell command runs the OpenVINO benchmark application to infer a model, specifying the model file, device, number of iterations, API type, report type, and report folder. It measures the average performance counters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/advanced-guides/low-precision-transformations.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n./benchmark_app -m resnet-50-tf.xml -d CPU -niter 1 -api sync -report_type average_counters  -report_folder pc_report_dir\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Overwrite (none) - C++\nDESCRIPTION: This code snippet demonstrates the 'none' reduction mode for the ScatterElementsUpdate operation, where elements in the data tensor are directly overwritten by the values from the updates tensor based on the provided indices. No additional operation is performed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[indices[i]] = updates[i], axis = 0\n```\n\n----------------------------------------\n\nTITLE: Verbose Output Format\nDESCRIPTION: This snippet describes the format of the verbose output generated by the OpenVINO CPU plugin when verbose mode is enabled. It shows the structure of the output string, including node implementer, name, type, algorithm, implementation type, input/output port information, post-operations, and execution time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/verbose.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nov_cpu_verbose,exec,<node_implemeter>,\\\n    <node_name>:<node_type>:<node_alg>,<impl_type>,\\\n    src:<port_id>:<precision>::<type>:<format>:f0:<shape> ...,\\\n    dst:<port_id>:<precision>::<type>:<format>:f0:<shape> ...,\\\n    post_ops:'<node_name>:<node_type>:<node_alg>;...;',\\\n    <execution_time>\n```\n\n----------------------------------------\n\nTITLE: Building Final Package (Windows)\nDESCRIPTION: This snippet builds the final OpenVINO package using cmake with the Release configuration. This step compiles the project after it has been configured with the desired options for conditional compilation and optimization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\ncmake --build . --config Release\n```\n\n----------------------------------------\n\nTITLE: Installing Python requirements for Wheel package\nDESCRIPTION: This command installs the Python requirements for building a wheel package for the OpenVINO Python API using `pip`. It uses the `requirements-dev.txt` file located in the `<openvino source tree>/src/bindings/python/wheel/` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_linux.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npip install -r <openvino source tree>/src/bindings/python/wheel/requirements-dev.txt\n```\n\n----------------------------------------\n\nTITLE: Define Target and Source Files in CMake\nDESCRIPTION: This CMake snippet defines the target name, collects source and header files using GLOB_RECURSE, and creates an executable from them. It is a fundamental part of the build configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/memcheck_tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset (TARGET_NAME \"MemCheckTests\")\n\nfile (GLOB_RECURSE SRC *.cpp)\nfile (GLOB_RECURSE HDR *.h)\n\n# Create library file from sources.\nadd_executable(${TARGET_NAME} ${HDR} ${SRC})\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries\nDESCRIPTION: Links the target against the openvino::npu_al and openvino::npu_common libraries. These libraries are linked privately, meaning that they are not exposed to other targets that link against this library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/backend/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME}\n    PRIVATE\n        openvino::npu_al\n        openvino::npu_common\n)\n```\n\n----------------------------------------\n\nTITLE: Conditional Filesystem Library Linking (CMake)\nDESCRIPTION: This block checks the results of the `try_compile` commands to determine which filesystem library, if any, needs to be linked. It sets the `STD_FS_LIB` variable accordingly or issues a warning if the correct library cannot be determined. It also avoids setting any flags for MSVC.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n  message(STATUS \"MSVC - No explicit filesystem linker setting required.\")\nelif(STD_FS_NEEDS_STDCXXFS)\n  message(STATUS \"STD_FS_NEEDS_STDCXXFS - Add explicit filesystem linker setting: 'stdc++fs'.\")\n  set(STD_FS_LIB stdc++fs)\nelif(STD_FS_NEEDS_CXXFS)\n  message(STATUS \"STD_FS_NEEDS_CXXFS - Add explicit filesystem linker setting: 'c++fs'.\")\n  set(STD_FS_LIB c++fs)\nelif(STD_FS_NO_LIB_NEEDED)\n  message(STATUS \"STD_FS_NO_LIB_NEEDED - No explicit filesystem linker setting required.\")\nelse()\n  message(WARNING \"Unknown C++ build setup - No explicit filesystem linker setting set\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target and ABI Free Target in CMake\nDESCRIPTION: This adds a clang-format target for the specified object library and also specifies this library as important for ABI free.  Includes source and include directories to be scanned by clang-format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME}_obj)\n\n# FEM is public API => need to mark this library as important for ABI free\nov_abi_free_target(${TARGET_NAME}_obj)\n\nov_ncc_naming_style(FOR_TARGET ${TARGET_NAME}_obj\n                    SOURCE_DIRECTORIES \"${FRONTEND_INCLUDE_DIR}\"\n                    ADDITIONAL_INCLUDE_DIRECTORIES\n                        $<TARGET_PROPERTY:openvino::core::dev,INTERFACE_INCLUDE_DIRECTORIES>)\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO with Fuzzing and Sanitizer Enabled (Bash)\nDESCRIPTION: This script builds OpenVINO with the `ENABLE_FUZZING` and `ENABLE_SANITIZER` options enabled, using the clang compiler. This is a prerequisite for building and running fuzz tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n(\\nmkdir -p build && cd build && \\\nCC=clang CXX=clang++ cmake .. -DENABLE_FUZZING=ON -DENABLE_SANITIZER=ON && \\\ncmake --build . \\\n)\n```\n\n----------------------------------------\n\nTITLE: CMake Subdirectory Inclusion\nDESCRIPTION: This snippet conditionally adds subdirectories based on the `ENABLE_TESTS` and `ENABLE_FUNCTIONAL_TESTS` CMake variables. The `add_subdirectory` command includes the specified directory in the build process if the corresponding variable is set to true.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(common_test_utils)\n    add_subdirectory(unit_test_utils)\nendif()\n\nif(ENABLE_FUNCTIONAL_TESTS)\n    add_subdirectory(functional_test_utils)\nendif()\n```\n\n----------------------------------------\n\nTITLE: AVX2 Optimization Flags in CMake\nDESCRIPTION: This CMake snippet conditionally applies AVX2 optimization flags if `ENABLE_AVX2` is set. It uses the `ov_avx2_optimization_flags` function to retrieve the necessary flags and then applies them to the target using `target_compile_options`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/unit/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_AVX2)\n    ov_avx2_optimization_flags(avx2_flags)\n    target_compile_options(${TARGET_NAME} PRIVATE \"${avx2_flags}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Add Clang Format Target CMake\nDESCRIPTION: This snippet adds a clang-format target to enforce code style for the 'paddle_fe_standalone_build_test' target. It utilizes the `ov_add_clang_format_target` function to create a target that checks and formats the code according to the project's coding standards.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/tests/standalone_build/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Querying Model with Device Properties in OpenVINO (TypeScript)\nDESCRIPTION: This snippet demonstrates the `queryModel` function, which queries a device to check if it supports a specified model with given properties. It takes the model, device name, and optional properties as input and returns a key-value pair representing the device's capabilities.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nqueryModel(model, deviceName, properties?): { [key: string]: string }\n```\n\nLANGUAGE: typescript\nCODE:\n```\nproperties: Record<string, OVAny>,\n```\n\n----------------------------------------\n\nTITLE: Create String Tensor with Initial Data (Python)\nDESCRIPTION: Creates an OpenVINO tensor populated with string data using a Python list. The strings from the initialization list are copied to a separately allocated storage underneath the tensor object. Requires the `openvino` package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/string-tensors.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\ntensor = ov.Tensor(['text', 'more text', 'even more text'])\n```\n\n----------------------------------------\n\nTITLE: Target Include Directories CMake\nDESCRIPTION: This snippet sets the include directories for the `openvino_snippets` target. It specifies that the public headers directory should be included when building the interface, and the shape inference include directory should be included privately when building the interface.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC $<BUILD_INTERFACE:${PUBLIC_HEADERS_DIR}>\n                                          PRIVATE $<BUILD_INTERFACE:${SHAPE_INFER_INCLUDE_DIR}>)\n```\n\n----------------------------------------\n\nTITLE: Creating a Virtual Disk Image (sh)\nDESCRIPTION: Creates a new virtual disk image in qcow2 format with a size of 20GB. This image is intended for the User role Guest VM and will be used to install the operating system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_21\n\nLANGUAGE: sh\nCODE:\n```\nsudo qemu-img create -f qcow2 <path>/ovsa_ovsa_runtime_vm_disk.qcow2 20G\n```\n\n----------------------------------------\n\nTITLE: Defining Simple Layout with ov::Layout in Python\nDESCRIPTION: This snippet demonstrates how to define a simple layout using the `ov::Layout` class in Python, assigning letters to each dimension (N, C, H, W). It provides a basic way to specify the meaning of each dimension in a tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nlayout = ov.Layout(\"NCHW\")\n```\n\n----------------------------------------\n\nTITLE: Using Predefined Layout Names in C++\nDESCRIPTION: This snippet shows how to use predefined layout names (N, C, H, W) in C++ with `ov::Layout` to easily specify common image tensor layouts. It utilizes established conventions to enhance code readability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nov::Layout layout(\"NCHW\");\nsize_t channel_idx = layout.get_index(\"channel\");\nsize_t height_idx = layout.get_index(\"height\");\n```\n\n----------------------------------------\n\nTITLE: Compiling Model (IR) with OpenVINO in Python\nDESCRIPTION: This code compiles a model in IR format using the `core.compile_model()` method. It specifies the path to the model and the device to use for inference (AUTO in this case). The compiled model is then used for creating inference requests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel_ir = core.compile_model(\"path_to_model.xml\", \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: cancel() Method C++\nDESCRIPTION: This code snippet shows the implementation of the `cancel()` method. This plugin-specific method allows interrupting the synchronous execution from the AsyncInferRequest, providing a mechanism to stop the inference process prematurely if needed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/synch-inference-request.rst#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nvoid InferRequest::cancel() {\n    // Plugin specific method to interrupt the synchronous execution from the AsyncInferRequest\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling SSE4.2 Optimization Flags\nDESCRIPTION: This snippet conditionally enables SSE4.2 optimization flags if 'ENABLE_SSE42' is enabled. It retrieves the flags using 'ov_sse42_optimization_flags' and applies them to the source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_SSE42)\n  ov_sse42_optimization_flags(sse4_2_flags)\n  set_source_files_properties(${SOURCES_ALL} PROPERTIES COMPILE_FLAGS \"${sse4_2_flags}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Float16 Conversion\nDESCRIPTION: Converts a value to float16 by applying a mask and divisor.  This snippet demonstrates the conversion process for float16 data types in the context of generating random numbers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\nmantissa_digits = 11 //(mantissa / significand bits count of float16 + 1, equal to 11)\nmask = uint32(uint64(1) << mantissa_digits - 1)\ndivisor = float(1) / (uint64(1) << mantissa_digits)\noutput = float16((x & mask) * divisor)\n```\n\n----------------------------------------\n\nTITLE: Link Libraries and Include Directories in CMake\nDESCRIPTION: This snippet configures include directories and links the MemCheckTests executable against the StressTestsCommon library.  This ensures that the executable has access to the necessary headers and libraries during compilation and linking.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/memcheck_tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})\ntarget_link_libraries(${TARGET_NAME} PRIVATE StressTestsCommon)\n```\n\n----------------------------------------\n\nTITLE: OVSA Model Receiving\nDESCRIPTION: Copies the model files (.dat and .lic) from the model developer's VM to the OVSA runtime artefacts directory using scp. Requires the developer's VM IP address and username.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_51\n\nLANGUAGE: sh\nCODE:\n```\ncd $OVSA_RUNTIME_ARTEFACTS\nscp username@<developer-vm-ip-address>:/<username-home-directory>/OVSA/artefacts/<name-of-the-model>.dat .\nscp username@<developer-vm-ip-address>:/<username-home-directory>/OVSA/artefacts/<name-of-the-model>.lic .\n```\n\n----------------------------------------\n\nTITLE: CMake Version Check\nDESCRIPTION: This snippet checks the CMake version and exits if it's less than 3.14, as the JS API requires at least this version. It prevents building the addon with older CMake versions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_VERSION VERSION_LESS 3.14)\n    message(WARNING \"JS API is not available with CMake version less than 3.14, skipping\")\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Define GitHub Actions Jobs in YAML\nDESCRIPTION: This YAML snippet defines the structure of jobs within a GitHub Actions workflow. It shows the jobs names like Build, Debian_Packages, Samples, Conformance, ONNX_Runtime, etc.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/overview.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\njobs:\n  Build: ...\n  Debian_Packages: ...\n  Samples: ...\n  Conformance: ...\n  ONNX_Runtime: ...\n  CXX_Unit_Tests: ...\n  Python_Unit_Tests: ...\n  CPU_Functional_Tests: ...\n  TensorFlow_Hub_Models_Tests: ...\n  PyTorch_Models_Tests: ...\n  NVIDIA_Plugin: ...\n```\n\n----------------------------------------\n\nTITLE: Exporting Whisper model with stateless flag\nDESCRIPTION: This snippet demonstrates how to export an Open AI Whisper model using Optimum-cli, with the `--disable-stateful` flag. This flag is required for using whisper models on NPU.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_4\n\nLANGUAGE: Console\nCODE:\n```\noptimum-cli export openvino --trust-remote-code --model openai/whisper-tiny whisper-tiny --disable-stateful\n```\n\n----------------------------------------\n\nTITLE: Example Workflow Configuration using Mirrored Docker Image\nDESCRIPTION: This configuration shows how to use the mirror of the ubuntu:22.04 DockerHub image in a GHA workflow. It replaces `docker.io` with `openvinogithubactions.azurecr.io` to use the mirrored image in the Azure Container Registry.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/docker_images.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nSomeJob:\n    ...\n    container:\n      image: openvinogithubactions.azurecr.io/library/ubuntu:22.04\n```\n\n----------------------------------------\n\nTITLE: NV12 Preprocessing with Two Planes (Single Batch) in OpenVINO (C++)\nDESCRIPTION: Demonstrates how to set input tensors with a vector of shared surfaces for each plane when batching and surface sharing are required simultaneously, using NV12 format with two planes in a single batch scenario. It utilizes the ov::InferRequest::set_tensors method to provide the shared surfaces to the OpenVINO inference engine. Requires OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_34\n\nLANGUAGE: cpp\nCODE:\n```\n//! [single_batch]\nauto input_tensor = ov::Tensor(ov::element::u8, {height + height / 2, width}, input_data);\nauto input_port = compiled_model.input();\ninfer_request.set_tensor(input_port, input_tensor);\n//! [single_batch]\n```\n\n----------------------------------------\n\nTITLE: ReduceL1 with keep_dims=false in OpenVINO XML\nDESCRIPTION: Example of ReduceL1 operation in OpenVINO XML format with keep_dims set to false. The input tensor has dimensions 6x12x10x24, and the reduction is performed along axes [2, 3]. The output tensor has dimensions 6x12, as the reduced dimensions are removed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-l1-4.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceL1\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Run Application under ITT Collector for Code Analysis\nDESCRIPTION: This snippet shows how to run an application (in this case, `benchmark_app`) under the ITT collector (`sea_runtool.py`) to collect statistics on code usage for a specific model.  `${OPENVINO_LIBRARY_DIR}` should point to the location of OpenVINO libraries, `${MY_MODEL_RESULT}` is the directory to store the results, and `${MY_MODEL}.xml` is the model file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npython thirdparty/itt_collector/runtool/sea_runtool.py --bindir ${OPENVINO_LIBRARY_DIR} -o ${MY_MODEL_RESULT} ! ./benchmark_app -niter 1 -nireq 1 -m ${MY_MODEL}.xml\n```\n\n----------------------------------------\n\nTITLE: Excluding KleidiAI or Snippets X64 Paths\nDESCRIPTION: This snippet excludes KleidiAI paths on x64, snippets paths on AARCH64\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_20\n\nLANGUAGE: cmake\nCODE:\n```\nif (AARCH64)\n    # this directory is reused on RISCV64\n    list(APPEND EXCLUDE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/transformations/snippets/x64/*)\nelse()\n    list(APPEND EXCLUDE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/nodes/executors/kleidiai/*)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Mandatory Unit Tests Libraries in CMake\nDESCRIPTION: This CMake snippet defines a list of mandatory libraries required for the unit tests.  These libraries provide testing utilities, mocking frameworks, and runtime support. They are linked during the build process to enable testing functionalities.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/unit/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(MANDATORY_UNIT_TESTS_LIBS\n        \"openvino::commonTestUtils\"\n        \"openvino::gmock\"\n        \"openvino::gtest\"\n        \"openvino::gtest_main\"\n        \"openvino::runtime\"\n        \"openvino::npu_common\"\n        \"openvino::npu_al\"\n        \"openvino::npu_logger_utils\"\n)\n```\n\n----------------------------------------\n\nTITLE: RandomUniform Output Example 3 (C++)\nDESCRIPTION: This is an example of the output from the RandomUniform operation with an i32 output type, with global_seed is set to 80, op_seed is set to 100, minval is set to 50, maxval is set to 100 and alignment is set to TENSORFLOW.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\ninput_shape = [ 2, 3 ]\nminval = 50\nmaxval = 100\noutput  = [[65 70 56]\\\n         [59 82 92]]\n```\n\n----------------------------------------\n\nTITLE: Adding Static Library Target CMake\nDESCRIPTION: This snippet adds a static library target for the `ov_subgraphs_dumper` tool's utilities. It specifies the source directory, includes, linked libraries, dependencies, compile definitions, and excludes certain source files from the build.  This allows for easier access to the core dumping functionality without needing the executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/subgraphs_dumper/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_target(\n        NAME \"${TARGET_NAME}_util\"\n        TYPE STATIC\n        ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/src\"\n        INCLUDES\n            PUBLIC\n                \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\"\n        LINK_LIBRARIES\n            PUBLIC\n                ${LIBRARIES}\n        EXCLUDED_SOURCE_PATHS\n            ${OpenVINO_SOURCE_DIR}/src/tests/functional/plugin/conformance/subgraphs_dumper/src/main.cpp\n            ${OpenVINO_SOURCE_DIR}/src/tests/functional/plugin/conformance/subgraphs_dumper/include/gflag_config.hpp\n        DEPENDENCIES\n            ov_frontends\n            ${LIBRARIES}\n        DEFINES\n            ${COMPILE_DEFINITIONS}\n        ADD_CPPLINT\n)\n```\n\n----------------------------------------\n\nTITLE: Shape Inference in InterpolateCalculation\nDESCRIPTION: The `shape_infer` method calculates the output shape based on the input shape, padding, sizes, and scales. It checks the shape calculation mode (SIZES or SCALES) and updates the result accordingly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef shape_infer(self, input_data, sizes, scales):\n    result = input_data.shape + self.pads_begin + self.pads_end\n\n    if self.shape_calculation_mode == ShapeCalculationMode.SIZES:\n        for i, axis in enumerate(self.axes):\n            result[axis] = sizes[i]\n    else:\n        for i, axis in enumerate(self.axes):\n            result[axis] = math.floor(scales[i] * result[axis])\n\n    return result\n```\n\n----------------------------------------\n\nTITLE: Convolution Test Parameters Declaration in C++\nDESCRIPTION: This snippet declares parameters for a functional test instantiation of the Convolution layer in the ``Template`` plugin. These parameters are used to configure and customize the test execution for different scenarios.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin-testing.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nconst std::vector<convLayerTestParamsSet> convParams_ExplicitPadding_FP32 = {\n    /* { inputShape, filters, kernel, stride, padBegin, padEnd, dilation, groups, Dtype, device, impl_name }*/\n    { {1, 3, 32, 32}, {16}, {3, 3}, {1, 1}, {1, 1}, {1, 1}, {1, 1}, 1, ov::element::f32, CommonTestUtils::DEVICE_TEMPLATE },\n    { {1, 3, 32, 32}, {16}, {5, 5}, {1, 1}, {2, 2}, {2, 2}, {1, 1}, 1, ov::element::f32, CommonTestUtils::DEVICE_TEMPLATE },\n    { {1, 3, 32, 32}, {16}, {3, 5}, {1, 1}, {1, 2}, {1, 2}, {1, 1}, 1, ov::element::f32, CommonTestUtils::DEVICE_TEMPLATE },\n    { {1, 3, 32, 32}, {16}, {5, 3}, {1, 1}, {2, 1}, {2, 1}, {1, 1}, 1, ov::element::f32, CommonTestUtils::DEVICE_TEMPLATE }\n};\n\n```\n\n----------------------------------------\n\nTITLE: Reproducing Findings (Bash)\nDESCRIPTION: This command runs the fuzzing test executable with a specific reproducer file to reproduce and debug a previously identified issue. The reproducer file is passed as a command-line argument.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./read_network-fuzzer crash-409b5eeed46a8445b7f7b7a2ce5b60a9ad895e3b\n```\n\n----------------------------------------\n\nTITLE: FakeQuantize XML Example\nDESCRIPTION: This XML code snippet illustrates the structure of a FakeQuantize layer definition within an OpenVINO model. It includes the 'levels' attribute and describes the input and output ports with their corresponding dimensions. It defines the data flow and quantization parameters of the layer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/quantization/fake-quantize-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer … type=\"FakeQuantize\"…>\n    <data levels=\"2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>56</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"4\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"5\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>56</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Library Version and Clang Format Target\nDESCRIPTION: This snippet adds version information to the library and creates a clang-format target for the library. It uses the ov_add_library_version and ov_add_clang_format_target custom CMake functions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/mock_py_frontend/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_library_version(${TARGET_NAME})\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Add Test Target\nDESCRIPTION: Adds the test target using the `ov_add_test_target` macro. This macro defines the test executable, includes directories, linked libraries, and other properties necessary to build and run the tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/functional/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n  NAME\n  ${TARGET_NAME}\n  ROOT\n  ${CMAKE_CURRENT_SOURCE_DIR}\n  EXCLUDED_SOURCE_PATHS\n  ${EXCLUDED_FUNC_TESTS_DIR}\n  INCLUDES\n  ${CMAKE_CURRENT_SOURCE_DIR}\n  ${OPTIONAL_FUNC_TESTS_INCLUDES}\n  \"${CMAKE_CURRENT_SOURCE_DIR}/shared_tests_instances\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/behavior\"\n  \"${CMAKE_CURRENT_SOURCE_DIR}/internal\"\n  LINK_LIBRARIES\n  ${OPTIONAL_FUNC_TESTS_LIBS}\n  openvino::func_test_utils\n  openvino::funcSharedTests\n  openvino::format_reader\n  openvino::reference\n  openvino::runtime\n  openvino::npu_common\n  openvino::npu_al)\n```\n\n----------------------------------------\n\nTITLE: Download and unpack Android SDK Platform Tools\nDESCRIPTION: Downloads and unpacks the Android SDK Platform Tools to the specified work directory.  The ANDROID_TOOLS_PATH environment variable is then set to point to the extracted platform-tools directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_android.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nwget https://dl.google.com/android/repository/platform-tools-latest-linux.zip --directory-prefix $OPV_HOME_DIR\n\nunzip $OPV_HOME_DIR/platform-tools-latest-linux.zip -d $OPV_HOME_DIR\nexport ANDROID_TOOLS_PATH=$OPV_HOME_DIR/platform-tools\n```\n\n----------------------------------------\n\nTITLE: SegmentMax with num_segments < max(segment_ids) + 1 in OpenVINO XML\nDESCRIPTION: This example demonstrates the SegmentMax operation where the provided `num_segments` value is less than `max(segment_ids) + 1`.  The operation calculates the maximum values for each segment defined by `segment_ids`, and the output dimension is determined by `num_segments`. The `fill_mode` is set to `ZERO`, meaning empty segments will be filled with zeros.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/segment-max-16.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"SegmentMax\" ... >\n    <data empty_segment_value=\"ZERO\">\n        <input>\n            <port id=\"0\" precision=\"F32\">   <!-- data -->\n                <dim>5</dim>\n            </port>\n            <port id=\"1\" precision=\"I32\">   <!-- segment_ids with 4 segments: [0, 0, 2, 3, 3] -->\n                <dim>5</dim> \n            </port>\n            <port id=\"2\" precision=\"I64\">   <!-- number of segments: 2 -->\n                <dim>0</dim> \n            </port>\n        </input>\n        <output>\n            <port id=\"3\" precision=\"F32\">\n                <dim>2</dim>\n            </port>\n        </output>\n    </layer>\n```\n\n----------------------------------------\n\nTITLE: Accessing Outputs property of CompiledModel (TypeScript)\nDESCRIPTION: This code shows the declaration of the `outputs` property within the `CompiledModel` interface. This property is an array of `Output` objects representing the output tensors of the compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\noutputs: Output []\n```\n\n----------------------------------------\n\nTITLE: Cloning ONNX Runtime Repository\nDESCRIPTION: This command clones the ONNX Runtime repository with a specific branch (rel-1.8.1) using git. The '--single-branch' option clones only the specified branch, and '--recursive' initializes and updates submodules.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_12\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone --branch rel-1.8.1 --single-branch --recursive https://github.com/microsoft/onnxruntime.git\n```\n\n----------------------------------------\n\nTITLE: Defining a New CC Domain in itt.hpp\nDESCRIPTION: Example of defining a new Conditional Compilation (CC) domain within the itt.hpp file. This snippet showcases how to add a macro for defining a new CC domain, allowing for specific profiling and filtering based on the defined conditions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/docs/develop_cc_for_new_component.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n#if defined(OV_CC_DOMAINS)\nOV_CC_DOMAINS(\n    OV_ITT_DOMAIN(CPU, )\n    OV_ITT_DOMAIN(GPU, )\n    OV_ITT_DOMAIN(NPU, )\n    OV_ITT_DOMAIN(HETERO, )\n    OV_ITT_DOMAIN(TEMPLATE, )\n    OV_ITT_DOMAIN(AUTO, )\n    OV_ITT_DOMAIN(AUTO_BATCH, )\n    OV_ITT_DOMAIN(MULTI, )\n)\n#endif\n```\n\n----------------------------------------\n\nTITLE: Deleting a Virtual Environment (Linux/macOS)\nDESCRIPTION: This command removes the virtual environment directory `openvino_env` and all its contents on Linux or macOS. It uses the `rm` command with the `-rf` options to recursively and forcefully delete the directory and its subdirectories and files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/run-notebooks.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf openvino_env\n```\n\n----------------------------------------\n\nTITLE: Signing Off Commits Using Git Commit Command\nDESCRIPTION: This code snippet shows how to sign off a commit using the 'git commit' command with the '-s' flag, which automatically adds the 'Signed-off-by' line to the commit message. This is a simple way to sign off individual commits.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/commit_signoff_policy.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ngit commit -s -m \"My commit message\"\n```\n\n----------------------------------------\n\nTITLE: Create Source Groups CMake\nDESCRIPTION: Organizes the source and header files into named groups within the Visual Studio project. This improves project organization and readability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/onnx_common/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${PUBLIC_HEADERS} ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Defining Output Directories in CMake\nDESCRIPTION: This CMake snippet defines the output directories for the Python bridge based on whether a multi-configuration generator is used. If `OV_GENERATOR_MULTI_CONFIG` is true, the output directory includes the configuration (e.g., Debug, Release); otherwise, it's a common directory. The library, archive, and PDB output directories are then set to this Python bridge directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(OV_GENERATOR_MULTI_CONFIG)\n    set(PYTHON_BRIDGE_OUTPUT_DIRECTORY ${CMAKE_LIBRARY_OUTPUT_DIRECTORY}/$<CONFIG>/python/openvino)\nelse()\n    set(PYTHON_BRIDGE_OUTPUT_DIRECTORY ${CMAKE_LIBRARY_OUTPUT_DIRECTORY}/python/openvino)\nendif()\n\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${PYTHON_BRIDGE_OUTPUT_DIRECTORY})\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${PYTHON_BRIDGE_OUTPUT_DIRECTORY})\nset(CMAKE_PDB_OUTPUT_DIRECTORY ${PYTHON_BRIDGE_OUTPUT_DIRECTORY})\n```\n\n----------------------------------------\n\nTITLE: Linking to oneDNN Library (CMake)\nDESCRIPTION: This conditional block links the `onednn_gpu_tgt` library to the target if the `ENABLE_ONEDNN_FOR_GPU` option is enabled. This allows using the oneDNN library for GPU acceleration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_ONEDNN_FOR_GPU)\n    target_link_libraries(${TARGET_NAME} PRIVATE onednn_gpu_tgt)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Interprocedural Optimization\nDESCRIPTION: This snippet sets the 'INTERPROCEDURAL_OPTIMIZATION_RELEASE' property for the target, enabling link-time optimization (LTO) based on the value of 'ENABLE_LTO'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: ReduceMax Layer Configuration in XML (keep_dims=true)\nDESCRIPTION: This XML configuration defines a ReduceMax layer with `keep_dims` set to `true`. The input tensor has dimensions 6x12x10x24, and the reduction is performed along axes 2 and 3 (values [2, 3]). The output tensor retains the reduced dimensions with size 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-max-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceMax\" ...>\n    <data keep_dims=\"true\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Get Rank of shape C++\nDESCRIPTION: This code snippet gets the rank of a `ov::PartialShape` using the `rank()` method and checks if the rank is static.\nIf the rank is static, it gets the rank value using `get_length()`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/docs/shape_propagation.md#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nov::PartialShape shape = ...;\nov::Rank = rank = shape.rank();\nif (rank.is_static())\n    auto rank_value = rank.get_length();\n    ...\n```\n\n----------------------------------------\n\nTITLE: Build Time Tests\nDESCRIPTION: This snippet builds the time tests using cmake.  It creates a build directory, navigates into it, configures the project using `cmake ..`, and then builds the `time_tests` target with 8 parallel jobs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build && cd build\ncmake .. && cmake --build . --target time_tests -j8\n```\n\n----------------------------------------\n\nTITLE: Build C++ OpenVINO Samples\nDESCRIPTION: This script builds the OpenVINO C++ sample applications. It is located in the /usr/share/openvino/samples/cpp/ directory. It compiles the sample applications for testing and demonstration purposes. Requires execution from a shell.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n/usr/share/openvino/samples/cpp/build_samples.sh\n```\n\n----------------------------------------\n\nTITLE: Unsqueeze 0D to 1D Tensor XML Configuration\nDESCRIPTION: This XML configuration demonstrates the use of the Unsqueeze operation to transform a 0D tensor (constant) into a 1D tensor. The input tensor has no dimensions, and the index [0] is used to insert a new dimension of size 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/unsqueeze-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Unsqueeze\">\n    <input>\n        <port id=\"0\">\n        </port>\n    </input>\n    <input>\n        <port id=\"1\">\n            <dim>1</dim>  <!-- value is [0] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Printing System Info in OpenVINO GitHub Action\nDESCRIPTION: This snippet utilizes a custom GitHub Action to print system information to the standard output. It retrieves details such as the operating system, machine architecture, and CPU, RAM, and memory information.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/custom_actions.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n  - name: System info\n    uses: ./openvino/.github/actions/system_info\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Environment Variables (macOS)\nDESCRIPTION: This script sets up the OpenVINO environment variables on macOS. It sources the `setupvars.sh` script from the installation directory, which configures the necessary paths and settings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\ncd <INSTALL_DIR>/\nsource setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Options CMake\nDESCRIPTION: This snippet sets compiler options for the 'openvino_cnpy' target. It suppresses warnings about unused variables and enables more comprehensive warnings (but suppresses them) when using GCC or Clang.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/cnpy/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES \"^(Apple)?Clang$\")\n    target_compile_options(${TARGET_NAME} PUBLIC -Wno-unused-variable\n                                          PRIVATE -Wno-all)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Define OutputInfo Interface TypeScript\nDESCRIPTION: Defines the `OutputInfo` interface, which provides access to output tensor information. It contains the `tensor()` method to retrieve an `OutputTensorInfo` object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/OutputInfo.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface OutputInfo {\n    tensor(): OutputTensorInfo;\n}\n```\n\n----------------------------------------\n\nTITLE: Shape Info Usage Example in Kernel\nDESCRIPTION: This code shows an example of how to use the shape_info[] array in a kernel to access dynamic shape information. The IS_DYNAMIC macro determines whether to use the shape_info or predefined shapes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/dynamic_shape/dynamic_impl.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nKERNEL(shape_of_ref)(\n    OPTIONAL_SHAPE_INFO_ARG\n    __global OUTPUT_TYPE* output\n    )\n{\n    const unsigned int i = (uint)get_global_id(2);\n\n#if IS_DYNAMIC\n    output[i] = TO_OUTPUT_TYPE(shape_info[i]);  // shape_info[] is directly used\n#else\n    size_t shapes[] = INPUT_DIMS_INIT;\n    output[i] = TO_OUTPUT_TYPE(shapes[i]);\n#endif\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: The `add_subdirectory` command in CMake adds a subdirectory to the build.  This includes the CMakeLists.txt file in the specified subdirectory and adds the targets defined within it to the current build process. This snippet adds two subdirectories: `sync_benchmark` and `throughput_benchmark`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/benchmark/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(sync_benchmark)\nadd_subdirectory(throughput_benchmark)\n```\n\n----------------------------------------\n\nTITLE: Install Static Library CMake\nDESCRIPTION: Installs the static library to the installation directory. The `OV_CPACK_COMP_CORE` variable likely specifies the component or package to which the library belongs during installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/onnx_common/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: BatchToSpace XML Example (5D input)\nDESCRIPTION: This example demonstrates the BatchToSpace operation with a 5D input tensor. It illustrates the input and output port configurations, including the data dimensions for batch and spatial dimensions, block shape, crops begin, and crops end values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/batch-to-space-2.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"BatchToSpace\" ...>\n    <input>\n        <port id=\"0\">       <!-- data -->\n            <dim>48</dim>   <!-- batch -->\n            <dim>3</dim>    <!-- spatial dimension 1 -->\n            <dim>3</dim>    <!-- spatial dimension 2 -->\n            <dim>1</dim>    <!-- spatial dimension 3 -->\n            <dim>3</dim>    <!-- spatial dimension 4 -->\n        </port>\n        <port id=\"1\">       <!-- block_shape value: [1, 2, 4, 3, 1] -->\n            <dim>5</dim>\n        </port>\n        <port id=\"2\">       <!-- crops_begin value: [0, 0, 1, 0, 0] -->\n            <dim>5</dim>\n        </port>\n        <port id=\"3\">       <!-- crops_end value: [0, 0, 1, 0, 0] -->\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>2</dim>    <!-- data.shape[0] / (block_shape.shape[0] * block_shape.shape[1] * ... * block_shape.shape[4]) -->\n            <dim>6</dim>    <!-- data.shape[1] * block_shape.shape[1] - crops_begin[1] - crops_end[1]-->\n            <dim>10</dim>   <!-- data.shape[2] * block_shape.shape[2] - crops_begin[2] - crops_end[2] -->\n            <dim>3</dim>    <!-- data.shape[3] * block_shape.shape[3] - crops_begin[3] - crops_end[3] -->\n            <dim>3</dim>    <!-- data.shape[4] * block_shape.shape[4] - crops_begin[4] - crops_end[4] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining OpenVINO Intel GPU Common Object Target in CMake\nDESCRIPTION: This CMake snippet defines a target named 'openvino_intel_gpu_common_obj' using the custom function 'ov_gpu_add_backend_target'. The function takes the 'NAME' argument and sets its value to the target name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_intel_gpu_common_obj\")\n\nov_gpu_add_backend_target(\n    NAME ${TARGET_NAME}\n)\n```\n\n----------------------------------------\n\nTITLE: ReduceLogicalOr Example 2 (keep_dims=false)\nDESCRIPTION: This XML example demonstrates the ReduceLogicalOr operation with `keep_dims` set to `false`.  The input is a 6x12x10x24 tensor, reduced along axes 2 and 3. The output tensor removes the reduced dimensions, resulting in a 6x12 tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-logical-or-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceLogicalOr\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Build Plugin\nDESCRIPTION: This command builds a specific plugin (replace 'lib_plugin_name' with the actual plugin name) for validation using the conformance tests. The `--jobs` flag uses all available processors for faster building.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/README.md#_snippet_2\n\nLANGUAGE: makefile\nCODE:\n```\nmake --jobs=$(nproc --all) lib_plugin_name\n```\n\n----------------------------------------\n\nTITLE: Copying and Preprocessing XeTLA Headers\nDESCRIPTION: This CMake snippet creates a custom command to copy the include directory containing XeTLA headers to the code generation cache directory and then preprocesses a specific header file (`cm_xetla.h`). The preprocessing step uses the C++ compiler to expand macros and remove comments, creating a simplified header file. This ensures that the generated code uses the correct header definitions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/cm/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(OUTPUT \"${XETLA_HEADER_FULL_PATH}\"\n    COMMAND \"${CMAKE_COMMAND}\" -E copy_directory \"${CMAKE_CURRENT_SOURCE_DIR}/include\" \"${CODEGEN_CACHE_DIR}/cm_kernels/include/\"\n    COMMAND \"${CMAKE_CXX_COMPILER}\" \"${XETLA_HEADER_FULL_PATH}\" ${PREPROCESSOR_OPTIONS} -I ${XETLA_INCLUDE_DIR} > \"${CODEGEN_CACHE_DIR}/${XETLA_HEADER}\"\n    COMMAND \"${CMAKE_COMMAND}\" -E rename \"${CODEGEN_CACHE_DIR}/${XETLA_HEADER}\" \"${XETLA_HEADER_FULL_PATH}\"\n    DEPENDS \"${COPIED_KERNELS}\" \"${HEADERS}\"\n    COMMENT \"Copying CM headers and preprocessing XeTLA headers ...\"\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags for MSVC\nDESCRIPTION: Adds specific compiler flags to suppress warnings when using the MSVC compiler. These flags disable specific warnings related to type conversions, data loss, and other potential issues. Requires MSVC compiler.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    ov_add_compiler_flags(/wd4244)\n    ov_add_compiler_flags(/wd4267)\n    ov_add_compiler_flags(/wd4305)\n    ov_add_compiler_flags(/wd4018)\n    ov_add_compiler_flags(/wd4050)\n    ov_add_compiler_flags(/wd4250)\n    ov_add_compiler_flags(/wd4334)\n    ov_add_compiler_flags(/wd4661)\n    ov_add_compiler_flags(/wd4273)\n    ov_add_compiler_flags(/wd4309)\n    ov_add_compiler_flags(/wd4804)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Check for Installed OpenVINO Packages\nDESCRIPTION: This command lists all currently installed packages that contain \"openvino\" in their name.  It allows verification of the installed OpenVINO runtime and samples.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-zypper.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nzypper se -i openvino\n```\n\n----------------------------------------\n\nTITLE: Clamp Layer Definition in XML - OpenVINO\nDESCRIPTION: This XML code snippet defines a Clamp layer in OpenVINO. It specifies the layer's ID, name, type, minimum and maximum values for clipping, input port with a dimension of 256, and output port also with a dimension of 256.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/clamp-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" name=\"clamp_node\" type=\"Clamp\">\n    <data min=\"10\" max=\"50\" />\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Linux: Setting OpenCV Library Path\nDESCRIPTION: This command sets the LD_LIBRARY_PATH environment variable on Linux to include the directory containing the OpenCV libraries. This ensures that Protopipe can find the necessary OpenCV shared libraries at runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\nexport LD_LIBRARY_PATH=<path-to-opencv>/build/lib/:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Setting Public Headers Directory CMake\nDESCRIPTION: This snippet defines the directory containing public header files for the library. The PUBLIC_HEADERS_DIR variable stores the absolute path to the 'include' directory within the current source directory, making headers available for other projects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/ov_helpers/ov_lpt_models/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(PUBLIC_HEADERS_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: Link Libraries\nDESCRIPTION: This links the necessary libraries to the `ov_onnx_frontend_tests` target. It uses private linking, meaning these libraries are not exposed to other targets that link against `ov_onnx_frontend_tests`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(ov_onnx_frontend_tests PRIVATE\n    gtest_main_manifest\n    frontend_shared_test_classes\n    openvino::frontend::onnx\n    func_test_utils)\n```\n\n----------------------------------------\n\nTITLE: Defining Dynamic Layout with Ellipsis in C++\nDESCRIPTION: This snippet demonstrates defining a dynamic layout in C++ using `ov::Layout`, using an ellipsis (`...`) to represent multiple unimportant dimensions. This is suitable for tensors with a variable number of dimensions that are not relevant.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nov::Layout layout(\"[N, ..., W]\");\n```\n\n----------------------------------------\n\nTITLE: Link Libraries\nDESCRIPTION: Links the target with the specified libraries. It links against `openvino_intel_gpu_graph` and `openvino::pugixml`. These libraries provide functionalities required by the Intel GPU plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino_intel_gpu_graph openvino::pugixml)\n```\n\n----------------------------------------\n\nTITLE: Nearest Pixel Calculation in Interpolate-4 (Python)\nDESCRIPTION: This Python code defines a class `GetNearestPixel` that encapsulates different modes for calculating the nearest pixel during interpolation. It supports modes like `round_prefer_floor`, `round_prefer_ceil`, `floor`, `ceil`, and `simple`, each with its specific rounding behavior. The class takes the mode as input during initialization and provides a callable object that returns the nearest pixel index based on the given original coordinate and a flag indicating whether it is a downsample operation. Dependencies: math, enum.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport math\nimport numpy as np\nfrom enum import Enum, unique\n\nclass GetNearestPixel:\n    def __init__(self, mode: str):\n        self.func = {\n            'round_prefer_floor': GetNearestPixel.prefer_floor_func,\n            'round_prefer_ceil': GetNearestPixel.prefer_ceil_func,\n            'floor': GetNearestPixel.floor_func,\n            'ceil': GetNearestPixel.ceil_func,\n            'simple': GetNearestPixel.simple_func\n        }[mode]\n\n    def __call__(self, x_original, is_downsample):\n        return self.func(x_original, is_downsample)\n\n    @staticmethod\n    def prefer_floor_func(x_original, is_downsample):\n        if x_original == int(x_original) + 0.5:\n            return int(math.floor(x_original))\n        else:\n            return int(round(x_original))\n\n    @staticmethod\n    def prefer_ceil_func(x_original, is_downsample):\n        return int(round(x_original))\n\n    @staticmethod\n    def floor_func(x_original, is_downsample):\n        return int(math.floor(x_original))\n\n    @staticmethod\n    def ceil_func(x_original, is_downsample):\n        return int(math.ceil(x_original))\n\n    @staticmethod\n    def simple_func(x_original, is_downsample):\n        if is_downsample:\n            return int(math.ceil(x_original))\n        else:\n            return int(x_original)\n```\n\n----------------------------------------\n\nTITLE: GRUSequence Layer Configuration XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a GRUSequence layer within an OpenVINO model. It shows how to define the layer's attributes, input ports with their dimensions, and output ports with their corresponding dimensions. The configuration includes settings like hidden_size and the dimensions of the input and output tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/gru-sequence-5.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"GRUSequence\" ...>\n    <data hidden_size=\"128\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>16</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n        </port>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>384</dim>\n            <dim>16</dim>\n        </port>\n        <port id=\"4\">\n            <dim>1</dim>\n            <dim>384</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"5\">\n            <dim>1</dim>\n            <dim>384</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"6\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"7\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Core Property C\nDESCRIPTION: This code snippet sets properties for a specified device on the OpenVINO core, allowing users to customize device behavior. Key value pairs need to be passed to this method.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_21\n\nLANGUAGE: C\nCODE:\n```\nov_core_t* core = nullptr;\nov_core_create(&core);\nconst char* key_1 = ov_property_key_inference_num_threads;\nconst char* value_1 = \"12\";\nconst char* key_2 = ov_property_key_num_streams;\nconst char* value_2 = \"7\";\nov_core_set_property(core, \"CPU\", key_1, value_1, key_2, value_2);\n...\nov_core_free(core);\n```\n\n----------------------------------------\n\nTITLE: Roll Operation Example\nDESCRIPTION: This example demonstrates the output of the Roll operation with a shift value of 1 along axis 0. The code shows how the elements of the input tensor are shifted to produce the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/roll-7.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ndata    = [[ 1,  2,  3],\n            [ 4,  5,  6],\n            [ 7,  8,  9],\n            [10, 11, 12]]\n    output  = [[10, 11, 12],\n            [ 1,  2,  3],\n            [ 4,  5,  6],\n            [ 7,  8,  9]]\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions for Static Library\nDESCRIPTION: If shared libraries are not being built (`BUILD_SHARED_LIBS` is false), this snippet adds the `OPENVINO_STATIC_LIBRARY` compile definition to the target library. This is important for correctly building and linking static libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME} PUBLIC OPENVINO_STATIC_LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring CPack for OpenVINOBenchmarkTool Packaging\nDESCRIPTION: This snippet checks if the current source directory is equal to the OpenVINOBenchmarkTool source directory, and if they are equal, it calls the `ov_cpack` function to configure CPack for packaging the entire application. This ensures that CPack is only configured when building the main project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/benchmark_tool/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_SOURCE_DIR STREQUAL OpenVINOBenchmarkTool_SOURCE_DIR)\n    ov_cpack(${OV_CPACK_COMPONENTS_ALL})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Static Library (CMake)\nDESCRIPTION: This snippet creates a static library named `openvino_shape_inference` from the specified source and header files. It also creates an alias for easier referencing and sets the export name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/shape_inference/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${LIBRARY_SRC} ${PUBLIC_HEADERS})\n\nadd_library(openvino::shape_inference ALIAS ${TARGET_NAME})\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME shape_inference)\n```\n\n----------------------------------------\n\nTITLE: Detect Python Tag with wheel.vendored.packaging.tags - Python\nDESCRIPTION: This snippet executes a Python command to detect the Python tag using the `wheel.vendored.packaging.tags` module. It captures the interpreter name and version and stores it in the `PYTHON_TAG` variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport wheel.vendored.packaging.tags as tags ; print(f'{tags.interpreter_name()}{tags.interpreter_version()}')\n```\n\n----------------------------------------\n\nTITLE: SpaceToDepth with blocks_first mode in Python\nDESCRIPTION: This Python code snippet demonstrates the transformation performed by the SpaceToDepth operation when mode is set to 'blocks_first'. It reshapes the input tensor, transposes it, and reshapes it again to obtain the output tensor. The input tensor has K spatial dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/space-to-depth-1.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nx' = reshape(data, [N, C, D1 / block_size, block_size, D2 / block_size, block_size, ... , DK / block_size, block_size])\n\nx'' = transpose(x',  [0,  3, 5, ..., K + (K + 1), 1,  2, 4, ..., K + K])\n\ny = reshape(x'', [N, C * (block_size ^ K), D1 / block_size, D2 / block_size, ... , DK / block_size])\n```\n\n----------------------------------------\n\nTITLE: Setting the Target Name in CMake\nDESCRIPTION: This snippet defines a variable named TARGET_NAME and sets its value to \"tensorflow_fe_standalone_build_test\". This variable is later used to refer to the target being built, which is a static library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/tests/standalone_build/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"tensorflow_fe_standalone_build_test\")\n```\n\n----------------------------------------\n\nTITLE: Gather-7 Example with default batch_dims (0) in shell\nDESCRIPTION: Demonstrates the Gather-7 operation with the default batch_dims value (0) and axis 0. The indices tensor is used to select elements from the data tensor, resulting in the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-7.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 0\naxis = 0\n\nindices = [0, 0, 4]\ndata    = [1, 2, 3, 4, 5]\noutput  = [1, 1, 5]\n```\n\n----------------------------------------\n\nTITLE: Atan Layer Definition XML\nDESCRIPTION: Defines an Atan layer in OpenVINO using XML. It specifies the input and output ports with their dimensions. The input and output tensors have the same shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/atan-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Atan\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Downloading OpenVINO GPG Key\nDESCRIPTION: This snippet shows how to download the GPG key required for adding the OpenVINO APT repository to the snap's snapcraft.yaml.  The key is used to verify the integrity of the packages from the repository.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/integrate-openvino-with-ubuntu-snap.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nwget https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries in CMake\nDESCRIPTION: This CMake snippet links the target against the `openvino::npu_al` and `openvino::npu_common` libraries.  These are linked privately, meaning they are not exposed to consumers of this library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/compiler_adapter/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME}\n    PRIVATE\n        openvino::npu_al\n        openvino::npu_common\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Options (CMake)\nDESCRIPTION: This sets private compile options for the target. The options are specific to the Release configuration and may differ depending on the compiler (MSVC or other).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_options(${TARGET_NAME} PRIVATE\n  $<$<CONFIG:Release>:$<IF:$<CXX_COMPILER_ID:MSVC>,/Os,-Os>>)\n```\n\n----------------------------------------\n\nTITLE: Using Optional with a predicate\nDESCRIPTION: This C++ snippet shows how to use Optional with a predicate. The predicate validates that the number of consumers for the wrapped node is 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\nshared_ptr<ov::Node> optional_predicate() {\n    // Creating nodes\n    auto relu_node = make_shared<WrapType<opset13::Relu>>();\n    auto optional_node = make_shared<Optional>(relu_node, [](const Output<Node>& output) {\n        return output.get_target_inputs().size() == 1;\n    });\n\n    return optional_node;\n}\n```\n\n----------------------------------------\n\nTITLE: Running Image Classification Sample (C++, macOS)\nDESCRIPTION: This command runs the `classification_sample_async` executable with a specified model (`googlenet-v1.xml`), input image (`dog.bmp`), and target device (`CPU`) on a macOS system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_34\n\nLANGUAGE: sh\nCODE:\n```\n./classification_sample_async -i ~/Downloads/dog.bmp -m ~/ir/googlenet-v1.xml -d CPU\n```\n\n----------------------------------------\n\nTITLE: Dynamic Dimension C++\nDESCRIPTION: These code snippets show how to create dynamic Dimension objects in OpenVINO using C++.\nThey initialize `ov::Dimension` objects representing dynamic dimensions with different initialization methods.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/docs/shape_propagation.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nov::Dimension::dynamic();\nov::Dimension(-1);  // special value for Dimension\nov::Dimension{0, MAX_INT};\n```\n\n----------------------------------------\n\nTITLE: Export Pruned Model - TensorFlow 2\nDESCRIPTION: This code snippet demonstrates how to export the pruned model to a frozen graph format in TensorFlow 2 for further inference. This makes the model compatible with OpenVINO and other inference engines.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntf.saved_model.save(model, \"saved_model\")\n```\n\n----------------------------------------\n\nTITLE: Building Yocto Image with BitBake\nDESCRIPTION: Builds a minimal Yocto image using the `bitbake` command. The `core-image-minimal` target creates a minimal root filesystem suitable for embedded devices. This command initiates the compilation and packaging process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yocto.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nbitbake core-image-minimal\n```\n\n----------------------------------------\n\nTITLE: Adding SYCL to Target\nDESCRIPTION: If the compiler is Intel LLVM, this snippet adds SYCL compilation to the target library and defines the `OV_GPU_WITH_SYCL` compile definition. This enables SYCL support in the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif(OV_COMPILER_IS_INTEL_LLVM)\n    add_sycl_to_target(TARGET ${TARGET_NAME} SOURCES ${SYCL_SOURCES})\n    target_compile_definitions(${TARGET_NAME} PUBLIC OV_GPU_WITH_SYCL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Comma Dangle\nDESCRIPTION: This rule enforces the use of trailing commas in multiline object literals in JavaScript and TypeScript code. It is enforced by ESLint with the configuration `comma-dangle: ['error', 'always-multiline']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_14\n\nLANGUAGE: JavaScript\nCODE:\n```\ncomma-dangle: ['error', 'always-multiline']\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name and Compiler Flags with CMake\nDESCRIPTION: This CMake snippet sets the target name for the functional tests and conditionally adds compiler flags for MSVC. It ensures that the tests are properly named and compiled with the necessary flags.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/tests/functional/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_template_func_tests)\n\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    ov_add_compiler_flags(/wd4305)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Listing Project Dependencies in constraints.txt\nDESCRIPTION: This snippet lists the dependencies of the project. It shows the libraries that need to be installed for the project to run correctly. It uses a constraints file and libraries like numpy, pytest, PyYAML, etc.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/test_runner/requirements.txt#_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n-c ../../constraints.txt\nnumpy\npytest\npy\nattrs\nPyYAML\njsonschema\ndistro\npymongo\npytest-html\npytest-timeout\n```\n\n----------------------------------------\n\nTITLE: Add Test Target\nDESCRIPTION: Adds a test target using the defined target name and specifies dependencies, link libraries, clang format, and labels.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n    NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        DEPENDENCIES\n            tensorflow_lite_test_models\n            tensorflow_lite_fe_standalone_build_test\n        LINK_LIBRARIES\n            gtest_main_manifest \n            frontend_shared_test_classes \n            openvino_tensorflow_lite_frontend\n        ADD_CLANG_FORMAT\n        LABELS\n            ${ctest_labels} TFL_FE\n)\n```\n\n----------------------------------------\n\nTITLE: Generating a runtime reference TCB\nDESCRIPTION: Generates a runtime reference TCB (Trusted Computing Base) signature using `ovsaruntime`. This is used for creating customer licenses and performing license checks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_42\n\nLANGUAGE: sh\nCODE:\n```\n/opt/ovsa/bin/ovsaruntime gen-tcb-signature -n \"Face Detect @ Runtime VM\" -v \"1.0\" -f model_inference_runtime_vm.tcb -k isv_keystore\n```\n\n----------------------------------------\n\nTITLE: Defining zlib Source Files\nDESCRIPTION: This snippet defines a list of source files that are part of the zlib library. These files will be compiled to create the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/zlib/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(zlib_srcs\n    zlib/adler32.c\n    zlib/compress.c\n    zlib/crc32.c\n    zlib/deflate.c\n    zlib/gzclose.c\n    zlib/gzlib.c\n    zlib/gzread.c\n    zlib/gzwrite.c\n    zlib/inflate.c\n    zlib/infback.c\n    zlib/inftrees.c\n    zlib/inffast.c\n    zlib/trees.c\n    zlib/uncompr.c\n    zlib/zutil.c)\n```\n\n----------------------------------------\n\nTITLE: Setting Installation Directories and Components (CMake)\nDESCRIPTION: This snippet sets the installation directory and component based on whether a shared library is being built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_SHARED_LIBS)\n  set(CACHE_JSON_INSTALL_DIR ${OV_CPACK_PLUGINSDIR})\n  set(CACHE_JSON_COMPONENT gpu)\nelse()\n  set(CACHE_JSON_INSTALL_DIR ${OV_CPACK_ARCHIVEDIR})\n  set(CACHE_JSON_COMPONENT ${OV_CPACK_COMP_CORE})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Linking OpenVINO Runtime Library in CMake\nDESCRIPTION: This command links the target library `${TARGET_NAME}` with the OpenVINO runtime library. The `PRIVATE` keyword specifies that the OpenVINO runtime is a private dependency of the library.  The extension needs this to access OpenVINO's API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/template_extension/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino::runtime)\n```\n\n----------------------------------------\n\nTITLE: Include Headers for Preprocessing & Saving - C++\nDESCRIPTION: Includes essential headers for OpenVINO preprocessing functionalities in C++. These headers allow to read models, specify preprocessing steps, and save the adjusted model to an OpenVINO IR file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details/integrate-save-preprocessing-use-case.rst#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include <openvino/openvino.hpp>\n#include <openvino/op/parameter.hpp>\n#include <openvino/core/preprocess/pre_post_process.hpp>\n```\n\n----------------------------------------\n\nTITLE: Setting Dependencies\nDESCRIPTION: This snippet sets the initial dependencies for the target. Further dependencies are added based on enabled features like ENABLE_HETERO, ENABLE_AUTO, ENABLE_MULTI, ENABLE_AUTO_BATCH, ENABLE_TEMPLATE, ENABLE_OV_IR_FRONTEND, and ENABLE_OV_ONNX_FRONTEND.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/shared/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(DEPENDENCIES mock_engine)\n```\n\n----------------------------------------\n\nTITLE: Get Friendly Name of Model in OpenVINO (C)\nDESCRIPTION: This function retrieves the friendly name of an OpenVINO model. It requires a pointer to the `ov_model_t` and returns the friendly name via the `friendly_name` parameter. A status code is returned to indicate success or failure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_34\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_model_get_friendly_name(const ov_model_t* model, char** friendly_name)\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Priority in Python\nDESCRIPTION: This snippet demonstrates how to configure model priority using the Auto-Device plugin in OpenVINO with Python. The implementation relies on code snippet located in `docs/articles_en/assets/snippets/ov_auto.py` section `[part4]`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndocs/articles_en/assets/snippets/ov_auto.py\n```\n\n----------------------------------------\n\nTITLE: Appending Compile Definitions in CMake\nDESCRIPTION: Appends a compile definition `TEST_CUSTOM_OP_CONFIG_PATH` to the `DEFINES` list. This definition specifies the path to the custom operator configuration file, which is located relative to the source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND DEFINES TEST_CUSTOM_OP_CONFIG_PATH=\"${CMAKE_CURRENT_SOURCE_DIR}/../../../plugins/intel_gpu/tests/functional/custom_op/custom_op.xml\")\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Executable for Python API Support\nDESCRIPTION: This snippet sets the Python executable path when enabling Python API support in CMake. This requires CMake 3.16 or higher. Example for Python 3.11.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_windows.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n-DPython3_EXECUTABLE=\"C:\\Program Files\\Python11\\python.exe\"\n```\n\n----------------------------------------\n\nTITLE: CMake Configure with Emscripten\nDESCRIPTION: This snippet configures the CMake build using the `emcmake` command, which is specific to Emscripten. It sets the build type to Release and specifies the OpenVINO source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_webassembly.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n$ mkdir build && cd build\n$ emcmake cmake -DCMAKE_BUILD_TYPE=Release /openvino\n```\n\n----------------------------------------\n\nTITLE: Per-Tensor Quantization Restriction in OpenVINO C++\nDESCRIPTION: This C++ code snippet shows how to enforce per-tensor quantization for specific operations in Low Precision Transformations (LPT) within OpenVINO, ensuring quantization is applied consistently across the entire tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/advanced-guides/low-precision-transformations.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nauto& transformation = *transformation;\n\n```\n\n----------------------------------------\n\nTITLE: Loading Extensions in C++\nDESCRIPTION: This snippet demonstrates how to load extensions into the OpenVINO Core object using C++. It adds both the custom operation class and a mapping extension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\n// [add_extension]\ncore.add_extension(std::make_shared<Identity>());\n// [add_extension]\n\n// [add_frontend_extension]\ncore.add_extension(std::make_shared<TemplateFrontend>());\n// [add_frontend_extension]\n```\n\n----------------------------------------\n\nTITLE: Add Subdirectory for Tests\nDESCRIPTION: This snippet conditionally adds the `tests` subdirectory to the build if testing is enabled. The `add_subdirectory` command includes the CMakeLists.txt file in the `tests` directory, which defines the tests for the proxy plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags (GCC/Clang)\nDESCRIPTION: This snippet adds the `-Wno-missing-declarations` compiler flag for GCC and Clang compilers to suppress warnings about missing declarations. This helps in a cleaner build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG)\n    ov_add_compiler_flags(-Wno-missing-declarations)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Smart CI Job Definition in Workflow (YAML)\nDESCRIPTION: This YAML snippet shows the basic structure of a Smart CI job within a GitHub workflow. It includes steps for checking out the action and getting affected components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_8\n\nLANGUAGE: YAML\nCODE:\n```\njobs:\n  Smart_CI:\n    ...\n    steps:\n      - name: Get affected components\n        id: smart_ci\n      ...\n```\n\n----------------------------------------\n\nTITLE: Install Python Dependencies (pip)\nDESCRIPTION: Installs the necessary Python modules required for running the end-to-end tests. It utilizes the `pip3` package manager and the `requirements.txt` file, which lists the required dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/e2e_tests/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Building for IA32 systems with CMake\nDESCRIPTION: This command uses CMake to configure the build for IA32 operation systems by specifying a toolchain file.  It uses the `-DCMAKE_TOOLCHAIN_FILE` flag to point to the `ia32.linux.toolchain.cmake` file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_linux.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_TOOLCHAIN_FILE=<openvino_repo>/cmake/toolchains/ia32.linux.toolchain.cmake ..\n```\n\n----------------------------------------\n\nTITLE: Enable Verbose Logging\nDESCRIPTION: This snippet illustrates how to enable more verbose logging by setting the `OV_VERBOSE_LOGGING` environment variable to `true`, in addition to `OV_MATCHER_LOGGING`. This provides more detailed information about the nodes involved in the matching process, useful for in-depth debugging.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/docs/debug_capabilities/matcher_logging.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nOV_MATCHER_LOGGING=true OV_VERBOSE_LOGGING=true ./your_amazing_program\n```\n\n----------------------------------------\n\nTITLE: Exclude Layers by Name\nDESCRIPTION: Quantizes the model, excluding specified layers from the quantization process by name. This can be used to preserve the accuracy of specific layers that are sensitive to quantization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_21\n\nLANGUAGE: sh\nCODE:\n```\nnames = ['layer_1', 'layer_2', 'layer_3']\nncf.quantize(model, dataset, ignored_scope=nncf.IgnoredScope(names=names))\n\n```\n\n----------------------------------------\n\nTITLE: Adding Source Subdirectory with CMake\nDESCRIPTION: This CMake command adds the 'src' subdirectory to the build process. It instructs CMake to process the CMakeLists.txt file located within the 'src' directory, effectively including the source code in the build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: OpenVINO: Create Calibration Dataset\nDESCRIPTION: Creates an instance of the nncf.Dataset class using an OpenVINO dataset. The transformation function prepares the data for inference, extracting the input tensor from the dataset sample.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndataset = nncf.Dataset(ov_dataset, transform_fn)\n\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for Shared Test Library in CMake\nDESCRIPTION: This CMake command adds a subdirectory named 'tests_shared_lib' located within the 'tests/lib' directory to the current build process. The subdirectory is renamed to 'tests_shared_lib' within the build. It depends on the OpenVINO_SOURCE_DIR variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(\"${OpenVINO_SOURCE_DIR}/tests/lib\" tests_shared_lib)\n```\n\n----------------------------------------\n\nTITLE: Convert TensorFlow Lite model to IR using ovc\nDESCRIPTION: This snippet shows how to convert a TensorFlow Lite model to OpenVINO Intermediate Representation (IR) format using the `ovc` command-line tool. The resulting IR can then be read by `read_model()` and inferred.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\novc <INPUT_MODEL>.tflite\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target\nDESCRIPTION: Adds a custom target for running clang-format on the source code associated with the specified target. This ensures consistent code formatting throughout the project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/backend/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Building the Subgraphs Dumper tool with CMake and Make\nDESCRIPTION: These commands configure and build the `ov_subgraphs_dumper` binary using CMake and Make. The build enables functional and unit tests.  The resulting binary is located in the build artifacts folder.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/subgraphs_dumper/README.md#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -DENABLE_FUNCTIONAL_TESTS=ON -DENABLE_TESTS=ON .\nmake --jobs=$(nproc --all) ov_subgraphs_dumper\n```\n\n----------------------------------------\n\nTITLE: Installing Requirement Files CMake\nDESCRIPTION: This snippet installs requirement files (requirements_pytorch, requirements_tensorflow, requirements_onnx, requirements_jax) to the tests directory. These files specify the dependencies needed for different frameworks within the testing environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES requirements_pytorch requirements_tensorflow requirements_onnx requirements_jax\n        DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Convert Tokenizer using Python API\nDESCRIPTION: Converts a Hugging Face tokenizer to OpenVINO Intermediate Representation (IR) using the Python API.  The AutoTokenizer loads the tokenizer, and convert_tokenizer creates the OpenVINO models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer\nfrom openvino_tokenizers import convert_tokenizer\n\nhf_tokenizer = AutoTokenizer.from_pretrained(model_id)\nov_tokenizer, ov_detokenizer = convert_tokenizer(hf_tokenizer, with_detokenizer=True)\n```\n\n----------------------------------------\n\nTITLE: Running a C++ Sample (Windows)\nDESCRIPTION: This command executes a C++ sample. It takes the path to the executable, the input media, the model, and the target device as arguments. This is for Windows.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_27\n\nLANGUAGE: bat\nCODE:\n```\n<sample.exe file> -i <path_to_media> -m <path_to_model> -d <target_device>\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files\nDESCRIPTION: Uses GLOB_RECURSE to find all C++ and header files within the current source directory and its subdirectories. The resulting list of files is stored in the SOURCES variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/backend/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES *.cpp *.hpp *.h)\n```\n\n----------------------------------------\n\nTITLE: Getting Layout from Model Input/Output in Python\nDESCRIPTION: This snippet demonstrates how to retrieve the layout from a model's input or output in Python using OpenVINO's helper functions. It simplifies the process of accessing layout information from a model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ninput_layout = ov.layout.get_layout(model.input(0))\noutput_layout = ov.layout.get_layout(model.output(0))\n```\n\n----------------------------------------\n\nTITLE: Cloning submodules via gitee mirrors\nDESCRIPTION: This script is an alternative for users in China to clone submodules via gitee mirrors. It first changes the permission of the submodule update script, then executes the script.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_linux.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nchmod +x scripts/submodule_update_with_gitee.sh\n./scripts/submodule_update_with_gitee.sh\n```\n\n----------------------------------------\n\nTITLE: Defining Output Port Dimension in OpenVINO XML\nDESCRIPTION: This XML snippet defines the output port of a layer within an OpenVINO model description. It specifies that the output port has a dimension of 3, which can correspond to the number of channels or elements in the output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/convert-promote-types-14.rst#_snippet_3\n\nLANGUAGE: XML\nCODE:\n```\n                  <dim>3</dim>\n            </port>\n        </output>\n    </layer>\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files CMake\nDESCRIPTION: Uses the `file(GLOB)` command to find all `.cpp` files in the current directory and store them in the `SRC` variable. This list of files will be used as source files for the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/lib/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile (GLOB SRC *.cpp)\n```\n\n----------------------------------------\n\nTITLE: Organizing Source Groups\nDESCRIPTION: This snippet uses the source_group command to organize the source and header files in the IDE.  It creates source groups named \"src\" and \"include\" containing the corresponding files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/mock_py_frontend/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${LIBRARY_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Run All End-to-End Tests (pytest)\nDESCRIPTION: Executes all end-to-end tests located within the `pipelines/` directory using the `pytest` testing framework. It invokes the `test_base.py` script, which is the main entry point for the tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/e2e_tests/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest test_base.py\n```\n\n----------------------------------------\n\nTITLE: Running Conditional Compilation Tests\nDESCRIPTION: This command executes the conditional compilation tests using pytest. It requires several parameters such as the path to sea_runtool.py, collector directory, artifacts directory, OpenVINO root directory, and Open Model Zoo repository.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/conditional_compilation/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest test_cc.py\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags for GCC and Clang\nDESCRIPTION: This CMake snippet adds the `-Wno-missing-declarations` compiler flag if the compiler is either GCC or Clang. This suppresses warnings about missing declarations during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG)\n    ov_add_compiler_flags(-Wno-missing-declarations)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Install Specific OpenVINO Runtime Version\nDESCRIPTION: This command installs a specific version of the OpenVINO Runtime using the YUM package manager.  Replace `<VERSION>.<UPDATE>.<PATCH>` with the desired version number. It requires sudo privileges to install software packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nsudo yum install openvino-<VERSION>.<UPDATE>.<PATCH>\n```\n\n----------------------------------------\n\nTITLE: Quantize with Mixed Preset\nDESCRIPTION: Quantizes the model using the MIXED preset. This uses symmetric quantization for weights and asymmetric quantization for activations, which is recommended for models with non-ReLU activation functions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\nnncf.quantize(model, dataset, preset=nncf.QuantizationPreset.MIXED)\n\n```\n\n----------------------------------------\n\nTITLE: Install CMake and Development Tools\nDESCRIPTION: This snippet installs CMake, SCons, fdupes, git-lfs, and Ninja using the `brew` package manager. These tools are required for building the OpenVINO project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n% brew install cmake scons fdupes git-lfs ninja\n```\n\n----------------------------------------\n\nTITLE: Finding the OpenVINO Package\nDESCRIPTION: This snippet uses CMake's `find_package` command to locate the OpenVINO package, specifying that it is required. This ensures that the necessary OpenVINO libraries and headers are available for the project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenVINO REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Enable Denormals Optimization (Python)\nDESCRIPTION: This snippet shows how to enable denormals optimization in the CPU plugin using the `ov::intel_cpu::denormals_optimization` property in Python.  This is set during the `compile_model` call.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\ncompiled_model = core.compile_model(model, \"CPU\", {ov.intel_cpu.denormals_optimization: True})\n```\n\n----------------------------------------\n\nTITLE: Adding a Custom Target for Time Tests (CMake)\nDESCRIPTION: This snippet adds a custom CMake target named `time_tests`. This target is used as a dependency for all individual time tests, allowing for a single command to build or run all tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/src/timetests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(time_tests)\n```\n\n----------------------------------------\n\nTITLE: ScaledDotProductAttention Example 1 (XML)\nDESCRIPTION: This XML snippet demonstrates a ScaledDotProductAttention layer with simple dimensions. It includes the input and output port definitions with their respective precisions and dimensions (N, L, S, E, Ev). The 'causal' attribute is set to false.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/scaled-dot-product-attention.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"285\" name=\"aten::scaled_dot_product_attention_0\" type=\"ScaledDotProductAttention\" version=\"opset13\">\n\t\t\t<data causal=\"false\" />\n\t\t\t<input>\n\t\t\t\t<!-- Example with simple dimensions, with N = 1, L = -1, S = -1, E = 80, Ev = 80-->\n\t\t\t\t<port id=\"0\" precision=\"FP32\"> <!-- query -->\n\t\t\t\t\t<dim>1</dim> <!-- N -->\n\t\t\t\t\t<dim>-1</dim> <!-- L -->\n\t\t\t\t\t<dim>80</dim> <!-- E -->\n\t\t\t\t</port>\n\t\t\t\t<port id=\"1\" precision=\"FP32\"> <!-- key -->\n\t\t\t\t\t<dim>1</dim> <!-- N -->\n\t\t\t\t\t<dim>-1</dim> <!-- S -->\n\t\t\t\t\t<dim>80</dim> <!-- E -->\n\t\t\t\t</port>\n\t\t\t\t<port id=\"2\" precision=\"FP32\"> <!-- value -->\n\t\t\t\t\t<dim>1</dim> <!-- N -->\n\t\t\t\t\t<dim>-1</dim> <!-- S -->\n\t\t\t\t\t<dim>80</dim> <!-- Ev -->\n\t\t\t\t</port>\n\t\t\t\t<port id=\"3\" precision=\"FP32\"> <!-- attention_mask -->\n\t\t\t\t\t<dim>1</dim> <!-- N -->\n\t\t\t\t\t<dim>-1</dim> <!-- L -->\n\t\t\t\t\t<dim>-1</dim> <!-- S -->\n\t\t\t\t</port>\n\t\t\t</input>\n\t\t\t<output>\n\t\t\t\t<port id=\"4\" precision=\"FP32\">\n\t\t\t\t\t<dim>1</dim> <!-- N -->\n\t\t\t\t\t<dim>-1</dim> <!-- L -->\n\t\t\t\t\t<dim>80</dim> <!-- Ev -->\n\t\t\t\t</port>\n\t\t\t</output>\n\t\t</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining DNNL usage\nDESCRIPTION: This snippet defines `-DOV_CPU_WITH_DNNL` if the `OV_CPU_WITH_DNNL` option is enabled, indicating that the CPU plugin will be built with oneDNN support.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nset(OV_CPU_WITH_DNNL ON)\nif(OV_CPU_WITH_DNNL)\n    add_definitions(-DOV_CPU_WITH_DNNL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Python with Static Library using pyenv\nDESCRIPTION: This shell command installs a specific Python version (3.10.7) using pyenv. This is for statically building OpenVINO™. The `--verbose` flag provides detailed output during the installation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/build.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npyenv install --verbose 3.10.7\n```\n\n----------------------------------------\n\nTITLE: Building AppVerifier Tests with CMake and MSBuild\nDESCRIPTION: This snippet shows the commands to build the AppVerifier tests. It requires setting up the OpenVINO environment using `setupvars.bat`, creating a build directory, running cmake to generate the build files, and then using cmake to build the project in Release configuration with 8 parallel jobs. This assumes OpenVINO is installed and `setupvars.bat` is available.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n<OpenVINO_install_dir>/setupvars.bat\nmkdir build && cd build\ncmake .. && cmake --build . --config Release -j8\n```\n\n----------------------------------------\n\nTITLE: Adding nlohmann_json Subdirectory in CMake\nDESCRIPTION: This CMake snippet adds the nlohmann_json directory as a subdirectory to the current project. The EXCLUDE_FROM_ALL option prevents the library from being built by default when the parent project is built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/json/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(nlohmann_json EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Roll Operation Example with Repeated Axis\nDESCRIPTION: This example demonstrates the Roll operation where an axis is specified multiple times with different shift values. The total shift for that axis is the sum of the individual shifts.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/roll-7.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ndata    = [[ 1,  2,  3],\n            [ 4,  5,  6],\n            [ 7,  8,  9],\n            [10, 11, 12]]\n    output  = [[ 8,  9,  7],\n            [11, 12, 10],\n            [ 2,  3,  1],\n            [ 5,  6,  4]]\n```\n\n----------------------------------------\n\nTITLE: Excluding ARM Paths\nDESCRIPTION: This snippet excludes ARM-specific paths if the target architecture is neither AARCH64 nor ARM.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_21\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT (AARCH64 OR ARM))\n    list(APPEND EXCLUDE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/transformations/cpu_opset/arm/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/transformations/tpp/aarch64/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/emitters/plugin/aarch64/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/emitters/tpp/aarch64/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/nodes/executors/aarch64/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/nodes/kernels/aarch64/*)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Define ov_color_format enum in C\nDESCRIPTION: This enum defines the supported color formats for image data in OpenVINO. It includes various YUV, RGB, and grayscale formats, supporting both single-plane and multi-plane representations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_12\n\nLANGUAGE: C\nCODE:\n```\ntypedef enum {\n\n    UNDEFINE = 0U,      //!< Undefine color format\n\n    NV12_SINGLE_PLANE,  //!< Image in NV12 format as single tensor\n\n    NV12_TWO_PLANES,    //!< Image in NV12 format represented as separate tensors for Y and UV planes.\n\n    I420_SINGLE_PLANE,  //!< Image in I420 (YUV) format as single tensor\n\n    I420_THREE_PLANES,  //!< Image in I420 format represented as separate tensors for Y, U and V planes.\n\n    RGB,                //!< Image in RGB interleaved format (3 channels)\n\n    BGR,                //!< Image in BGR interleaved format (3 channels)\n\n    GRAY,               //!< Image in GRAY format (1 channel)\n\n    RGBX,               //!< Image in RGBX interleaved format (4 channels)\n\n    BGRX                //!< Image in BGRX interleaved format (4 channels)\n\n} ov_color_format_e;\n```\n\n----------------------------------------\n\nTITLE: AvgPool Layer Configuration with explicit auto_pad and exclude-pad false\nDESCRIPTION: This XML snippet illustrates the configuration of an AvgPool layer with explicit padding and `exclude-pad` set to `false`. The `kernel` size is 5x5, `pads_begin` and `pads_end` are 1x1, and the stride is 2x2.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/avg-pool-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"AvgPool\" ... >\n    <data auto_pad=\"explicit\" exclude-pad=\"false\" kernel=\"5,5\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>15</dim>\n            <dim>15</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: LogicalNot XML Layer Definition\nDESCRIPTION: This XML code defines a LogicalNot layer in OpenVINO. It specifies the input and output ports with their dimensions, indicating that the operation will perform element-wise logical negation on a tensor of shape (256, 56). The input and output tensors are boolean.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/logical/logical-not-1.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"LogicalNot\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Excluding oneDNN Source Files\nDESCRIPTION: This snippet conditionally excludes source files from the 'onednn' directory if 'ENABLE_ONEDNN_FOR_GPU' is not enabled. It iterates through the 'SOURCES_MAIN' list and removes any files that match the exclude directory pattern.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT ENABLE_ONEDNN_FOR_GPU)\n    set(EXCLUDE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/onednn/\")\n    foreach (SOURCE_FILE IN LISTS SOURCES_MAIN)\n        if (SOURCE_FILE MATCHES \"${EXCLUDE_DIR}.*\")\n            list (REMOVE_ITEM SOURCES_MAIN ${SOURCE_FILE})\n        endif ()\n    endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Library Target in CMake\nDESCRIPTION: Creates an object library target named ${TARGET_NAME}_obj using the source files and header files defined earlier. Object libraries are intermediate targets that can be linked into other libraries or executables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${TARGET_NAME}_obj OBJECT\n            ${LIBRARY_SRC}\n            ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Apply Naming Convention Check for OpenVINO Proxy Plugin\nDESCRIPTION: This snippet applies a naming convention check to the OpenVINO proxy plugin source directories.  It checks the code in the specified directories against a predefined naming style. `ov_ncc_naming_style` is a custom CMake function (presumably defined elsewhere).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nov_ncc_naming_style(FOR_TARGET ${TARGET_NAME}\n                    SOURCE_DIRECTORIES \"${CMAKE_CURRENT_SOURCE_DIR}/dev_api\"\n                                       \"${CMAKE_CURRENT_SOURCE_DIR}/src\")\n```\n\n----------------------------------------\n\nTITLE: GatherElements Example 2 (axis=1, indices > data)\nDESCRIPTION: Shows GatherElements with axis=1 and indices tensor having a greater shape than the data tensor. The output is constructed by selecting elements from 'data' based on 'indices' along the 1st axis.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-elements-6.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndata = [\n    [1, 7],\n    [4, 3],\n]\nindices = [\n    [1, 1, 0],\n    [1, 0, 1],\n]\naxis = 1\noutput = [\n    [7, 7, 1],\n    [3, 4, 3],\n]\n```\n\n----------------------------------------\n\nTITLE: Defining the getPartialShape Method in Output Interface\nDESCRIPTION: Defines the `getPartialShape` method within the `Output` interface. This method returns the partial shape information of the output tensor as a `PartialShape` object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Output.rst#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ngetPartialShape(): PartialShape\n```\n\n----------------------------------------\n\nTITLE: Protopipe Reference Mode Execution\nDESCRIPTION: This command line execution invokes Protopipe in reference mode. It loads the configuration from 'config.yaml', specifies the mode as 'reference', and runs the inference for 10 iterations (-niter 10).  This mode is used for generating reference output data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n./protopipe -cfg config.yaml -mode reference -niter 10\n```\n\n----------------------------------------\n\nTITLE: Increase Test Verbosity with -v Flag Shell\nDESCRIPTION: Increases the verbosity of the test output using the `-v` flag of `pytest`. This provides more detailed information about each test being executed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npytest tests/test_runtime/test_core.py -v\n```\n\n----------------------------------------\n\nTITLE: NV12 to Grey Conversion (Multiple Batches) in OpenVINO (C++)\nDESCRIPTION: Demonstrates NV12 to Grayscale conversion using OpenVINO in a multiple batches scenario. The provided input data in NV12 format is converted to grayscale for each batch of images. Requires OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_40\n\nLANGUAGE: cpp\nCODE:\n```\n//! [batched_case]\nstd::vector<ov::Tensor> input_tensors;\nfor (size_t i = 0; i < batch_size; ++i) {\n    input_tensors.emplace_back(ov::element::u8, {height + height / 2, width}, input_data + i * height * width * 3 / 2);\n}\ninfer_request.set_tensor(input_port, input_tensors);\n//! [batched_case]\n```\n\n----------------------------------------\n\nTITLE: Setting OpenCL Headers Directory CMake\nDESCRIPTION: This snippet sets the path to OpenCL C headers, overriding the default path for the icd_loader project. It uses the CMAKE_CURRENT_SOURCE_DIR to define the location of the headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ocl/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(OPENCL_ICD_LOADER_HEADERS_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/cl_headers/\"\n    CACHE PATH \"Path to OCL includes\" FORCE)\n```\n\n----------------------------------------\n\nTITLE: Creating OpenVINO GPU Backend Target in CMake\nDESCRIPTION: This CMake code snippet creates a target named openvino_intel_gpu_cpu_obj using the ov_gpu_add_backend_target function. It links the target against the openvino::reference library, which is a prerequisite for using the backend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/cpu/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_intel_gpu_cpu_obj\")\n\nov_gpu_add_backend_target(\n    NAME ${TARGET_NAME}\n    LINK_LIBRARIES openvino::reference\n)\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific Source File Inclusion in CMake\nDESCRIPTION: This code uses conditional logic based on the operating system (Windows, Apple, or Linux) to include platform-specific source files and headers. This allows the OpenVINO runtime to adapt to different operating system environments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n    file (GLOB LIBRARY_SRC\n         ${LIBRARY_SRC}\n         ${CMAKE_CURRENT_SOURCE_DIR}/src/os/win/*.cpp)\n    file (GLOB LIBRARY_HEADERS\n         ${LIBRARY_HEADERS}\n         ${CMAKE_CURRENT_SOURCE_DIR}/src/os/win/*.hpp)\nelseif(APPLE)\n    file (GLOB LIBRARY_SRC\n        ${LIBRARY_SRC}\n        ${CMAKE_CURRENT_SOURCE_DIR}/src/os/mac/*.cpp)\n    file (GLOB LIBRARY_HEADERS\n        ${LIBRARY_HEADERS}\n        ${CMAKE_CURRENT_SOURCE_DIR}/src/os/mac/*.hpp)\nelseif(NOT EMSCRIPTEN)\n    file (GLOB LIBRARY_SRC\n          ${LIBRARY_SRC}\n          ${CMAKE_CURRENT_SOURCE_DIR}/src/os/lin/*.cpp)\n    file (GLOB LIBRARY_HEADERS\n         ${LIBRARY_HEADERS}\n         ${CMAKE_CURRENT_SOURCE_DIR}/src/os/lin/*.hpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Azure Blob Storage: Enabling sccache for C/C++ Files\nDESCRIPTION: This YAML snippet shows how to enable `sccache` for caching C and C++ build files within a CMake project. It sets the `CMAKE_CXX_COMPILER_LAUNCHER` and `CMAKE_C_COMPILER_LAUNCHER` environment variables to `sccache` within the `env` block of the job configuration. This instructs CMake to use `sccache` when invoking the C and C++ compilers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/caches.md#_snippet_6\n\nLANGUAGE: YAML\nCODE:\n```\nBuild:\n  ...\n  env:\n    ...\n    CMAKE_CXX_COMPILER_LAUNCHER: sccache\n    CMAKE_C_COMPILER_LAUNCHER: sccache\n    ...\n    SCCACHE_AZURE_KEY_PREFIX: ubuntu20_x86_64_Release\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties in CMake\nDESCRIPTION: This snippet sets the target properties for the 'compile_tool' executable. It defines the folder where the target is located and sets the C++ standard to 17. The target is named compile_tool and the folder property is set to the current source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/compile_tool/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES\n                          FOLDER ${CMAKE_CURRENT_SOURCE_DIR}\n                          CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: Using AnyInput\nDESCRIPTION: This Python snippet demonstrates the usage of AnyInput to indicate that a specific input for a node in a pattern does not need to be explicitly specified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino.tools.ovc.composite.pattern import AnyInput\n\ndef any_input():\n    any_input_node = AnyInput()\n\n    return any_input_node\n```\n\n----------------------------------------\n\nTITLE: GatherND with Batch Dimensions & Leading Dimensions\nDESCRIPTION: This example illustrates a complex scenario where GatherND operates with both batch_dims (set to 2) and leading dimensions in the indices tensor.  This configuration impacts the way slices are selected and assembled from the data tensor within each batch.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-5.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 2\nindices = [[[[1]], <--- this is applied to the first batch\n               [[0]],\n               [[2]]],\n              [[[0]],\n               [[2]],\n               [[2]]] <--- this is applied to the sixth batch\n             ], shape = (2, 3, 1, 1)\ndata    = [[[1,   2,  3,  4], <--- this is the first batch\n               [ 5,  6,  7,  8],\n               [ 9, 10, 11, 12]]\n              [[13, 14, 15, 16],\n               [17, 18, 19, 20],\n               [21, 22, 23, 24]] <--- this is the sixth batch\n             ] <--- the second batch, shape = (2, 3, 4)\noutput  = [[2], [5], [11], [13], [19], [23]], shape = (6, 1)\n```\n\n----------------------------------------\n\nTITLE: Mod Operation with No Broadcasting XML - OpenVINO\nDESCRIPTION: This XML snippet demonstrates the Mod operation in OpenVINO with auto_broadcast set to \"none\".  It shows the layer configuration with two input ports and one output port, all having the same dimensions.  The 'data' tag specifies the auto_broadcast attribute, and the 'input' and 'output' tags define the tensor shapes for the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/mod-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Mod\">\n    <data auto_broadcast=\"none\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Range Generation with Floating-Point Values in OpenVINO (XML)\nDESCRIPTION: This XML snippet demonstrates the Range operation with floating-point values, generating a sequence from start (1) to stop (2.5) with a step of 0.5. The output is a floating-point tensor of dimension 3.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/range-4.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Range\">\n    <data output_type=\"f32\">\n    <input>\n        <port id=\"0\">  <!-- start value: 1 -->\n        </port>\n        <port id=\"1\">  <!-- stop value: 2.5 -->\n        </port>\n        <port id=\"2\">  <!-- step value: 0.5 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>3</dim> <!-- [ 1.0,  1.5,  2.0] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Filesystem Test Program (C++)\nDESCRIPTION: This snippet writes a minimal C++ program to `main.cpp` in the binary directory to check for filesystem library support. It includes the `<experimental/filesystem>` header, creates a path object, and returns its length as an integer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n#include <experimental/filesystem>\nint main(int argc, char ** argv) {\n  std::experimental::filesystem::path p(argv[0]);\n  return p.string().length();\n}\n```\n\n----------------------------------------\n\nTITLE: Set Test Models Directory\nDESCRIPTION: Sets a compile definition to define the directory where the TensorFlow Lite test models are located.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_TENSORFLOW_LITE_MODELS_DIRNAME ${TEST_MODEL_ZOO}/tensorflow_lite_test_models)\ntarget_compile_definitions(${TARGET_NAME} PRIVATE -D TEST_TENSORFLOW_LITE_MODELS_DIRNAME=\\\"${TEST_TENSORFLOW_LITE_MODELS_DIRNAME}/\\\")\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory thirdparty\nDESCRIPTION: This snippet adds the `thirdparty` subdirectory to the build process, likely containing external dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(thirdparty)\n```\n\n----------------------------------------\n\nTITLE: Setting Element Type\nDESCRIPTION: The `setElementType` method sets the element type of the output tensor. It takes an `elementType` parameter, which can be either an `elementTypeString` or an `element` object, and returns an `InputTensorInfo` object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/OutputTensorInfo.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nsetElementType(elementType): InputTensorInfo\n```\n\n----------------------------------------\n\nTITLE: Setting MLAS/SHL Libraries\nDESCRIPTION: This snippet conditionally sets the MLAS and SHL libraries based on the `ENABLE_MLAS_FOR_CPU` and `ENABLE_SHL_FOR_CPU` flags, respectively. If enabled, it sets the corresponding library name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/vectorized/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_MLAS_FOR_CPU)\n    set(MLAS_LIBRARY \"mlas\")\nendif()\n\nif (ENABLE_SHL_FOR_CPU)\n    set(SHL_LIBRARY \"shl\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Unit Test Target in CMake\nDESCRIPTION: This snippet adds dependencies to the unit test target based on the UNIT_TESTS_DEPENDENCIES variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/tests/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_dependencies(${TARGET_NAME} ${UNIT_TESTS_DEPENDENCIES})\n```\n\n----------------------------------------\n\nTITLE: Skipping Git Pre-Commit Hook\nDESCRIPTION: This bash command sets the `SKIP` environment variable to `generate_stubs` before running `git commit`. This configuration bypasses the pre-commit hook responsible for generating stub files, allowing commits without triggering the stub generation process. It is useful for specific scenarios where stub generation is not needed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/stubs.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nSKIP=generate_stubs git commit -m \"Your commit message\"\n```\n\n----------------------------------------\n\nTITLE: Running a Python Sample (Linux)\nDESCRIPTION: This command executes a Python sample. It takes the path to the Python file, the model, the input media, and the target device as arguments. This is for Linux\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\npython <sample.py file> -m <path_to_model> -i <path_to_media> -d <target_device>\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Addition in CMake\nDESCRIPTION: This CMake code block conditionally adds the 'zero' subdirectory if the `ENABLE_NPU_PLUGIN_ENGINE` variable is set to true.  This allows for optional components to be included in the build based on configuration settings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_NPU_PLUGIN_ENGINE)\n    add_subdirectory(zero)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Image Classification Sample (C++, Windows)\nDESCRIPTION: This command runs the `classification_sample_async.exe` executable with a specified model (`googlenet-v1.xml`), input image (`dog.bmp`), and target device (`CPU`) on a Windows system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_33\n\nLANGUAGE: bat\nCODE:\n```\n.\\classification_sample_async.exe -i %USERPROFILE%\\Downloads\\dog.bmp -m %USERPROFILE%\\Documents\\ir\\googlenet-v1.xml -d CPU\n```\n\n----------------------------------------\n\nTITLE: Handling Reserved Keyword in MinGW\nDESCRIPTION: This snippet checks if the compiler is MinGW and, if so, defines a compile definition for the `onnx` target to replace the reserved word `OPTIONAL` with a placeholder. This avoids compilation errors related to the use of `OPTIONAL` as a keyword in MinGW.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/onnx/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(MINGW)\n    # OPTIONAL is a reserved word for mingw at least\n    target_compile_definitions(onnx PRIVATE OPTIONAL=OPTIONAL_PLACEHOLDER)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Linking to OpenCL and OpenVINO Libraries (CMake)\nDESCRIPTION: This snippet links the library to the OpenCL and OpenVINO runtime libraries. It uses the `OpenCL::OpenCL`, `openvino::runtime`, and `openvino::runtime::dev` targets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC OpenCL::OpenCL openvino::runtime PRIVATE openvino::runtime::dev)\n```\n\n----------------------------------------\n\nTITLE: GreaterEqual Layer Configuration (No Broadcast) XML\nDESCRIPTION: This example shows the configuration of a GreaterEqual layer where no broadcasting is applied because the input tensors have matching shapes. The layer takes two input tensors of shape 256x56 and produces an output tensor of the same shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/greater-equal-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"GreaterEqual\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing Target with CMake\nDESCRIPTION: This snippet installs the `sea_itt_lib` target to the `tests/sea_itt_lib` directory under the `itt_collector` component and excludes it from the default installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/itt_collector/sea_itt_lib/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n        DESTINATION tests/sea_itt_lib COMPONENT itt_collector EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Setting Output Directories\nDESCRIPTION: This snippet sets the output directories for libraries, archives, program database (PDB) files, and executables, based on the OV_MAIN_SAMPLES_DIR and BIN_FOLDER variables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset (CMAKE_LIBRARY_OUTPUT_DIRECTORY ${OV_MAIN_SAMPLES_DIR}/${BIN_FOLDER})\nset (CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${OV_MAIN_SAMPLES_DIR}/${BIN_FOLDER})\nset (CMAKE_PDB_OUTPUT_DIRECTORY ${OV_MAIN_SAMPLES_DIR}/${BIN_FOLDER})\nset (CMAKE_RUNTIME_OUTPUT_DIRECTORY ${OV_MAIN_SAMPLES_DIR}/${BIN_FOLDER})\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO tools with pip\nDESCRIPTION: This command installs OpenVINO tools using pip, specifying the path to the tool's directory containing the setup.py file.  This command ensures that all dependencies for the tool are also installed, and the tool is made available within the current Python environment (either system-wide or within a virtual environment).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/README.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\npython3 -m pip install <tools_folder>/\n```\n\n----------------------------------------\n\nTITLE: Defining DirectX Libraries for Windows\nDESCRIPTION: Conditionally adds the `ENABLE_DX11` definition and links against the Direct3D 11 and DXGI libraries if the target platform is Windows. This enables DirectX 11 support.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/functional/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n    target_compile_definitions(${TARGET_NAME} PRIVATE ENABLE_DX11)\n    target_link_libraries(${TARGET_NAME} PRIVATE d3d11 dxgi)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Add Test Subdirectory CMake\nDESCRIPTION: This conditional block adds the 'tests' subdirectory to the build process if the 'ENABLE_TESTS' variable is set to true. This allows the inclusion of unit tests and integration tests in the build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Reshape Empty Tensor Example in OpenVINO XML\nDESCRIPTION: This XML snippet demonstrates reshaping an empty tensor using the Reshape operation in OpenVINO. The 'special_zero' attribute is set to 'false', indicating that zero values in the shape tensor are interpreted as-is, resulting in an empty output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/reshape-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Reshape\" ...>\n    <data special_zero=\"false\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>5</dim>\n            <dim>5</dim>\n            <dim>0</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>   <!--The tensor contains 2 elements: 0, 4 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>0</dim>\n            <dim>4</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Build with Custom OpenCV - CMake\nDESCRIPTION: This command demonstrates how to specify the path to OpenCV configuration files during the CMake configuration of OpenVINO, allowing OpenVINO samples to utilize the provided OpenCV installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/cmake_options_for_custom_compilation.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DOpenCV_DIR=<path to OpenCVConfig.cmake> ...\n```\n\n----------------------------------------\n\nTITLE: Einsum Diagonal Extraction Example C++\nDESCRIPTION: This example demonstrates how Einsum extracts a diagonal for each batch object using the Einstein summation convention.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/einsum-7.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nA = [[[1.0, 2.0, 3.0],\n      [4.0, 5.0, 6.0],\n      [7.0, 8.0, 9.0]],\n     [[2.0, 4.0, 6.0],\n      [8.0, 10.0, 12.0],\n      [14.0, 16.0, 18.0]]]\nequation = \"kii->ki\"\noutput = [[1.0, 5.0, 9.0],\n            [2.0, 10.0, 18.0]]\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for Specific Sources in CMake\nDESCRIPTION: This snippet sets include directories for a specific set of source files (`MIXED_SRC`). It appends include directories based on the properties of the `openvino_runtime_obj` target. This allows specific source files to access headers from locations defined by the `openvino_runtime_obj` target, useful for complex project structures.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset_property(SOURCE ${MIXED_SRC}\n    APPEND PROPERTY INCLUDE_DIRECTORIES\n        $<TARGET_PROPERTY:openvino_runtime_obj,SOURCE_DIR>/src\n        $<TARGET_PROPERTY:openvino_runtime_obj,SOURCE_DIR>/dev_api\n        $<TARGET_PROPERTY:openvino_runtime_obj,SOURCE_DIR>/include)\n```\n\n----------------------------------------\n\nTITLE: Defining Workflow Job Runner in YAML\nDESCRIPTION: This code snippet shows how to define the runner for a job within a GitHub Actions workflow using the `runs-on` key in a YAML file.  The specified runner determines the environment in which the job will execute. This example uses the `aks-linux-16-cores-32gb` runner group.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/runners.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nBuild:\n  ...\n  runs-on: aks-linux-16-cores-32gb\n  ...\n```\n\n----------------------------------------\n\nTITLE: Exclude Subgraphs from Quantization\nDESCRIPTION: Excludes a specific subgraph from the quantization process. All nodes along simple paths from the input nodes to the output nodes of the subgraph are excluded from quantization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_24\n\nLANGUAGE: sh\nCODE:\n```\nsubgraph = nncf.Subgraph(inputs=['layer_1', 'layer_2'], outputs=['layer_3'])\nncf.quantize(model, dataset, ignored_scope=nncf.IgnoredScope(subgraphs=[subgraph]))\n\n```\n\n----------------------------------------\n\nTITLE: Defining Constant Layer with Data XML\nDESCRIPTION: This XML snippet defines a constant layer with an offset and size attribute that specifies the location and size of the data, and its output port specifying data dimensions and precision for the layer's output. This layer provides constant data to the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/tensor-iterator-1.rst#_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"5\" type=\"Const\" ...>\n    <data offset=\"16\" size=\"3145728\"/>\n    <output>\n        <port id=\"1\" precision=\"FP32\">\n            <dim>1024</dim>\n            <dim>768</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Define ov_version struct in C\nDESCRIPTION: This struct represents the version information for OpenVINO components. It contains pointers to the build number and description strings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_0\n\nLANGUAGE: C\nCODE:\n```\ntypedef struct ov_version {\n\n    const char* buildNumber;\n\n    const char* description;\n\n} ov_version_t;\n```\n\n----------------------------------------\n\nTITLE: Pad Output Example - Symmetric Mode - C++\nDESCRIPTION: Illustrates the output of the Pad operation in 'symmetric' mode, where the padded values are symmetrically added from the input tensor (values on edges are duplicated). It shows how the input tensor is extended based on pads_begin and pads_end attributes, using symmetric values for padding.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-1.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nINPUT =\n    [[ 1  2  3  4 ]\n    [ 5  6  7  8 ]\n    [ 9 10 11 12 ]]\n\npads_begin = [0, 1]\npads_end = [2, 3]\n\nOUTPUT =\n    [[ 1  1  2  3  4  4  3  2 ]\n    [ 5  5  6  7  8  8  7  6 ]\n    [ 9  9 10 11 12 12 11 10 ]\n    [ 9  9 10 11 12 12 11 10 ]\n    [ 5  5  6  7  8  8  7  6 ]]\n```\n\n----------------------------------------\n\nTITLE: Adding Tests Subdirectory CMake\nDESCRIPTION: This snippet adds the tests subdirectory to the build process if the ENABLE_TESTS flag is enabled. This allows for building and running unit tests for the NPU plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Shellcheck Skip List CMake\nDESCRIPTION: This snippet configures the `shellcheck_skip_list` variable, which contains a list of directories to be excluded from Shellcheck analysis. These directories include binary, build, third-party, and Python binding directories within the OpenVINO source tree, as well as temporary directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/scripts/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(shellcheck_skip_list\n    \"${OpenVINO_SOURCE_DIR}/bin\"\n    \"${OpenVINO_SOURCE_DIR}/build\"\n    \"${OpenVINO_SOURCE_DIR}/thirdparty\"\n    \"${OpenVINO_SOURCE_DIR}/src/plugins/intel_cpu/thirdparty\"\n    \"${OpenVINO_SOURCE_DIR}/src/plugins/intel_gpu/thirdparty\"\n    \"${OpenVINO_SOURCE_DIR}/src/plugins/intel_npu/thirdparty\"\n    \"${OpenVINO_SOURCE_DIR}/src/bindings/python/thirdparty/pybind11\"\n    \"${TEMP}\")\n\nov_shellcheck_process(DIRECTORY \"${OpenVINO_SOURCE_DIR}\"\n                      SKIP ${shellcheck_skip_list})\n```\n\n----------------------------------------\n\nTITLE: Glob Source and Header Files CMake\nDESCRIPTION: This snippet uses `file(GLOB_RECURSE)` to find all C++ source files (`*.cpp`) and header files (`*.hpp`) within the specified directories. The results are stored in the `LIBRARY_SRC` and `PUBLIC_HEADERS` variables respectively, which are used in subsequent steps.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE PUBLIC_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/include/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: ReduceLogicalOr Example 4 (keep_dims=false, axis=-2)\nDESCRIPTION: This XML example shows ReduceLogicalOr with `keep_dims` set to `false` and reduction along axis -2. With an input tensor of 6x12x10x24, the output tensor is 6x12x24 after reduction of the third-to-last dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-logical-or-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceLogicalOr\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [-2] that means independent reduction in each channel, batch and second spatial dimension -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining NOMINMAX for Windows\nDESCRIPTION: This snippet defines the `NOMINMAX` macro for Windows builds to prevent conflicts with the standard library's `min` and `max` functions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DNOMINMAX\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Build Directory for OpenVINO Samples (Linux)\nDESCRIPTION: This command creates a new directory named `build` which will be used for building the OpenVINO samples. It assumes the user has write access to the current directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nmkdir build\n```\n\n----------------------------------------\n\nTITLE: Organizing Source Files in CMake\nDESCRIPTION: This snippet uses the source_group command to organize the source files in the IDE. It groups the files under the current source directory, making the project structure more readable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/common/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(TREE ${CMAKE_CURRENT_SOURCE_DIR} FILES ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Disabling Functional Tests During Plugin Build\nDESCRIPTION: This snippet demonstrates how to disable functional tests during the plugin build process using a CMake option.  It utilizes the OpenVINODeveloperPackage and modifies the ENABLE_FUNCTIONAL_TESTS variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/build-plugin-using-cmake.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n$ cmake -DENABLE_FUNCTIONAL_TESTS=OFF -DOpenVINODeveloperPackage_DIR=../openvino-release-build ../template-plugin\n```\n\n----------------------------------------\n\nTITLE: Install GnuPG\nDESCRIPTION: Installs the GnuPG (GNU Privacy Guard) software, which is required for managing and using GPG keys. This step is necessary if GnuPG is not already installed on the system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt-get install gnupg\n```\n\n----------------------------------------\n\nTITLE: Defining Partially Defined Layout in Python\nDESCRIPTION: This snippet demonstrates defining a partially defined layout in Python using `ov::Layout`, where some dimensions are marked as unimportant using `?`. This is useful when certain dimensions don't require specific labeling.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlayout = ov.Layout(\"[?, C, ?, W]\")\n```\n\n----------------------------------------\n\nTITLE: Conditional JS API Build Check CMake\nDESCRIPTION: This snippet checks if the `ENABLE_JS` option is not set. If it's not set (meaning JavaScript support is disabled), the function returns, skipping the rest of the API build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT ENABLE_JS)\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Starting the License Server\nDESCRIPTION: Sets up the environment variables and starts the license server. This server is required for validating certificates. It sources the setupvars.sh script and executes the license_server executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_35\n\nLANGUAGE: sh\nCODE:\n```\nsource /opt/ovsa/scripts/setupvars.sh\ncd /opt/ovsa/bin\n./license_server\n```\n\n----------------------------------------\n\nTITLE: Load Checkpoint - TensorFlow\nDESCRIPTION: Restores the TensorFlow model from a checkpoint created during QAT using NNCF's API. This enables resuming training or loading a quantized model for further processing.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/quantization-aware-training.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnncf.load_checkpoint(model, checkpoint_path)\n```\n\n----------------------------------------\n\nTITLE: Reshape by Input Name - Python\nDESCRIPTION: This Python snippet shows how to reshape an OpenVINO model by specifying the input by its name. The new shape is passed as a string.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/changing-input-shape.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino.runtime as ov\n\ncore = ov.Core()\nmodel = core.read_model(\"model.xml\")\n\n# ! [name_to_shape]\ninput_name = \"input\"  # Name of the input\nnew_shape = \"[1,3,256,256]\"\nmodel.reshape({input_name: new_shape})\n# ! [name_to_shape]\n```\n\n----------------------------------------\n\nTITLE: Adding the Library Target\nDESCRIPTION: This snippet adds the library target with the specified name (TARGET_NAME), type (SHARED), and source files (SRC and HDR). It creates a shared library from the listed source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/thread_local/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} SHARED ${SRC} ${HDR})\n```\n\n----------------------------------------\n\nTITLE: Setting Code Generation Script Paths\nDESCRIPTION: Defines variables for the paths to the code generation and test scripts written in Python.  These scripts are used to generate and test the kernel database.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nset(CODEGEN_SCRIPT \"${CMAKE_CURRENT_SOURCE_DIR}/common_utils/kernels_db_gen.py\")\nset(CODEGEN_TEST_SCRIPT \"${CMAKE_CURRENT_SOURCE_DIR}/common_utils/test_kernels_db_gen.py\")\n```\n\n----------------------------------------\n\nTITLE: Creating Source Groups in CMake\nDESCRIPTION: Organizes the source files within the Visual Studio project using `source_group`. This helps to logically structure the project in the IDE by grouping source files and headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/format_reader/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC} ${LIBRARY_HEADERS})\nsource_group(\"include\" FILES ${LIBRARY_PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Activate Virtual Environment (Linux/macOS)\nDESCRIPTION: This command activates the previously created virtual environment `openvino_env` on Linux and macOS.  It modifies the shell environment to use the environment's Python interpreter and packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-pip.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nsource openvino_env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Convert MetaGraph Model (TF1) using CLI\nDESCRIPTION: This snippet demonstrates how to convert a TensorFlow 1 MetaGraph model to OpenVINO IR using the `ovc` command-line tool. The input is the path to the ``.meta`` file. The output is the OpenVINO IR model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-tensorflow.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\novc path_to_meta_graph.meta\n```\n\n----------------------------------------\n\nTITLE: Using AnyInput\nDESCRIPTION: This C++ snippet demonstrates the usage of AnyInput to indicate that a specific input for a node in a pattern does not need to be explicitly specified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nshared_ptr<ov::Node> any_input() {\n    auto any_input_node = make_shared<AnyInput>();\n\n    return any_input_node;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining CMake Option for PostgreSQL Reporting\nDESCRIPTION: Defines a CMake option to enable PostgreSQL-based reporting from test tools. This allows for storing and analyzing test results in a PostgreSQL database. Requires CMake and a PostgreSQL installation if enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nov_option(ENABLE_CONFORMANCE_PGQL \"Enables support of PostgreSQL-based reporting from test tools\" OFF)\nmark_as_advanced(FORCE ENABLE_CONFORMANCE_PGQL)\n```\n\n----------------------------------------\n\nTITLE: Functional Test Directory\nDESCRIPTION: This snippet shows the directory where functional tests for the GPU plugin are located. Most tests are shared across plugins, with each plugin adding specific test instances with unique parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/source_code_structure.md#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nsrc/tests/functional/plugin/gpu\n```\n\n----------------------------------------\n\nTITLE: Linking System Libraries (CMake)\nDESCRIPTION: This conditional block links to system libraries (setupapi for Windows, pthread for Unix) based on the operating system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n  target_link_libraries(${TARGET_NAME} PRIVATE setupapi)\nelif((NOT ANDROID) AND UNIX)\n  target_link_libraries(${TARGET_NAME} PRIVATE pthread)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO Samples with PowerShell (Windows)\nDESCRIPTION: This PowerShell command executes the `build_samples.ps1` script, which automates the process of building OpenVINO samples on Windows using Visual Studio. It detects the highest installed Visual Studio version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n& <path-to-build-samples-folder>/build_samples.ps1\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO Plugin Using OpenVINODeveloperPackage\nDESCRIPTION: This snippet shows how to build an OpenVINO plugin using the OpenVINO Developer Package.  It configures CMake to find the Developer Package and then builds the plugin source tree.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/build-plugin-using-cmake.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n$ mkdir template-plugin-release-build\n$ cd template-plugin-release-build\n$ cmake -DOpenVINODeveloperPackage_DIR=../openvino-release-build ../template-plugin\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Reflect Mode Example (Mixed Pads) - C++\nDESCRIPTION: Demonstrates reflect padding with mixed positive and negative pad values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nOUTPUT =\n[[10, 11, 12, 11, 10, 9],\n[6,   7,  8,  7,  6, 5],\n[2,   3,  4,  3,  2, 1],\n[6,   7,  8,  7,  6, 5]]\nShape(4, 6)\n```\n\n----------------------------------------\n\nTITLE: File architecture check\nDESCRIPTION: This snippet demonstrates how to check the architecture of a library file using the `file` command. It's used to verify if a library is built for the correct architecture during cross-compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nfile /opt/homebrew/Cellar/tbb/2021.5.0_2/lib/libtbb.12.5.dylib\n```\n\n----------------------------------------\n\nTITLE: Enable Colored Verbose Output\nDESCRIPTION: This snippet shows how to enable colored verbose output for the OpenVINO CPU plugin by duplicating the level's digit in the `OV_CPU_VERBOSE` environment variable. Shell color codes are used for colored output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/verbose.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_VERBOSE=11 binary ...\n```\n\n----------------------------------------\n\nTITLE: Include Specific Tests CMake\nDESCRIPTION: This snippet includes the 'cmake/specific_tests.cmake' and 'cmake/target_per_test.cmake' files. These files likely contain additional configurations and functions related to specific tests and target configurations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/functional/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(cmake/specific_tests.cmake)\ninclude(cmake/target_per_test.cmake)\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenEmbedded Build Environment\nDESCRIPTION: Sets up the OpenEmbedded build environment by sourcing the `oe-init-build-env` script within the `poky` directory. This script initializes the environment variables and configures the build system for Yocto development.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yocto.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsource poky/oe-init-build-env\n```\n\n----------------------------------------\n\nTITLE: Deleting a Virtual Environment (Windows)\nDESCRIPTION: This command removes the virtual environment directory `openvino_env` and all its contents on Windows. It uses the `rmdir` command with the `/s` option to recursively delete the directory and its subdirectories and files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/run-notebooks.rst#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nrmdir /s openvino_env\n```\n\n----------------------------------------\n\nTITLE: Determine Platform Tag for Linux - CMake\nDESCRIPTION: This CMake code block determines the platform tag for Linux. It checks for `OPENVINO_GNU_LIBC` or `OPENVINO_MUSL_LIBC` to determine the libc type, constructs the platform string, and then sets the `PLATFORM_TAG` to `manylinux/musllinux_<libc_version>_<arch>`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nelseif(LINUX)\n    _ov_platform_arch()\n\n    if(OPENVINO_GNU_LIBC)\n        set(platform \"manylinux\")\n    elseif(OPENVINO_MUSL_LIBC)\n        set(platform \"musllinux\")\n    else()\n        message(FATAL_ERROR \"Undefined libc type\")\n    endif()\n\n    string(REPLACE \".\" \"_\" _ov_libc_version \"${OV_LIBC_VERSION}\")\n    set(platform \"${platform}_${_ov_libc_version}\")\n\n    # convert to well-known formats according to PEP 600\n    if(platform STREQUAL \"manylinux_2_5\")\n        set(platform \"manylinux1\")\n    elseif(platform STREQUAL \"manylinux_2_12\")\n        set(platform \"manylinux2010\")\n    elseif(platform STREQUAL \"manylinux_2_17\")\n        set(platform \"manylinux2014\")\n    endif()\n\n    set(PLATFORM_TAG \"${platform}_${_arch}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Removing Symbolic Link - Shell\nDESCRIPTION: This code snippet removes a symbolic link to the OpenVINO installation directory. It is necessary to remove the symbolic link before deleting the files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\nsudo rm /opt/intel/openvino_2025\n```\n\n----------------------------------------\n\nTITLE: OVSA Model Artifacts Copying\nDESCRIPTION: Copies the model artefacts (.dat, .lic, and keystore) to the model directory for the model server.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_53\n\nLANGUAGE: sh\nCODE:\n```\ncd $OVSA_RUNTIME_ARTEFACTS/../ovms\ncp $OVSA_RUNTIME_ARTEFACTS/<name-of-the-model>.dat model/fd/1/.\ncp $OVSA_RUNTIME_ARTEFACTS/<name-of-the-model>.lic model/fd/1/.\ncp $OVSA_RUNTIME_ARTEFACTS/custkeystore model/fd/1/.\n```\n\n----------------------------------------\n\nTITLE: Applying Install Interface Filter\nDESCRIPTION: This applies the install interface filter function to each of the relevant targets: gtest, gtest_main, gmock, and gmock_main. This ensures that the include paths are correctly set for both building and installing the library and its associated main functions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/gtest/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n_ov_gtest_filter_install_interface(gtest gtest)\n_ov_gtest_filter_install_interface(gtest_main gtest)\n_ov_gtest_filter_install_interface(gmock gmock)\n_ov_gtest_filter_install_interface(gmock_main gmock)\n```\n\n----------------------------------------\n\nTITLE: Configuring Protobuf Build Options\nDESCRIPTION: This snippet configures various Protobuf build options, such as verbosity, building tests, building shared libraries, ZLIB support, and Abseil propagation. It also unsets HAVE_ZLIB if defined.  These options are set using CMake's `set` command with the `CACHE` option, which allows them to be configured by the user.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/protobuf/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(protobuf_VERBOSE ON)\nset(protobuf_BUILD_TESTS OFF CACHE BOOL \"Build tests\" FORCE)\nset(protobuf_BUILD_SHARED_LIBS OFF CACHE BOOL \"Build shared libs\" FORCE)\nset(protobuf_WITH_ZLIB OFF CACHE BOOL \"Build with zlib support\" FORCE)\nset(ABSL_PROPAGATE_CXX_STD ON CACHE BOOL \"Abseil protogate CXX standard to dependent targets\" FORCE)\n\n# some projects define HAVE_ZLIB, which affects protobuf. Let's explicitly unset it\nunset(HAVE_ZLIB CACHE)\n```\n\n----------------------------------------\n\nTITLE: LogicalXor XML Layer Definition (No Broadcast)\nDESCRIPTION: This XML snippet defines a LogicalXor layer with two input ports of the same shape, demonstrating the case where no broadcasting is needed. The output port has the same shape as the inputs. This example showcases the basic structure of the layer definition, specifying the input and output tensor dimensions when the input shapes match.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/logical/logical-xor-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"LogicalXor\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: PyTorch: Quantize Model\nDESCRIPTION: Quantizes the PyTorch model using NNCF's `quantize` function with a calibration dataset. The function applies 8-bit quantization to the model, preparing it for efficient inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nquantized_model = nncf.quantize(model=model, calibration_dataset=dataset)\n\n```\n\n----------------------------------------\n\nTITLE: List of Python Dependencies\nDESCRIPTION: This snippet defines the project's Python dependencies, including constraint files, testing libraries like pytest, and scientific computing libraries like NumPy and TensorFlow Hub.  It's likely used by pip or a similar package manager to install the project's requirements. The `constraints.txt` file probably specifies version constraints for these packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/model_hub_tests/performance_tests/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-c ../../constraints.txt\nnumpy\npytest\npytest-html\ntensorflow-hub\npy\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Edge Mode Example (Mixed Pads) - C++\nDESCRIPTION: Demonstrates edge padding with mixed positive and negative pad values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nOUTPUT Shape(4, 6) =\n[[2, 3, 4, 4, 4, 4],\n[2, 3, 4, 4, 4, 4],\n[2, 3, 4, 4, 4, 4],\n[6, 7, 8, 8, 8, 8]]\nShape(4, 6)\n```\n\n----------------------------------------\n\nTITLE: Defining Dynamic Layout with Ellipsis in Python\nDESCRIPTION: This snippet demonstrates defining a dynamic layout in Python using `ov::Layout`, using an ellipsis (`...`) to represent multiple unimportant dimensions. This is suitable for tensors with a variable number of irrelevant dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/layout-api-overview.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlayout = ov.Layout(\"[N, ..., W]\")\n```\n\n----------------------------------------\n\nTITLE: Activate Virtual Environment (Windows)\nDESCRIPTION: This command activates the Python virtual environment 'openvino_env' on Windows. After activation, the system will use the Python interpreter and packages installed within the virtual environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/pypi_publish/pypi-openvino-rt.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nopenvino_env\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Runtime and Samples\nDESCRIPTION: This command installs the OpenVINO Runtime and sample applications using the zypper package manager. It requires root privileges and installs both the development headers and sample code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-zypper.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nsudo zypper install openvino-devel openvino-sample\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies in CMake\nDESCRIPTION: This command adds a build dependency of the `${TARGET_NAME}` library on the `test_builtin_extensions` target. It ensures that `test_builtin_extensions` is built before `${TARGET_NAME}`. This is used to enforce build order.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_dependencies(${TARGET_NAME} test_builtin_extensions)\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Constant Mode Example (Mixed Pads) - C++\nDESCRIPTION: Demonstrates constant padding with mixed positive and negative pad values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\npad_mode = \"constant\"\n\nOUTPUT =\n[[0, 0, 0, 0, 0, 0],\n[0, 0, 0, 0, 0, 0],\n[2, 3, 4, 0, 0, 0],\n[6, 7, 8, 0, 0, 0]]\nShape(4, 6)\n```\n\n----------------------------------------\n\nTITLE: Installing C Samples Build Scripts (UNIX) in CMake\nDESCRIPTION: Installs the build_samples.sh script for C samples on UNIX systems. This script is placed in the designated C samples directory within the installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif(UNIX)\n    install(PROGRAMS cpp/build_samples.sh\n            DESTINATION ${OV_CPACK_SAMPLESDIR}/c\n            COMPONENT ${OV_CPACK_COMP_C_SAMPLES}\n            ${OV_CPACK_COMP_C_SAMPLES_EXCLUDE_ALL})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Float16 Conversion in Mersenne-Twister (C++)\nDESCRIPTION: This code snippet provides the conversion process for a 32-bit unsigned integer value to a float16 number within the Mersenne-Twister algorithm.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nmantissa_digits = 11 //(mantissa / significand bits count of float16 + 1, equal to 11)\nmask = uint32(uint64(1) << mantissa_digits - 1)\n```\n\n----------------------------------------\n\nTITLE: Setting up OVSA release path\nDESCRIPTION: Defines the OVSA_RELEASE_PATH environment variable to point to the current working directory. This path is used in subsequent steps for installing the OpenVINO™ Security Add-on.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_29\n\nLANGUAGE: sh\nCODE:\n```\nexport OVSA_RELEASE_PATH=$PWD\n```\n\n----------------------------------------\n\nTITLE: Get Input Tensor from InferRequest TypeScript\nDESCRIPTION: Retrieves the input tensor of the InferRequest. It allows to specify the index of the input tensor. If the model has only one input, the index is optional. Returns a Tensor object representing the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ngetInputTensor(): Tensor\n```\n\nLANGUAGE: typescript\nCODE:\n```\ngetInputTensor(idx: number): Tensor\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate 2D Tensor Update - C++ (axis=1)\nDESCRIPTION: This code snippet shows how the ScatterElementsUpdate operation updates a 2D tensor when axis is set to 1. It demonstrates the element update logic based on the indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i][indices[i][j]] = reduction(updates[i][j], output[indices[i][j]][j]) if axis = 1\n```\n\n----------------------------------------\n\nTITLE: Compile ONNX model using compile_model in C\nDESCRIPTION: This snippet shows how to compile an ONNX model using the OpenVINO C API. The model is compiled from a file for execution on the specified device (AUTO).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_16\n\nLANGUAGE: c\nCODE:\n```\nov_compiled_model_t* compiled_model = NULL;\nov_core_compile_model_from_file(core, \"<INPUT_MODEL>.onnx\", \"AUTO\", 0, &compiled_model);\n```\n\n----------------------------------------\n\nTITLE: Set Blob Dump Environment Variables\nDESCRIPTION: Sets environment variables to control blob dumping. It configures the dump directory, format, and node ports to be dumped for CPU execution in OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_DIR=dump_dir OV_CPU_BLOB_DUMP_FORMAT=TEXT OV_CPU_BLOB_DUMP_NODE_PORTS=OUT binary ...\n```\n\n----------------------------------------\n\nTITLE: Setting Tuning Cache Path (CMake)\nDESCRIPTION: This snippet sets the path for the tuning cache based on whether a multi-configuration generator is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nif(OV_GENERATOR_MULTI_CONFIG)\n  set(TUNING_CACHE_PATH \"${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/$<CONFIG>\")\nelse()\n  set(TUNING_CACHE_PATH \"${CMAKE_LIBRARY_OUTPUT_DIRECTORY}/\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Python dependencies\nDESCRIPTION: This command installs the required Python packages for building the OpenVINO Runtime Python API, using the `requirements.txt` file found in the OpenVINO source tree.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n% python3 -m pip install -r <openvino source tree>/src/bindings/python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Define Target and Sources\nDESCRIPTION: This snippet defines the target name, sources, and headers for the OpenVINO Hetero plugin. It uses GLOB_RECURSE to find all .cpp and .hpp files in the src directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME openvino_hetero_plugin)\n\nfile(GLOB_RECURSE SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/src/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Defining Reshape Layer XML\nDESCRIPTION: This XML snippet defines a Reshape layer with its input and output ports, specifying the data dimensions for the reshaping operation. The layer takes an input with dimensions 1x256 and a reshape dimension of 3, outputting a tensor with dimensions 1x1x256.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/tensor-iterator-1.rst#_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"11\" type=\"Reshape\" ...>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>256</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>256</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining OpenVINO IR Frontend with CMake\nDESCRIPTION: This CMake code snippet defines the OpenVINO IR frontend using the `ov_add_frontend` function. It sets the name to \"ir\", provides a description, and links the `openvino::pugixml` and `openvino::core::dev` libraries.  This allows the OpenVINO toolkit to load models in the OpenVINO IR format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/ir/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_frontend(NAME ir\n                FILEDESCRIPTION \"FrontEnd to load OpenVINO IR file format\"\n                LINK_LIBRARIES openvino::pugixml\n                               openvino::core::dev)\n```\n\n----------------------------------------\n\nTITLE: Standalone project configuration\nDESCRIPTION: Configures the project if it's built standalone. It sets the minimum CMake version and includes a standalone configuration file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT DEFINED PROJECT_NAME)\n    if(WIN32)\n        cmake_minimum_required(VERSION 3.16)\n    else()\n        cmake_minimum_required(VERSION 3.13)\n    endif()\n    project(single-image-test_standalone)\n    include(\"cmake/standalone.cmake\")\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries CMake\nDESCRIPTION: Links the `${TARGET_NAME}` target to the libraries specified in the `ov_link_libraries` variable. This ensures that the library has access to the necessary dependencies at link time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/lib/src/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC ${ov_link_libraries})\n```\n\n----------------------------------------\n\nTITLE: Query Model C++\nDESCRIPTION: This snippet showcases the `query_model` method, responsible for analyzing a model and determining the operations supported by the plugin. It applies transformations to the model and returns a map of supported operations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nov::SupportedOpsMap Plugin::query_model(const std::shared_ptr<const ov::Model>& model, const ov::AnyMap& properties) const {\n    auto cfg = Configuration{properties, get_stream_executor(properties)};\n    auto transformed_model = transform_model(model, properties);\n    ov::SupportedOpsMap supported_ops;\n\n    for (const auto& op : transformed_model->get_ops()) {\n        supported_ops[op->get_friendly_name()] = m_backend->is_operation_supported(op);\n    }\n\n    return supported_ops;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory CMake\nDESCRIPTION: Adds the `level-zero` subdirectory to the build process, excluding it from the `ALL` target. This means it needs to be built explicitly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/level_zero/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(level-zero EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation Scope for OpenVINO Operation Methods in C++\nDESCRIPTION: This macro adds support for conditional compilation to each method of the new operation. It provides a scoping mechanism based on the operation version, operation name, and method name, ensuring that the code can be selectively compiled based on specific requirements.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/docs/operation_enabling_flow.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nOV_OP_SCOPE(<operation_version>_<operation_name>_<method_name>);\n```\n\n----------------------------------------\n\nTITLE: Running MatcherPass on a Single Node (C++)\nDESCRIPTION: This code snippet demonstrates how to run a MatcherPass on a single node.  This is useful when you want to integrate a MatcherPass into another transformation or apply it selectively to specific parts of the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/matcher-pass.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n// [matcher_pass:run_on_node]\n#include <openvino/core/rt_info.hpp>\n#include <openvino/opsets/opset10.hpp>\n#include <openvino/pass/matcher_pass.hpp>\n\n#include \"openvino/pass/graph_rewrite.hpp\"\n\nusing namespace std;\nusing namespace ov::opset10;\n\n{\n    auto pass = make_shared<TemplateTransformation>();\n    // Create node\n    auto input0 = make_shared<Parameter>(element::f32, Shape{1, 3, 64, 64});\n    auto input1 = make_shared<Parameter>(element::f32, Shape{1, 3, 64, 64});\n    auto add = make_shared<Add>(input0, input1);\n\n    pass->run_on_node(add);\n}\n// [matcher_pass:run_on_node]\n```\n\n----------------------------------------\n\nTITLE: Run Benchmark App (C++)\nDESCRIPTION: This command runs the OpenVINO benchmark application with default options on a model specified by its XML file. The application performs inference on CPU for 60 seconds using randomly generated data inputs. It displays the benchmark parameters and reports the minimum, average, and maximum inference latency and the average throughput.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./benchmark_app -m model.xml\n```\n\n----------------------------------------\n\nTITLE: Accessing Model Input Information in TypeScript\nDESCRIPTION: This code snippet defines the `model()` method within the `InputInfo` interface.  It is used to retrieve the `InputModelInfo` object, which contains information about the model's expected input. The return type is explicitly defined as `InputModelInfo`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InputInfo.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nmodel(): InputModelInfo\n```\n\n----------------------------------------\n\nTITLE: Install Conan Package Manager with pip\nDESCRIPTION: This command installs Conan Package Manager version 2.0.8 or higher using pip, the Python package installer. This is a prerequisite for installing OpenVINO with Conan.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conan.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython3 -m pip install 'conan>=2.0.8'\n```\n\n----------------------------------------\n\nTITLE: RDFT Layer XML Configuration (With signal_size, 3D Input)\nDESCRIPTION: Configures an RDFT layer in XML with a signal_size input, processing a 3D input tensor. The signal_size input provides the desired output dimensions after the Fourier transform. It shows how the signal_size parameter affects the shape of the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/rdft-9.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"RDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>320</dim>\n            <dim>320</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- axes input contains [1, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>2</dim> <!-- signal_size input contains [512, 100] -->\n        </port>\n    <output>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>512</dim>\n            <dim>51</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This snippet sets the target name for the library to be built.  The `TARGET_NAME` variable is later used in subsequent CMake commands to refer to this target. It is a fundamental step in defining a build target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"frontend_shared_test_classes\")\n```\n\n----------------------------------------\n\nTITLE: Adding Static Library (CMake)\nDESCRIPTION: Adds a static library target with the specified name, source files, and public headers.  It also creates an alias for the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${LIBRARY_SRC} ${PUBLIC_HEADERS})\n\nadd_library(openvino::reference ALIAS ${TARGET_NAME})\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME reference)\n```\n\n----------------------------------------\n\nTITLE: Creating Alias Target in CMake\nDESCRIPTION: This code snippet creates an alias target named `openvino::offline_transformations` that refers to the `TARGET_NAME`. Alias targets allow users to link against a more descriptive name than the actual library name. This also sets an `EXPORT_NAME` property, which seems redundant with the alias.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/offline_transformations/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(openvino::offline_transformations ALIAS ${TARGET_NAME})\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME offline_transformations)\n```\n\n----------------------------------------\n\nTITLE: Defining Target and Sources (CMake)\nDESCRIPTION: This snippet defines the target library name, gathers source files, and organizes them into a source group. It then creates a static library with the specified name and source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/zero/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME openvino_npu_zero_utils)\n\nfile(GLOB_RECURSE SOURCES *.cpp)\nsource_group(TREE ${CMAKE_CURRENT_SOURCE_DIR} FILES ${SOURCES})\n\nadd_library(${TARGET_NAME} STATIC ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Setting Code Generation Directories and Files (CMake)\nDESCRIPTION: This section defines the directories used for generated code and sets variables for specific generated files like `ks_primitive_db.inc` and `ks_primitive_db_batch_headers.inc`. It also sets the path to the Python script used for code generation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(CODEGEN_DIR \"${CMAKE_CURRENT_BINARY_DIR}/codegen\")\nset(CODEGEN_CACHE_DIR  \"${CODEGEN_DIR}/cache\")\n\n# Path which points to automatically included directory with code generated elements\n# (to support \"copy-if-different\" optimization).\nset(CODEGEN_INCDIR  \"${CODEGEN_DIR}/include\")\n\n\nset(PRIM_DB \"ks_primitive_db.inc\")\nset(PRIM_DB_BATCH_HEADERS \"ks_primitive_db_batch_headers.inc\")\nset(CODEGEN_CACHE_SOURCES \"${CODEGEN_INCDIR}/${PRIM_DB}\"\n                          \"${CODEGEN_INCDIR}/${PRIM_DB_BATCH_HEADERS}\")\n\nset(CODEGEN_SCRIPT \"${CMAKE_CURRENT_SOURCE_DIR}/primitive_db_gen.py\")\n# Helping with some generators.\nset_property(SOURCE ${CODEGEN_CACHE_SOURCES} PROPERTY GENERATED TRUE)\n```\n\n----------------------------------------\n\nTITLE: Adding Unit Test Target with CMake\nDESCRIPTION: This snippet uses the `ov_add_test_target` macro to define a unit test target. It specifies the target name, root directory, include directories, object files, link libraries, dependencies, and labels for the test target. It also adds a clang format check and the labels `OV`, `UNIT`, and `HETERO`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/tests/unit/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_test_target(\n        NAME\n            ${TARGET_NAME}\n        ROOT\n            ${CMAKE_CURRENT_SOURCE_DIR}\n        INCLUDES\n            PUBLIC\n                $<TARGET_PROPERTY:openvino_hetero_plugin,SOURCE_DIR>/src\n            ${CMAKE_CURRENT_SOURCE_DIR}\n        OBJECT_FILES\n            ${OBJ_LIB}\n        LINK_LIBRARIES\n            unit_test_utils\n        DEPENDENCIES\n            mock_engine\n        ADD_CLANG_FORMAT\n        LABELS\n            OV UNIT HETERO\n)\n```\n\n----------------------------------------\n\nTITLE: Float64 Conversion\nDESCRIPTION: Converts two uint32 values (x and y) to a float64 (double) by combining them into a uint64, applying a mask and divisor. It relies on `std::numeric_limits<double>::digits == DBL_MANT_DIG == 53`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\nvalue = uint64(x) << 32 + y\n\nmantissa_digits = 53 //(mantissa / significand bits count of double + 1, equal to std::numeric_limits<double>::digits == DBL_MANT_DIG == 53)\nmask = uint64(1) << mantissa_digits - 1\ndivisor = double(1) / (uint64(1) << mantissa_digits)\noutput = double((x & mask) * divisor)\n```\n\n----------------------------------------\n\nTITLE: Generating node.lib (MSVC)\nDESCRIPTION: This snippet generates the `node.lib` file on Windows using the `CMAKE_AR` archiver and the `node-lib.def` definition file. This is specific to MSVC compiler.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC AND CMAKE_JS_NODELIB_DEF AND CMAKE_JS_NODELIB_TARGET)\n    # Generate node.lib\n    execute_process(COMMAND ${CMAKE_AR} /def:${CMAKE_JS_NODELIB_DEF} /out:${CMAKE_JS_NODELIB_TARGET} ${CMAKE_STATIC_LINKER_FLAGS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation Definitions\nDESCRIPTION: This snippet conditionally defines the `OPENVINO_STATIC_LIBRARY` compilation definition if `BUILD_SHARED_LIBS` is not enabled, indicating a static library build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common_translators/src/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME} PRIVATE OPENVINO_STATIC_LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Version Defines for OpenVINO Core\nDESCRIPTION: This snippet adds version defines to the `openvino_core_obj` target. It uses a custom function `ov_add_version_defines` to generate the necessary definitions from the `src/version.cpp` file. This ensures that the compiled code has access to version information.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_version_defines(src/version.cpp openvino_core_obj)\n```\n\n----------------------------------------\n\nTITLE: Setting Shared Headers Directory (CMake)\nDESCRIPTION: Sets the directory containing shared header files used by the unit tests. This directory is then included in the include paths during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/tests/unit/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(SHARED_HEADERS_DIR \"${OpenVINO_SOURCE_DIR}/src/tests/ie_test_util\")\n```\n\n----------------------------------------\n\nTITLE: Building C/C++ OpenVINO Samples (Linux)\nDESCRIPTION: This script builds the C and C++ sample applications for Linux. It navigates to the samples directory and executes the `build_samples.sh` script which handles the compilation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nbuild_samples.sh\n```\n\n----------------------------------------\n\nTITLE: EmbeddingBagOffsets Example 3 (XML)\nDESCRIPTION: This XML snippet provides an example configuration for the EmbeddingBagOffsets operation with the reduction attribute set to \"mean\". It showcases a scenario where the average of the values in the bag is computed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sparse/embedding-bag-offsets-15.rst#_snippet_3\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"EmbeddingBagOffsets\" ... >\n       <data reduction=\"mean\"/>\n       <input>\n           <port id=\"0\">     <!-- emb_table value is: [[-0.2, -0.6], [-0.1, -0.4], [-1.9, -1.8], [-1.,  1.5], [ 0.8, -0.7]] -->\n               <dim>5</dim>\n               <dim>2</dim>\n           </port>\n           <port id=\"1\">     <!-- indices value is: [0, 2, 3, 4] -->\n               <dim>4</dim>\n           </port>\n           <port id=\"2\">     <!-- offsets value is: [0, 2, 2] - 3 \"bags\" containing [2,0,4-2] elements, second \"bag\" is empty -->\n               <dim>3</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"3\">     <!-- output value is: [[-1.05, -1.2], [0., 0.], [-0.1, 0.4]] -->\n               <dim>3</dim>\n               <dim>2</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Broadcast with Bidirectional Mode in XML\nDESCRIPTION: This XML snippet demonstrates the Broadcast operation with the 'bidirectional' mode. It broadcasts a tensor of shape [16, 1, 1] to [1, 16, 50, 50]. The third input is not provided when mode is set to 'bidirectional'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/broadcast-3.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Broadcast\" ...>\n    <data mode=\"bidirectional\"/>\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n       </port>\n        <port id=\"1\">\n            <dim>4</dim>   <!--The tensor contains 4 elements: [1, 1, 50, 50] -->\n        </port>\n        <!-- the 3rd input shouldn't be provided with mode=\"bidirectional\" -->\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>16</dim>\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Protopipe Validation Mode Command\nDESCRIPTION: This command executes Protopipe in 'validation' mode to perform accuracy validation using a specified configuration file (config.yaml). The `-t` parameter sets the time limit for validation to 15 seconds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n./protopipe --cfg config.yaml --mode validation -t 15\n```\n\n----------------------------------------\n\nTITLE: VariadicSplit XML Example 1\nDESCRIPTION: This XML example demonstrates the use of the VariadicSplit operation with a split along axis 0. The input tensor with dimensions [6, 12, 10, 24] is split into three output tensors with dimensions [1, 12, 10, 24], [2, 12, 10, 24], and [3, 12, 10, 24] respectively, based on the split_lengths input [1, 2, 3].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/variadic-split-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"VariadicSplit\" ...>\n    <input>\n        <port id=\"0\">            <!-- some data -->\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">            <!-- axis: 0 -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim>         <!-- split_lengths: [1, 2, 3] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"4\">\n            <dim>2</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"5\">\n            <dim>3</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Unique Operation without Axis Input - XML Configuration\nDESCRIPTION: This XML snippet demonstrates the Unique operation without an axis input. The operation finds unique elements in the flattened input tensor. It showcases the input and output port configurations including their precision and dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/unique-10.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Unique\" ... >\n    <input>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"FP32\">\n            <dim>-1</dim>\n        </port>\n        <port id=\"2\" precision=\"I64\">\n            <dim>-1</dim>\n        </port>\n        <port id=\"3\" precision=\"I64\">\n            <dim>9</dim>\n        </port>\n        <port id=\"4\" precision=\"I64\">\n            <dim>-1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: AvgPool Layer Configuration with valid auto_pad\nDESCRIPTION: This XML snippet shows the configuration of an AvgPool layer with `auto_pad` set to `valid`. The `kernel` size is 5x5, `pads_begin` and `pads_end` are set to 1x1, and the stride is 2x2.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/avg-pool-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"AvgPool\" ... >\n    <data auto_pad=\"valid\" exclude-pad=\"true\" kernel=\"5,5\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>14</dim>\n            <dim>14</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files (CMake)\nDESCRIPTION: This snippet uses `file(GLOB_RECURSE)` to find all C++ source files in the `src` directory and all header files in the `include` directory within the root directory. These files are then stored in the `LIBRARY_SRC` and `LIBRARY_HEADERS` variables, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC ${root_dir}/src/*.cpp)\nfile(GLOB_RECURSE LIBRARY_HEADERS ${root_dir}/include/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Enable DX12 Support (Windows)\nDESCRIPTION: Enables DirectX 12 support on Windows if the `ENABLE_DX12` variable is set. This adds a compile definition and links the necessary DirectX libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/functional/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n    if(ENABLE_DX12)\n        target_compile_definitions(${TARGET_NAME} PRIVATE ENABLE_DX12)\n        target_link_libraries(${TARGET_NAME} PRIVATE d3d12 dxcore)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Dynamic Shape Test Definition C++\nDESCRIPTION: This code illustrates how to define dynamic shape tests to verify the correct behavior of the runtime parameters cache.  It sets up different shapes for the inputs, including repeating shapes to test the cache effectively. Note that identical shapes right after each other will not trigger the cache due to shape optimization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/runtime_parameters_cache.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n{\n    {{-1, -1, -1}, {{10, 10, 10}, {5, 5, 5}, {10, 10, 10}}}, // input 0\n    {{-1, -1, 5}, {{10, 10, 5}, {5, 5, 5}, {10, 10, 5}}}  // input 1\n},\n```\n\n----------------------------------------\n\nTITLE: Install Requirements (Windows)\nDESCRIPTION: This command installs all the required Python packages listed in the `requirements.txt` file. The packages are installed within the activated virtual environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Adding Include Directories for RapidJSON (CMake)\nDESCRIPTION: This snippet adds the interface include directories from the `rapidjson` target to the target library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PRIVATE $<TARGET_PROPERTY:rapidjson,INTERFACE_INCLUDE_DIRECTORIES>)\n```\n\n----------------------------------------\n\nTITLE: Filter Tests with -k Flag Shell\nDESCRIPTION: Filters tests to run only those matching the specified keyword using the `-k` flag of `pytest`. In this case it will run tests that contains \"test_available_devices\" in their name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npytest tests/test_runtime/test_core.py -k \"test_available_devices\"\n```\n\n----------------------------------------\n\nTITLE: ONNX: Quantize Model\nDESCRIPTION: Quantizes the ONNX model using the specified calibration dataset.  The `nncf.quantize` function applies 8-bit quantization, optimizing the model for deployment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nquantized_model = nncf.quantize(model=model, calibration_dataset=dataset)\n\n```\n\n----------------------------------------\n\nTITLE: Adding CPack Component for Requirements Files in CMake\nDESCRIPTION: This CMake snippet adds a CPack component for OpenVINO's requirement files.  It defines `${OV_CPACK_COMP_OPENVINO_REQ_FILES}` as a hidden component, which is presumably for internal packaging purposes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nov_cpack_add_component(${OV_CPACK_COMP_OPENVINO_REQ_FILES} HIDDEN)\n```\n\n----------------------------------------\n\nTITLE: Adding Version Defines in CMake\nDESCRIPTION: This uses `ov_add_version_defines` to inject version information into the compiled binary. It reads version information from `src/plugins/auto/src/plugin.cpp` and makes it available to the `ov_auto_unit_tests` target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/unit/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_version_defines(${OpenVINO_SOURCE_DIR}/src/plugins/auto/src/plugin.cpp ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Creating Directory for OpenVINO Installation in macOS\nDESCRIPTION: This command creates the `/opt/intel` directory, the recommended location for OpenVINO installation. If the directory already exists, the command can be skipped.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-macos.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nsudo mkdir /opt/intel\n```\n\n----------------------------------------\n\nTITLE: GreaterEqual Layer Configuration (Numpy Broadcast) XML\nDESCRIPTION: This example demonstrates the configuration of a GreaterEqual layer using numpy broadcasting. The input tensors have shapes 8x1x6x1 and 7x1x5, respectively. The output tensor has a shape of 8x7x6x5, which is the result of numpy broadcasting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/greater-equal-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"GreaterEqual\">\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: getDimensions Method Definition (TypeScript)\nDESCRIPTION: Defines the `getDimensions` method of the `PartialShape` interface, which returns an array of `Dimension` objects. This method allows access to the individual dimensions of the partial shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PartialShape.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ngetDimensions(): Dimension\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties CMake\nDESCRIPTION: This snippet sets the target properties for 'openvino_cnpy', specifically setting the FOLDER property to 'thirdparty', which likely organizes the library within the IDE or build system's project structure.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/cnpy/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES FOLDER thirdparty)\n```\n\n----------------------------------------\n\nTITLE: Defining Proxy Plugin\nDESCRIPTION: Conditionally adds the `PROXY_PLUGIN_ENABLED` definition to the compile definitions of the target if `ENABLE_PROXY` is enabled. This enables proxy plugin functionality during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/functional/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_PROXY)\n    target_compile_definitions(${TARGET_NAME} PUBLIC PROXY_PLUGIN_ENABLED)\nendif()\n```\n\n----------------------------------------\n\nTITLE: CMake Build with Emscripten\nDESCRIPTION: This snippet builds the OpenVINO project using the `emmake` command, which is also specific to Emscripten. It uses the `make` command with parallel compilation enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_webassembly.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n$ emmake make -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Configure YAML-CPP Library (CMake)\nDESCRIPTION: This snippet adds the YAML-CPP library as a subdirectory, disables shared library building, and suppresses a specific compiler warning when `ENABLE_INTEL_NPU_PROTOPIPE` is enabled and `SUGGEST_OVERRIDE_SUPPORTED` is true.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/thirdparty/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_INTEL_NPU_PROTOPIPE)\n    set(YAML_BUILD_SHARED_LIBS OFF)\n    add_subdirectory(yaml-cpp EXCLUDE_FROM_ALL)\n    # NB: Suppress warnings in yaml-cpp\n    if(SUGGEST_OVERRIDE_SUPPORTED)\n        target_compile_options(yaml-cpp PRIVATE -Wno-suggest-override)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Source Subdirectory CMake\nDESCRIPTION: This snippet adds the src subdirectory to the build process. This is where the main source code for the NPU plugin resides.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: XML IR Snippet Example\nDESCRIPTION: This XML snippet demonstrates how to specify the IR version and operation set version within an OpenVINO Intermediate Representation (IR) file. The 'version' attribute in the '<net>' tag indicates the overall IR version, while the 'version' attribute in the '<layer>' tag specifies the operation set version for that particular layer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<?xml version=\"1.0\" ?>\n<net name=\"model_file_name\" version=\"10\">  <!-- Version of the whole IR file is here; it is 10 -->\n    <layers>\n        <!-- Version of operation set that the layer belongs to is described in <layer>\n            tag attributes. For this operation, it is version=\"opset1\". -->\n        <layer id=\"0\" name=\"input\" type=\"Parameter\" version=\"opset1\">\n            <data element_type=\"f32\" shape=\"1,3,32,100\"/> <!-- attributes of operation -->\n            <output>\n                <!-- description of output ports with type of element and tensor dimensions -->\n                <port id=\"0\" precision=\"FP32\">\n                    <dim>1</dim>\n                    <dim>3</dim>\n\n                     ...\n\n```\n\n----------------------------------------\n\nTITLE: Convolution Test Parameters C++\nDESCRIPTION: This C++ code defines a struct `convolution_test_params` to store the parameters for convolution fusion tests, including input/output shapes, kernel size, stride, padding, dilation, groups, data types, and formats. It also defines a macro `CASE_CONV_FP32_2` to initialize the struct with specific values, excluding the expected fused and non-fused primitives. The `INSTANTIATE_TEST_SUITE_P` macro is used to create a test suite with specific parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_unit_test.md#_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\nstruct convolution_test_params {\n    tensor in_shape;\n    tensor out_shape;\n    tensor kernel;\n    tensor stride;\n    tensor pad;\n    tensor dilation;\n    uint32_t groups;\n    data_types data_type;\n    format input_format;\n    data_types weights_type;\n    format weights_format;\n    data_types default_type;\n    format default_format;\n    size_t expected_fused_primitives;\n    size_t expected_not_fused_primitives;\n};\n\n\n// in_shape; out_shape; kernel; stride; pad; dilation; groups; data_type; input_format; weights_type; weights_format; default_type; default_format;\n#define CASE_CONV_FP32_2 { 1, 16, 4, 5 }, { 1, 32, 2, 3 }, { 1, 1, 3, 3 }, tensor{ 1 }, tensor{ 0 }, tensor{ 1 }, 1, data_types::f32, format::b_fs_yx_fsv16, data_types::f32, format::os_is_yx_isv16_osv16, data_types::f32, format::bfyx\n\n\nINSTANTIATE_TEST_SUITE_P(fusings_gpu, conv_fp32_scale, ::testing::ValuesIn(std::vector<convolution_test_params>{\n    convolution_test_params{ CASE_CONV_FP32_2, 2, 3 }, // CASE_CONV_FP32_2, # of fused executed primitives, # of non fused networks\n    convolution_test_params{ CASE_CONV_FP32_3, 2, 3 },\n}));\n```\n\n----------------------------------------\n\nTITLE: Einsum with Ellipsis Example 2 C++\nDESCRIPTION: This example shows how Einsum operates on a single input with an equation containing an ellipsis.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/einsum-7.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nA = [[1.0, 2.0, 3.0],\n     [4.0, 5.0, 6.0],\n     [7.0, 8.0, 9.0]]\nequation = \"a...->a\"\nalternative_equation = \"...a->...\"\noutput = [6.0, 15.0, 24.0]\n```\n\n----------------------------------------\n\nTITLE: Dependency List\nDESCRIPTION: This section outlines the dependencies for the OpenVINO project, specifying the name and version constraints of each package. It ensures the project has the required libraries for Python bindings, testing, and frontend support for various frameworks like TensorFlow, ONNX, and PaddlePaddle.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/constraints.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nnumpy>=1.16.6,<2.3.0  # Python bindings, frontends\n\n# pytest\npytest>=5.0,<8.4\npytest-dependency==0.6.0\npytest-html==4.1.1\npytest-timeout==2.3.1\n\n# Python bindings\nbuild<1.3\npygments>=2.8.1\nsetuptools>=70.1,<79.1\nsympy>=1.10\nwheel>=0.38.1\npatchelf<=0.17.2.1\npackaging>=22.0\n\n# Frontends\nh5py>=3.1.0,<3.14.0\ndocopt~=0.6.2\npaddlepaddle==2.6.2\ntensorflow>=1.15.5,<2.18.0\nprotobuf>=3.18.1,<7.0.0\nonnx==1.17.0\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO™ Security Add-on on Host\nDESCRIPTION: Navigates to the directory defined by OVSA_RELEASE_PATH, unpacks the ovsa-kvm-host.tar.gz archive, navigates into the unpacked directory, and executes the install.sh script to install the OpenVINO™ Security Add-on software.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_30\n\nLANGUAGE: sh\nCODE:\n```\ncd $OVSA_RELEASE_PATH\ntar xvfz ovsa-kvm-host.tar.gz\ncd ovsa-kvm-host\n./install.sh\n```\n\n----------------------------------------\n\nTITLE: OneHot Layer Definition Example 2 XML\nDESCRIPTION: This XML code defines another OneHot layer in OpenVINO. It sets the axis attribute to 1. The input ports define the indices, depth, on_value, and off_value, similar to the first example. This example illustrates the flexibility in specifying the axis along which the one-hot encoding is applied. The output shows how the dimensions are modified by the OneHot operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/one-hot-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"OneHot\" ...>\n    <data axis=\"1\"/>\n    <input>\n        <port id=\"0\">    <!-- indices value: [[0, 3, 1], [1, 2, 4]] -->\n            <dim>2</dim>\n            <dim>3</dim>\n        </port>\n        <port id=\"1\">    <!-- depth value: 3 -->\n        </port>\n        <port id=\"2\">    <!-- on_value 1 -->\n        </port>\n        <port id=\"3\">    <!-- off_value 0 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">    <!-- output value: [[[1, 0, 0], [0, 0, 1], [0, 0, 0]], -->\n            <dim>2</dim> <!--                [[0, 0, 0], [1, 0, 0], [0, 1, 0]]] -->\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Get Device Name Implementation C++\nDESCRIPTION: Implements the get_device_name() method, which returns the device name associated with the remote context. This allows querying the device name from the remote context object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/remote-context.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nstd::string TemplateRemoteContext::get_device_name() const {\n    return m_name;\n}\n```\n\n----------------------------------------\n\nTITLE: Slicing 3D Tensor with Last Axes Default XML\nDESCRIPTION: This XML snippet shows a Slice layer configuration for a 3D tensor where the last axis is defaulted. Only the start, stop, step, and axes for the first two dimensions are explicitly defined. The input is a 20x10x5 tensor, and the output remains 4x10x5, implying the last axis is not sliced.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-8.rst#_snippet_11\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Slice\" ...>\n       <input>\n           <port id=\"0\">       <!-- data -->\n             <dim>20</dim>\n             <dim>10</dim>\n             <dim>5</dim>\n           </port>\n           <port id=\"1\">       <!-- start: [0, 0] -->\n             <dim>2</dim>\n           </port>\n           <port id=\"2\">       <!-- stop: [4, 10] -->\n             <dim>2</dim>\n           </port>\n           <port id=\"3\">       <!-- step: [1, 1] -->\n             <dim>2</dim>\n           </port>\n           <port id=\"4\">       <!-- axes: [0, 1] -->\n             <dim>2</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">       <!-- output -->\n               <dim>4</dim>\n               <dim>10</dim>\n               <dim>5</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Warnings as Errors for GNU/Clang/IntelLLVM CMake\nDESCRIPTION: This snippet sets -Werror flag, which treats warnings as errors for GNU, Clang or IntelLLVM compilers if CMAKE_COMPILE_WARNING_AS_ERROR is enabled.  It also handles removing compiler specific flags for Intel Compiler related to diagnostic messages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILE_WARNING_AS_ERROR)\n    if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"IntelLLVM\")\n        remove_compiler_flags(-Wno-error=deprecated-declarations)\n        ov_add_compiler_flags(-Werror)\n\n    elseif(CMAKE_CXX_COMPILER_ID STREQUAL \"Intel\")\n        if(WIN32)\n            remove_compiler_flags(/Qdiag-disable:1478,1786)\n        else()\n            remove_compiler_flags(-diag-disable=1478,1786)\n        endif()\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for Unit Test Target in CMake\nDESCRIPTION: This snippet sets the include directories for the unit test target, including the source directory of openvino_core_obj and the current source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PRIVATE $<TARGET_PROPERTY:openvino_core_obj,SOURCE_DIR>/src\n                                                  ${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Linking Library Dependencies in CMake\nDESCRIPTION: This code snippet links the `openvino_offline_transformations` library against other OpenVINO libraries. It uses `target_link_libraries` to link against `openvino::core::dev`, `openvino::reference`, and `openvino::runtime` as private dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/offline_transformations/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino::core::dev openvino::reference openvino::runtime)\n```\n\n----------------------------------------\n\nTITLE: Install Python (x86_64)\nDESCRIPTION: This snippet shows how to find available Python versions using `brew search` and install a specific version (e.g., 3.11) using `brew install` on x86_64 macOS systems. It is a prerequisite for building the OpenVINO Runtime Python API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n% # let's have a look what python versions are available in brew\n% brew search python\n% # select preferred version of python based on available ones, e.g. 3.11\n% brew install python@3.11\n```\n\n----------------------------------------\n\nTITLE: Set Interprocedural Optimization\nDESCRIPTION: Sets the `INTERPROCEDURAL_OPTIMIZATION_RELEASE` property for the target to the value of `ENABLE_LTO`. This enables or disables link-time optimization (LTO) for release builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Extension Class Entry Point\nDESCRIPTION: This snippet demonstrates how to define an entry point to a library with OpenVINO Extensions using the OPENVINO_CREATE_EXTENSIONS() macro.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// [ov_extension:entry_point]\nOPENVINO_CREATE_EXTENSIONS(std::vector<ov::Extension::Ptr>{\n    std::make_shared<Identity>(),\n    std::make_shared<FrontEnd>(),\n});\n// [ov_extension:entry_point]\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties for LTO\nDESCRIPTION: This snippet sets the `INTERPROCEDURAL_OPTIMIZATION_RELEASE` property for the target, enabling Link Time Optimization (LTO) if `ENABLE_LTO` is set.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common_translators/src/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES\n    INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Free Output Port in OpenVINO (C)\nDESCRIPTION: This function releases the memory occupied by an output port object. It takes a pointer to the `ov_output_port_t` structure as input. This function does not return any value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_40\n\nLANGUAGE: C\nCODE:\n```\nvoid ov_output_port_free(ov_output_port_t* port)\n```\n\n----------------------------------------\n\nTITLE: Uninstalling OpenVINO - Removing Symbolic Link\nDESCRIPTION: This command removes the symbolic link `openvino_2025` from the `/opt/intel` directory, if it exists, as part of the OpenVINO uninstallation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-macos.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nsudo rm /opt/intel/openvino_2025\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags for MSVC\nDESCRIPTION: If the compiler is MSVC, this line adds the '/wd5051' flag to suppress warning 5051. This is a compiler-specific flag.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    ov_add_compiler_flags(/wd5051)\nendif()\n```\n\n----------------------------------------\n\nTITLE: isStatic Method Definition (TypeScript)\nDESCRIPTION: Defines the `isStatic` method of the `PartialShape` interface, which returns a boolean indicating whether the shape is static. A static shape has all dimensions fixed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PartialShape.rst#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nisStatic(): boolean\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for Paddle Frontend (CMake)\nDESCRIPTION: This snippet conditionally adds the 'paddle' subdirectory to the build if the ENABLE_OV_PADDLE_FRONTEND CMake option is enabled. This includes the Paddle frontend functionality.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_OV_PADDLE_FRONTEND)\n    add_subdirectory(paddle)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: This snippet sets the include directories for the target, defining both build and install interfaces.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}>\n                                                 $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/ops/>\n                                                 $<INSTALL_INTERFACE:developer_package/include/${TARGET_NAME}>)\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target (CMake)\nDESCRIPTION: This snippet adds a clang-format target for the `openvino_tensorflow_common` library. This allows for easy formatting of the code using clang-format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/src/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Upgrade PIP (Linux/macOS)\nDESCRIPTION: This command upgrades pip, wheel and setuptools to the latest versions within the virtual environment. Upgrading ensures the latest package management features.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_15\n\nLANGUAGE: console\nCODE:\n```\npython -m pip install --upgrade pip\npip install wheel setuptools\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target (CMake)\nDESCRIPTION: This snippet adds a custom target for running clang-format on the source code of the library. This helps maintain consistent code formatting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/shape_inference/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Install Pre-release OpenVINO Tokenizers\nDESCRIPTION: Installs a pre-release version of OpenVINO and OpenVINO Tokenizers from the OpenVINO nightly builds repository. This allows access to the latest features and bug fixes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npip install --pre -U openvino openvino-tokenizers --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n```\n\n----------------------------------------\n\nTITLE: Convert Tokenizer using CLI\nDESCRIPTION: Converts a tokenizer to OpenVINO Intermediate Representation (IR) using a CLI tool. The model ID specifies the Hugging Face model to convert, and the output is saved to the tokenizer directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n!convert_tokenizer $model_id --with-detokenizer -o tokenizer\n```\n\n----------------------------------------\n\nTITLE: OVSA Runtime Artefacts Directory Creation\nDESCRIPTION: Creates and navigates to the OVSA runtime artefacts directory, setting up the necessary environment variables using `setupvars.sh`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_47\n\nLANGUAGE: sh\nCODE:\n```\nmkdir -p ~/OVSA/artefacts\ncd ~/OVSA/artefacts\nexport OVSA_RUNTIME_ARTEFACTS=$PWD\nsource /opt/ovsa/scripts/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: Project Setup and Minimum CMake Version\nDESCRIPTION: This snippet initializes the CMake project named 'openvino_fuzzing' and sets the minimum required CMake version to 3.13.  It is the very beginning of the CMake configuration file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nproject(openvino_fuzzing)\n\ncmake_minimum_required(VERSION 3.13)\n```\n\n----------------------------------------\n\nTITLE: Building Target Faster in CMake\nDESCRIPTION: This snippet uses the `ov_build_target_faster` macro to configure the target for faster compilation. It uses the UNITY build system to improve build times.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/api_conformance_runner/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME} UNITY)\n```\n\n----------------------------------------\n\nTITLE: ITT Target Configuration CMake\nDESCRIPTION: This snippet configures the `ittnotify` target. It defines a compile definition to enable ITT profiling. If the compiler is GCC or Clang, it disables a specific warning.  It also defines the interface include directories for the `ittnotify` target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ittapi/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(ittnotify INTERFACE ENABLE_PROFILING_ITT)\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG)\n    target_compile_options(ittnotify PRIVATE -Wno-undef)\nendif()\n\n# override INTERFACE_INCLUDE_DIRECTORIES\nset_property(TARGET ittnotify PROPERTY INTERFACE_INCLUDE_DIRECTORIES\n                $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/ittapi/src/ittnotify>\n                $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/ittapi/include>)\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO™ Model Server Docker Images\nDESCRIPTION: Clones the OpenVINO™ Model Server repository, navigates to the directory, and builds the Docker images using the `make docker_build` command.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_26\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/openvinotoolkit/model_server.git\ncd model_server\nmake docker_build\n```\n\n----------------------------------------\n\nTITLE: Define ov_core_version struct in C\nDESCRIPTION: This struct represents the version information for a specific device within the OpenVINO core. It contains the device name and a nested `ov_version_t` struct.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_1\n\nLANGUAGE: C\nCODE:\n```\ntypedef struct {\n\n    const char* device_name;\n\n    ov_version_t version;\n\n} ov_core_version_t;\n```\n\n----------------------------------------\n\nTITLE: Disabling Graph Transformations\nDESCRIPTION: This environment variable disables specified graph transformations. This is useful for debugging and comparing performance with and without certain transformations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nOV_CPU_DISABLE=\"transformations=common,preLpt,lpt,postLpt,snippets,specific\"\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries in CMake\nDESCRIPTION: Links the `mock_engine` library against the `openvino::runtime::dev` library. This makes the symbols and functions available in `openvino::runtime::dev` available to the mock engine.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/unit_test_utils/mocks/mock_engine/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino::runtime::dev)\n```\n\n----------------------------------------\n\nTITLE: Checking Available Devices - C++\nDESCRIPTION: This C++ snippet shows how to retrieve a list of available devices using the `get_available_devices()` method of the `Core` class in OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nov::runtime::Core::get_available_devices()\n```\n\n----------------------------------------\n\nTITLE: Setting include directories\nDESCRIPTION: Sets the include directories for the target.  Uses a generator expression to define the include directory for the build interface. `NPU_UTILS_SOURCE_DIR` is assumed to be a variable defined elsewhere.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/logger/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}\n    PUBLIC\n        $<BUILD_INTERFACE:${NPU_UTILS_SOURCE_DIR}/include>\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target in CMake\nDESCRIPTION: This snippet adds a clang-format target for the object library `${TARGET_NAME}_obj`. This allows for easy code formatting using clang-format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME}_obj)\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries\nDESCRIPTION: This snippet links the `OpenCL::OpenCL`, `openvino::itt`, and `openvino::runtime::dev` libraries to the target library as private dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE\n    OpenCL::OpenCL\n    openvino::itt\n    openvino::runtime::dev\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining Standalone Project Requirements in CMake\nDESCRIPTION: This snippet defines the minimum CMake version required based on the platform (Windows or other). It then sets up a standalone project named 'protopipe_standalone' and includes a standalone CMake configuration file. The `return()` statement stops execution of the current CMake script, indicating that the project is being built in standalone mode.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT DEFINED PROJECT_NAME)\n    if(WIN32)\n        cmake_minimum_required(VERSION 3.16)\n    else()\n        cmake_minimum_required(VERSION 3.13)\n    endif()\n    project(protopipe_standalone)\n    include(\"cmake/standalone.cmake\")\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Include Directories in CMake\nDESCRIPTION: This snippet sets the include directories for the test target using `target_include_directories`. It includes the directory containing the developer API headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/tests/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/../dev_api)\n```\n\n----------------------------------------\n\nTITLE: Tensor Constructor with Type and Shape\nDESCRIPTION: Constructs a tensor with the specified element type and shape. The tensor data is allocated by default. The type parameter specifies the element type, and the shape parameter specifies the dimensions of the tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/TensorConstructor.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nnew TensorConstructor(type, shape): Tensor\n```\n\n----------------------------------------\n\nTITLE: Project Definition with Description\nDESCRIPTION: This snippet defines the CMake project name and provides a description.  It sets the name of the project to 'OpenVINOPython' and adds a description for clarity.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nproject(OpenVINOPython DESCRIPTION \"OpenVINO Runtime Python bindings\")\n```\n\n----------------------------------------\n\nTITLE: Fix code style issues with ESLint\nDESCRIPTION: Automatically fixes many of the code style issues detected by ESLint in a specified file. This command runs ESLint with the fix option on a single file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/test_examples.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpx eslint --fix \"tests/unit/core.test.js\"\n```\n\n----------------------------------------\n\nTITLE: Define ov_core_version_list struct in C\nDESCRIPTION: This struct represents a list of `ov_core_version_t` structs, along with its size. It is used to return version information for multiple devices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_2\n\nLANGUAGE: C\nCODE:\n```\ntypedef struct {\n\n    ov_core_version_t* versions;\n\n    size_t size;\n\n} ov_core_version_list_t;\n```\n\n----------------------------------------\n\nTITLE: Define Model isDynamic Method (TypeScript)\nDESCRIPTION: This code defines the `isDynamic` method of the `Model` interface. It returns true if any of the ops defined in the model contains a partial shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nisDynamic(): boolean\n```\n\n----------------------------------------\n\nTITLE: Building ITT collector with CMake\nDESCRIPTION: This snippet shows how to build the ITT collector library using CMake. The ITT collector is used to collect statistics during the analysis phase.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/conditional_compilation.md#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncmake --build .\ncmake --build . --target sea_itt_lib\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version\nDESCRIPTION: Specifies the minimum required version of CMake for the project. This ensures that the build system has the necessary features to process the CMakeLists.txt file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions for OpenCL CMake\nDESCRIPTION: This snippet sets compile definitions for the OpenCL target, enabling the use of CL/opencl.hpp and checking for UUID support.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ocl/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(OpenCL INTERFACE OV_GPU_USE_OPENCL_HPP\n                                            OV_GPU_OPENCL_HPP_HAS_UUID)\n```\n\n----------------------------------------\n\nTITLE: get_runtime_model() Implementation C++\nDESCRIPTION: This code shows the implementation of the `get_runtime_model()` method. This function retrieves the runtime model containing backend-specific information.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/compiled-model.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nstd::shared_ptr<const ov::Model> CompiledModel::get_runtime_model() const {\n    return m_model;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining u64 data type in TypeScript\nDESCRIPTION: Defines the u64 data type as a number in TypeScript. This represents an unsigned 64-bit integer, providing the largest range for non-negative integer values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/element.rst#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nu64: number\n```\n\n----------------------------------------\n\nTITLE: BitwiseOr Example with Boolean Tensors in Python\nDESCRIPTION: This example demonstrates the BitwiseOr operation with boolean tensors. It performs a logical OR operation on corresponding elements of two boolean input tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-or-13.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# For given boolean inputs:\na = [True, False, False]\nb = [True, True, False]\n# Perform logical OR operation same as in LogicalOr operator:\noutput = [True, True, False]\n```\n\n----------------------------------------\n\nTITLE: Element Property Declaration\nDESCRIPTION: Declares the element property within the NodeAddon interface. This property provides access to the enum that defines element types supported by OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/addon.rst#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nelement: typeof element\n```\n\n----------------------------------------\n\nTITLE: Run benchmark (C++)\nDESCRIPTION: This command executes the throughput_benchmark application with a specified model file path. The application will perform the benchmark and report the throughput.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/throughput-benchmark.rst#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nthroughput_benchmark ./models/googlenet-v1.xml\n```\n\n----------------------------------------\n\nTITLE: ExtractImagePatches Output Example 2 C++\nDESCRIPTION: This C++ snippet shows the output of the ExtractImagePatches operation with sizes=\"4,4\", strides=\"8,8\", rates=\"1,1\", and auto_pad=\"valid\". The resulting output shape is [1, 16, 1, 1].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/extract-image-patches-3.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n[[[[ 1]]\n\n [[ 2]]\n\n [[ 3]]\n\n [[ 4]]\n\n [[11]]\n\n [[12]]\n\n [[13]]\n\n [[14]]\n\n [[21]]\n\n [[22]]\n\n [[23]]\n\n [[24]]\n\n [[31]]\n\n [[32]]\n\n [[33]]\n\n [[34]]]]\n```\n\n----------------------------------------\n\nTITLE: EmbeddingBagOffsetsSum Example with default_index -1\nDESCRIPTION: This XML snippet shows another example of the EmbeddingBagOffsetsSum operation, this time with `default_index` set to -1. This indicates that empty bags should be filled with zeros. It also uses `per_sample_weights` to weight the embeddings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sparse/embedding-bag-offsets-sum-3.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"EmbeddingBagOffsets\" ... >\n       <input>\n           <port id=\"0\">     <!-- emb_table value is: [[-0.2, -0.6], [-0.1, -0.4], [-1.9, -1.8], [-1.,  1.5], [ 0.8, -0.7]] -->\n               <dim>5</dim>\n               <dim>2</dim>\n           </port>\n           <port id=\"1\">     <!-- indices value is: [0, 2, 3, 4] -->\n               <dim>4</dim>\n           </port>\n           <port id=\"2\">     <!-- offsets value is: [0, 2, 2] - 3 \"bags\" containing [2,0,4-2] elements, second \"bag\" is empty -->\n               <dim>3</dim>\n           </port>\n           <port id=\"3\"/>    <!-- default_index value is: -1 - fill empty bag with 0-->\n           <port id=\"4\"/>    <!-- per_sample_weights value is: [0.5, 0.5, 0.5, 0.5] -->\n               <dim>4</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">     <!-- output value is: [[-1.05, -1.2], [0., 0.], [-0.1, 0.4]] -->\n               <dim>3</dim>\n               <dim>2</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Applying CPU Plugin Workaround\nDESCRIPTION: This workaround addresses a segmentation fault issue encountered when compiling an OpenVINO model performing weight quantization on Intel® Core™ Ultra 200V processors. It involves setting an environment variable before running the application to specify the maximum CPU instruction set architecture.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/release-notes-openvino.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport DNNL_MAX_CPU_ISA=AVX2_VNNI\n```\n\n----------------------------------------\n\nTITLE: Removing Specific Source Files in CMake\nDESCRIPTION: Excludes `dllmain.cpp` from the list of source files when building on UNIX systems.  This is likely because `dllmain.cpp` is specific to Windows DLLs and not required/compatible on UNIX-like platforms.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/unit_test_utils/mocks/mock_engine/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(UNIX)\n    list(REMOVE_ITEM LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/dllmain.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Constant Mode Example (Positive Pads) - XML\nDESCRIPTION: XML example demonstrating the Pad operation in constant mode with positive padding. It specifies the input dimensions, padding amounts (pads_begin and pads_end), and a pad_value of 15.0.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_10\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Pad\" ...>\n    <data pad_mode=\"constant\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>40</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>     <!-- pads_begin = [0, 5, 2, 1]  -->\n        </port>\n        <port id=\"2\">\n            <dim>4</dim>     <!-- pads_end = [1, 0, 3, 7] -->\n        </port>\n        <port id=\"3\">\n                            <!-- pad_value = 15.0 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>2</dim>     <!-- 2 = 0 + 1 + 1 = pads_begin[0] + input.shape[0] + pads_end[0] -->\n            <dim>8</dim>     <!-- 8 = 5 + 3 + 0 = pads_begin[1] + input.shape[1] + pads_end[1] -->\n            <dim>37</dim>    <!-- 37 = 2 + 32 + 3 = pads_begin[2] + input.shape[2] + pads_end[2] -->\n            <dim>48</dim>    <!-- 48 = 1 + 40 + 7 = pads_begin[3] + input.shape[3] + pads_end[3] -->\n                            <!-- all new elements are filled with 15.0 value -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries in CMake\nDESCRIPTION: This snippet links the object library `${TARGET_NAME}_obj` with other OpenVINO libraries. It establishes dependencies for reference, itt, core::dev, and shape_inference components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME}_obj PRIVATE openvino::reference openvino::itt openvino::core::dev openvino::shape_inference)\n```\n\n----------------------------------------\n\nTITLE: RandomUniform Output Example 1 (C++)\nDESCRIPTION: This code block shows an example of the output from the RandomUniform operation when global_seed is set to 150, op_seed is set to 10, output_type is set to f32 and alignment is set to TENSORFLOW.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\ninput_shape = [3, 3]\noutput  = [[0.7011236  0.30539632 0.93931055]\\\n            [0.9456035   0.11694777 0.50770056]\\\n            [0.5197197   0.22727466 0.991374  ]]\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name and Source Files in CMake\nDESCRIPTION: This snippet defines the target name for the OpenVINO runtime library and uses GLOB to collect source files and headers from specified directories. It ensures all relevant source and header files are included for the library build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset (TARGET_NAME \"openvino_runtime\")\n\nfile (GLOB LIBRARY_SRC\n        ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/src/cpp/*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/src/dev/*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/src/dev/preprocessing/*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/src/dev/threading/*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/src/threading/*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/src/cpp/*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/src/cpp_interfaces/interface/*.cpp\n      )\n\nfile (GLOB LIBRARY_HEADERS\n       ${CMAKE_CURRENT_SOURCE_DIR}/src/*.h\n       ${CMAKE_CURRENT_SOURCE_DIR}/src/*.hpp\n      )\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions for Static Library (CMake)\nDESCRIPTION: This conditional block defines a compile definition `OPENVINO_STATIC_LIBRARY` if the `BUILD_SHARED_LIBS` option is not enabled. This allows different behavior for static library builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME} PUBLIC OPENVINO_STATIC_LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Multiply Layer Definition - Numpy Broadcast XML\nDESCRIPTION: This example illustrates a Multiply layer definition with numpy broadcasting enabled. The input tensors have different shapes and will be broadcast to a common shape before multiplication.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/multiply-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Multiply\">\n    <data auto_broadcast=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining Test Target with CMake\nDESCRIPTION: This CMake snippet defines a test target named `ov_snippets_func_tests`. It configures include directories, links required libraries (openvino::runtime::dev, common_test_utils, and ov_snippets_models), adds cpplint checks, and assigns labels for test categorization. The `ov_add_test_target` macro is used to simplify the target creation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_snippets_func_tests)\n\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        INCLUDES\n            ${CMAKE_CURRENT_SOURCE_DIR}/include\n            $<TARGET_PROPERTY:openvino::snippets,INTERFACE_INCLUDE_DIRECTORIES>\n        LINK_LIBRARIES\n            openvino::runtime::dev\n            common_test_utils\n            ov_snippets_models\n        ADD_CPPLINT\n        LABELS\n            OV UNIT SNIPPETS\n)\n```\n\n----------------------------------------\n\nTITLE: Cross-Compiled File Configuration\nDESCRIPTION: This snippet configures cross-compilation for a specific set of architecture files.  It defines the architecture (AVX2), API files, a list of function names, and the namespace for those functions.  It uses `cross_compiled_file` to handle differences based on the target architecture.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/plugin/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ncross_compiled_file(${TARGET_NAME}\n        ARCH AVX2 ANY\n                    npuw/util_xarch.cpp\n        API         npuw/util_xarch.hpp\n        NAME        unpack_i4i8 unpack_u4i8 unpack_i4f16 unpack_i4f16_scale unpack_i4f16_z unpack_u4f16 unpack_u4f16_scale_zp unpack_u4f16_asymm_zp unpack_u4f16_z unpack_u4f32 unpack_i8f16 unpack_i8f16_scale unpack_u8f16 to_f16 copy_row_as_column\n        NAMESPACE   ov::npuw::util::XARCH\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties CMake\nDESCRIPTION: Appends the `INTERFACE_INCLUDE_DIRECTORIES` property to the `ze_loader` target, making the specified include directory available to dependent targets. It uses a generator expression to ensure the directory is only used during the build interface.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/level_zero/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset_property(TARGET ze_loader APPEND PROPERTY INTERFACE_INCLUDE_DIRECTORIES $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/level-zero/include>)\n```\n\n----------------------------------------\n\nTITLE: Creating a New Job in GitHub Actions (YAML)\nDESCRIPTION: This example demonstrates the creation of a new job within a GitHub Actions workflow. It includes configurations such as the job's name, dependencies (needs), timeout, runner type (runs-on), container setup (including image, volumes, and options), environment variables, conditional execution (if), and the steps to be executed. This job is specifically designed for NVIDIA plugin tests, showcasing how to configure jobs for specific hardware and software environments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/adding_tests.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nNVIDIA_Plugin:\n  name: NVIDIA plugin\n  needs: [ Build, Smart_CI ]\n  timeout-minutes: 15\n  defaults:\n    run:\n      shell: bash\n  runs-on: aks-linux-16-cores-32gb\n  container:\n    image: openvinogithubactions.azurecr.io/dockerhub/nvidia/cuda:11.8.0-runtime-ubuntu20.04\n    volumes:\n      - /mount:/mount\n    options: -e SCCACHE_AZURE_BLOB_CONTAINER -e SCCACHE_AZURE_CONNECTION_STRING\n  env:\n    CMAKE_BUILD_TYPE: 'Release'\n    CMAKE_GENERATOR: 'Ninja Multi-Config'\n    CMAKE_CUDA_COMPILER_LAUNCHER: sccache\n    CMAKE_CXX_COMPILER_LAUNCHER: sccache\n    CMAKE_C_COMPILER_LAUNCHER: sccache\n    INSTALL_DIR: /__w/openvino/openvino/install\n    OPENVINO_DEVELOPER_PACKAGE: /__w/openvino/openvino/install/developer_package\n    OPENVINO_REPO: /__w/openvino/openvino/openvino\n    OPENVINO_CONTRIB_REPO: /__w/openvino/openvino/openvino_contrib\n    NVIDIA_BUILD_DIR: /__w/openvino/openvino/nvidia_plugin_build\n    DEBIAN_FRONTEND: 'noninteractive'\n    SCCACHE_AZURE_KEY_PREFIX: ubuntu20_x86_64_Release\n  if: fromJSON(needs.smart_ci.outputs.affected_components).NVIDIA\n\n  steps:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Change Directory to OpenVINO\nDESCRIPTION: Navigates to the cloned OpenVINO repository directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_raspbian.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd openvino/\n```\n\n----------------------------------------\n\nTITLE: Include Headers for Preprocessing & Saving - Python\nDESCRIPTION: Includes necessary libraries for working with OpenVINO preprocessing capabilities in Python. These libraries provide functionalities for reading models, defining preprocessing steps, and saving the modified model to an OpenVINO IR file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details/integrate-save-preprocessing-use-case.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom openvino.runtime import Core\nfrom openvino.runtime import Layout\nfrom openvino.preprocess import PrePostProcessor, ColorFormat\nfrom openvino.runtime import Model\nfrom openvino.runtime import serialize\n```\n\n----------------------------------------\n\nTITLE: ReorgYolo Layer XML Configuration\nDESCRIPTION: This XML snippet demonstrates the configuration of a ReorgYolo layer with a stride of 2. It defines the input and output ports, specifying their dimensions.  The input tensor has dimensions [1, 64, 26, 26], and the output tensor has dimensions [1, 256, 13, 13] with f32 precision.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/reorg-yolo-1.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer id=\"89\" name=\"reorg\" type=\"ReorgYolo\">\n    <data stride=\"2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>26</dim>\n            <dim>26</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"f32\">\n            <dim>1</dim>\n            <dim>256</dim>\n            <dim>13</dim>\n            <dim>13</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding a Dummy Fuzz Target in CMake\nDESCRIPTION: This snippet adds a custom target named 'fuzz'. This target acts as an aggregator for all the individual fuzz test targets, allowing for a single command to build or run all fuzz tests. This simplifies the build and execution process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(fuzz)\n```\n\n----------------------------------------\n\nTITLE: AvgPool Layer Configuration with same_upper and exclude-pad false\nDESCRIPTION: This XML snippet configures an AvgPool layer with `auto_pad` set to `same_upper` and `exclude-pad` set to `false`. The `kernel` size is 5x5, and the stride is 2x2.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/avg-pool-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"AvgPool\" ... >\n    <data auto_pad=\"same_upper\" exclude-pad=\"false\" kernel=\"5,5\" pads_begin=\"0,0\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Cos Layer Configuration XML OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of a Cos layer in OpenVINO. It shows the input and output ports with their dimensions, defining how the cosine operation is applied to the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/cos-1.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"Cos\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Serialize a Model to IR (C++)\nDESCRIPTION: This snippet demonstrates how to serialize an OpenVINO model to an Intermediate Representation (IR) file using C++. Serialization is useful for debugging purposes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n// [ov:serialize]\n```\n\n----------------------------------------\n\nTITLE: I420toRGB Layer Definition (Three Planes) XML\nDESCRIPTION: Defines an I420toRGB layer in XML format, illustrating the conversion from three separate planes (Y, U, V) to an RGB output. The Y plane input is 1x480x640x1, U and V planes are 1x240x320x1, and the output is 1x480x640x3.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/i420-to-rgb-8.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"I420toRGB\">\n       <input>\n           <port id=\"0\"> \t<!-- Y plane -->\n               <dim>1</dim>\n               <dim>480</dim>\n               <dim>640</dim>\n               <dim>1</dim>\n           </port>\n           <port id=\"1\"> \t<!-- U plane -->\n               <dim>1</dim>\n               <dim>240</dim>\n               <dim>320</dim>\n               <dim>1</dim>\n           </port>\n           <port id=\"2\"> \t<!-- V plane -->\n             <dim>1</dim>\n             <dim>240</dim>\n             <dim>320</dim>\n             <dim>1</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"1\">\n               <dim>1</dim>\n               <dim>480</dim>\n               <dim>640</dim>\n               <dim>3</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Building the ITT Collector\nDESCRIPTION: This snippet builds the ITT collector, required for generating the statistics file. This step must be performed before running the benchmark application to generate statistics. The resulting `libIntelSEAPI.so` library is used by the `sea_runtool.py` script.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/selective_build.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# in ./build\ncd ./thirdparty/itt_collector && make\n```\n\n----------------------------------------\n\nTITLE: AUGRUCell Formula in Python Style\nDESCRIPTION: This code block presents the mathematical formulas for the AUGRUCell operation, including the calculation of reset gate (rt), update gate (zt), hidden gate (ht), and the final hidden state (Ht). It clarifies the role of each gate and attention score in the computation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/internal/augru-cell.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nAUGRU formula:\n *  - matrix multiplication\n (.) - Hadamard product (element-wise)\n\n f, g - activation functions\n z - update gate, r - reset gate, h - hidden gate\n a - attention score\n\n rt = f(Xt*(Wr^T) + Ht-1*(Rr^T) + Wbr + Rbr)\n zt = f(Xt*(Wz^T) + Ht-1*(Rz^T) + Wbz + Rbz)\n ht = g(Xt*(Wh^T) + (rt (.) Ht-1)*(Rh^T) + Rbh + Wbh)  # 'linear_before_reset' is False\n\n zt' = (1 - at) (.) zt  # multiplication by attention score\n\n Ht = (1 - zt') (.) ht + zt' (.) Ht-1\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: This snippet sets the include directories for the target '${TARGET_NAME}'. It includes the current source directory, test utility directories, interface include directories from 'openvino_intel_gpu_kernels' and 'openvino_intel_gpu_runtime' targets, and the reference include directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}\n                                                  ${CMAKE_CURRENT_SOURCE_DIR}/test_utils/\n                                                  $<TARGET_PROPERTY:openvino_intel_gpu_kernels,INTERFACE_INCLUDE_DIRECTORIES>\n                                                  $<TARGET_PROPERTY:openvino_intel_gpu_runtime,INTERFACE_INCLUDE_DIRECTORIES>\n                                                  ${CMAKE_HOME_DIRECTORY}/src/core/reference/include/\n                                                  ${TEST_COMMON_INCLUDE_DIR})\n```\n\n----------------------------------------\n\nTITLE: ReduceL1 along negative axis in OpenVINO XML\nDESCRIPTION: Example of ReduceL1 operation in OpenVINO XML where keep_dims is false and reduction occurs along axis -2. The input tensor shape is 6x12x10x24 and the output tensor shape becomes 6x12x24.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-l1-4.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceL1\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [-2] that means independent reduction in each channel, batch and second spatial dimension -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Determine Platform Architecture - CMake\nDESCRIPTION: This CMake macro determines the platform architecture based on CMake variables like `AARCH64`, `UNIVERSAL2`, `ARM`, `X86_64`, `X86`, and `RISCV64`. It sets the `_arch` variable accordingly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nmacro(_ov_platform_arch)\n    if(AARCH64)\n        if(APPLE)\n            set(_arch \"arm64\")\n        else()\n            set(_arch \"aarch64\")\n        endif()\n    elseif(UNIVERSAL2)\n        set(_arch \"universal2\")\n    elseif(ARM)\n        set(_arch \"armv7l\")\n    elseif(X86_64)\n        set(_arch \"x86_64\")\n    elseif(X86)\n        set(_arch \"i686\")\n    elseif(RISCV64)\n        set(_arch \"riscv64\")\n    else()\n        message(FATAL_ERROR \"Unknown architecture: ${CMAKE_SYSTEM_PROCESSOR}\")\n    endif()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Adding Link Libraries\nDESCRIPTION: This snippet appends private link libraries, 'openvino::util' and 'openvino::runtime::dev', to the 'LINK_LIBRARIES_PRIVATE' list. These libraries are linked privately to the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/shared/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND LINK_LIBRARIES_PRIVATE\n        openvino::util\n        openvino::runtime::dev)\n```\n\n----------------------------------------\n\nTITLE: Get/Set Tensor by Name in Python\nDESCRIPTION: This Python code shows how to get and set tensors by name using `get_tensor(tensor_name)` and `set_tensor(tensor_name, tensor)`. Requires OpenVINO and a compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nie = Core()\nmodel = ie.read_model(\"model.xml\")\ncompiled_model = ie.compile_model(model, \"CPU\")\ninfer_request = compiled_model.create_infer_request()\n\ninput_tensor = infer_request.get_tensor(\"input_tensor_name\")\ninput_tensor.data[:] = np.random.normal(0, 1, input_tensor.shape)\n\ninfer_request.infer()\n\noutput_tensor = infer_request.get_tensor(\"output_tensor_name\")\nprint(output_tensor.data)\n```\n\n----------------------------------------\n\nTITLE: Protopipe Configuration Example (YAML)\nDESCRIPTION: This YAML configuration file demonstrates how to define multiple inference streams with network sequences and dependency graphs. It specifies model paths, input precision, device, and execution parameters such as target FPS and execution time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_dir:\n  local: C:\\workspace\\models\ndevice_name: NPU\ncompiler_type: MLIR\nlog_level: INFO\n\nmulti_inference:\n- input_stream_list:\n  - network:\n    - { name: A.xml, ip: FP16, il: NCHW, device: CPU }\n    - [{ name: B.xml, ip: FP16, op: FP16 }, { name: C.xml, ip: FP16, op: FP16 }]\n    - { name: D.xml, ip: FP16, op: FP16, config: { PEROFMRANCE_HINT: LATENCY } }\n    target_fps: 30\n    exec_time_in_secs: 15\n  - op_desc:\n    - { tag: E, path: E.onnx, framework: onnxrt, ep: { name: OV, device_type: NPU_U8 } }\n    - { tag: F, type: CPU, time_in_us: 5000 }\n    - { tag: G, path: G.xml, ip: FP16, op: FP16, priority: HIGH }\n    connections:\n    - [E, F, G]\n    target_fps: 100\n    exec_time_in_secs: 15\n```\n\n----------------------------------------\n\nTITLE: Warning as Errors CMake\nDESCRIPTION: Sets the `CMAKE_COMPILE_WARNING_AS_ERROR` option to `OFF`, preventing compiler warnings from being treated as errors. This is generally done to allow the build to proceed despite the presence of warnings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/level_zero/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_COMPILE_WARNING_AS_ERROR OFF)\n```\n\n----------------------------------------\n\nTITLE: CMake Project Setup\nDESCRIPTION: This CMake snippet sets the minimum required CMake version, defines the project name as 'e2e_tests', and configures the installation directory for test files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/e2e_tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\n\nproject(e2e_tests)\n\ninstall(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This snippet sets the target name for the unit tests to `ov_auto_unit_tests`.  This name will be used in subsequent CMake commands to refer to this build target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/unit/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_auto_unit_tests)\n```\n\n----------------------------------------\n\nTITLE: ExtractImagePatches Output Example 1 C++\nDESCRIPTION: This C++ snippet shows the output of the ExtractImagePatches operation with sizes=\"3,3\", strides=\"5,5\", rates=\"1,1\", and auto_pad=\"valid\" applied to an input image. The output shape is [1, 9, 2, 2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/extract-image-patches-3.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n[[[[ 1  6]\n   [51 56]]\n\n  [[ 2  7]\n   [52 57]]\n\n  [[ 3  8]\n   [53 58]]\n\n  [[11 16]\n   [61 66]]\n\n  [[12 17]\n   [62 67]]\n\n  [[13 18]\n   [63 68]]\n\n  [[21 26]\n   [71 76]]\n\n  [[22 27]\n   [72 77]]\n\n  [[23 28]\n   [73 78]]]]\n```\n\n----------------------------------------\n\nTITLE: ReduceLogicalAnd XML Example with axis [-2]\nDESCRIPTION: XML example illustrating ReduceLogicalAnd reducing along axis -2.  The 'keep_dims' attribute is 'false', so the dimension is removed from the output. The reduction is performed independently in the channel, batch and second spatial dimension\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-logical-and-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceLogicalAnd\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [-2] that means independent reduction in each channel, batch and second spatial dimension -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Add Tests Subdirectory Conditionally with CMake\nDESCRIPTION: Adds the 'tests' subdirectory to the project only if the CMake variable ENABLE_TESTS is set to true.  This allows conditional inclusion of tests during the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Deactivate Current Swap\nDESCRIPTION: Deactivates the current swap file on the system.  This is needed before modifying the swap size.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_raspbian.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo dphys-swapfile swapoff\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for PyTorch Frontend (CMake)\nDESCRIPTION: This snippet conditionally adds the 'pytorch' subdirectory to the build if the ENABLE_OV_PYTORCH_FRONTEND CMake option is enabled. This includes the PyTorch frontend functionality.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_OV_PYTORCH_FRONTEND)\n    add_subdirectory(pytorch)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting MSVC Compiler Flags\nDESCRIPTION: This snippet sets compiler flags specifically for the MSVC compiler. It disables some security warnings, disables asynchronous structured exception handling, enables large address awareness, and disables noisy warnings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    set (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -D_SCL_SECURE_NO_WARNINGS -D_CRT_SECURE_NO_WARNINGS\")\n    set (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /EHsc\") # no asynchronous structured exception handling\n    set (CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} /LARGEADDRESSAWARE\")\n\n    # disable some noisy warnings\n    set (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd4251 /wd4275 /wd4267 /wd4819\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Add Alias CMake\nDESCRIPTION: This snippet creates an alias for the `openvino_snippets` library, allowing it to be referenced as `openvino::snippets`. This simplifies linking and dependency management.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(openvino::snippets ALIAS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Defining ov_add_samples_to_build Function\nDESCRIPTION: This CMake function iterates through the provided directories, checks if they contain a CMakeLists.txt file, and adds them as subdirectories to the project if they do. It also supports skipping specific samples/demos based on the BUILD_SAMPLE_NAME variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ov_add_samples_to_build)\n    # check each passed sample subdirectory\n    foreach (dir ${ARGN})\n        if (IS_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/${dir})\n            # check if a subdirectory contains CMakeLists.txt. In this case we can build it.\n            file(GLOB is_sample_dir \"${CMAKE_CURRENT_SOURCE_DIR}/${dir}/CMakeLists.txt\")\n            if(is_sample_dir)\n                # check if specified sample/demo is found.\n                if (BUILD_SAMPLE_NAME)\n                    list(FIND BUILD_SAMPLE_NAME ${dir} index)\n                endif()\n                if (index EQUAL -1)\n                    message(STATUS \"${dir} SKIPPED\")\n                else()\n                    # Include subdirectory to the project.\n                    add_subdirectory(${dir})\n                endif()\n            endif()\n        endif()\n    endforeach()\nendfunction(ov_add_samples_to_build)\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Symmetric Mode Example (Positive Pads) - C++\nDESCRIPTION: Illustrates padding with the symmetric mode, similar to reflect, but values on the edges are duplicated. pads_begin and pads_end must be not greater than data.shape[D] for any valid D.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\npads_begin = [0, 1]\npads_end = [2, 3]\n\nDATA =\n[[1,  2,  3,  4]\n[5,  6,  7,  8]\n[9, 10, 11, 12]]\n\npad_mode = \"symmetric\"\n\nOUTPUT =\n[[ 1,  1,  2,  3,  4,  4,  3,  2 ]\n[ 5,  5,  6,  7,  8,  8,  7,  6 ]\n[ 9,  9, 10, 11, 12, 12, 11, 10 ]\n[ 9,  9, 10, 11, 12, 12, 11, 10 ]\n[ 5,  5,  6,  7,  8,  8,  7,  6 ]]\n```\n\n----------------------------------------\n\nTITLE: Conditional Compile Definitions CMake\nDESCRIPTION: This snippet conditionally adds compile definitions based on whether various frontends (IR, ONNX, Paddle, TF, TF Lite, PyTorch) are enabled. If a frontend is enabled, a corresponding compile definition is added to the COMPILE_DEFINITIONS list, affecting the build based on the frontends being used. For example, `ENABLE_OV_IR_FRONTEND`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/subgraphs_dumper/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_OV_IR_FRONTEND)\n    list(APPEND COMPILE_DEFINITIONS ENABLE_OV_IR_FRONTEND)\nendif()\n\nif(ENABLE_OV_ONNX_FRONTEND)\n    list(APPEND COMPILE_DEFINITIONS ENABLE_OV_ONNX_FRONTEND)\nendif()\n\nif(ENABLE_OV_PADDLE_FRONTEND)\n    list(APPEND COMPILE_DEFINITIONS ENABLE_OV_PADDLE_FRONTEND)\nendif()\n\nif(ENABLE_OV_TF_FRONTEND)\n    list(APPEND COMPILE_DEFINITIONS ENABLE_OV_TF_FRONTEND)\nendif()\n\nif(ENABLE_OV_TF_LITE_FRONTEND)\n    list(APPEND COMPILE_DEFINITIONS ENABLE_OV_TF_LITE_FRONTEND)\nendif()\n\nif(ENABLE_OV_PYTORCH_FRONTEND)\n    list(APPEND COMPILE_DEFINITIONS ENABLE_OV_PYTORCH_FRONTEND)\nendif()\n```\n\n----------------------------------------\n\nTITLE: IDFT Layer XML (5D, signal_size, unsorted axes, -1 in signal_size) - 2\nDESCRIPTION: Another XML configuration for IDFT layer with 5D input, signal_size (containing -1), and unsorted axes. This example provides a different configuration scenario for the same context as the previous snippet.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/idft-7.rst#_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>580</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>\t<!-- axes input contains  [3, 0, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim>\t<!-- signal_size input contains [258, -1, 2056] -->\n        </port>\n    <output>\n        <port id=\"3\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>2056</dim>\n            <dim>258</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: OVSA Example Client Copying\nDESCRIPTION: Copies the example client files from the OVSA installation directory to the user's OVMS directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_57\n\nLANGUAGE: sh\nCODE:\n```\ncd ~/OVSA/ovms\ncp /opt/ovsa/example_client/* .\n```\n\n----------------------------------------\n\nTITLE: Define ov_status enum in C\nDESCRIPTION: This enum defines the possible status codes returned by OpenVINO C API functions. It includes codes for success, general errors, unimplemented features, and various specific error conditions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_11\n\nLANGUAGE: C\nCODE:\n```\ntypedef enum {\n\n    OK = 0,  //!< SUCCESS\n\n    GENERAL_ERROR = -1,       //!< GENERAL_ERROR\n\n    NOT_IMPLEMENTED = -2,     //!< NOT_IMPLEMENTED\n\n    NETWORK_NOT_LOADED = -3,  //!< NETWORK_NOT_LOADED\n\n    PARAMETER_MISMATCH = -4,  //!< PARAMETER_MISMATCH\n\n    NOT_FOUND = -5,           //!< NOT_FOUND\n\n    OUT_OF_BOUNDS = -6,       //!< OUT_OF_BOUNDS\n\n    UNEXPECTED = -7,          //!< UNEXPECTED\n\n    REQUEST_BUSY = -8,        //!< REQUEST_BUSY\n\n    RESULT_NOT_READY = -9,    //!< RESULT_NOT_READY\n\n    NOT_ALLOCATED = -10,      //!< NOT_ALLOCATED\n\n    INFER_NOT_STARTED = -11,  //!< INFER_NOT_STARTED\n\n    NETWORK_NOT_READ = -12,   //!< NETWORK_NOT_READ\n\n    INFER_CANCELLED = -13,    //!< INFER_CANCELLED\n\n    INVALID_C_PARAM = -14,         //!< INVALID_C_PARAM\n\n    UNKNOWN_C_ERROR = -15,         //!< UNKNOWN_C_ERROR\n\n    NOT_IMPLEMENT_C_METHOD = -16,  //!< NOT_IMPLEMENT_C_METHOD\n\n    UNKNOW_EXCEPTION = -17,        //!< UNKNOW_EXCEPTION\n\n} ov_status_e;\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Target in CMake\nDESCRIPTION: Defines a custom CMake target named `memory_tests`. This target acts as an aggregator for all individual memory test targets, allowing for convenient execution of all memory tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/src/memory_tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(memory_tests)\n```\n\n----------------------------------------\n\nTITLE: CMake Plugin Definition\nDESCRIPTION: This snippet defines the core configurations for the OpenVINO template plugin. It sets the target name, includes source and header files, and conditionally skips installation and registration based on the ENABLE_TEMPLATE_REGISTRATION flag.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_template_plugin\")\n\nfile(GLOB_RECURSE SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp)\nfile(GLOB_RECURSE HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp)\n\nif (NOT ENABLE_TEMPLATE_REGISTRATION)\n    # Skip install and registration of template component\n    set(skip_plugin SKIP_INSTALL SKIP_REGISTRATION)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries (CMake)\nDESCRIPTION: This snippet links the `openvino_tensorflow_common` library to other OpenVINO libraries, specifically `openvino::util` and `openvino::frontend::common_translators` as private dependencies, and `openvino::core::dev` as a public dependency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/src/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino::util openvino::frontend::common_translators\n                                     PUBLIC openvino::core::dev)\n```\n\n----------------------------------------\n\nTITLE: Removing Compiler Flags CMake\nDESCRIPTION: This macro removes specific compiler flags from the CMAKE_CXX_FLAGS, CMAKE_C_FLAGS, CMAKE_CXX_FLAGS_RELEASE, and CMAKE_C_FLAGS_RELEASE variables. It's used to disable certain warnings or features during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nmacro(remove_compiler_flags FLAGS)\n    string(REPLACE \" \" \";\" _tokened_list \"${FLAGS}\")\n    foreach(flag IN LISTS _tokened_list)\n        string(REPLACE \"${flag}\" \"\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\n        string(REPLACE \"${flag}\" \"\" CMAKE_C_FLAGS \"${CMAKE_C_FLAGS}\")\n        string(REPLACE \"${flag}\" \"\" CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE}\")\n        string(REPLACE \"${flag}\" \"\" CMAKE_C_FLAGS_RELEASE \"${CMAKE_C_FLAGS_RELEASE}\")\n    endforeach()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Reshape Tensor with First Dimension Calculation in OpenVINO XML\nDESCRIPTION: This XML demonstrates reshaping a tensor where the first dimension is calculated and the second dimension is preserved from the input tensor using the Reshape operation. The 'special_zero' attribute is set to 'true'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/reshape-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Reshape\" ...>\n    <data special_zero=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>   <!--The tensor contains 2 elements: -1, 0 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>3</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Add Test Model Zoo Dependency\nDESCRIPTION: This makes the ONNX frontend tests depend on the test model zoo, ensuring that the models are available before the tests are run.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nadd_dependencies(ov_onnx_frontend_tests test_model_zoo)\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Capabilities with CMake\nDESCRIPTION: This CMake option enables the debug capabilities for OpenVINO. This flag needs to be set during the build process to include the necessary debug features.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/README.md#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n-DENABLE_DEBUG_CAPS=ON\n```\n\n----------------------------------------\n\nTITLE: OVSA Certificate Copying\nDESCRIPTION: Copies the customer certificate to the model developer's VM using scp. Requires the developer's VM IP address and username.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_50\n\nLANGUAGE: sh\nCODE:\n```\ncd $OVSA_RUNTIME_ARTEFACTS\nscp custkeystore.csr.crt username@<developer-vm-ip-address>:/<username-home-directory>/OVSA/artefacts\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard in CMake\nDESCRIPTION: This line sets the C++ standard to C++17 for the project. It ensures that the compiler uses C++17 features during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/template_extension/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: Defining i32 data type in TypeScript\nDESCRIPTION: Defines the i32 data type as a number in TypeScript. This represents a 32-bit integer, offering a wider range of integer values compared to i16.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/element.rst#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ni32: number\n```\n\n----------------------------------------\n\nTITLE: Defining u8 data type in TypeScript\nDESCRIPTION: Defines the u8 data type as a number in TypeScript. This represents an unsigned 8-bit integer, used for storing small, non-negative integer values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/element.rst#_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nu8: number\n```\n\n----------------------------------------\n\nTITLE: Changing Directory to Downloads Folder\nDESCRIPTION: This command changes the current directory to the user's Downloads folder, where the OpenVINO archive is expected to be downloaded.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-macos.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncd <user_home>/Downloads\n```\n\n----------------------------------------\n\nTITLE: Define Model output Method (TypeScript) - Index Parameter\nDESCRIPTION: This code defines the `output` method of the `Model` interface with an index parameter. It gets the output of the model identified by the index.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\noutput(index): Output\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO Node package\nDESCRIPTION: This command installs the openvino-node package using npm. This package allows you to use the OpenVINO JavaScript API in your Node.js application.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/nodejs_api.rst#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nnpm install openvino-node\n```\n\n----------------------------------------\n\nTITLE: Adding the certificate to the key store\nDESCRIPTION: Adds the generated certificate to the keystore, enabling the use of the certificate for signing and verification operations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_40\n\nLANGUAGE: sh\nCODE:\n```\n/opt/ovsa/bin/ovsatool keygen -storecert -c isv_keystore.csr.crt -k isv_keystore\n```\n\n----------------------------------------\n\nTITLE: Low Precision Transformations Execution in OpenVINO C++\nDESCRIPTION: This C++ code snippet shows how to configure and run Low Precision Transformations (LPT) within OpenVINO for optimizing model inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/advanced-guides/low-precision-transformations.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nauto& transformation = *transformation;\n\n```\n\n----------------------------------------\n\nTITLE: Setting Hostname (sh)\nDESCRIPTION: Sets the hostname of the Guest VM to ovsa_runtime using the hostnamectl command. This is part of configuring the Guest VM for the User role.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_20\n\nLANGUAGE: sh\nCODE:\n```\nsudo hostnamectl set-hostname ovsa_runtime\n```\n\n----------------------------------------\n\nTITLE: Conditional Documentation Build (CMake)\nDESCRIPTION: This snippet checks if the `ENABLE_DOCS` option is enabled. If it is, the `build_docs` function is called to initiate the documentation build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_DOCS)\n    build_docs()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Uninstall OpenVINO using vcpkg\nDESCRIPTION: This command uninstalls OpenVINO from the system using vcpkg. It removes the installed OpenVINO packages and their dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-vcpkg.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nvcpkg uninstall openvino\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This code snippet sets the target name for the library. It uses the `set` command in CMake to assign the string \"openvino_offline_transformations\" to the variable `TARGET_NAME`. This variable is later used when defining the library target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/offline_transformations/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_offline_transformations\")\n```\n\n----------------------------------------\n\nTITLE: Declaring PyTorch Dependency\nDESCRIPTION: This snippet declares PyTorch as a dependency for the OpenVINO Toolkit project. It indicates that PyTorch needs to be installed in order for the project to function correctly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/python/benchmark/bert_benchmark/requirements.txt#_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\ntorch\n```\n\n----------------------------------------\n\nTITLE: Create Virtual Environment (Linux/macOS)\nDESCRIPTION: This command creates a virtual environment named 'openvino_env' using Python 3's venv module. It isolates project dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_13\n\nLANGUAGE: console\nCODE:\n```\npython3 -m venv openvino_env\n```\n\n----------------------------------------\n\nTITLE: MaxPool Layer with 'explicit' auto_pad (XML)\nDESCRIPTION: This XML snippet defines a MaxPool layer with 'explicit' auto_pad, a kernel size of 2x2, padding of 1x1 on both sides, and strides of 2x2. It processes a 4D input tensor with dimensions 1x3x32x32 and produces a 4D output tensor with dimensions 1x3x17x17, along with a second output port for indices. The explicit auto_pad means that the specified pads_begin and pads_end values are used directly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-14.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MaxPool\" ... >\n    <data auto_pad=\"explicit\" kernel=\"2,2\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>17</dim>\n            <dim>17</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>17</dim>\n            <dim>17</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This snippet sets the target name to \"StressMemLeaksTests\". This name is later used to define the executable and its installation target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/memleaks_tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset (TARGET_NAME \"StressMemLeaksTests\")\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library\nDESCRIPTION: Creates an object library named `${TARGET_NAME}_obj` from the source files and headers. This object library is used when building shared libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_44\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_SHARED_LIBS)\n    add_library(${TARGET_NAME}_obj OBJECT ${SOURCES} ${HEADERS})\n\n    set(CPU_OBJ_LINK_SYSTEM LIBRARIES dnnl openvino::pugixml)\n    if(RISCV64)\n        list(APPEND CPU_OBJ_LINK_SYSTEM xbyak_riscv::xbyak_riscv)\n    endif()\n\n    ov_link_system_libraries(${TARGET_NAME}_obj PUBLIC ${CPU_OBJ_LINK_SYSTEM})\n\n    ov_add_version_defines(src/plugin.cpp ${TARGET_NAME}_obj)\n\n    target_include_directories(${TARGET_NAME}_obj\n        PRIVATE\n            $<TARGET_PROPERTY:openvino::runtime::dev,INTERFACE_INCLUDE_DIRECTORIES>\n            $<TARGET_PROPERTY:openvino::itt,INTERFACE_INCLUDE_DIRECTORIES>\n            $<TARGET_PROPERTY:openvino::shape_inference,INTERFACE_INCLUDE_DIRECTORIES>\n            $<TARGET_PROPERTY:openvino::snippets,INTERFACE_INCLUDE_DIRECTORIES>\n            $<TARGET_PROPERTY:openvino::reference,INTERFACE_INCLUDE_DIRECTORIES>\n        PUBLIC\n            ${CMAKE_CURRENT_SOURCE_DIR}/src\n            $<TARGET_PROPERTY:openvino::conditional_compilation,INTERFACE_INCLUDE_DIRECTORIES>)\n\n    target_include_directories(${TARGET_NAME}_obj SYSTEM PUBLIC $<TARGET_PROPERTY:dnnl,INCLUDE_DIRECTORIES>)\n    if(ENABLE_SNIPPETS_LIBXSMM_TPP)\n        target_include_directories(${TARGET_NAME}_obj SYSTEM PUBLIC $<TARGET_PROPERTY:xsmm,INCLUDE_DIRECTORIES>)\n    endif()\n\n    if(ENABLE_MLAS_FOR_CPU)\n        target_include_directories(${TARGET_NAME}_obj SYSTEM PUBLIC $<TARGET_PROPERTY:mlas,INCLUDE_DIRECTORIES>)\n    endif()\n\n    if(ENABLE_SHL_FOR_CPU)\n        target_include_directories(${TARGET_NAME}_obj SYSTEM PUBLIC $<TARGET_PROPERTY:shl,INTERFACE_INCLUDE_DIRECTORIES>)\n    endif()\n\n    if(ENABLE_KLEIDIAI_FOR_CPU)\n        target_include_directories(${TARGET_NAME}_obj SYSTEM PUBLIC $<TARGET_PROPERTY:kleidiai,INTERFACE_INCLUDE_DIRECTORIES>)\n    endif()\n    if(RISCV64)\n        target_include_directories(${TARGET_NAME}_obj SYSTEM PUBLIC $<TARGET_PROPERTY:xbyak_riscv::xbyak_riscv,INTERFACE_INCLUDE_DIRECTORIES>)\n    endif()\n\n    ov_set_threading_interface_for(${TARGET_NAME}_obj)\n\n    target_compile_definitions(${TARGET_NAME}_obj PRIVATE USE_STATIC_IE)\n\n    set_target_properties(${TARGET_NAME}_obj PROPERTIES EXCLUDE_FROM_ALL ON)\n\n    # LTO\n    set_target_properties(${TARGET_NAME}_obj PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\nendif()\n```\n\n----------------------------------------\n\nTITLE: EmbeddingBagPacked with Sum Reduction, With Weights - XML\nDESCRIPTION: Demonstrates the EmbeddingBagPacked operation with 'sum' reduction and with per_sample_weights. The input embedding table, indices, per-sample weights, and the resulting output are defined within the layer's input and output ports.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sparse/embedding-bag-packed-15.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"EmbeddingBagPacked\" ... >\n    <data reduction=\"sum\"/>\n    <input>\n        <port id=\"0\">     <!-- emb_table value is: [[-0.2, -0.6], [-0.1, -0.4], [-1.9, -1.8], [-1.,  1.5], [ 0.8, -0.7]] -->\n            <dim>5</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">     <!-- indices value is: [[0, 2], [1, 2], [3, 4]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"2\"/>    <!-- per_sample_weights value is: [[0.5, 0.5], [0.3, 0.7], [2., -1.]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">     <!-- output value is: [[-1.05, -1.2], [-1.36, -1.38], [-2.8, 3.7]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Disable Strict Aliasing Warning (GCC)\nDESCRIPTION: Disables the strict aliasing warning for the GNU C++ compiler. This warning can be triggered by certain code patterns that violate strict aliasing rules.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX)\n    ov_add_compiler_flags(-Wno-strict-aliasing)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining f32 data type in TypeScript\nDESCRIPTION: Defines the f32 data type as a number in TypeScript. This snippet represents a 32-bit floating-point number, commonly used in numerical computations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/element.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nf32: number\n```\n\n----------------------------------------\n\nTITLE: Define ov_available_devices struct in C\nDESCRIPTION: This struct represents a list of available devices. It contains an array of device name strings and the size of the array.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_3\n\nLANGUAGE: C\nCODE:\n```\ntypedef struct {\n\n    char** devices;\n\n    size_t size;\n\n} ov_available_devices_t;\n```\n\n----------------------------------------\n\nTITLE: Install Requirements\nDESCRIPTION: This command installs the necessary Python packages listed in the `requirements.txt` file. These packages are dependencies required by the BERT benchmark sample.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/bert-benchmark.rst#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\npython -m pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Linking libraries\nDESCRIPTION: Links the target against other libraries.  In this case, it's linking against `openvino::runtime::dev` as a private dependency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/logger/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME}\n    PRIVATE\n        openvino::runtime::dev\n)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO GenAI via PyPI\nDESCRIPTION: This command installs the OpenVINO GenAI package using pip. It is the standard way to install Python packages from the Python Package Index.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-genai.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython -m pip install openvino-genai\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory 'frontend' in CMake\nDESCRIPTION: This snippet adds the 'frontend' subdirectory to the build process. This allows for the inclusion of CMakeLists.txt located inside that folder.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/tests/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(frontend)\n```\n\n----------------------------------------\n\nTITLE: Setting target name\nDESCRIPTION: Sets the target name to 'single-image-test'. This name is used throughout the CMake script to refer to the executable being built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME single-image-test)\n```\n\n----------------------------------------\n\nTITLE: Uninstall OpenVINO Runtime version 2025.1.0\nDESCRIPTION: Uninstalls the OpenVINO Runtime version 2025.1.0 using the APT package manager. This is an example of uninstalling a specific version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt autoremove openvino-2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Getting a property of CompiledModel (TypeScript)\nDESCRIPTION: This code shows the declaration of the `getProperty` method within the `CompiledModel` interface. This method retrieves a property value by property name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ngetProperty(propertyName): OVAny\n```\n\n----------------------------------------\n\nTITLE: Define Model getFriendlyName Method (TypeScript)\nDESCRIPTION: This code defines the `getFriendlyName` method of the `Model` interface. It returns the friendly name of the model as a string.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ngetFriendlyName(): string\n```\n\n----------------------------------------\n\nTITLE: Negative Layer XML Configuration in OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of a Negative layer in OpenVINO. It defines the input and output ports, specifying the dimensions of the tensors involved.  The input tensor is passed to the Negative layer, and the output tensor contains the element-wise negation of the input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/negative-1.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"Negative\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Original Coordinate Calculation in Interpolate-4 (Python)\nDESCRIPTION: This Python code defines a class `GetOriginalCoordinate` that calculates the original coordinate in the input tensor based on the resized coordinate and scaling factors. It supports different coordinate transformation modes, including `half_pixel`, `pytorch_half_pixel`, `asymmetric`, `tf_half_pixel_for_nn`, and `align_corners`. The class takes the mode as input during initialization and provides a callable object that returns the original coordinate. Dependencies: None.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass GetOriginalCoordinate:\n    def __init__(self, mode: str):\n        self.func = {\n            'half_pixel': GetOriginalCoordinate.half_pixel_func,\n            'pytorch_half_pixel': GetOriginalCoordinate.pytorch_half_pixel_func,\n            'asymmetric': GetOriginalCoordinate.asymmetric_func,\n            'tf_half_pixel_for_nn': GetOriginalCoordinate.tf_half_pixel_for_nn_func,\n            'align_corners': GetOriginalCoordinate.align_corners_func\n        }[mode]\n\n    def __call__(self, x_resized, x_scale, length_resized, length_original):\n        return self.func(x_resized, x_scale, length_resized, length_original)\n\n    @staticmethod\n    def half_pixel_func(x_resized, x_scale, length_resized, length_original):\n        return ((x_resized + 0.5) / x_scale) - 0.5\n\n    @staticmethod\n    def pytorch_half_pixel_func(x_resized, x_scale, length_resized, length_original):\n        return (x_resized + 0.5) / x_scale - 0.5 if length_resized > 1 else 0.0\n\n    @staticmethod\n    def asymmetric_func(x_resized, x_scale, length_resized, length_original):\n        return x_resized / x_scale\n\n    @staticmethod\n    def tf_half_pixel_for_nn_func(x_resized, x_scale, length_resized, length_original):\n        return (x_resized + 0.5) / x_scale\n\n    @staticmethod\n    def align_corners_func(x_resized, x_scale, length_resized, length_original):\n        return  0 if length_resized == 1 else  x_resized * (length_original - 1) / (length_resized - 1)\n```\n\n----------------------------------------\n\nTITLE: Visualize First Error Map\nDESCRIPTION: Visualizes the first error map between two dumps of output tensors from the CPU plugin. Replace `/path/to/model` with the path to the model file, and `dump1` and `dump2` with the names of the dump files to compare. The `-v` flag enables visualization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/dump_check/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 dump_check.py -m=/path/to/model dump1 dump2 -v\n```\n\n----------------------------------------\n\nTITLE: Concatenating Output Tensors in TensorIterator (C++)\nDESCRIPTION: This code demonstrates how the output tensors are concatenated based on the 'stride' attribute in the TensorIterator operation. If the stride is positive, the tensors are concatenated in the forward order. If the stride is negative, the tensors are concatenated in reverse order. This snippet shows the logic for forming the final output from the intermediate results of the TensorIterator's body.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/tensor-iterator-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput = Concat(S[0], S[1], ..., S[N-1])\n```\n\n----------------------------------------\n\nTITLE: Defining Dimension Type Alias in TypeScript\nDESCRIPTION: This code snippet defines the `Dimension` type alias in TypeScript. It specifies that a `Dimension` can be either a single number or an array containing two numbers. This is likely used to represent the shape of a tensor where a single number indicates a fixed dimension size and an array of two numbers indicates a dimension with a variable size within a range.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/types/Dimension.rst#_snippet_0\n\nLANGUAGE: ts\nCODE:\n```\nDimension: number|[number,number]\n```\n\n----------------------------------------\n\nTITLE: Running Python Script in CMake\nDESCRIPTION: This CMake block executes a Python script (`check_supported_ops.py`) to validate supported operations. It checks the `op_table.cpp` file against the `supported_ops.md` file. The script's result, output, and errors are captured and checked, failing the cmake configuration if the Python script fails.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(Python3_Interpreter_FOUND)\n    execute_process(\n        COMMAND ${Python3_EXECUTABLE} \n                       ${CMAKE_CURRENT_SOURCE_DIR}/docs/check_supported_ops.py\n                       ${CMAKE_CURRENT_SOURCE_DIR}/src/op_table.cpp\n                       ${CMAKE_CURRENT_SOURCE_DIR}/docs/supported_ops.md\n        RESULT_VARIABLE SCRIPT_RESULT\n        OUTPUT_VARIABLE SCRIPT_OUTPUT\n        ERROR_VARIABLE SCRIPT_ERROR\n    )\n\n    if(NOT SCRIPT_RESULT EQUAL 0)\n        message(FATAL_ERROR \"Python script failed with return code ${SCRIPT_RESULT}\\nOutput: ${SCRIPT_OUTPUT}\\nError: ${SCRIPT_ERROR}\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: ReduceMean Example with axis [-2]\nDESCRIPTION: Illustrates the ReduceMean operation with a negative axis index ([-2]), demonstrating reduction along a specific dimension from the end of the tensor's shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-mean-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceMean\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>     <!-- value is [-2] that means independent reduction in each channel, batch and second spatial dimension -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags for Non-MSVC Compilers\nDESCRIPTION: Adds specific compiler flags to suppress warnings when using non-MSVC compilers. These flags disable warnings related to missing declarations, one definition rule violations, and other general warnings. Requires a non-MSVC compiler like GCC or Clang.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nelse()\n    ov_add_compiler_flags(-Wno-missing-declarations)\n    ov_add_compiler_flags(-Wno-odr)\n    ov_add_compiler_flags(-Wno-all)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Referencing Reusable Workflow as a Job\nDESCRIPTION: This YAML snippet provides an example of referencing and using a reusable workflow as a job within another workflow. The `Python_Unit_Tests` job is defined by specifying its `name`, `needs` (dependencies), the path to the reusable workflow using `uses`, and input parameters passed to the reusable workflow defined using `with`. This setup reduces duplication and improves maintainability across multiple workflows.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/reusable_workflows.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n  Python_Unit_Tests:\n    name: Python unit tests\n    needs: [ Build, Smart_CI ]\n    uses: ./.github/workflows/job_python_unit_tests.yml\n    with:\n      runner: 'aks-linux-4-cores-16gb'\n      container: '{\"image\": \"openvinogithubactions.azurecr.io/dockerhub/ubuntu:20.04\", \"volumes\": [\"/mount:/mount\"]}'\n      affected-components: ${{ needs.smart_ci.outputs.affected_components }}\n```\n\n----------------------------------------\n\nTITLE: TopK Computation\nDESCRIPTION: Illustrates the computation of TopK values for each slice along a specified axis. It shows how the output tensor is populated based on the input tensor and the TopK operation's parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/top-k-11.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i1, ..., i(axis-1), j, i(axis+1) ..., iN] = top_k(input[i1, ...., i(axis-1), :, i(axis+1), ..., iN], k, sort, mode)\n```\n\n----------------------------------------\n\nTITLE: Copying Tuning Cache (CMake)\nDESCRIPTION: This adds a post-build command to copy the tuning cache JSON file. This cache is used for performance optimization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(\n    TARGET ${TARGET_NAME} POST_BUILD\n    COMMAND \"${CMAKE_COMMAND}\" -E copy_if_different ${CMAKE_CURRENT_SOURCE_DIR}/cache/cache.json ${TUNING_CACHE_PATH}/cache.json)\n```\n\n----------------------------------------\n\nTITLE: Accessing Tensor Data Property in TypeScript\nDESCRIPTION: Shows how to access the tensor's data using the `data` property, which returns a `TypedArray` subclass corresponding to the tensor's element type. The setter copies data from a `TypedArray` subclass into the tensor's underlying memory, throwing an error if size or type mismatches.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Tensor.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ndata: SupportedTypedArray\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags in CMake\nDESCRIPTION: This CMake snippet adds compiler flags based on the compiler being used. For AppleClang, it adds `-stdlib=libc++`. For GCC, it disables the warning related to 'PYBIND11_NOINLINE inline' when using GCC 7.5 by using `ov_add_compiler_flags` to add `-Wno-error=attributes`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(OV_COMPILER_IS_APPLECLANG)\n    add_link_options(-stdlib=libc++)\nelif(CMAKE_COMPILER_IS_GNUCXX)\n    # WA for GCC 7.5 \"PYBIND11_NOINLINE inline\" warning\n    ov_add_compiler_flags(-Wno-error=attributes)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Emulating Sapphire Rapids CPU with SDE\nDESCRIPTION: This snippet demonstrates how to emulate a Sapphire Rapids CPU using Intel SDE while running a benchmark application. It includes blob dumping for debugging accuracy issues. It requires the SDE binary and the benchmark_app executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/cpu_emulation.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_FORMAT=TEXT OV_CPU_BLOB_DUMP_NODE_TYPE=Convolution \\\n/path/to/sde -spr -- ./benchmark_app --niter 1 --nstreams 1 -m path/to/model.xml\n```\n\n----------------------------------------\n\nTITLE: Conditional Source Exclusion for OpenCL in CMake\nDESCRIPTION: Conditionally excludes `ov_remote_context_test.cpp` from the source files if the `OpenCL::OpenCL` target is not defined. This is done to avoid compilation errors when OpenCL support is not available.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT TARGET OpenCL::OpenCL)\n    list(FILTER SOURCES EXCLUDE REGEX ov_remote_context_test.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific Linking\nDESCRIPTION: This snippet conditionally links platform-specific libraries. On Windows, it links `setupapi`, and on Unix-like systems (excluding Android), it links `pthread`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n  target_link_libraries(${TARGET_NAME} PRIVATE setupapi)\nelif((NOT ANDROID) AND (UNIX))\n  target_link_libraries(${TARGET_NAME} PRIVATE pthread)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Cloning OpenVINO repository\nDESCRIPTION: This snippet clones the OpenVINO repository from GitHub with the `--recursive` flag to initialize submodules, then changes the current directory to the cloned repository.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/openvinotoolkit/openvino.git\ncd openvino\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet adds the 'unit' subdirectory to the current CMake project. The add_subdirectory command instructs CMake to process the CMakeLists.txt file located in the 'unit' directory, effectively incorporating its build instructions into the current project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(unit)\n```\n\n----------------------------------------\n\nTITLE: Fuzzy Tests and Standalone Build Integration\nDESCRIPTION: This snippet conditionally adds a dependency on the OpenVINO Intel CPU plugin for fuzzy tests and includes a subdirectory for standalone builds. If `ENABLE_INTEL_CPU` is true, it adds a dependency to the `paddle_tests` target.  It also adds a subdirectory called 'standalone_build' which contains additional configurations for standalone test builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n# Fuzzy tests for PaddlePaddle use IE_CPU engine\nif(ENABLE_INTEL_CPU)\n    add_dependencies(${TARGET_NAME} openvino_intel_cpu_plugin)\nendif()\n\nadd_subdirectory(standalone_build)\n```\n\n----------------------------------------\n\nTITLE: Setting Plugin Target and Name\nDESCRIPTION: This snippet sets the target name for the NPU plugin (`openvino_intel_npu_plugin`) and defines the main source file (`src/plugin.cpp`) used for version definition generation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/plugin/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(NPU_PLUGIN_TARGET \"openvino_intel_npu_plugin\")\nset(NPU_PLUGIN_ENGINE_SOURCE_FILE \"src/plugin.cpp\")\nset(TARGET_NAME ${NPU_PLUGIN_TARGET})\n```\n\n----------------------------------------\n\nTITLE: Including Subdirectories in CMake\nDESCRIPTION: This snippet includes the 'shared' and 'conformance' subdirectories into the current CMake project. The `add_subdirectory` command tells CMake to process the `CMakeLists.txt` file in the specified subdirectory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(shared)\nadd_subdirectory(conformance)\n```\n\n----------------------------------------\n\nTITLE: Azure Blob Storage: Providing sccache Prefix\nDESCRIPTION: This YAML snippet demonstrates setting the `SCCACHE_AZURE_KEY_PREFIX` environment variable to specify the directory in Azure Blob Storage where cached artifacts for a particular OS and architecture should be stored. This environment variable is set within the `env` block of the job configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/caches.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\nBuild:\n  ...\n  env:\n    ...\n    CMAKE_CXX_COMPILER_LAUNCHER: sccache\n    CMAKE_C_COMPILER_LAUNCHER: sccache\n    ...\n    SCCACHE_AZURE_KEY_PREFIX: ubuntu20_x86_64_Release\n```\n\n----------------------------------------\n\nTITLE: CMake Install Directory Setup\nDESCRIPTION: This snippet defines an installation rule to copy the contents of the current source directory to the 'tests' directory under the installation prefix.  It also excludes the installed files from all default build targets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/layer_tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: NonMaxSuppression Layer Definition in C++\nDESCRIPTION: This code snippet demonstrates the XML representation of a NonMaxSuppression layer in OpenVINO, showcasing its attributes, input ports, and output ports. It defines the layer's type, data encoding, and input/output tensor dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/non-max-suppression-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"NonMaxSuppression\" ... >\n    <data box_encoding=\"corner\" sort_result_descending=\"1\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>1000</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>1000</dim>\n        </port>\n        <port id=\"2\"/>\n        <port id=\"3\"/>\n        <port id=\"4\"/>\n    </input>\n    <output>\n        <port id=\"5\" precision=\"I32\">\n            <dim>1000</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO Model Parameters with YAML\nDESCRIPTION: This code snippet demonstrates how to configure OpenVINO model parameters such as input precision, input layout, and device using YAML format in Protopipe. It also shows how to set model priority and plugin-specific configuration options.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- { name: model.xml, ip: FP16, iml: NHWC, il: NCHW }\n- { name: model.xml, ip: { data: FP16 }, priority: HIGH }\n- { name: model.xml, device: NPU, config: { PERFORMANCE_HINT: THROUGHPUT } }\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for hello_classification_c Sample\nDESCRIPTION: This CMake code snippet uses the `ov_add_sample` macro to define the 'hello_classification_c' sample. It specifies the source files (`main.c`, `infer_result_util.c`), header file (`infer_result_util.h`), and the dependency `opencv_c_wrapper`. This setup ensures the sample is correctly built with OpenVINO and its dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/c/hello_classification/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_sample(NAME hello_classification_c\n              SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/main.c;${CMAKE_CURRENT_SOURCE_DIR}/infer_result_util.c\"\n              HEADERS \"${CMAKE_CURRENT_SOURCE_DIR}/infer_result_util.h\"\n              DEPENDENCIES opencv_c_wrapper)\n```\n\n----------------------------------------\n\nTITLE: Suppress Override Suggestion Warning\nDESCRIPTION: Suppresses the `-Wno-suggest-override` warning if `SUGGEST_OVERRIDE_SUPPORTED` is set, preventing compiler warnings related to override suggestions in C++ code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/functional/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(SUGGEST_OVERRIDE_SUPPORTED)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-suggest-override\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries\nDESCRIPTION: Links the target library with other OpenVINO libraries. It specifies the libraries `openvino::npu_al`, `openvino::npu_logger_utils`, and `openvino::npu_zero_utils` as public dependencies, which means they are required for both building and using the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/common/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME}\n    PUBLIC\n        openvino::npu_al\n        openvino::npu_logger_utils\n        openvino::npu_zero_utils\n)\n```\n\n----------------------------------------\n\nTITLE: Broadcast with explicit mode and axes_mapping in OpenVINO (XML)\nDESCRIPTION: This XML code showcases the 'explicit' mode of the Broadcast operation with a different axes_mapping configuration.  A tensor with shape [50, 50] is broadcasted to [1, 50, 50, 16] according to target shape [4] and the 3rd input which provides the axes mapping [1,2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/broadcast-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Broadcast\" ...>\n    <data mode=\"explicit\"/>\n    <input>\n        <port id=\"0\">\n            <dim>50</dim>\n            <dim>50</dim>\n       </port>\n        <port id=\"1\">\n            <dim>4</dim>   <!--The tensor contains 4 elements: [1, 50, 50, 16] -->\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>   <!--The tensor contains 2 elements: [1, 2] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>50</dim>\n            <dim>50</dim>\n            <dim>16</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Save Checkpoint - TensorFlow 2\nDESCRIPTION: This code snippet shows how to save a model checkpoint during training in TensorFlow 2. This uses NNCF's API for handling model checkpoints when the model is wrapped by NNCF.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncompression_ctrl.save_checkpoint(\"checkpoint.pth\")\n```\n\n----------------------------------------\n\nTITLE: MatMul Batched Matrix Multiplication with Broadcasting XML\nDESCRIPTION: Configuration example demonstrating MatMul operation for multiplication of a batch of 5 matrices by a single matrix with broadcasting. The input ports define the dimensions of the input matrices (5x10x1024 and 1024x1000). The output port defines the resulting matrix (5x10x1000).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/matmul-1.rst#_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MatMul\">\n    <input>\n        <port id=\"0\">\n            <dim>5</dim>\n            <dim>10</dim>\n            <dim>1024</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1024</dim>\n            <dim>1000</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>5</dim>\n            <dim>10</dim>\n            <dim>1000</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Set ONNX Opset Version\nDESCRIPTION: This snippet sets the ONNX opset version as a compile definition for the target. It defines the `ONNX_OPSET_VERSION` macro with a value of 21, which is used during the compilation of the ONNX frontend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/frontend/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(ONNX_OPSET_VERSION 21 CACHE INTERNAL \"Supported version of ONNX operator set\")\ntarget_compile_definitions(${TARGET_NAME} PRIVATE ONNX_OPSET_VERSION=${ONNX_OPSET_VERSION})\n```\n\n----------------------------------------\n\nTITLE: GatherND XML Layer Definition (Example 1)\nDESCRIPTION: This example demonstrates the XML layer definition for the GatherND operation within the OpenVINO framework. It specifies the data shape, indices shape, and the resulting output shape of the GatherND operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-5.rst#_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"GatherND\">\n    <data batch_dims=0 />\n    <input>\n        <port id=\"0\">\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>10</dim>\n            <dim>15</dim>\n        </port>\n        <port id=\"1\">\n            <dim>25</dim>\n            <dim>125</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>25</dim>\n            <dim>125</dim>\n            <dim>15</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Wrapping with C++ Shared Pointer in C\nDESCRIPTION: This code snippet demonstrates how to wrap a C++ class in C using a struct that contains a shared pointer to the C++ class. This approach is useful for objects that need to be hidden from the user. The struct `ov_class_name` contains a `std::shared_ptr` to an `ov::ClassName` object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/how_to_wrap_openvino_objects_with_c.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nstruct ov_class_name {\n    std::shared_ptr<ov::ClassName> object;\n};\n```\n\n----------------------------------------\n\nTITLE: TopK Operation Computation\nDESCRIPTION: This code snippet shows how the output of the TopK operation is computed. It iterates over the input tensor and applies the top_k function to each slice along the specified axis, based on the given mode and sort criteria. The output contains the top k values and indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/top-k-3.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i1, ..., i(axis-1), j, i(axis+1) ..., iN] = top_k(input[i1, ...., i(axis-1), :, i(axis+1), ..., iN]), k, sort, mode)\n```\n\n----------------------------------------\n\nTITLE: Declaring the anyName Property in Output Interface\nDESCRIPTION: Defines the `anyName` property within the `Output` interface as a string. This property represents the name assigned to the model output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Output.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nanyName: string\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Multiple Spaces\nDESCRIPTION: This rule discourages the use of multiple spaces, except for indentation, in JavaScript and TypeScript code. It promotes cleaner code and is enforced by ESLint with the configuration `no-multi-spaces: ['error']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_9\n\nLANGUAGE: JavaScript\nCODE:\n```\nno-multi-spaces: ['error']\n```\n\n----------------------------------------\n\nTITLE: Add ONNX Frontend\nDESCRIPTION: This snippet uses the `ov_add_frontend` macro to add the ONNX frontend to the build. It specifies the frontend name, required features (PROTOBUF, PROTOBUF_LITE), a description, and linked libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/frontend/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_frontend(NAME onnx\n                LINKABLE_FRONTEND\n                PROTOBUF_REQUIRED\n                PROTOBUF_LITE\n                SKIP_NCC_STYLE\n                FILEDESCRIPTION \"FrontEnd to load and convert ONNX file format\"\n                LINK_LIBRARIES openvino_onnx_common openvino::core::dev)\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example 6 (valid padding, ceil rounding)\nDESCRIPTION: This example demonstrates how MaxPool operates with a 4D input, 2D kernel, `auto_pad` set to 'valid', and `rounding_type` set to 'ceil'. It illustrates the input data, strides, kernel size, padding, and the expected output after the MaxPool operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/pooling_shape_rules.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[-1, 2, 3],\n                 [4, 5, -6],\n                 [-7, 8, 9]]]]   # shape: (1, 1, 3, 3)\n      strides = [2, 2]\n      kernel = [2, 2]\n      rounding_type = \"ceil\"\n      auto_pad = \"valid\"\n      output0 = [[[[5, 3],\n                   [8, 9]]]]   # shape: (1, 1, 2, 2)\n      output1 = [[[[4, 2],\n                   [7, 8]]]]   # shape: (1, 1, 2, 2)\n```\n\n----------------------------------------\n\nTITLE: Installing Target CMake\nDESCRIPTION: This snippet installs the target library, including runtime, archive and library files, and header files. It also handles component assignments for different parts of the installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/src/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_cpack_add_component(${OV_CPACK_COMP_CORE_C} HIDDEN)\nov_cpack_add_component(${OV_CPACK_COMP_CORE_C_DEV} HIDDEN)\n\nif(BUILD_SHARED_LIBS)\n    set(archive_comp CORE_C_DEV)\nelse()\n    set(archive_comp CORE_C)\nendif()\n\ninstall(TARGETS ${TARGET_NAME} EXPORT OpenVINOTargets\n        RUNTIME DESTINATION ${OV_CPACK_RUNTIMEDIR} COMPONENT ${OV_CPACK_COMP_CORE_C} ${OV_CPACK_COMP_CORE_C_EXCLUDE_ALL}\n        ARCHIVE DESTINATION ${OV_CPACK_ARCHIVEDIR} COMPONENT ${OV_CPACK_COMP_${archive_comp}} ${OV_CPACK_COMP_${archive_comp}_EXCLUDE_ALL}\n        LIBRARY DESTINATION ${OV_CPACK_LIBRARYDIR} COMPONENT ${OV_CPACK_COMP_CORE_C} ${OV_CPACK_COMP_CORE_C_EXCLUDE_ALL}\n        NAMELINK_COMPONENT ${OV_CPACK_COMP_LINKS} ${OV_CPACK_COMP_LINKS_EXCLUDE_ALL}\n        INCLUDES DESTINATION ${OV_CPACK_INCLUDEDIR})\n\nov_install_pdb(${TARGET_NAME})\n\ninstall(DIRECTORY ${OpenVINO_C_API_SOURCE_DIR}/include/openvino/\n        DESTINATION ${OV_CPACK_INCLUDEDIR}/openvino\n        COMPONENT ${OV_CPACK_COMP_CORE_C_DEV}\n        ${OV_CPACK_COMP_CORE_C_DEV_EXCLUDE_ALL})\n```\n\n----------------------------------------\n\nTITLE: Link Libraries\nDESCRIPTION: This command links the target 'pybind_mock_frontend' with the 'openvino::frontend::mock_py' library. It ensures that the necessary OpenVINO frontend components are available during compilation and runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/pyngraph_fe_mock_api/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${PYBIND_FE_NAME} PRIVATE openvino::frontend::mock_py)\n```\n\n----------------------------------------\n\nTITLE: Installing Target in CMake\nDESCRIPTION: This command installs the target library to the specified destination during the installation process. It places the library under the `tests` directory and includes it in the `tests` component. `EXCLUDE_FROM_ALL` ensures it's not built in the default build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/template_extension/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n    LIBRARY DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: RDFT Layer XML Configuration (No signal_size, 2D Input)\nDESCRIPTION: Configures an RDFT layer in XML without a signal_size input, processing a 2D input tensor. The axes [0, 1] are used for the Fourier transform. The example demonstrates how a 2D input tensor is transformed into a 3D output tensor, with the last dimension being 2 (representing the real and imaginary parts of the complex output).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/rdft-9.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"RDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>320</dim>\n            <dim>320</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- axes input contains [0, 1] -->\n        </port>\n    <output>\n        <port id=\"2\">\n            <dim>320</dim>\n            <dim>161</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Conditional Test Inclusion in CMake\nDESCRIPTION: This CMake conditional block adds the 'tests' subdirectory to the build process if the `ENABLE_TESTS` variable is set. This allows for test targets to be included or excluded during build configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Cpplint Target in CMake\nDESCRIPTION: Adds a Cpplint target for code style checking. It relies on add_cpplint_target macro.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nadd_cpplint_target(${TARGET_NAME}_cpplint FOR_TARGETS ${TARGET_NAME}_obj)\n```\n\n----------------------------------------\n\nTITLE: Setting Library Naming Convention CMake\nDESCRIPTION: This CMake function defines the library naming convention based on the target platform (Windows or other). It uses the OV_BUILD_POSTFIX and appropriate library suffixes to construct the final library name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ocl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(get_lib_name TARGET_NAME LIBRARY_NAME)\n    if(WIN32)\n        set(LIB_SUFFIX \"${OV_BUILD_POSTFIX}${CMAKE_LINK_LIBRARY_SUFFIX}\")\n    else()\n        set(LIB_SUFFIX \"${OV_BUILD_POSTFIX}${CMAKE_SHARED_LIBRARY_SUFFIX}\")\n    endif()\n\n    set(\"${LIBRARY_NAME}\" \"${CMAKE_SHARED_MODULE_PREFIX}${TARGET_NAME}${LIB_SUFFIX}\" PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Installing Protobuf Targets for ONNX Export Set\nDESCRIPTION: This snippet addresses a workaround for ONNX.  It ensures that protobuf libraries (either `libprotobuf-lite` or `libprotobuf`) are included in the same export set as the ONNX targets if system protobuf is disabled and a static build is performed. This is necessary because protobuf must be in the same export set when using imported protobuf targets. It installs the correct protobuf target, specifying archive destination and component for packaging. OV_CPACK_COMP_CORE_EXCLUDE_ALL is used to mark this for component-specific exclusion during packaging.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/onnx/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT ENABLE_SYSTEM_PROTOBUF AND NOT BUILD_SHARED_LIBS)\n    if(ONNX_USE_LITE_PROTO)\n        set(protobuf_target_name libprotobuf-lite)\n    else()\n        set(protobuf_target_name libprotobuf)\n    endif()\n\n    install(TARGETS ${protobuf_target_name} EXPORT ONNXTargets\n            ARCHIVE DESTINATION ${OV_CPACK_ARCHIVEDIR} COMPONENT ${OV_CPACK_COMP_CORE}\n            ${OV_CPACK_COMP_CORE_EXCLUDE_ALL})\nendif()\n```\n\n----------------------------------------\n\nTITLE: infer_preprocess() Method C++\nDESCRIPTION: This code snippet shows the implementation of the `infer_preprocess()` method. It checks user input/output tensors and demonstrates the conversion from user tensors to a backend-specific representation, ensuring compatibility between the user-provided data and the backend inference engine.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/synch-inference-request.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nvoid InferRequest::infer_preprocess() {\n    // Check user input/output tensors\n\n    // Conversion from user tensor to backend specific representation\n}\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Space Before Blocks\nDESCRIPTION: This rule requires a space before blocks in JavaScript and TypeScript code. Consistent spacing improves readability and is enforced by ESLint with the configuration `space-before-blocks: ['error']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_12\n\nLANGUAGE: JavaScript\nCODE:\n```\nspace-before-blocks: ['error']\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for TensorFlow Lite Frontend (CMake)\nDESCRIPTION: This snippet conditionally adds the 'tensorflow_lite' subdirectory to the build if the ENABLE_OV_TF_LITE_FRONTEND CMake option is enabled. This includes the TensorFlow Lite frontend functionality.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif (ENABLE_OV_TF_LITE_FRONTEND)\n    add_subdirectory(tensorflow_lite)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating CNPY Library Target CMake\nDESCRIPTION: This code snippet creates a static library target named 'openvino_cnpy' from the source file 'cnpy.cpp'. It also defines an alias 'openvino::cnpy' for easier referencing in other parts of the project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/cnpy/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(cnpy)\n\nset(TARGET_NAME \"openvino_cnpy\")\n\nadd_library(${TARGET_NAME} STATIC cnpy.cpp)\nadd_library(openvino::cnpy ALIAS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Second Correct TopK Output\nDESCRIPTION: Displays a second possible correct output for the TopK operation with the example input and configuration, showing the values and indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/top-k-11.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\noutput_values  = [3, 1, 2, 5]\noutput_indices = [1, 2, 3, 4]\n```\n\n----------------------------------------\n\nTITLE: StridedSlice New Axis Mask Example in XML\nDESCRIPTION: Shows how to add new dimensions to the output tensor using the `new_axis_mask` attribute. This example demonstrates the equivalent of array[np.newaxis, 0:2, np.newaxis, 0:4]. The mask effectively inserts dimensions of size 1 into the output shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/strided-slice-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StridedSlice\" ...>\n    <data begin_mask=\"0,0,0,0\" end_mask=\"0,0,0,0\" new_axis_mask=\"1,0,1,0\" shrink_axis_mask=\"0,0,0,0\" ellipsis_mask=\"0,0,0,0\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim> <!-- begin: [1234, 0, -1, 0] - new_axis_mask skips the value -->\n        </port>\n        <port id=\"2\">\n            <dim>4</dim> <!-- end: [1234, 2, 9876, 4] - new_axis_mask skips the value -->\n        </port>\n        <port id=\"3\">\n            <dim>4</dim> <!-- stride: [132, 1, 241, 1] - new_axis_mask skips the value -->\n        </port>\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>1</dim> <!-- new dimension appears -->\n            <dim>2</dim> <!-- second dimension created from first dimension of the input -->\n            <dim>1</dim> <!-- new dimension appears -->\n            <dim>4</dim> <!-- fourth dimension created from second dimension of the input -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Loading Extensions from Library in C++\nDESCRIPTION: This snippet demonstrates how to load extensions from a library into OpenVINO Runtime using C++.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nov::Core core;\n// [add_extension_lib]\ncore.add_extension(\"path_to_the_extension/libtemplate_extension.so\");\n// [add_extension_lib]\n```\n\n----------------------------------------\n\nTITLE: Optional Install\nDESCRIPTION: This snippet installs the built OpenVINO artifacts to a specified installation location. It uses the `cmake_install.cmake` script.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_INSTALL_PREFIX=<installation location> -P cmake_install.cmake\n```\n\n----------------------------------------\n\nTITLE: Setting Threading Interface\nDESCRIPTION: This snippet sets the threading interface for the target library using the `ov_set_threading_interface_for` function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: Defines the target name for the library, which is used in subsequent CMake commands to refer to the library being built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset (TARGET_NAME \"openvino_lp_transformations\")\n```\n\n----------------------------------------\n\nTITLE: Install specific OpenVINO Runtime version\nDESCRIPTION: Installs a specific version of the OpenVINO Runtime using the APT package manager. Requires knowing the exact version number to install.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt install openvino-<VERSION>.<UPDATE>.<PATCH>\n```\n\n----------------------------------------\n\nTITLE: Visualize Model Graph (C++)\nDESCRIPTION: This snippet demonstrates how to visualize an OpenVINO model graph by converting it to an image using C++.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-representation.rst#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n// [ov:visualize]\n```\n\n----------------------------------------\n\nTITLE: Enforce Standards Conformance (MSVC)\nDESCRIPTION: Enforces standards conformance on MSVC by adding the `/Zc:preprocessor` compile option.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/functional/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC)\n  # Enforce standards conformance on MSVC\n  target_compile_options(${TARGET_NAME} PRIVATE /Zc:preprocessor)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name\nDESCRIPTION: Defines the target name for the library being built. This variable is later used to define the library target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/thread_local/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_thread_local)\n```\n\n----------------------------------------\n\nTITLE: Defining the toString Method in Output Interface\nDESCRIPTION: Defines the `toString` method within the `Output` interface. This method returns a string representation of the `Output` object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Output.rst#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\ntoString(): string\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties\nDESCRIPTION: This snippet sets target properties for the zlib library, such as the folder in which it should be located in the IDE.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/zlib/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES FOLDER thirdparty)\n```\n\n----------------------------------------\n\nTITLE: Import NNCF API - TensorFlow 2\nDESCRIPTION: This code snippet demonstrates how to import the necessary NNCF modules in a TensorFlow 2 training script to enable filter pruning. These imports are required to use NNCF's compression features and integrate them into the model training process. The NNCF package and tensorflow must be installed before the script is run.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/filter-pruning.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom nncf import NNCFConfig\nfrom nncf import create_compressed_model\nfrom nncf import load_state\n```\n\n----------------------------------------\n\nTITLE: Creating Inference Request in C++\nDESCRIPTION: This snippet shows how to create an inference request from a compiled model in C++.  The `create_infer_request` function creates an `InferRequest` object, which is then used to set input tensors, execute the model, and retrieve output tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/README.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nov::InferRequest request = compiled_model.create_infer_request();\n```\n\n----------------------------------------\n\nTITLE: Enabling ONNX Frontend Compilation Definition in CMake\nDESCRIPTION: This snippet adds a compile definition to the target if the OV_ONNX_FRONTEND is enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/tests/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_OV_ONNX_FRONTEND)\n    target_compile_definitions(${TARGET_NAME} PRIVATE ENABLE_OV_ONNX_FRONTEND)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Dump FP32 Precision\nDESCRIPTION: Dumps FP32 precision data using the CPU plugin. Filters by the 'Convolution' layer. Replace `/path/model.xml` with the path to the model and saves the dump to `./dump_fp32`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/dump_check/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 dump_check.py -m /path/model.xml  -f Convolution ./dump_fp32\n```\n\n----------------------------------------\n\nTITLE: Building kleidiai as Static Library\nDESCRIPTION: This CMake function, `ov_build_kleidiai_static`, builds the kleidiai library as a static library and disables its tests. It adds the kleidiai subdirectory to the build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/thirdparty/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ov_build_kleidiai_static)\n    set(BUILD_SHARED_LIBS OFF)\n    set(KLEIDIAI_BUILD_TESTS OFF CACHE BOOL \"\" FORCE)\n    add_subdirectory(kleidiai EXCLUDE_FROM_ALL)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Example Protopipe Configuration (YAML)\nDESCRIPTION: This YAML configuration file demonstrates how to configure Protopipe for model validation, including specifying the model directory, device, compiler, log level, and metric parameters. It also includes an example of overwriting the global metric for a specific model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_dir:\n  local: C:\\workspace\\models\ndevice_name: NPU\ncompiler_type: MLIR\nlog_level: INFO\n\nsave_validation_outputs: actual-outputs/\nmetric: { name: norm, tolerance: 0.01 }\n\nmulti_inference:\n- input_stream_list:\n  - network:\n    - { name: A.xml, ip: FP16, input_data: A-inputs/, output_data: A-outputs/ }\n      # overwrites the global metric for the model B.xml\n    - { name: B.xml, ip: FP16, input_data: B-inputs/, output_data: B-outputs/, metric: { name: norm, tolerance: 0.0 }\n```\n\n----------------------------------------\n\nTITLE: Finding Coverage Report Index Files (find)\nDESCRIPTION: This snippet demonstrates how to find the generated `index.html` files within the `coverage` directory using the `find` command.  This allows the user to quickly locate the entry point for browsing the generated coverage reports.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/test_coverage.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ find coverage -maxdepth 2 -name index.html\ncoverage/core/index.html\ncoverage/transformations/index.html\ncoverage/paddle_frontend/index.html\ncoverage/tf_frontend/index.html\ncoverage/openvino/index.html\ncoverage/onnx_frontend/index.html\ncoverage/ir_frontend/index.html\ncoverage/low_precision_transformations/index.html\ncoverage/template_plugin/index.html\ncoverage/inference/index.html\ncoverage/frontend_common/index.html\n```\n\n----------------------------------------\n\nTITLE: Defining PreProcessSteps Interface in TypeScript\nDESCRIPTION: Defines the PreProcessSteps interface, which includes the resize method. The resize method is used to resize images during preprocessing.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PreProcessSteps.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface PreProcessSteps {\n    resize(algorithm): PreProcessSteps;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining the getAnyName Method in Output Interface\nDESCRIPTION: Defines the `getAnyName` method within the `Output` interface. This method returns the name of the output as a string.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Output.rst#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ngetAnyName(): string\n```\n\n----------------------------------------\n\nTITLE: Building the Extension Library\nDESCRIPTION: These are the shell commands used to build the extension library using CMake.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n$ cd src/core/template_extension/new\n$ mkdir build\n$ cd build\n$ cmake -DOpenVINO_DIR=<OpenVINO_DIR> ../\n$ cmake --build .\n```\n\nLANGUAGE: sh\nCODE:\n```\n$ cd src/core/template_extension/new\n$ mkdir build\n$ cd build\n$ cmake -DOpenVINO_DIR=$(python3 -c \"from openvino.utils import get_cmake_path; print(get_cmake_path(), end='')\") ../\n$ cmake --build .\n```\n\n----------------------------------------\n\nTITLE: Searching for OpenCL package\nDESCRIPTION: This command searches for the OpenCL package in the vcpkg repository.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/use_device_mem.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n> vcpkg search opencl\n```\n\n----------------------------------------\n\nTITLE: Setup Environment Variables PowerShell\nDESCRIPTION: Sets the environment variables for OpenVINO using the setupvars.ps1 script in PowerShell.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-windows.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n. <path-to-setupvars-folder>/setupvars.ps1\n```\n\n----------------------------------------\n\nTITLE: ExperimentalDetectronPriorGridGenerator XML Example\nDESCRIPTION: This XML snippet demonstrates the configuration of the ExperimentalDetectronPriorGridGenerator operation within an OpenVINO model. It showcases the usage of attributes like `flatten`, `h`, `stride_x`, `stride_y`, and `w`, as well as the input and output port definitions with their respective dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/experimental-detectron-prior-grid-generator-6.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ExperimentalDetectronPriorGridGenerator\" version=\"opset6\">\n    <data flatten=\"true\" h=\"0\" stride_x=\"32.0\" stride_y=\"32.0\" w=\"0\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>256</dim>\n            <dim>25</dim>\n            <dim>42</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>800</dim>\n            <dim>1344</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\" precision=\"FP32\">\n            <dim>3150</dim>\n            <dim>4</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name and CI Build Number (CMake)\nDESCRIPTION: Defines the target name for the unit tests and sets the CI build number. The target name is used to identify the test executable, and the CI build number can be used for versioning or identification purposes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/tests/unit/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_auto_batch_unit_tests)\n\nset(CI_BUILD_NUMBER \"unittest\")\n```\n\n----------------------------------------\n\nTITLE: Setting FlatBuffers Library Variable\nDESCRIPTION: This snippet sets the `flatbuffers_LIBRARY` variable to `FlatBuffers` in the parent scope. This variable is typically used to link against the FlatBuffers library in other parts of the project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/flatbuffers/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(flatbuffers_LIBRARY FlatBuffers PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries CMake\nDESCRIPTION: Links the target library against the OpenVINO runtime (`openvino::runtime`) and gflags (`${GFLAGS_TARGET}`). The `PUBLIC` keyword makes the OpenVINO runtime dependency available to other targets that link against this library, while `PRIVATE` makes gflags a private dependency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/utils/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino::runtime PRIVATE ${GFLAGS_TARGET})\n```\n\n----------------------------------------\n\nTITLE: Run Time Tests with Pytest\nDESCRIPTION: This snippet runs time tests using the `pytest` framework. It executes the `test_timetest.py` test suite, passing the path to the `timetest_infer` executable as an argument.  The second command runs the `run_timetest.py` script directly, likely for parse_stat testing.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest ./test_runner/test_timetest.py --exe ../../bin/intel64/Release/timetest_infer\n\n# For parse_stat testing:\npytest ./scripts/run_timetest.py\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: These commands add the `gtest_main_manifest` and `test_builtin_extensions` directories to the build process. `add_subdirectory` includes the CMakeLists.txt file within these directories, allowing them to be built as part of the overall project. This is used for including external dependencies or sub-projects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(gtest_main_manifest)\nadd_subdirectory(test_builtin_extensions)\n```\n\n----------------------------------------\n\nTITLE: Reshape Tensor with Second Dimension Calculation in OpenVINO XML\nDESCRIPTION: This XML snippet shows how to reshape a tensor while preserving the first dimension and calculating the second dimension.  The attribute 'special_zero' is set to true allowing the first dimension to be copied from the input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/reshape-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Reshape\" ...>\n    <data special_zero=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>   <!--The tensor contains 2 elements: 0, -1 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>3</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Running executable file with riscv64 emulation\nDESCRIPTION: This snippet shows how to launch an executable file with RISC-V 64-bit emulation using QEMU.  It specifies the path to the QEMU emulator, the target CPU, and the path to the executable file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\n<xuantie_install_path>/bin/qemu-riscv64 -cpu=<target_cpu> <executable_file_path>\n```\n\n----------------------------------------\n\nTITLE: Conditional Snippets Libxsmm TPP Configuration\nDESCRIPTION: If 'ENABLE_SNIPPETS_LIBXSMM_TPP' is enabled, this adds preprocessor definitions, compile definitions, and links the 'xsmm' library to the target. It also adds the include directories for 'xsmm'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_SNIPPETS_LIBXSMM_TPP)\n    add_definitions(-DSNIPPETS_LIBXSMM_TPP -DLIBXSMM_DEFAULT_CONFIG)\n    target_compile_definitions(xsmm PRIVATE __BLAS=0)\n    target_link_libraries(${TARGET_NAME} PRIVATE xsmm)\n    target_include_directories(${TARGET_NAME} SYSTEM PRIVATE $<TARGET_PROPERTY:xsmm,INCLUDE_DIRECTORIES>)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO GenAI from Archive - Ubuntu 22.04\nDESCRIPTION: These commands download and extract the OpenVINO GenAI archive for Ubuntu 22.04. The `curl` command downloads the tarball, and the `tar` command extracts its contents.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-genai.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino_genai/packages/2025.1/linux/openvino_genai_ubuntu22_2025.1.0.0_x86_64.tar.gz --output openvino_genai_2025.1.0.0.tgz\ntar -xf openvino_genai_2025.1.0.0.tgz\n```\n\n----------------------------------------\n\nTITLE: Protopipe Reference Mode Command\nDESCRIPTION: This command executes Protopipe in 'reference' mode, generating random input data and calculating reference outputs. It requires a configuration file (config.yaml) and specifies the number of iterations to run (-niter 10).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n./protopipe --cfg config.yaml --mode reference -niter 10\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target\nDESCRIPTION: Adds a clang-format target using `ov_add_clang_format_target` to format the code. It excludes files matching the pattern `.*\\.cxx`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME}\n                           EXCLUDE_PATTERNS \".*\\\\.cxx\")\n```\n\n----------------------------------------\n\nTITLE: Globbing Library Sources in CMake\nDESCRIPTION: This code snippet uses `file(GLOB_RECURSE)` to find all C++ source files (`*.cpp`) within the `src` directory and its subdirectories and stores them in the `LIBRARY_SRC` variable. It also finds all header files (`*.hpp`) within the `include` directory and stores them in the `PUBLIC_HEADERS` variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/offline_transformations/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE PUBLIC_HEADERS ${PUBLIC_HEADERS_DIR}/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Intel GPU Plugin Enable Check\nDESCRIPTION: This snippet checks if the Intel GPU is enabled, and returns if it is not. It ensures that the subsequent configuration steps are only executed when GPU support is enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT ENABLE_INTEL_GPU)\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Common Compiler Options\nDESCRIPTION: Sets common compiler options for different build configurations (Release). It conditionally applies `/Os` for MSVC compiler or `-Os -Wno-suggest-override` (if `SUGGEST_OVERRIDE_SUPPORTED` is enabled) for other compilers in Release configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(SUGGEST_OVERRIDE_SUPPORTED)\n    set(COMMON_COMPILE_OPTIONS $<$<CONFIG:Release>:$<IF:$<CXX_COMPILER_ID:MSVC>,/Os,-Os -Wno-suggest-override>>)\nelse()\n    set(COMMON_COMPILE_OPTIONS $<$<CONFIG:Release>:$<IF:$<CXX_COMPILER_ID:MSVC>,/Os,-Os>>)\nendif()\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Trailing Spaces\nDESCRIPTION: This rule disallows trailing spaces at the end of lines in JavaScript and TypeScript code. Removing trailing spaces ensures cleaner code and is enforced by ESLint with the configuration `no-trailing-spaces: ['error']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_11\n\nLANGUAGE: JavaScript\nCODE:\n```\nno-trailing-spaces: ['error']\n```\n\n----------------------------------------\n\nTITLE: Add Subdirectories for Components CMake\nDESCRIPTION: These snippets add subdirectories for various OpenVINO components like common, core, frontends, plugins, inference, and bindings. Each subdirectory contains a separate CMakeLists.txt file that defines the build rules for that specific component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(common)\nadd_subdirectory(core)\nadd_subdirectory(frontends)\nadd_subdirectory(plugins)\nadd_subdirectory(inference)\nadd_subdirectory(bindings)\n```\n\n----------------------------------------\n\nTITLE: Adding Library Target in CMake\nDESCRIPTION: This CMake code finds all `.cpp` and `.h` files recursively, then creates a shared library target named `sea_itt_lib` using the discovered source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/itt_collector/sea_itt_lib/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES \"*.cpp\" \"*.h\")\n\nadd_library(${TARGET_NAME} SHARED ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Define OV Component Macro CMake\nDESCRIPTION: This snippet defines the IN_OV_COMPONENT macro, which is likely used to indicate that the current code is part of the OpenVINO component. This allows for conditional compilation or behavior within the OpenVINO project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_definitions(-DIN_OV_COMPONENT)\n```\n\n----------------------------------------\n\nTITLE: Double Value Calculation (C++)\nDESCRIPTION: This code shows how to construct a double-precision floating-point number from two 32-bit unsigned integers (x0 and x1), representing the high and low parts of the mantissa, respectively. The code then combines these parts with the exponent to calculate the final double value.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nmantissa_h = x0 & 0xfffffu; // upper 20 bits of mantissa\nmantissa_l = x1; // lower 32 bits of mantissa\nmantissa = (mantissa_h << 32) | mantissa_l;\nval = ((exponent << 52) | mantissa) - 1.0,\n```\n\n----------------------------------------\n\nTITLE: Add Custom Command to Build Wheel - CMake\nDESCRIPTION: This CMake code adds a custom command to build the Python wheel using the `wheel_build_command` defined earlier. It specifies the output file (`openvino_wheel_path`), dependencies, working directory, and a comment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(OUTPUT ${openvino_wheel_path}\n    COMMAND ${setup_py_env} ${wheel_build_command}\n    DEPENDS ${ov_setup_py_deps}\n    WORKING_DIRECTORY \"${CMAKE_CURRENT_SOURCE_DIR}\"\n    COMMENT \"Building Python wheel ${openvino_wheel_name}\"\n    VERBATIM)\nset(ie_wheel_deps ${openvino_wheel_path})\n```\n\n----------------------------------------\n\nTITLE: LessEqual Layer Configuration (Numpy Broadcast) - XML\nDESCRIPTION: This XML snippet showcases the configuration of a LessEqual layer in OpenVINO utilizing numpy broadcasting rules. The `auto_broadcast` attribute is set to 'numpy', allowing for automatic broadcasting of input tensors according to numpy conventions. The input and output ports specify the tensor dimensions before and after broadcasting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/lessequal-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"LessEqual\">\n    <data auto_broadcast=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding SHL Subdirectory\nDESCRIPTION: This snippet conditionally includes the SHL subdirectory and installs its static library if the `ENABLE_SHL_FOR_CPU` flag is set.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/thirdparty/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_SHL_FOR_CPU)\n    add_subdirectory(shl)\n    ov_install_static_lib(shl ${OV_CPACK_COMP_CORE})\nendif()\n```\n\n----------------------------------------\n\nTITLE: DFT Layer XML Definition (signal_size, 5D input, -1, unsorted axes)\nDESCRIPTION: Another example of a DFT layer with a 5-dimensional input tensor, including a signal_size input with '-1' and unsorted axes. Demonstrates flexibility in specifying DFT parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/dft-7.rst#_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"DFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>580</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim> <!-- axes input contains  [3, 0, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim> <!-- signal_size input contains [258, -1, 2056] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>2056</dim>\n            <dim>258</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating Pybind11 Module\nDESCRIPTION: Adds a Pybind11 module using `pybind11_add_module`. This command creates a Python module named `${TARGET_NAME}` (test_utils_api) from the specified source files. `NO_EXTRAS` indicates that no extra arguments are passed to the linker.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\npybind11_add_module(${TARGET_NAME} MODULE NO_EXTRAS ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Defining OpenVINO Sample Project with CMake\nDESCRIPTION: This CMake code snippet defines an OpenVINO sample project named `hello_query_device`. It specifies the source file (`main.cpp`) and the required dependencies (`ie_samples_utils`). The `ov_add_sample` macro simplifies the process of creating OpenVINO sample projects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_query_device/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_sample(NAME hello_query_device\n              SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/main.cpp\"\n              DEPENDENCIES ie_samples_utils)\n```\n\n----------------------------------------\n\nTITLE: Exporting Targets for Developer Package (CMake)\nDESCRIPTION: These snippets export the specified targets (openvino::npu_zero_utils, level-zero-ext, level-zero-headers and ze_loader when defined) for use in developer packages, including setting installation include directories and installing the corresponding static libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/zero/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET openvino::npu_zero_utils\n                                    INSTALL_INCLUDE_DIRECTORIES\n                                        $<BUILD_INTERFACE:${NPU_UTILS_SOURCE_DIR}/include>)\n\nov_developer_package_export_targets(TARGET level-zero-ext)\nov_install_static_lib(level-zero-ext ${NPU_PLUGIN_COMPONENT})\n\nov_developer_package_export_targets(TARGET level-zero-headers)\nov_install_static_lib(level-zero-headers ${NPU_PLUGIN_COMPONENT})\n\nif(TARGET ze_loader)\n    ov_developer_package_export_targets(TARGET ze_loader)\n    ov_install_static_lib(ze_loader ${NPU_PLUGIN_COMPONENT})\n    \n    add_dependencies(${TARGET_NAME} ze_loader)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditional Step Execution Based on Smart CI Output (YAML)\nDESCRIPTION: This YAML snippet shows how to conditionally execute a specific step within a job based on the output of the Smart CI job, useful for running only the required steps for a particular component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_11\n\nLANGUAGE: YAML\nCODE:\n```\njob_that_validates_your_component:\n  needs: [Build, Smart_CI]\n  ...\n  steps:\n    # The step below will start if YOUR_COMPONENT is affected and the \"build\" scope is required\n    - name: step_name\n      if: fromJSON(needs.smart_ci.outputs.affected_components).YOUR_COMPONENT.build # or <...>.test, if needed\n```\n\n----------------------------------------\n\nTITLE: Installing libpython3-dev:riscv64\nDESCRIPTION: This snippet installs the `libpython3-dev:riscv64` package using the apt package manager. This package is required to cross-compile the OpenVINO Runtime Python API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\napt-get install -y --no-install-recommends libpython3-dev:riscv64\n```\n\n----------------------------------------\n\nTITLE: 3D GroupConvolutionBackpropData Example (OpenVINO XML)\nDESCRIPTION: Example of a 3D GroupConvolutionBackpropData layer configuration in OpenVINO XML format. This configuration specifies the layer's ID, name, type, attributes (dilations, pads_begin, pads_end, strides), input port dimensions, and output port dimensions with precision.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/group-convolution-backprop-data-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"5\" name=\"upsampling_node\" type=\"GroupConvolutionBackpropData\">\n    <data dilations=\"1,1,1\" pads_begin=\"1,1,1\" pads_end=\"1,1,1\" strides=\"2,2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>20</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>\n            <dim>5</dim>\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>8</dim>\n            <dim>447</dim>\n            <dim>447</dim>\n            <dim>447</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties in CMake\nDESCRIPTION: This snippet uses set_target_properties to set the FOLDER and CXX_STANDARD properties of the target library. The FOLDER property specifies the directory where the library should be placed in the IDE, and CXX_STANDARD sets the C++ standard to 17.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/common/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES\n                          FOLDER ${CMAKE_CURRENT_SOURCE_DIR}\n                          CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: MaxPool Layer with 'same_upper' auto_pad (XML)\nDESCRIPTION: This XML snippet defines a MaxPool layer with 'same_upper' auto_pad, a kernel size of 2x2, padding of 1x1 on both sides, and strides of 2x2. It processes a 4D input tensor with dimensions 1x3x32x32 and produces a 4D output tensor with the same dimensions, along with a second output port for indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-14.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MaxPool\" ... >\n    <data auto_pad=\"same_upper\" kernel=\"2,2\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Using Or for pattern matching\nDESCRIPTION: This C++ snippet demonstrates how to use the Or operator to match either one sequence of nodes or another, effectively creating two distinct branches in the pattern matching process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\nshared_ptr<ov::Node> pattern_or() {\n    // Creating two different branches for the pattern\n    auto relu_node = make_shared<WrapType<opset13::Relu>>();\n    auto sigmoid_node = make_shared<WrapType<opset13::Sigmoid>>();\n\n    // Using Or to create two different branches for matching\n    auto pattern_or_node = make_shared<Or>(OutputVector{relu_node, sigmoid_node});\n\n    return pattern_or_node;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining i64 data type in TypeScript\nDESCRIPTION: Defines the i64 data type as a number in TypeScript. This represents a 64-bit integer, providing the largest range for integer values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/element.rst#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ni64: number\n```\n\n----------------------------------------\n\nTITLE: TensorFlow: Quantize Model\nDESCRIPTION: Applies post-training quantization to a TensorFlow model using NNCF. It takes the model and a representative dataset as input and quantizes the model to 8-bit precision, optimizing it for efficient deployment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/quantizing-models-post-training/basic-quantization-flow.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nquantized_model = nncf.quantize(model=model, calibration_dataset=dataset)\n\n```\n\n----------------------------------------\n\nTITLE: StringTensorUnpack with Empty String in XML\nDESCRIPTION: This XML example illustrates the StringTensorUnpack operation when the input tensor contains an empty string. It shows how the 'begins', 'ends', and 'symbols' tensors are affected by the presence of an empty string in the input batch. The output dimensions and values are adjusted accordingly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/string-tensor-unpack-15.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StringTensorUnpack\" ... >\n    <input>\n        <port id=\"0\" precision=\"STRING\">\n            <dim>5</dim>     <!-- batch of strings -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"I32\">\n            <dim>2</dim>     <!-- begins = [0, 3, 3, 8, 9] -->\n        </port>\n        <port id=\"1\" precision=\"I32\">\n            <dim>2</dim>     <!-- ends = [3, 3, 8, 9, 13] -->\n        </port>\n        <port id=\"2\" precision=\"U8\">\n            <dim>13</dim>    <!-- symbols = \"OMZGenAI 2024\" encoded in an utf-8 array -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Multiple Empty Lines\nDESCRIPTION: This rule limits the number of consecutive empty lines in JavaScript and TypeScript code. It allows a maximum of one empty line, with none at the beginning or end of a file, configured as `no-multiple-empty-lines: ['error', { max: 1, maxBOF: 0, maxEOF: 0 }]`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_17\n\nLANGUAGE: JavaScript\nCODE:\n```\nno-multiple-empty-lines: ['error', { max: 1, maxBOF: 0, maxEOF: 0 }]\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files\nDESCRIPTION: Uses `file(GLOB_RECURSE)` to find all C++ and header files in the current directory and its subdirectories and stores them in the `SOURCES` variable. This list is then used to populate the source group.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/common/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES *.cpp *.hpp *.h)\nsource_group(TREE ${CMAKE_CURRENT_SOURCE_DIR} FILES ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Exporting Developer Package CMake\nDESCRIPTION: This snippet configures the export of the library target for the developer package using 'ov_developer_package_export_targets'.  It specifies which target to export and sets the installation include directories for the exported target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/ov_helpers/ov_lpt_models/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nov_developer_package_export_targets(TARGET ${TARGET_NAME}\n                                    INSTALL_INCLUDE_DIRECTORIES \"${PUBLIC_HEADERS_DIR}/\")\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target (CMake)\nDESCRIPTION: Adds a clang-format target for the specified target.  The target name is suffixed with '_clang'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Link to Paddle Frontend CMake\nDESCRIPTION: This snippet links the 'paddle_fe_standalone_build_test' target to the OpenVINO Paddle frontend library using `target_link_libraries`.  It specifies that the dependency is public, meaning it will be transitively linked to other targets that depend on this library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/tests/standalone_build/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino::frontend::paddle)\n```\n\n----------------------------------------\n\nTITLE: Build Memory Tests with CMake and Make\nDESCRIPTION: This snippet demonstrates how to build the OpenVINO memory tests using CMake and Make. It involves creating a build directory, navigating into it, running CMake to generate the Makefiles, and then using Make to compile the tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build && cd build\ncmake .. && make memory_tests\n```\n\n----------------------------------------\n\nTITLE: Constant Layer Definition in XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a Constant layer in OpenVINO. It defines the layer's type as \"Constant\" and specifies attributes such as the offset in the binary file, the size of the constant content, the element type of the tensor, and the shape of the output tensor. The output port defines the dimensions of the resulting tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/constant-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Constant\">\n    <data offset=\"1000\" size=\"256\" element_type=\"f32\" shape=\"8,8\"/>\n    <output>\n        <port id=\"1\">\n            <dim>8</dim>\n            <dim>8</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard to 17 CMake\nDESCRIPTION: This snippet sets the C++ standard to version 17. It ensures that the code is compiled using the C++17 standard, enabling the use of features and functionalities available in this version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific File Handling (Linux) in CMake\nDESCRIPTION: This conditional block handles platform-specific file inclusion. On non-Windows systems, it removes Windows-specific files from the `LIBRARY_SRC` variable.  It uses `file(GLOB_RECURSE)` to find Windows-specific files and then `list(REMOVE_ITEM)` to exclude them.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nelse()\n    # Remove windows specific files\n    file(GLOB_RECURSE WIN_FILES ${CMAKE_CURRENT_SOURCE_DIR}/src/os/win/*.cpp\n                                ${CMAKE_CURRENT_SOURCE_DIR}/src/os/win/*.hpp)\n    list(REMOVE_ITEM LIBRARY_SRC ${WIN_FILES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Broadcast with Numpy Mode in XML\nDESCRIPTION: This XML snippet demonstrates the Broadcast operation with the 'numpy' mode. It broadcasts a tensor of shape [16, 1, 1] to the shape [1, 16, 50, 50]. The third input (axes_mapping) is not provided when using 'numpy' mode.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/broadcast-3.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Broadcast\" ...>\n    <data mode=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n       </port>\n        <port id=\"1\">\n            <dim>4</dim>   <!--The tensor contains 4 elements: [1, 16, 50, 50] -->\n        </port>\n        <!-- the 3rd input shouldn't be provided with mode=\"numpy\" -->\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>16</dim>\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties for Optimization in CMake\nDESCRIPTION: This snippet sets the interprocedural optimization property for the target. `INTERPROCEDURAL_OPTIMIZATION_RELEASE` is set based on the value of `ENABLE_LTO` (Likely Link Time Optimization), which improves performance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/compiler_adapter/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Setting Initial Files Runtime\nDESCRIPTION: This snippet sets the `INIT_FILES_RUNTIME` variable to a list of paths to `__init__.py` files within the OpenVINO project structure. It is used as an argument in function `ov_check_init_files_alignment` to check file alignment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(INIT_FILES_RUNTIME \"${OpenVINOPython_SOURCE_DIR}/src/openvino/__init__.py\"\n                       \"${OpenVINO_SOURCE_DIR}/tools/ovc/openvino/__init__.py\"\n                       \"${OpenVINO_SOURCE_DIR}/tools/benchmark_tool/openvino/__init__.py\")\n```\n\n----------------------------------------\n\nTITLE: CMake Project Setup\nDESCRIPTION: This snippet initializes the CMake project. It sets the minimum required CMake version and declares the project name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/conditional_compilation/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\n\nproject(conditional_compilation_tests)\n```\n\n----------------------------------------\n\nTITLE: Check for dynamic dimensions in input layers - C++\nDESCRIPTION: This C++ code snippet demonstrates how to determine if a model has dynamic dimensions. It iterates through each input layer, obtains the partial shape, and prints the name and dimensions.  Dynamic dimensions are represented as '?'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/model-input-output/dynamic-shapes.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n//! [ov_dynamic_shapes:check_inputs]\nauto model = core.read_model(\"model.xml\");\nfor (const auto& input : model->inputs()) {\n    std::cout << \"Input: \" << input.get_any_name() << \" \" << input.get_partial_shape() << std::endl;\n}\n//! [ov_dynamic_shapes:check_inputs]\n```\n\n----------------------------------------\n\nTITLE: Handling Protoc Binary Build for Cross-compilation\nDESCRIPTION: This snippet handles building the `protoc` binary based on cross-compilation status and target architectures (Apple Silicon, MSVC with ARM).  If cross-compiling or targeting specific architectures with MSVC, it disables building `protoc` directly and defers to a native build.  Otherwise, `protoc` will be built as part of the protobuf build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/protobuf/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CROSSCOMPILING OR\n    (APPLE AND (HOST_X86_64 AND AARCH64)) OR\n    (MSVC AND (HOST_X86_64 AND (AARCH64 OR ARM))))\n    set(protobuf_BUILD_PROTOC_BINARIES OFF CACHE BOOL \"Build protoc binaries\" FORCE)\nelse()\n    set(protobuf_BUILD_PROTOC_BINARIES ON CACHE BOOL \"Build protoc binaries\" FORCE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Alias Library\nDESCRIPTION: This snippet creates an alias for the target, providing a more user-friendly name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(openvino::interpreter_backend ALIAS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Building C++ Samples on Windows (Command Prompt)\nDESCRIPTION: These commands navigate to the C++ samples directory and execute the `build_samples_msvc.bat` batch file. This batch file compiles the sample applications for Windows using the MSVC compiler.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncd <INSTALLDIR>\\samples\\cpp\nbuild_samples_msvc.bat\n```\n\n----------------------------------------\n\nTITLE: Excluding RISCV64 Paths\nDESCRIPTION: This snippet excludes RISCV64 specific paths when architecture is not RISCV64.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_23\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT RISCV64)\n    list(APPEND EXCLUDE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/emitters/plugin/riscv64/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/nodes/kernels/riscv64/*)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Project Name\nDESCRIPTION: Sets the name of the CMake project to 'memory_tests'. This name is used for generating build system artifacts.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nproject(memory_tests)\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library (CMake)\nDESCRIPTION: This snippet installs the static library into the specified NPU plugin component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/zero/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${NPU_PLUGIN_COMPONENT})\n```\n\n----------------------------------------\n\nTITLE: Defining Target Name CMake\nDESCRIPTION: Defines the target name for the library to be built. The variable TARGET_NAME is set to 'tests_shared_lib'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/lib/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset (TARGET_NAME \"tests_shared_lib\")\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library\nDESCRIPTION: This snippet installs the static library as part of the core OpenVINO component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: Changing directory to vcpkg\nDESCRIPTION: This command changes the current directory to the vcpkg directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/use_device_mem.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n> cd vcpkg\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library (CMake)\nDESCRIPTION: Installs the static library to the specified location, under the OV_CPACK_COMP_CORE component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: OVSA Sample Image Retrieval\nDESCRIPTION: Downloads a sample image for inferencing using curl.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_58\n\nLANGUAGE: sh\nCODE:\n```\ncurl --create-dirs https://raw.githubusercontent.com/openvinotoolkit/model_server/master/example_client/images/people/people1.jpeg -o images/people1.jpeg\n```\n\n----------------------------------------\n\nTITLE: Assign-6 XML Example\nDESCRIPTION: This XML snippet demonstrates the usage of the Assign-6 operation within an OpenVINO model.  It shows how to define the layer with the 'Assign' type, specify the 'variable_id', and connect the input port with its dimensions.  The 'force' directive in the reStructuredText ensures that the XML is displayed as a code block without interpretation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/assign-6.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Assign\" ...>\n    <data variable_id=\"lstm_state_1\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </input>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Define Model input Method (TypeScript) - Index Parameter\nDESCRIPTION: This code defines the `input` method of the `Model` interface with an index parameter. It gets the input of the model identified by the index.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\ninput(index: number): Output\n```\n\n----------------------------------------\n\nTITLE: Add timetests Subdirectory CMake\nDESCRIPTION: Adds the 'timetests' subdirectory to the current CMake build process.  This incorporates the CMakeLists.txt file within the 'timetests' directory into the current build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(timetests)\n```\n\n----------------------------------------\n\nTITLE: Adding Cpplint Target (CMake)\nDESCRIPTION: This conditional block adds a cpplint target for the library if the `add_cpplint_target` command is available. This helps enforce coding style guidelines.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif(COMMAND add_cpplint_target)\n  add_cpplint_target(${TARGET_NAME}_cpplint FOR_TARGETS ${TARGET_NAME})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCL package\nDESCRIPTION: This command installs the OpenCL package using vcpkg.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/use_device_mem.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n> vcpkg install opencl\n```\n\n----------------------------------------\n\nTITLE: Updating pip and setuptools\nDESCRIPTION: These commands update pip and setuptools to newer versions using the python3 package manager. It is a prerequisite step before installing python requirements to build the OpenVINO Runtime Python API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n% python3 -m pip install -U pip\n```\n\n----------------------------------------\n\nTITLE: Creating Static Library Target with CMake\nDESCRIPTION: This CMake snippet creates a static library target named `func_test_utils`. It specifies include directories, source directories, and libraries to link against (common_test_utils, openvino::runtime, openvino::runtime::dev, openvino::pugixml). The include path is configured as a build interface property.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/functional_test_utils/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME func_test_utils)\n\nov_add_target(\n    NAME ${TARGET_NAME}\n    TYPE STATIC\n    ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n    ADD_CLANG_FORMAT\n    INCLUDES\n        PUBLIC\n            \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\"\n    ADDITIONAL_SOURCE_DIRS\n        ${CMAKE_CURRENT_SOURCE_DIR}/src\n    LINK_LIBRARIES\n        PUBLIC\n            common_test_utils\n            openvino::runtime\n            openvino::runtime::dev\n        PRIVATE\n            openvino::pugixml\n)\n```\n\n----------------------------------------\n\nTITLE: Copying CM Kernels to Cache Directory\nDESCRIPTION: This CMake snippet iterates through the list of found CM kernels and creates a custom command for each kernel to copy it to the code generation cache directory. The `copy_if_different` command ensures that the file is only copied if it has changed, optimizing the build process. The copied kernels are added to the `COPIED_KERNELS` list for later use.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/cm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(KERNEL IN LISTS KERNELS)\n    get_filename_component(FILENAME ${KERNEL} NAME)\n    add_custom_command(\n        OUTPUT \"${CODEGEN_CACHE_DIR}/cm_kernels/${FILENAME}\"\n        COMMAND ${CMAKE_COMMAND} -E copy_if_different ${KERNEL} \"${CODEGEN_CACHE_DIR}/cm_kernels/${FILENAME}\"\n        DEPENDS ${KERNEL} \"${CODEGEN_CACHE_DIR}/cm_kernels\"\n        COMMENT \"Copying ${FILE} ${CODEGEN_CACHE_DIR}/cm_kernels\"\n    )\n    list(APPEND COPIED_KERNELS \"${CODEGEN_CACHE_DIR}/cm_kernels/${FILENAME}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Conditional Source Path Exclusion\nDESCRIPTION: This snippet conditionally excludes source paths based on the architecture.  If the architecture is not X86_64, `paged_attn_cache_rotation.cpp` is excluded; otherwise, `stub.cpp` is excluded. This uses `EXCLUDED_SOURCE_PATHS_FOR_UNIT_TEST` list.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/vectorized/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT X86_64)\n    list(APPEND EXCLUDED_SOURCE_PATHS_FOR_UNIT_TEST\n      ${CMAKE_CURRENT_SOURCE_DIR}/paged_attn_cache_rotation.cpp)\nelse()\n    list(APPEND EXCLUDED_SOURCE_PATHS_FOR_UNIT_TEST\n      ${CMAKE_CURRENT_SOURCE_DIR}/stub.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Reference Implementation Include Directories\nDESCRIPTION: Adds the interface include directories of the `openvino::reference` target as private include directories to the current target. This allows the target to access template reference implementations when optimized implementations are not available.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_38\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PRIVATE $<TARGET_PROPERTY:openvino::reference,INTERFACE_INCLUDE_DIRECTORIES>)\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions and Link Libraries\nDESCRIPTION: This snippet sets compile definitions and link libraries for the target. `target_compile_definitions` adds the definitions specified in the `DEFINITIONS` variable, while `target_link_libraries` links the libraries specified in the `DEPENDENCIES` variable to the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/test_builtin_extensions/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME} PRIVATE ${DEFINITIONS})\ntarget_link_libraries(${TARGET_NAME} PRIVATE ${DEPENDENCIES})\n```\n\n----------------------------------------\n\nTITLE: AvgPool Configuration: explicit padding, exclude-pad true\nDESCRIPTION: This XML snippet sets up an AvgPool layer with 'explicit' auto_pad, setting 'exclude-pad' to true.  It configures a 5x5 kernel, 1x1 padding on both begin and end, a 3x3 stride, and defines input/output port dimensions. This config uses the padding values specified directly and excludes padded values from the average calculation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/avg-pool-14.rst#_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"AvgPool\" ... >\n    <data auto_pad=\"explicit\" exclude-pad=\"true\" kernel=\"5,5\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"3,3\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: Adds subdirectories for third-party libraries (gflags, gtest, pugixml) and a tests library. These subdirectories are excluded from the ALL target, meaning they are not built by default.  The thirdparty libraries are located in `${OpenVINO_SOURCE_DIR}/thirdparty/...` and built in `${CMAKE_CURRENT_BINARY_DIR}/..._build`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/common/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(${OpenVINO_SOURCE_DIR}/thirdparty/gflags\n                 ${CMAKE_CURRENT_BINARY_DIR}/gflags_build\n                 EXCLUDE_FROM_ALL)\nadd_subdirectory(${OpenVINO_SOURCE_DIR}/thirdparty/gtest\n                 ${CMAKE_CURRENT_BINARY_DIR}/gtest_build\n                 EXCLUDE_FROM_ALL)\nadd_subdirectory(${OpenVINO_SOURCE_DIR}/thirdparty/pugixml\n                 ${CMAKE_CURRENT_BINARY_DIR}/pugixml_build\n                 EXCLUDE_FROM_ALL)\n\nadd_subdirectory(\"${OpenVINO_SOURCE_DIR}/tests/lib\" tests_shared_lib)\n```\n\n----------------------------------------\n\nTITLE: Adding DNNL Include Directories\nDESCRIPTION: This adds the DNNL include directories to the target's include path. This ensures that the compiler can find the DNNL headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} SYSTEM PRIVATE\n    $<TARGET_PROPERTY:dnnl,INCLUDE_DIRECTORIES>)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for LIR Dump (Bash)\nDESCRIPTION: This environment variable activates the LIR dumping feature. The options control which passes are dumped, the directory to store the dumps, and the output format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/docs/debug_capabilities/linear_ir_passes_serialization.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOV_SNIPPETS_DUMP_LIR=<space_separated_options> binary ...\n```\n\n----------------------------------------\n\nTITLE: Maximum-1 XML Example - No Broadcasting\nDESCRIPTION: This example demonstrates the Maximum-1 operation with no broadcasting.  The input tensors have matching shapes. The auto_broadcast attribute is set to \"none\".\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/maximum-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Maximum\">\n    <data auto_broadcast=\"none\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Add Cpplint Target CMake\nDESCRIPTION: This snippet adds a custom target for running Cpplint on the source code of the `openvino_snippets` library. Cpplint is a style checker for C++ code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nadd_cpplint_target(${TARGET_NAME}_cpplint FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory\nDESCRIPTION: This snippet adds the 'src' subdirectory to the build process.  This implies that the source code for the 'time_tests' project is located within this 'src' directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties for Optimization CMake\nDESCRIPTION: This snippet sets target properties for interprocedural optimization. It sets the `INTERPROCEDURAL_OPTIMIZATION_RELEASE` property to the value of `ENABLE_LTO`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/src/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Finding CM Kernels and Headers\nDESCRIPTION: This CMake snippet uses `file(GLOB_RECURSE)` to find all `.cm` kernel files and `.h` header files in the current source directory and its subdirectories. The paths to these files are stored in the `KERNELS` and `HEADERS` variables, respectively. This ensures that all relevant files are included in the code generation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/cm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE KERNELS \"${CMAKE_CURRENT_SOURCE_DIR}/*.cm\")\nfile(GLOB_RECURSE HEADERS \"${CMAKE_CURRENT_SOURCE_DIR}/include/*.h\")\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Arrow Function Spacing\nDESCRIPTION: This rule enforces consistent spacing around arrow functions in JavaScript and TypeScript code. It enhances code readability and is enforced by ESLint with the configuration `arrow-spacing: ['error']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\narrow-spacing: ['error']\n```\n\n----------------------------------------\n\nTITLE: Defining Compiler Definitions for the Test Target\nDESCRIPTION: This CMake snippet defines a private compiler definition `CI_BUILD_NUMBER` for the test target, setting its value to \"mock_version\". This definition can be used within the test code to access the build number during execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/tests/functional/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME} PRIVATE CI_BUILD_NUMBER=\\\"mock_version\\\")\n```\n\n----------------------------------------\n\nTITLE: create_sync_infer_request() Implementation C++\nDESCRIPTION: This code snippet shows how to create a synchronous inference request.  It increments the request ID counter and creates an instance of the SyncInferRequest class, passing the necessary context. The SyncInferRequest class defines the pipeline stages and executes them synchronously in its `infer` method.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/compiled-model.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nov::InferRequest CompiledModel::create_infer_request() {\n    return create_infer_request_impl({});\n}\n\nov::InferRequest CompiledModel::create_infer_request_impl(const ov::AnyMap& request_cfg) {\n    m_request_id++;\n    return std::make_shared<SyncInferRequest>(m_model, shared_from_this(), request_cfg, m_request_id);\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Source Files\nDESCRIPTION: This snippet explicitly lists the source files that make up the interpreter backend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset (SRC\n    backend.cpp\n    backend.hpp\n    executable.cpp\n    executable.hpp\n    int_backend.cpp\n    int_executable.cpp\n    evaluates_map.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for Tests in CMake\nDESCRIPTION: This CMake snippet conditionally adds a subdirectory named `tests` to the build process when the `ENABLE_TESTS` flag is set. It allows integrating and building the tests as part of the overall project build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Less-1 Layer Definition (Broadcast) - XML\nDESCRIPTION: Illustrates a Less-1 layer in OpenVINO using broadcasting.  The input tensors have different dimensions, and broadcasting rules are applied to perform the element-wise comparison. The output port reflects the resulting dimensions after broadcasting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/less-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Less\">\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting PYTHONPATH for OpenVINO\nDESCRIPTION: This bash command sets the `PYTHONPATH` environment variable to include the path to the OpenVINO Python API build directory. This is necessary to ensure that `pybind11-stubgen` can find the latest changes when generating stub files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/stubs.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nexport PYTHONPATH=$PYTHONPATH:/home/pwysocki/openvino/bin/intel64/Release/python\n```\n\n----------------------------------------\n\nTITLE: Define Model Interface (TypeScript)\nDESCRIPTION: This code snippet defines the `Model` interface in TypeScript. It includes method signatures for cloning, accessing inputs/outputs, retrieving names and shapes, checking dynamism, and setting a friendly name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface Model {\n    clone(): Model;\n    inputs: Output[];\n    outputs: Output[];\n    getFriendlyName(): string;\n    getName(): string;\n    getOutputShape(index): number[];\n    getOutputSize(): number;\n    getOutputElementType(index): string;\n    input(): Output;\n    input(name): Output;\n    input(index): Output;\n    isDynamic(): boolean;\n    output(): Output;\n    output(name): Output;\n    output(index): Output;\n    setFriendlyName(name): void;\n}\n```\n\n----------------------------------------\n\nTITLE: Calling ov_add_onednn Function\nDESCRIPTION: This line invokes the `ov_add_onednn` function, which configures and includes the oneDNN library in the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/thirdparty/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_onednn()\n```\n\n----------------------------------------\n\nTITLE: Add Static Library CMake\nDESCRIPTION: This snippet creates a static library target named `openvino_snippets` from the source files and public headers. The `STATIC` keyword specifies that it is a static library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC\n            ${LIBRARY_SRC}\n            ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Setting CMake variables for code generation paths\nDESCRIPTION: Defines CMake variables to manage directories for generated code, including the main directory, cache directory, and include directory.  These paths are used in subsequent commands to locate and store generated OpenCL kernel files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/ocl_v2/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_intel_gpu_ocl_v2_obj\")\n\n# Path which points to root directory where code generated elements are created\n# (specific to build configuration).\nset(CODEGEN_DIR \"${CMAKE_CURRENT_BINARY_DIR}/codegen\")\nset(CODEGEN_CACHE_DIR  \"${CODEGEN_DIR}/cache\")\n\n# Path which points to automatically included directory with code generated elements\n# (to support \"copy-if-different\" optimization).\nset(CODEGEN_INCDIR  \"${CODEGEN_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: ExperimentalDetectronROIFeatureExtractor XML Example\nDESCRIPTION: This XML snippet demonstrates the configuration of the ExperimentalDetectronROIFeatureExtractor operation within an OpenVINO model.  It defines the layer type, attributes such as `output_size`, `pyramid_scales`, and `sampling_ratio`, and specifies the input and output ports with their respective dimensions. The `aligned` attribute is set to `false`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/experimental-detectron-roi-feature-extractor-6.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ExperimentalDetectronROIFeatureExtractor\" version=\"opset6\">\n    <data aligned=\"false\" output_size=\"7\" pyramid_scales=\"4,8,16,32,64\" sampling_ratio=\"2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1000</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>256</dim>\n            <dim>200</dim>\n            <dim>336</dim>\n        </port>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>256</dim>\n            <dim>100</dim>\n            <dim>168</dim>\n        </port>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>256</dim>\n            <dim>50</dim>\n            <dim>84</dim>\n        </port>\n        <port id=\"4\">\n            <dim>1</dim>\n            <dim>256</dim>\n            <dim>25</dim>\n            <dim>42</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"5\" precision=\"FP32\">\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>7</dim>\n            <dim>7</dim>\n        </port>\n        <port id=\"6\" precision=\"FP32\">\n            <dim>1000</dim>\n            <dim>4</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Conditional Object Library Setting for Shared Libs\nDESCRIPTION: This snippet conditionally sets the object library based on whether shared libraries are being built. If `BUILD_SHARED_LIBS` is true, it sets the `OBJ_LIB` variable to the object files of the `openvino_intel_cpu_plugin_obj` target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/vectorized/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_SHARED_LIBS)\n    set (OBJ_LIB $<TARGET_OBJECTS:openvino_intel_cpu_plugin_obj>)\nendif()\n```\n\n----------------------------------------\n\nTITLE: MVN-6 Layer Configuration in XML\nDESCRIPTION: This XML snippet demonstrates the configuration of an MVN-6 layer within an OpenVINO model. It shows how to set the `eps`, `eps_mode`, and `normalize_variance` attributes.  The input ports define the dimensions of the input data and the axes for normalization. The output port defines the dimensions of the resulting normalized tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/mvn-6.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MVN\">\n    <data eps=\"1e-9\" eps_mode=\"inside_sqrt\" normalize_variance=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim> <!-- value of [0,2,3] means independent normalization per channels -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries in CMake\nDESCRIPTION: Links the `${TARGET_NAME}` library to its dependencies. Public dependencies (gtest, pugixml, gflags, tests_shared_lib) are linked to any target that links to `${TARGET_NAME}`, while private dependencies (gtest_main) are only linked to `${TARGET_NAME}` itself.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/common/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME}\n    PUBLIC\n        gtest\n        pugixml\n        gflags\n        tests_shared_lib\n    PRIVATE\n        gtest_main)\n```\n\n----------------------------------------\n\nTITLE: Adding Install Dependencies Component with CPack CMake\nDESCRIPTION: This snippet configures the installation of the `install_openvino_dependencies.sh` script on Linux systems.  The script is placed in the `install_dependencies/` subdirectory and assigned to the `OV_CPACK_COMP_INSTALL_DEPENDENCIES` component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/scripts/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(LINUX)\n    ov_cpack_add_component(${OV_CPACK_COMP_INSTALL_DEPENDENCIES} HIDDEN)\n\n    install(PROGRAMS \"${CMAKE_CURRENT_SOURCE_DIR}/install_dependencies/install_openvino_dependencies.sh\"\n            DESTINATION install_dependencies/\n            COMPONENT ${OV_CPACK_COMP_INSTALL_DEPENDENCIES}\n            ${OV_CPACK_COMP_INSTALL_DEPENDENCIES_EXCLUDE_ALL})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Cloning the OpenVINO repository and submodules\nDESCRIPTION: These commands clone the OpenVINO repository from GitHub and initialize its submodules. This is the initial step to get the OpenVINO source code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/openvinotoolkit/openvino.git\ncd openvino\ngit submodule update --init\n```\n\n----------------------------------------\n\nTITLE: Float16 Conversion in Philox Algorithm (C++)\nDESCRIPTION: This code snippet shows how to convert a 32-bit integer generated by the Philox algorithm to a float16 value. It sets the sign to 0, exponent to 15 (zero exponent representation), and the mantissa to the rightmost 10 bits of the generated integer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nsign = 0\nexponent = 15 // representation of a zero exponent.\nmantissa = 10 // right bits from generated uint32 random value.\n```\n\n----------------------------------------\n\nTITLE: Obtaining TorchScript Graph in Python\nDESCRIPTION: This Python code demonstrates how to obtain the TorchScript graph of a PyTorch model using tracing and scripting. It uses `torch.jit.trace` and `torch.jit.script` to generate the graph representations and prints them for analysis and debugging purposes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/pytorch/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom openvino.frontend.pytorch.ts_decoder import TorchScriptPythonDecoder\n\nmodel = SomeModel()\nmodel.eval()\nexample = <...>  # use the valid model inputs\n\n# get traced model graph\ntraced_model = torch.jit.trace(model, example)\nprint(traced_model.inlined_graph)\n\n# get scripted model graph\nscripted_model = torch.jit.script(model)\nprint(scripted_model.inlined_graph)\n```\n\n----------------------------------------\n\nTITLE: Adding FlatBuffers as Subdirectory\nDESCRIPTION: This command adds the FlatBuffers source code as a subdirectory to the current CMake project. It excludes the FlatBuffers directory from the ALL target, which prevents it from being built by default.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/flatbuffers/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(flatbuffers EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Inclusion in CMake\nDESCRIPTION: This CMake snippet conditionally includes the 'benchmark_tool' and 'ovc' subdirectories in the build process. It checks if the 'ENABLE_PYTHON' option is enabled before adding these subdirectories. The 'add_subdirectory' command adds a subdirectory to the build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_PYTHON)\n    # Benchmark Tool\n    add_subdirectory(benchmark_tool)\n\n    # OpenVino Conversion Tool\n    add_subdirectory(ovc)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Getting Device Property (Property Name)\nDESCRIPTION: Retrieves a device property based on the provided property name, returning the value as OVAny. This overloaded method doesn't take in a deviceName.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\ngetProperty(propertyName): OVAny\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for TensorFlow Common (CMake)\nDESCRIPTION: This snippet conditionally adds the 'tensorflow_common' subdirectory to the build if either the ENABLE_OV_TF_FRONTEND or ENABLE_OV_TF_LITE_FRONTEND CMake options are enabled. This likely contains common code shared between the TensorFlow and TensorFlow Lite frontends.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_OV_TF_FRONTEND OR ENABLE_OV_TF_LITE_FRONTEND)\n    add_subdirectory(tensorflow_common)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Connecting OpenVINO Plugs to Slots\nDESCRIPTION: This snippet demonstrates how to manually connect the application's plugs to the OpenVINO snap's slots using the `snap connect` command. This is necessary if the snaps are not published by the same user.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/integrate-openvino-with-ubuntu-snap.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nsnap connect app:openvino-libs openvino-libs:openvino-libs\nsnap connect app:openvino-3rdparty-libs openvino-libs:openvino-3rdparty-libs\n```\n\n----------------------------------------\n\nTITLE: Disable Deprecated Function Errors\nDESCRIPTION: This snippet calls a function `ov_deprecated_no_errors()`.  This function likely disables errors related to deprecated OpenVINO functionalities. It's noted that this is temporary and will be removed when deprecated features are fully removed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n# TODO: remove this when all the deprecated functions are removed\nov_deprecated_no_errors()\n```\n\n----------------------------------------\n\nTITLE: ReduceMax Layer Configuration in XML (axes=[-2], keep_dims=false)\nDESCRIPTION: This XML configuration defines a ReduceMax layer with `keep_dims` set to `false`. The input tensor has dimensions 6x12x10x24, and the reduction is performed along axis -2 (value [-2]). The output tensor removes the reduced dimension.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-max-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceMax\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [-2] that means independent reduction in each channel, batch and second spatial dimension -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Define Project Name CMake\nDESCRIPTION: This snippet sets the project name for the JavaScript API to 'OpenVINO_JS_API'. This project name is used for the generated build system and other build-related tasks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nproject(OpenVINO_JS_API)\n```\n\n----------------------------------------\n\nTITLE: PartialShape Interface Definition (TypeScript)\nDESCRIPTION: Defines the `PartialShape` interface with methods for getting dimensions (`getDimensions`), checking if the shape is dynamic (`isDynamic`) or static (`isStatic`), and converting to a string representation (`toString`). The interface is located in `addon.ts` of the OpenVINO JavaScript bindings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PartialShape.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface PartialShape {\n    getDimensions(): Dimension[];\n    isDynamic(): boolean;\n    isStatic(): boolean;\n    toString(): string;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Tests Directory in CMake\nDESCRIPTION: This CMake snippet installs the tests directory into `tests/${PROJECT_NAME}`, marking it as a tests component and excluding it from the default build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_18\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY ${OpenVINOPython_SOURCE_DIR}/tests\n        DESTINATION tests/${PROJECT_NAME}\n        COMPONENT tests\n        EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Sigmoid Layer Definition in XML\nDESCRIPTION: This XML snippet defines a Sigmoid layer in OpenVINO. It specifies the input and output ports with their dimensions, defining how data flows through the Sigmoid operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/sigmoid-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Sigmoid\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Input Tensor Example\nDESCRIPTION: Shows an example input tensor with values to demonstrate the behavior of TopK when configured with min mode and index sort, highlighting potential correct results.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/top-k-11.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ninput = [5, 3, 1, 2, 5, 5]\n```\n\n----------------------------------------\n\nTITLE: Starting vTPM and Writing HW TPM Data (sh)\nDESCRIPTION: Starts the swtpm socket, clears and initializes the TPM, writes the HW quote data into NVRAM using a Python script, and restarts the vTPM for QEMU. The script uses specific ports (8280 and 8281) for communication and the /var/OVSA/vtpm/vtpm_isv_dev directory for TPM state.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\nsudo swtpm socket --tpm2 --server port=8280 \\\n                         --ctrl type=tcp,port=8281 \\\n                         --flags not-need-init --tpmstate dir=/var/OVSA/vtpm/vtpm_isv_dev &\n\nsudo tpm2_startup --clear -T swtpm:port=8280\nsudo tpm2_startup -T swtpm:port=8280\npython3 <path to Security-Addon source>/Scripts/host/OVSA_write_hwquote_swtpm_nvram.py 8280\nsudo pkill -f vtpm_isv_dev\n\nswtpm socket --tpmstate dir=/var/OVSA/vtpm/vtpm_isv_dev \\\n       --tpm2 \\\n       --ctrl type=unixio,path=/var/OVSA/vtpm/vtpm_isv_dev/swtpm-sock \\\n       --log level=20\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This CMake snippet sets the target name for the unit tests to `ov_npu_unit_tests`. This variable is used throughout the script to refer to the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/unit/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"ov_npu_unit_tests\")\n```\n\n----------------------------------------\n\nTITLE: Install Target\nDESCRIPTION: Installs the test executable, skip configuration file, and blob files to the specified destination directories. These are marked as `tests` component and excluded from the `ALL` target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/functional/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(\n  TARGETS ${TARGET_NAME}\n  RUNTIME DESTINATION tests\n          COMPONENT tests\n          EXCLUDE_FROM_ALL)\n\ninstall(\n  FILES ${SKIP_CONFIG_PATH}\n  DESTINATION tests\n  COMPONENT tests\n  EXCLUDE_FROM_ALL)\n\ninstall(\n  FILES ${BLOBS}\n  DESTINATION tests/intel_npu_blobs\n  COMPONENT tests\n  EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Benchmark App Execution with AUTO:GNA,CPU\nDESCRIPTION: This snippet executes the OpenVINO benchmark_app with the AUTO plugin, prioritizing GNA but including CPU as a fallback. It loads the 'add_abc.xml' model and runs for 10 seconds. The output demonstrates that CPU is selected as the execution device due to GNA's lack of FP32 support, even though GNA has higher priority in the device list.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/docs/tests.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nopenvino/bin/intel64/Release$ ./benchmark_app -m openvino/src/core/tests/models/ir/add_abc.xml -d AUTO:GNA,CPU -t 10\n[Step 1/11] Parsing and validating input arguments\n[ INFO ] Parsing input parameters\n[Step 2/11] Loading OpenVINO Runtime\n[ INFO ] OpenVINO:\n[ INFO ] Build ................................. <OpenVINO version>-<Branch name>\n[ INFO ] \n[ INFO ] Device info:\n[ INFO ] AUTO\n[ INFO ] Build ................................. <OpenVINO version>-<Branch name>\n[ INFO ] \n[ INFO ] CPU\n[ INFO ] Build ................................. <OpenVINO version>-<Branch name>\n[ INFO ] \n[ INFO ] GNA\n[ INFO ] Build ................................. <OpenVINO version>-<Branch name>\n...\n[Step 11/11] Dumping statistics report\n[ INFO ] Execution Devices: [ CPU ]\n...\n[ INFO ] Latency:\n[ INFO ]    Median:           8.90 ms\n[ INFO ]    Average:          8.95 ms\n[ INFO ]    Min:              5.16 ms\n[ INFO ]    Max:              32.78 ms\n[ INFO ] Throughput:          446.08 FPS\n```\n\n----------------------------------------\n\nTITLE: Define IR Frontend Test Target in CMake\nDESCRIPTION: This CMake code defines the test target `ov_ir_frontend_tests`. It specifies the source directory, dependencies (openvino_ir_frontend), required libraries (gtest, gtest_main, openvino::runtime::dev, common_test_utils, frontend_shared_test_classes), include directories, and labels for test categorization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/ir/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_ir_frontend_tests)\n\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        DEPENDENCIES\n            openvino_ir_frontend\n        LINK_LIBRARIES\n            gtest\n            gtest_main\n            openvino::runtime::dev\n            common_test_utils\n            frontend_shared_test_classes\n        INCLUDES\n            \"${CMAKE_CURRENT_SOURCE_DIR}/../include\"\n        ADD_CLANG_FORMAT\n        LABELS\n            OV UNIT IR_FE\n)\n```\n\n----------------------------------------\n\nTITLE: Smart CI Output Example\nDESCRIPTION: This example shows the output of the Smart CI job for a pull request that changes only the TensorFlow Frontend component. It includes the names of changed components and their validation scopes: 'test' and/or 'build'. The TF_FE alias is used for the TensorFlow Frontend component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nchanged_component_names: {'TF_FE'}  # TF_FE is an alias we chose for TensorFlow Frontend component\naffected_components={\n    \"TF_FE\": {\"test\": true, \"build\": true},\n    \"OVC\": {\"test\": true, \"build\": true},\n    \"CPU\": {\"build\": true},\n    \"Python_API\": {\"build\": true},\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Source Directory\nDESCRIPTION: This snippet sets a variable pointing to the OpenVINO source directory. It assumes the current CMake file is located within a subdirectory of the OpenVINO source tree.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(OpenVINO_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../\")\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for ONNX Frontend (CMake)\nDESCRIPTION: This snippet conditionally adds the 'onnx' subdirectory to the build if the ENABLE_OV_ONNX_FRONTEND CMake option is enabled. This includes the ONNX frontend functionality.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_OV_ONNX_FRONTEND)\n    add_subdirectory(onnx)\nendif()\n```\n\n----------------------------------------\n\nTITLE: PaddlePaddle Dependency Check and Model Generation\nDESCRIPTION: This snippet checks for the PaddlePaddle dependency using Python and executes a process to check the requirements. If PaddlePaddle is found, it sets a flag and labels for testing. Otherwise, it issues a warning message that some tests might fail due to missing models. This also sets the `ctest_labels` variable based on whether PaddlePaddle is found.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"paddle_tests\")\n\n# Test model generating\nset(PADDLE_REQ \"${CMAKE_CURRENT_SOURCE_DIR}/requirements.txt\")\nif(Python3_Interpreter_FOUND)\n    execute_process(\n        COMMAND ${Python3_EXECUTABLE} \"${CMAKE_CURRENT_SOURCE_DIR}/paddle_pip_check.py\" ${PADDLE_REQ}\n        RESULT_VARIABLE EXIT_CODE\n        OUTPUT_VARIABLE OUTPUT_TEXT\n        ERROR_VARIABLE ERROR_TEXT)\nendif()\n\nif(NOT EXIT_CODE EQUAL 0)\n    set(paddlepaddle_FOUND OFF)\n    message(WARNING \"Python requirement file ${PADDLE_REQ} is not installed, PaddlePaddle testing models weren't generated, some tests will fail due models not found\")\nelse()\n    set(paddlepaddle_FOUND ON)\nendif()\n\nif(paddlepaddle_FOUND)\n    set(ctest_labels OV UNIT)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Define Target Name CMake\nDESCRIPTION: Sets the target name for the library to 'onnx_fe_standalone_build_test'. This name is used in subsequent CMake commands to refer to the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/standalone_build/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"onnx_fe_standalone_build_test\")\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINO Package in CMake\nDESCRIPTION: This block attempts to find the OpenVINO package. It first tries to determine the path using Python. If that fails, it relies on other CMake mechanisms to find OpenVINO.  The `REQUIRED` keyword ensures that the build fails if OpenVINO is not found.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/template_extension/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT CMAKE_CROSSCOMPILING)\n    find_package(Python3 QUIET COMPONENTS Interpreter)\n    if(Python3_Interpreter_FOUND)\n        execute_process(\n            COMMAND ${Python3_EXECUTABLE} -c \"from openvino.utils import get_cmake_path; print(get_cmake_path(), end='')\"\n            OUTPUT_VARIABLE OpenVINO_DIR_PY\n            ERROR_QUIET)\n    endif()\nendif()\n\nfind_package(OpenVINO REQUIRED PATHS \"${OpenVINO_DIR_PY}\")\n```\n\n----------------------------------------\n\nTITLE: Define Static Library Target CMake\nDESCRIPTION: This snippet defines a static library target named 'paddle_fe_standalone_build_test' using the `add_library` command. The source file for the library is 'standalone_build_test.cpp'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/tests/standalone_build/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"paddle_fe_standalone_build_test\")\n\nadd_library(${TARGET_NAME} STATIC standalone_build_test.cpp)\n```\n\n----------------------------------------\n\nTITLE: Accessing Input without parameters of CompiledModel (TypeScript)\nDESCRIPTION: This code shows the declaration of the `input` method within the `CompiledModel` interface to retrieve a single input without specifying the index or name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\ninput(): Output\n```\n\n----------------------------------------\n\nTITLE: Standalone Protopipe Build\nDESCRIPTION: These commands build Protopipe as a standalone project. They create a build directory, configure CMake with paths to OpenCV and gflags installations, and then build the `protopipe` target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_25\n\nLANGUAGE: cmake\nCODE:\n```\nmkdir \"protopipe_build\" && cd \"protopipe_build\"\ncmake <OpenVINO-dir>/src/plugins/intel_npu/tools/protopipe ^\n         -DOpenCV_DIR=<opencv-build-dir>                      ^\n         -Dgflags_DIR=<gflags-build-dir>\n\ncmake --build . --config Release --target protopipe --parallel\n```\n\n----------------------------------------\n\nTITLE: Creating and entering the build directory\nDESCRIPTION: This snippet creates a directory named `build` and then changes the current directory to it. This is a common practice for out-of-source builds using CMake.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nmkdir build && cd build\n```\n\n----------------------------------------\n\nTITLE: Resetting Compilation Options CMake\nDESCRIPTION: This snippet resets specific compilation options to prevent the removal of symbols from the ICD loader library. It unsets CMAKE_C_VISIBILITY_PRESET and CMAKE_DEBUG_POSTFIX.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ocl/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nunset(CMAKE_C_VISIBILITY_PRESET)\nunset(CMAKE_DEBUG_POSTFIX)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties CMake\nDESCRIPTION: Sets the `FOLDER` property of the target library to \"src\", which helps organize the project in IDEs like Visual Studio. This places the target within a folder in the IDE's project view.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/utils/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES FOLDER \"src\")\n```\n\n----------------------------------------\n\nTITLE: STFT Configuration for 2D Signal, transpose_frames=true (XML)\nDESCRIPTION: This XML snippet demonstrates the configuration of the STFT operation for a 2D signal input where the transpose_frames attribute is set to true. The input signal has dimensions [3, 56], the window has a dimension of 7, frame_size is 11 and frame_step is 3. The output shape is [3, 6, 16, 2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/stft-15.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"STFT\" ... >\n    <data transpose_frames=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n        </port>\n        <port id=\"2\"></port> <!-- value: 11 -->\n        <port id=\"3\"></port> <!-- value: 3 -->\n    <output>\n        <port id=\"4\">\n            <dim>3</dim>\n            <dim>6</dim>\n            <dim>16</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: CMake Configure\nDESCRIPTION: This snippet runs the CMake configuration step, specifying the `Ninja Multi-Config` generator and enabling system-wide dependencies like PugiXML, Snappy, and Protobuf. It sets up the build rules for the OpenVINO project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -G \"Ninja Multi-Config\" -DENABLE_SYSTEM_PUGIXML=ON -DENABLE_SYSTEM_SNAPPY=ON -DENABLE_SYSTEM_PROTOBUF=ON ..\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags (MSVC)\nDESCRIPTION: This snippet adds compiler flags to suppress specific warnings when using the MSVC compiler.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    ov_add_compiler_flags(/wd4244)\n    ov_add_compiler_flags(/wd4018)\nendif()\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Quotes\nDESCRIPTION: This rule enforces the use of single quotes for strings in JavaScript and TypeScript code. It promotes consistency in string usage and is enforced by ESLint with the configuration `quotes: ['error', 'single']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_10\n\nLANGUAGE: JavaScript\nCODE:\n```\nquotes: ['error', 'single']\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Build Commands\nDESCRIPTION: These commands build OpenVINO from source, enabling XML plugins, setting the install prefix, and enabling debug capabilities. They create a build directory, run CMake to generate build files, and then build and install the project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\nmkdir \"build\" && cd \"build\"\ncmake ../ -DCMAKE_BUILD_TYPE=Release     ^\n          -DENABLE_PLUGINS_XML=ON        ^\n          -DCMAKE_INSTALL_PREFIX=install ^\n          -DENABLE_DEBUG_CAPS=ON         ^\n          -DENABLE_NPU_DEBUG_CAPS=ON ..\n\ncmake --build . --config Release --target install --parallel\n```\n\n----------------------------------------\n\nTITLE: GatherND with Batch Dimensions Example (Slices)\nDESCRIPTION: This example demonstrates GatherND with batch_dims set to 1, gathering slices. The first dimension is treated as the batch, and the operation selects slices from the data tensor based on the indices within each batch.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-5.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 1\nindices = [[1], <--- this is applied to the first batch\n              [0]] <--- this is applied to the second batch, shape = (2, 1)\ndata    = [[[1,   2,  3,  4], [ 5,  6,  7,  8], [ 9, 10, 11, 12]]  <--- the first batch\n              [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]] <--- the second batch, shape = (2, 3, 4)\noutput  = [[ 5,  6,  7,  8], [13, 14, 15, 16]], shape = (2, 4)\n```\n\n----------------------------------------\n\nTITLE: Adding Dependencies to Target in CMake\nDESCRIPTION: Adds a dependency on the `mock_engine` target to the `ov_capi_test` target. This ensures that `mock_engine` is built before `ov_capi_test`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/tests/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nadd_dependencies(${TARGET_NAME} mock_engine)\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO after configuring CMake\nDESCRIPTION: This snippet demonstrates building the OpenVINO binaries after the CMake configuration step is complete, applying the conditional compilation settings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/conditional_compilation.md#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ncmake --build .\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files and Kernels (CMake)\nDESCRIPTION: This snippet uses `file(GLOB_RECURSE)` to collect all header, source, and OpenCL kernel files within the specified directories. These files are then used during the library build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC\n    \"${CMAKE_CURRENT_SOURCE_DIR}/*.h\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\"\n  )\n\nfile(GLOB_RECURSE KERNELS\n    \"${CMAKE_CURRENT_SOURCE_DIR}/cl_kernels/*.cl\"\n  )\n```\n\n----------------------------------------\n\nTITLE: AUGRUCell Layer Configuration in XML\nDESCRIPTION: This code provides an example of how to configure an AUGRUCell layer using XML. It shows the definition of input and output ports, specifying the dimensions required for each tensor, along with the hidden_size parameter.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/internal/augru-cell.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"AUGRUCell\" ...>\n    <data hidden_size=\"128\"/>\n     <input>\n        <port id=\"0\"> <!-- `X` input data -->\n            <dim>1</dim>\n            <dim>16</dim>\n        </port>\n        <port id=\"1\"> <!-- `H_t` input -->\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n         <port id=\"3\"> <!-- `W` weights input -->\n            <dim>384</dim>\n            <dim>16</dim>\n        </port>\n         <port id=\"4\"> <!-- `R` recurrence weights input -->\n            <dim>384</dim>\n            <dim>128</dim>\n        </port>\n         <port id=\"5\"> <!-- `B` bias input -->\n            <dim>384</dim>\n        </port>\n        <port id=\"6\"> <!-- `A` attention score input -->\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"7\"> <!-- `Y` output -->\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"8\"> <!-- `Ho` output -->\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Disabling Errors for Deprecated Code\nDESCRIPTION: Configures the build system to treat deprecated code as warnings instead of errors.  This can be useful for maintaining compatibility with older codebases while still flagging potential issues. Requires CMake and custom ov_deprecated_no_errors function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_deprecated_no_errors()\n```\n\n----------------------------------------\n\nTITLE: Enabling SHL for CPU\nDESCRIPTION: This snippet determines whether to enable SHL (Sheldon) for the CPU plugin based on whether it is building for RISCV64_XUANTIE.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(RISCV64_XUANTIE)\n    set(ENABLE_SHL_FOR_CPU_DEFAULT ON)\nelse()\n    set(ENABLE_SHL_FOR_CPU_DEFAULT OFF)\nendif()\nov_dependent_option(ENABLE_SHL_FOR_CPU \"Enable SHL for OpenVINO CPU Plugin\" ${ENABLE_SHL_FOR_CPU_DEFAULT} \"RISCV64\" OFF)\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target in CMake\nDESCRIPTION: Adds a custom target for running clang-format on the source code of the `ov_capi_test` target, facilitating consistent code style.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/tests/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Linking Target Libraries in CMake\nDESCRIPTION: This snippet links the `TARGET_NAME` library with the required dependencies: `CMAKE_DL_LIBS`, `openvino::pugixml`, and `STD_FS_LIB` (which is the filesystem library determined earlier). On Windows, it also links against `Shlwapi`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE ${CMAKE_DL_LIBS} PUBLIC openvino::pugixml ${STD_FS_LIB})\nif (WIN32)\n    target_link_libraries(${TARGET_NAME} PRIVATE Shlwapi)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Install Licensing Files\nDESCRIPTION: This CMake code snippet installs licensing files from the current source directory to a destination directory specified by `${OV_CPACK_LICENSESDIR}`. It uses patterns to exclude specific files like `CMakeLists.txt` and various `third-party-programs.txt` files, associating the installation with the `${OV_CPACK_COMP_LICENSING}` component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/\n        DESTINATION ${OV_CPACK_LICENSESDIR}\n        COMPONENT ${OV_CPACK_COMP_LICENSING}\n        ${OV_CPACK_COMP_LICENSING_EXCLUDE_ALL}\n        PATTERN CMakeLists.txt EXCLUDE\n        PATTERN dev-third-party-programs.txt EXCLUDE\n        PATTERN documentation-third-party-programs.txt EXCLUDE\n        PATTERN third-party-programs.txt EXCLUDE)\n```\n\n----------------------------------------\n\nTITLE: Installing RISC-V toolchain packages\nDESCRIPTION: This snippet updates the package list and installs the GCC, G++, and Binutils toolchain packages for RISC-V 64-bit Linux using the apt package manager. This method is used when building without optimized primitives.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\napt-get update\napt-get install -y  gcc-riscv64-linux-gnu g++-riscv64-linux-gnu binutils-riscv64-linux-gnu\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: This snippet sets the include directories for the target ${TARGET_NAME}. It adds the ${CMAKE_CURRENT_SOURCE_DIR}/include directory as a public include directory, making the headers available to other targets that link to this library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/mock_py_frontend/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include)\n```\n\n----------------------------------------\n\nTITLE: Build Target Optimization CMake\nDESCRIPTION: This snippet optimizes the build process for the `ov_subgraphs_dumper` target using the UNITY build system. This optimization is typically used to reduce compile times by combining multiple source files into a single compilation unit.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/subgraphs_dumper/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME} UNITY)\n```\n\n----------------------------------------\n\nTITLE: Get Compiled Model from InferRequest TypeScript\nDESCRIPTION: Retrieves the CompiledModel associated with the InferRequest.  This allows access to the model's metadata and configuration after the InferRequest has been created. The method returns a CompiledModel object.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ngetCompiledModel(): CompiledModel\n```\n\n----------------------------------------\n\nTITLE: Installing Smoke Tests Directory\nDESCRIPTION: This command installs the 'smoke_tests' directory to the 'tests/smoke_tests' destination. It excludes files matching the pattern 'requirements.txt' and adds the installed files to the 'tests' component, excluding them from default build targets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/samples_tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(DIRECTORY smoke_tests/ DESTINATION tests/smoke_tests COMPONENT tests EXCLUDE_FROM_ALL PATTERN \"requirements.txt\" EXCLUDE)\n```\n\n----------------------------------------\n\nTITLE: Bucketize Layer XML Configuration\nDESCRIPTION: This XML snippet demonstrates how to configure the Bucketize layer in an OpenVINO model. It defines the input and output ports with their respective dimensions, allowing the OpenVINO runtime to correctly execute the bucketization operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/condition/bucketize-3.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Bucketize\">\n    <input>\n        <port id=\"0\">\n            <dim>49</dim>\n            <dim>11</dim>\n        </port>\n        <port id=\"1\">\n            <dim>5</dim>\n        </port>\n     </input>\n    <output>\n        <port id=\"1\">\n            <dim>49</dim>\n            <dim>11</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Windows: Setting OpenCV Path\nDESCRIPTION: This command sets the PATH environment variable on Windows to include the directory containing the OpenCV libraries. This ensures that Protopipe can find the necessary OpenCV DLLs at runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_26\n\nLANGUAGE: batch\nCODE:\n```\nset PATH=<path-to-opencv>\\build\\bin\\Release\\;%PATH%\n```\n\n----------------------------------------\n\nTITLE: Make Stateful Model Using Tensor Names in Python\nDESCRIPTION: This snippet demonstrates how to apply the MakeStateful transformation to an OpenVINO model using tensor names in Python. It uses the 'param_res_names' argument to specify pairs of input and output tensors that should be converted to state variables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/stateful-models/obtaining-stateful-openvino-model.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom openvino.tools.ovc.composite_transformation import CompositeTransform\n\nmodel = CompositeTransform(transformations=[(\"MakeStateful\", {\"param_res_names\": {\"tensor_name_1\": \"tensor_name_4\", \"tensor_name_3\": \"tensor_name_6\"}})]).apply(model)\n```\n\n----------------------------------------\n\nTITLE: Installing product and samples dependencies using Brew (arm64 only)\nDESCRIPTION: This command installs TBB, pugixml, flatbuffers, snappy and protobuf using Brew, which are dependencies specifically required for building OpenVINO on arm64 systems.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n% brew install tbb pugixml flatbuffers snappy protobuf\n```\n\n----------------------------------------\n\nTITLE: Linking Pugixml Library\nDESCRIPTION: Links the `openvino::pugixml` library to the specified target privately. This ensures that the target can use the Pugixml library for XML processing without exposing it as a public dependency.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_40\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino::pugixml)\n```\n\n----------------------------------------\n\nTITLE: Install Numpy\nDESCRIPTION: Installs the necessary Python packages using pip and the requirements.txt file located in the OpenVINO's python directory. This step is required for using the Python API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-windows.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncd \"C:\\Program Files (x86)\\Intel\\openvino_2025.1.0\"\npython -m pip install -r .\\python\\requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Parameter Layer XML Configuration Example\nDESCRIPTION: This XML snippet shows how to configure a Parameter layer in OpenVINO. The `data` tag specifies the element type (`element_type`) and shape (`shape`) of the input tensor. The `output` tag defines the dimensions of the output port.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/parameter-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Parameter\" ...>\n    <data element_type=\"f32\" shape=\"1,3,224,224\"></data>\n    <output>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Suppressing Missing Declarations Warning (CMake)\nDESCRIPTION: This snippet disables the `-Wmissing-declarations` warning for GCC and Clang compilers. This is done because the frontend implementation might have specific reasons for not declaring certain symbols.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/src/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG)\n    target_compile_options(${TARGET_NAME} PRIVATE -Wno-missing-declarations)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Emulating Alder Lake for AVX/SSE Transition Check with SDE\nDESCRIPTION: This snippet shows how to emulate an Alder Lake CPU using Intel SDE for checking AVX/SSE transitions while running a benchmark application. This is suggested as the best way to check AVX/SSE transitions. It requires the SDE binary and the benchmark_app executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/cpu_emulation.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n/path/to/sde -adl -- ./benchmark_app -m path/to/model.xml\n```\n\n----------------------------------------\n\nTITLE: Emulating RVV 1.0\nDESCRIPTION: This snippet shows how to emulate RVV 1.0 using QEMU with the Xuantie toolchain. It specifies the path to the QEMU emulator, the target CPU with RVV 1.0 extensions enabled, and the path to the executable file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\n<xuantie_install_path>/bin/qemu-riscv64 -cpu rv64,x-v=true,vext_spec=v1.0 <executable_file_path>\n```\n\n----------------------------------------\n\nTITLE: Mermaid Diagram Example\nDESCRIPTION: This is the diagram generated by the previous mermaid code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/dev_doc_guide.md#_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR;\n    A-->B;\n    A-->C;\n    B-->D;\n    C-->D;\n```\n\n----------------------------------------\n\nTITLE: Adding Library Target in CMake\nDESCRIPTION: Creates the library target named by `TARGET_NAME` which is `mock_engine` of type `MODULE`. It specifies the source files and headers to be compiled into the library. It defines a module library file from the source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/unit_test_utils/mocks/mock_engine/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} MODULE\n            ${LIBRARY_SRC}\n            ${LIBRARY_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: ReduceLogicalOr C++ Implementation\nDESCRIPTION: This C++ code block illustrates the core logic of the ReduceLogicalOr operation. It shows how the output is calculated by performing a logical OR operation across the specified dimensions of the input data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-logical-or-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i0, i1, ..., iN] = or[j0, ..., jN](x[j0, ..., jN]))\n```\n\n----------------------------------------\n\nTITLE: Try Compile with no Library (CMake)\nDESCRIPTION: This snippet attempts to compile the `main.cpp` program without linking any filesystem library. The result is stored in `STD_FS_NO_LIB_NEEDED`, indicating whether the compilation succeeded without needing a library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\ntry_compile(STD_FS_NO_LIB_NEEDED ${CMAKE_CURRENT_BINARY_DIR}\n            SOURCES ${CMAKE_CURRENT_BINARY_DIR}/main.cpp\n            COMPILE_DEFINITIONS -std=c++11)\n```\n\n----------------------------------------\n\nTITLE: Creating a Static Library with CMake\nDESCRIPTION: This snippet uses the `add_library` command to create a static library named after the `TARGET_NAME` variable (tensorflow_fe_standalone_build_test). The source file for this library is specified as `standalone_build_test.cpp`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/tests/standalone_build/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC standalone_build_test.cpp)\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Options for flatc\nDESCRIPTION: This snippet sets the `-Wno-shadow` compiler option for the `flatc` target when using the GNU C++ compiler. This option suppresses warnings about shadowed variables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/flatbuffers/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX)\n    set_target_properties(flatc PROPERTIES COMPILE_OPTIONS \"-Wno-shadow\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Broadcast with Explicit Mode and Different Shape in XML\nDESCRIPTION: This XML snippet demonstrates the Broadcast operation with the 'explicit' mode, broadcasting a tensor of shape [50, 50] to [1, 50, 50, 16] with axes mapping [1,2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/broadcast-3.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Broadcast\" ...>\n    <data mode=\"explicit\"/>\n    <input>\n        <port id=\"0\">\n            <dim>50</dim>\n            <dim>50</dim>\n       </port>\n        <port id=\"1\">\n            <dim>4</dim>   <!--The tensor contains 4 elements: [1, 50, 50, 16] -->\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>   <!--The tensor contains 2 elements: [1, 2] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>50</dim>\n            <dim>50</dim>\n            <dim>16</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Install Missing Ubuntu Dependency\nDESCRIPTION: This command installs the libpython3.10 package on Ubuntu, resolving a missing external dependency error. This is required if the system reports that it cannot open the shared object file libpython3.10.so.1.0.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/pypi_publish/pypi-openvino-rt.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt-get install libpython3.10\n```\n\n----------------------------------------\n\nTITLE: Run Benchmark App with Average Counters\nDESCRIPTION: This command runs the `benchmark_app` with the `--report_type average_counters` option, which instructs the application to generate a CSV report containing average counter values.  The output CSV file can then be used as input to the `aggregate-average-counters.py` script.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/aggregate-average-counters/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbenchmark_app --report_type average_counters\n```\n\n----------------------------------------\n\nTITLE: Installing development Python dependencies for wheel package building\nDESCRIPTION: This command installs additional Python packages required for building OpenVINO Python API as wheel packages, utilizing the `requirements-dev.txt` file within the OpenVINO source tree.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n% python3 -m pip install -r <openvino source tree>/src/bindings/python/wheel/requirements-dev.txt\n```\n\n----------------------------------------\n\nTITLE: ReduceMin Layer Configuration (XML, axes=[-2], keep_dims=false)\nDESCRIPTION: This XML snippet demonstrates the ReduceMin layer with `keep_dims` set to false and reduction along axis -2. The input tensor is 6x12x10x24, and the output after reduction is 6x12x24.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-min-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceMin\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [-2] that means independent reduction in each channel, batch and second spatial dimension -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: GatherElements Example 3 (indices < data)\nDESCRIPTION: Illustrates GatherElements where the indices tensor has a lesser shape than the data tensor. Gathering occurs along axis 0.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-elements-6.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndata = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n]\nindices = [\n    [1, 0, 1],\n    [1, 2, 0],\n]\naxis = 0\noutput = [\n    [4, 2, 6],\n    [4, 8, 3],\n]\n```\n\n----------------------------------------\n\nTITLE: MyTensor Creation Test Python\nDESCRIPTION: This test case verifies the creation of a `MyTensor` object. It imports necessary modules (`pytest`, `numpy`, `openvino`) and asserts that the created tensor is not `None`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nimport numpy as np \nimport openvino as ov\n\ndef test_mytensor_creation():\n    tensor = ov.MyTensor([1, 2, 3])\n\n    assert tensor is not None\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name\nDESCRIPTION: Defines the target name for the library being built. This is a straightforward assignment of a string literal to a variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/backend/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_npu_level_zero_backend\")\n```\n\n----------------------------------------\n\nTITLE: Creating CMake Test Target\nDESCRIPTION: This CMake snippet defines a test target named `ov_conditional_compilation_tests` using the custom `ov_add_test_target` macro. It specifies the root directory, dependencies (gtest, gmock, and openvino::conditional_compilation), include directories, and labels for the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME ov_conditional_compilation_tests)\n\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        DEPENDENCIES\n        LINK_LIBRARIES\n            gtest\n            gtest_main\n            gmock\n            openvino::conditional_compilation\n        INCLUDES\n            \"${CMAKE_CURRENT_SOURCE_DIR}/../include\"\n        ADD_CLANG_FORMAT\n        LABELS\n            OV UNIT\n)\n```\n\n----------------------------------------\n\nTITLE: RDFT Layer XML Configuration (With signal_size, 2D Input)\nDESCRIPTION: Configures an RDFT layer in XML with a signal_size input, processing a 2D input tensor. The example demonstrates how the output dimensions are determined by the signal_size parameter. The axes input defines the dimensions over which the Fourier transform is computed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/rdft-9.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"RDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>320</dim>\n            <dim>320</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- axes input contains [0, 1] -->\n        </port>\n        <port id=\"2\">\n            <dim>2</dim> <!-- signal_size input contains [512, 100] -->\n        </port>\n    <output>\n        <port id=\"3\">\n            <dim>512</dim>\n            <dim>51</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding the Target\nDESCRIPTION: This snippet uses the 'ov_add_target' function to create the target with specified properties such as name, type (STATIC), root directory, source directories, excluded paths, definitions, includes, link libraries, and dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/shared/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_target(\n        NAME ${TARGET_NAME}\n        TYPE STATIC\n        ROOT ${PUBLIC_HEADERS_DIR}\n        ADDITIONAL_SOURCE_DIRS\n            ${CMAKE_CURRENT_SOURCE_DIR}/src\n        ADD_CPPLINT\n        EXCLUDED_SOURCE_PATHS ${EXCLUDED_SOURCE_PATHS}\n        DEFINES ${DEFINES}\n        INCLUDES\n            PUBLIC\n                \"$<BUILD_INTERFACE:${PUBLIC_HEADERS_DIR}>\"\n        LINK_LIBRARIES\n            PUBLIC\n                openvino::pugixml\n                common_test_utils\n                func_test_utils\n                ov_lpt_models\n                sharedTestClasses\n            PRIVATE\n                ${LINK_LIBRARIES_PRIVATE}\n        DEPENDENCIES\n            ${DEPENDENCIES}\n)\n```\n\n----------------------------------------\n\nTITLE: Building Target Faster with Precompiled Header\nDESCRIPTION: This snippet uses the 'ov_build_target_faster' function to enable precompiled headers for faster build times. It specifies the 'src/precomp.hpp' file as the precompiled header.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/shared/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME}\n    PCH PRIVATE \"src/precomp.hpp\"\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Target\nDESCRIPTION: Installs the target library to the specified destination directories for runtime and library components. It is installed under the tests component and excluded from all by default. The destination paths depend on `OV_CPACK_PYTHONDIR` variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n        RUNTIME DESTINATION ${OV_CPACK_PYTHONDIR}/openvino/test_utils\n        COMPONENT tests EXCLUDE_FROM_ALL\n        LIBRARY DESTINATION tests/${OV_CPACK_PYTHONDIR}/openvino/test_utils\n        COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for gflags Dependency\nDESCRIPTION: This CMake snippet adds the gflags library as a subdirectory to the build.  It uses `add_subdirectory` and specifies the source and build directories for gflags, excluding it from the ALL target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/src/timetests_helper/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(${OpenVINO_SOURCE_DIR}/thirdparty/gflags\n                 ${CMAKE_CURRENT_BINARY_DIR}/gflags_build\n                 EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Defining Input Layer Dimensions XML\nDESCRIPTION: This XML snippet defines the input layer with its output port, specifying the data dimensions and precision for the layer's output. The dimensions dictate the shape of the data that will be passed from this layer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/infrastructure/tensor-iterator-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"0\" type=\"Parameter\" ...>\n    <output>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>512</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Paddle Models Directory and Compile Definitions\nDESCRIPTION: This snippet sets the directory for the PaddlePaddle test models and defines a compile definition for the target. The `TEST_PADDLE_MODELS_DIRNAME` variable is defined, and a compile definition is added to the target `paddle_tests` to expose this directory at compile time. This allows the tests to locate the models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_PADDLE_MODELS_DIRNAME ${TEST_MODEL_ZOO}/paddle_test_models)\ntarget_compile_definitions(${TARGET_NAME} PRIVATE -D TEST_PADDLE_MODELS_DIRNAME=\\\"${TEST_PADDLE_MODELS_DIRNAME}/\\\")\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories CMake\nDESCRIPTION: Specifies that the current source directory should be added to the include path for the `${TARGET_NAME}` target. This allows the library to find header files located in the same directory as the source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/lib/src/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC ${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name and Include Directories in CMake\nDESCRIPTION: This snippet defines the target name for the library and sets the include directories for both development and general use. The `FRONTEND_INCLUDE_DIR` and `FRONTEND_DEV_INCLUDE_DIR` variables are defined, and file globbing is used to collect source and header files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_frontend_common\")\n\nset(FRONTEND_INCLUDE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/include)\nset(FRONTEND_DEV_INCLUDE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/dev_api)\n\nfile(GLOB_RECURSE LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE LIBRARY_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/src/*.hpp)\nfile(GLOB_RECURSE LIBRARY_PUBLIC_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/include/*.hpp)\n\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${LIBRARY_HEADERS})\nsource_group(\"public include\" FILES ${LIBRARY_PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Swish Layer Configuration without Beta Parameter in OpenVINO (XML)\nDESCRIPTION: This XML snippet shows how to configure a Swish layer in OpenVINO without specifying the beta parameter. When the beta parameter is not provided, the default value of 1.0 is used. The layer accepts a 1D tensor of size 128 as input and outputs a tensor of the same size.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/swish-4.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Swish\">\n    <input>\n        <port id=\"0\">\n            <dim>128</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Float32 Conversion in Philox Algorithm (XML)\nDESCRIPTION: This code snippet shows how to set the sign, exponent, and mantissa to generate float32 value from the Philox algorithm, focusing on the XML format used for configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\nsign = 0\nexponent = 127 - representation of a zero exponent.\nmantissa = 23 right bits from generated uint32 random value.\n```\n\n----------------------------------------\n\nTITLE: CMake Install Directories\nDESCRIPTION: This snippet installs directories for tests and utility functions. It specifies the source directory, destination directory, component, and excludes it from the 'all' target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/conditional_compilation/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\ninstall(DIRECTORY ../utils/ DESTINATION tests/utils COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Creating Tensors in OpenVINO Python\nDESCRIPTION: This snippet demonstrates how to create `Tensor` objects in the OpenVINO Python API. It shows how data is passed as tensors and how the `dtype` of NumPy arrays is automatically converted to OpenVINO types. The `Tensor` object holds a copy of the data from the given array.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-request/python-api-exclusives.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport openvino.runtime as ov\nimport numpy as np\n\n# Create a numpy array\ndata = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n\n# Create a Tensor object from the numpy array\ntensor = ov.Tensor(data)\n\n# Print the shape of the tensor\nprint(tensor.shape)\n```\n\n----------------------------------------\n\nTITLE: Set Target Properties CMake\nDESCRIPTION: Sets the `INTERPROCEDURAL_OPTIMIZATION_RELEASE` property for the target, enabling link-time optimization (LTO) during release builds.  LTO can improve performance by optimizing across multiple compilation units.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/onnx_common/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES\n    INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: EmbeddingBagOffsetsSum Example with default_index\nDESCRIPTION: This XML snippet provides an example of the EmbeddingBagOffsetsSum operation in OpenVINO's model representation. It demonstrates the use of `default_index` to fill empty bags with values from the embedding table at the specified index. The `per_sample_weights` input provides weights for each index.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sparse/embedding-bag-offsets-sum-3.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"EmbeddingBagOffsetsSum\" ... >\n       <input>\n           <port id=\"0\">     <!-- emb_table value is: [[-0.2, -0.6], [-0.1, -0.4], [-1.9, -1.8], [-1.,  1.5], [ 0.8, -0.7]] -->\n               <dim>5</dim>\n               <dim>2</dim>\n           </port>\n           <port id=\"1\">     <!-- indices value is: [0, 2, 3, 4] -->\n               <dim>4</dim>\n           </port>\n           <port id=\"2\">     <!-- offsets value is: [0, 2, 2] - 3 \"bags\" containing [2,0,4-2] elements, second \"bag\" is empty -->\n               <dim>3</dim>\n           </port>\n           <port id=\"3\"/>    <!-- default_index value is: 0 -->\n           <port id=\"4\"/>    <!-- per_sample_weights value is: [0.5, 0.5, 0.5, 0.5] -->\n               <dim>4</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">     <!-- output value is: [[-1.05, -1.2], [-0.2, -0.6], [-0.1, 0.4]] -->\n               <dim>3</dim>\n               <dim>2</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Dumping IR with Transformations\nDESCRIPTION: This environment variable configures the dumping of the Intermediate Representation (IR) of the model, specifying the transformation directory and formats (svg, xml, dot). This helps in understanding the transformations applied to the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nOV_CPU_DUMP_IR=\"transformations dir=path/dumpdir formats=svg,xml,dot\"\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Naming Conventions\nDESCRIPTION: This rule enforces the use of camelCase for variable and function names in JavaScript and TypeScript code. It promotes a consistent naming convention and is enforced by ESLint with the configuration `camelcase: ['error']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\ncamelcase: ['error']\n```\n\n----------------------------------------\n\nTITLE: Create Build Directory (Bash)\nDESCRIPTION: This set of commands creates a dedicated `build` directory and navigates into it. This practice helps keep the source directory clean by isolating the build artifacts and temporary files generated during the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build && cd build\n```\n\n----------------------------------------\n\nTITLE: Squeeze 2D Tensor to Dynamic Shape using OpenVINO XML\nDESCRIPTION: This example demonstrates squeezing a 2D tensor with dynamic and static shape elements to a dynamic shape output based on the opset15 rules. The 'allow_axis_skip' attribute is set to true. Axis 1 is specified for squeezing and the output has dynamic rank because the actual value of the dynamic dimension may or may not be squeezable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/squeeze-15.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Squeeze\" version=\"opset15\">\n    <data allow_axis_skip=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>-1</dim>\n        </port>\n    </input>\n    <input>\n        <port id=\"1\">\n            <dim>1</dim>  <!-- value is [1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\" />    <!-- Output with dynamic rank. Actual value of <dim>-1</dim> may or may not be squeezable -->\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring openvino_itt Library (CMake)\nDESCRIPTION: Creates a static library named openvino_itt and configures its properties, including dependencies, compilation definitions, and include directories. This snippet defines the core settings for building the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/itt/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME openvino_itt)\n\nfile(GLOB_RECURSE SOURCES \"src/*.cpp\" \"src/*.hpp\" \"include/*.hpp\")\n\nadd_library(${TARGET_NAME} STATIC ${SOURCES})\n\nadd_library(openvino::itt ALIAS ${TARGET_NAME})\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME itt)\n\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino::util)\n\ntarget_include_directories(${TARGET_NAME} PUBLIC\n    $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>)\n```\n\n----------------------------------------\n\nTITLE: Create RemoteTensor from ID3D11Texture2D (C++)\nDESCRIPTION: This C++ snippet illustrates how to create an OpenVINO RemoteTensor from an ID3D11Texture2D using the D3DContext and process NV12 format. It sets up preprocessing to convert the color format and wraps the texture into a RemoteTensor for inference. Requires OpenVINO, DirectX 11, and OpenCL libraries. The shared D3D context facilitates interoperability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_28\n\nLANGUAGE: cpp\nCODE:\n```\nusing namespace ov::preprocess;\nauto p = PrePostProcessor(model);\np.input().tensor().set_element_type(ov::element::u8)\n                  .set_color_format(ov::preprocess::ColorFormat::NV12_TWO_PLANES, {\"y\", \"uv\"})\n                  .set_memory_type(ov::intel_gpu::memory_type::surface);\np.input().preprocess().convert_color(ov::preprocess::ColorFormat::BGR);\np.input().model().set_layout(\"NCHW\");\nmodel = p.build();\n\nCComPtr<ID3D11Device> device_ptr = get_d3d_device_ptr()\n// create the shared context object\nauto shared_d3d_context = ov::intel_gpu::ocl::D3DContext(core, device_ptr);\n// compile model within a shared context\nauto compiled_model = core.compile_model(model, shared_d3d_context);\n\nauto param_input_y = model->get_parameters().at(0);\nauto param_input_uv = model->get_parameters().at(1);\n\nD3D11_TEXTURE2D_DESC texture_description = get_texture_desc();\nCComPtr<ID3D11Texture2D> dx11_texture = get_texture();\n//     ...\n//wrap decoder output into RemoteBlobs and set it as inference input\nauto nv12_blob = shared_d3d_context.create_tensor_nv12(texture_description.Heights, texture_description.Width, dx11_texture);\n\nauto infer_request = compiled_model.create_infer_request();\ninfer_request.set_tensor(param_input_y->get_friendly_name(), nv12_blob.first);\ninfer_request.set_tensor(param_input_uv->get_friendly_name(), nv12_blob.second);\ninfer_request.start_async();\ninfer_request.wait();\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example 1 (explicit padding)\nDESCRIPTION: This example demonstrates how MaxPool operates with a 4D input, a 2D kernel, and `auto_pad` set to 'explicit'. It showcases the input data, strides, padding, kernel size, and the resulting output shapes and values after the MaxPool operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/pooling_shape_rules.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[-1, 2, 3],\n                 [4, 5, -6],\n                 [-7, 8, 9]]]]   # shape: (1, 1, 3, 3)\n      strides = [1, 1]\n      pads_begin = [1, 1]\n      pads_end = [1, 1]\n      kernel = [2, 2]\n      rounding_type = \"floor\"\n      auto_pad = \"explicit\"\n      output0 = [[[[-1, 2, 3, 3],\n                   [4, 5, 5, -6],\n                   [4, 8, 9, 9],\n                   [-7, 8, 9, 9]]]]   # shape: (1, 1, 4, 4)\n      output1 = [[[[0, 1, 2, 2],\n                   [3, 4, 4, 5],\n                   [3, 7, 8, 8],\n                   [6, 7, 8, 8]]]]   # shape: (1, 1, 4, 4)\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions in CMake\nDESCRIPTION: Adds a private compile definition IMPLEMENT_OPENVINO_API to the library target. This definition might be used to enable or disable certain features or optimizations within the library's source code.  Additionally, it sets OPENVINO_STATIC_LIBRARY if BUILD_SHARED_LIBS is not enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME}_obj PRIVATE IMPLEMENT_OPENVINO_API)\n\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME}_obj PUBLIC OPENVINO_STATIC_LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Exporting Targets for Developer Package\nDESCRIPTION: This snippet exports the target for inclusion in the OpenVINO developer package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET openvino::interpreter_backend)\n```\n\n----------------------------------------\n\nTITLE: Defining Target Name and Root Directory\nDESCRIPTION: This snippet sets the target name for the library and retrieves the root directory of the current source directory using CMake commands.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common_translators/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME \"openvino_common_translators\")\nget_filename_component(root_dir \"${CMAKE_CURRENT_SOURCE_DIR}\" DIRECTORY)\n```\n\n----------------------------------------\n\nTITLE: Installing and Exporting the Library (CMake)\nDESCRIPTION: Installs the static library and exports the target for use in other projects. The installation location is determined by the OV_CPACK_COMP_CORE variable. This snippet sets up the necessary configurations for packaging and distribution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/itt/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n\n# install & export\n\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n\nov_developer_package_export_targets(TARGET openvino::itt\n                                    INSTALL_INCLUDE_DIRECTORIES \"${CMAKE_CURRENT_SOURCE_DIR}/include/\")\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example 3 (same_lower padding)\nDESCRIPTION: This example demonstrates how MaxPool operates with a 4D input, a 2D kernel, and `auto_pad` set to 'same_lower'. It illustrates the input data, strides, kernel size, and the corresponding output shapes and values after applying the MaxPool operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/pooling_shape_rules.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[-1, 2, 3],\n               [4, 5, -6],\n               [-7, 8, 9]]]]   # shape: (1, 1, 3, 3)\n      strides = [1, 1]\n      kernel = [2, 2]\n      rounding_type = \"floor\"\n      auto_pad = \"same_lower\"\n      output0 = [[[[-1, 2, 3],\n                  [4, 5, 5]\n                  [4, 8, 9]]]]   # shape: (1, 1, 3, 3)\n      output1 = [[[[0, 1, 2],\n                  [3, 4, 4],\n                  [3, 7, 8]]]]   # shape: (1, 1, 3, 3)\n```\n\n----------------------------------------\n\nTITLE: StringTensorUnpack with 1D Input in XML\nDESCRIPTION: This XML snippet demonstrates the StringTensorUnpack operation with a 1D input tensor containing a batch of strings. It defines the input and output ports, including their precision and dimensions. The output tensors 'begins', 'ends', and 'symbols' represent the unpacked string data.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/string-tensor-unpack-15.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StringTensorUnpack\" ... >\n    <input>\n        <port id=\"0\" precision=\"STRING\">\n            <dim>2</dim>     <!-- batch of strings -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"I32\">\n            <dim>2</dim>     <!-- begins = [0, 5] -->\n        </port>\n        <port id=\"1\" precision=\"I32\">\n            <dim>2</dim>     <!-- ends = [5, 13] -->\n        </port>\n        <port id=\"2\" precision=\"U8\">\n            <dim>13</dim>     <!-- symbols = \"IntelOpenVINO\" encoded in an utf-8 array -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories and Linking Libraries\nDESCRIPTION: Sets the include directories for the target library to the current source directory. It also links the target library with the specified libraries in the `link_libraries` list.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}\")\ntarget_link_libraries(${TARGET_NAME} PRIVATE ${link_libraries})\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target in CMake\nDESCRIPTION: This CMake snippet adds a Clang format target for the pyopenvino module by calling the custom function `ov_add_clang_format_target`. This creates a target (named `${PROJECT_NAME}_clang`) that can be used to format the source code of the pyopenvino module according to Clang's formatting rules.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${PROJECT_NAME}_clang FOR_TARGETS ${PROJECT_NAME})\n```\n\n----------------------------------------\n\nTITLE: Project Definition\nDESCRIPTION: Defines the project name as 'appverifier_tests'. This is a standard CMake command that sets the project's name, which is used in various build processes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nproject(appverifier_tests)\n```\n\n----------------------------------------\n\nTITLE: Set Includes, Dependencies, and Link Libraries\nDESCRIPTION: This snippet sets the include directories, dependencies, and link libraries for the main test target. The INCLUDES variable specifies the include paths, the DEPENDENCIES variable specifies the target dependencies, and the LINK_LIBRARIES variable specifies the libraries to link against.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/functional/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(INCLUDES ${CMAKE_CURRENT_SOURCE_DIR} $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/src)\nset(DEPENDENCIES openvino_intel_cpu_plugin openvino_template_extension)\nset(LINK_LIBRARIES funcSharedTests cpuUtils openvino::snippets ov_snippets_models)\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions: Saving Test Execution Time\nDESCRIPTION: This YAML snippet demonstrates how to save test execution time using GitHub Actions' caching feature. It utilizes the `actions/cache/save` action to store the contents of the `PARALLEL_TEST_CACHE` environment variable under a key that includes the runner's OS, architecture, and the Git SHA. The cache is saved only when the workflow is triggered on the 'master' branch.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/caches.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nCPU_Functional_Tests:\n  name: CPU functional tests\n  ...\n  steps:\n    - name: Save tests execution time\n      uses: actions/cache/save@v3\n      if: github.ref_name == 'master'\n      with:\n        path: ${{ env.PARALLEL_TEST_CACHE }}\n        key: ${{ runner.os }}-${{ runner.arch }}-tests-functional-cpu-stamp-${{ github.sha }}\n    ...\n```\n\n----------------------------------------\n\nTITLE: Defining Target Name in CMake\nDESCRIPTION: This line defines the name of the target library to be built. The target name is used in subsequent CMake commands to refer to the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/template_extension/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_template_extension\")\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Paddle Test Target Definition\nDESCRIPTION: This snippet defines a test target named `paddle_tests` using the `ov_add_test_target` macro. It specifies dependencies, link libraries, and labels for the test target.  It links the test target with necessary libraries like openvino::cnpy, frontend_shared_test_classes and openvino::runtime. It also uses clang format for code styling and adds labels for test categorization.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n    NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        DEPENDENCIES\n            paddle_test_models\n            openvino_paddle_frontend\n            paddle_fe_standalone_build_test\n        LINK_LIBRARIES\n            openvino::cnpy\n            frontend_shared_test_classes\n            openvino_paddle_frontend\n            openvino::runtime\n            gtest_main_manifest\n            func_test_utils\n        ADD_CLANG_FORMAT\n        LABELS\n            ${ctest_labels} PADDLE_FE\n)\n```\n\n----------------------------------------\n\nTITLE: Example Model Creation Sample (C++)\nDESCRIPTION: This console command provides an example of running the `model_creation_sample` executable with the `lenet.bin` weights file and the `GPU` device.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/model-creation.rst#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nmodel_creation_sample lenet.bin GPU\n```\n\n----------------------------------------\n\nTITLE: Adding a Static Library in CMake\nDESCRIPTION: Adds a static library named `${TARGET_NAME}` (StressTestsCommon) using the source files and header files found by the `GLOB_RECURSE` commands. This creates the library target within the CMake project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/common/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${SRC} ${HDR})\n```\n\n----------------------------------------\n\nTITLE: Configuring Naming Style for OpenVINO Core in CMake\nDESCRIPTION: This snippet configures the naming style for the `openvino_core_obj` target, ensuring consistent code formatting. The `ov_ncc_naming_style` function applies a specific naming convention to the source code, improving readability and maintainability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nov_ncc_naming_style(FOR_TARGET openvino_core_obj\n                    SOURCE_DIRECTORIES \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: Convert flax.linen.Module with Custom Flags (Python)\nDESCRIPTION: This code demonstrates how to handle a `flax.linen.Module` with custom flags (e.g., `training`) in the `__call__` method. A helper function is created to remove the custom flags, and then traced with `jax.make_jaxpr` before converting to OpenVINO. Dependencies include `jax`, `jax.numpy`, `openvino`, `flax.linen`, and `flax.core`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-jax.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport openvino as ov\nfrom flax import linen as nn\nfrom flax.core import freeze, unfreeze\n\nclass SimpleModuleWithExtraFlag(nn.Module):\n    features: int\n\n    @nn.compact\n    def __call__(self, x, training):\n        x = nn.Dense(self.features)(x)\n        x = nn.BatchNorm(use_running_average=not training)(x)\n        return x\n\n\n# 1. Initialize the model\nmodule = SimpleModuleWithExtraFlag(features=10)\nkey = jax.random.PRNGKey(0)\ninput_data = jnp.ones((4, 5))  # Batch of 4 samples, each with 5 features\nparams = module.init(key, input_data, training=False)\n\n# 2. Create helper function with only input data parameter\ndef helper_function(x):\n    return module.apply(params, x, training=False)\n\n# 3. Trace the helper function\njaxpr = jax.make_jaxpr(helper_function)(input_data)\n\n# 4. Convert to OpenVINO\nov_model = ov.convert_model(jaxpr)\n```\n\n----------------------------------------\n\nTITLE: Install Target\nDESCRIPTION: This command installs the 'pybind_mock_frontend' target to the 'tests' directory. It is included as part of the 'tests' component and is excluded from the 'ALL' target, likely meaning it is only built when tests are specifically targeted.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/pyngraph_fe_mock_api/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${PYBIND_FE_NAME} DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Setting Build Options CMake\nDESCRIPTION: Sets the `BUILD_SHARED_LIBS` option to `OFF`, indicating that static libraries should be built instead of shared libraries. It also clears the `CMAKE_DEBUG_POSTFIX` to avoid debug postfixes, particularly important for avoiding issues with the loader.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/level_zero/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(BUILD_SHARED_LIBS OFF)\nset(CMAKE_DEBUG_POSTFIX \"\")\n```\n\n----------------------------------------\n\nTITLE: AvgPool Configuration: explicit padding, exclude-pad false\nDESCRIPTION: This XML snippet configures an AvgPool layer with 'explicit' auto_pad and 'exclude-pad' set to false. It has a 5x5 kernel, 1x1 padding at the beginning and end, and a 2x2 stride. This means that padding is explicitly defined and padded values *are* included in the average calculation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/avg-pool-14.rst#_snippet_3\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"AvgPool\" ... >\n    <data auto_pad=\"explicit\" exclude-pad=\"false\" kernel=\"5,5\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>15</dim>\n            <dim>15</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target in CMake\nDESCRIPTION: This CMake snippet adds a clang-format target for the library, allowing for automated code formatting according to the project's coding style.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/compiler_adapter/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: BitwiseLeftShift Layer Definition (No Broadcast) XML\nDESCRIPTION: This XML snippet defines a BitwiseLeftShift layer with matching input shapes, demonstrating the case where no broadcasting is needed. The input and output ports specify the dimensions of the tensors involved in the operation.  The layer type is 'BitwiseLeftShift'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-left-shift-15.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"BitwiseLeftShift\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target in CMake\nDESCRIPTION: This snippet uses the `ov_add_clang_format_target` function (likely a custom function defined elsewhere in the OpenVINO project) to add a target that checks the code style of the `TARGET_NAME` library using Clang Format. The `FOR_TARGETS` keyword specifies the target(s) to which the code style check applies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/tests/standalone_build/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: CMake Module Inclusion\nDESCRIPTION: This CMake script dynamically includes subdirectories as modules based on the existence of 'CMakeLists.txt' files within them. It prevents the inclusion of files or directories lacking a 'CMakeLists.txt', and it skips pre-defined template modules using the variable 'skip_module'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nov_deprecated_no_errors()\n\nset(skip_module \"${CMAKE_CURRENT_SOURCE_DIR}/template\")\nfile(GLOB local_modules \"${CMAKE_CURRENT_SOURCE_DIR}/*\")\nforeach(module_path IN LISTS local_modules)\n    if( # Skip files\n        NOT IS_DIRECTORY ${module_path} OR\n        # or directories without cmake\n        NOT EXISTS \"${module_path}/CMakeLists.txt\" OR\n        # module in the skip list\n        ${module_path} STREQUAL ${skip_module})\n        continue()\n    endif()\n    add_subdirectory(${module_path})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories CMake\nDESCRIPTION: Specifies the include directories for the target library. The `PUBLIC` keyword makes these include directories available to other targets that link against this library. The `$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>` expression ensures that the include directory is only used when building the library, not when installing it.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/utils/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}\n    PUBLIC \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\")\n```\n\n----------------------------------------\n\nTITLE: Setting Build Type and Property\nDESCRIPTION: This snippet sets the default CMake build type to 'Release' and defines allowed values for the build type.  This allows users to select from 'Release', 'Debug', 'RelWithDebInfo', or 'MinSizeRel' when configuring the build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"CMake build type\")\nset_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Release\" \"Debug\" \"RelWithDebInfo\" \"MinSizeRel\")\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for Gflags\nDESCRIPTION: This snippet sets a compiler flag to suppress warnings about unused variables when using GCC or Clang compilers. It appends the `-Wno-unused-variable` flag to the `CMAKE_CXX_FLAGS` variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/gflags/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-unused-variable\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration with Repeat Count\nDESCRIPTION: This code snippet demonstrates how to use the `repeat_count` parameter in a Protopipe YAML configuration file to specify that a particular operation should be executed multiple times. This feature is useful for simulating scenarios where a particular model or CPU load needs to be repeated within a pipeline.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- op_desc:\n  - { tag: A, path: Model_A.xml, ... }\n  - { tag: B, path: Model_B.xml, repeat_count: 20 }\n  - { tag: C, path: Model_C.xml, ... }\n  connections:\n    - [A, B, C]\n```\n\n----------------------------------------\n\nTITLE: Defining zlib Header Files\nDESCRIPTION: This snippet defines lists of header files that are part of the zlib library. These header files provide the interface to the library functions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/zlib/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(zlib_hdrs\n    zlib/crc32.h\n    zlib/deflate.h\n    zlib/gzguts.h\n    zlib/inffast.h\n    zlib/inffixed.h\n    zlib/inflate.h\n    zlib/inftrees.h\n    zlib/trees.h\n    zlib/zutil.h)\n\nset(zlib_ext_hdrs\n    zlib/zlib.h\n    zlib/zconf.h)\n```\n\n----------------------------------------\n\nTITLE: Disabling PythonInterp Package Find\nDESCRIPTION: This snippet disables the automatic finding of the Python interpreter package by CMake. This is useful when the project doesn't require Python integration or when a specific Python version needs to be controlled manually.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/gtest/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_DISABLE_FIND_PACKAGE_PythonInterp ON)\n```\n\n----------------------------------------\n\nTITLE: Setting Initial Dependencies\nDESCRIPTION: This snippet sets an initial dependency for the target library.  The runtime dev dependency is required for the test library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/test_builtin_extensions/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(DEPENDENCIES openvino::runtime::dev)\n```\n\n----------------------------------------\n\nTITLE: Aggregate Average Counters Report\nDESCRIPTION: This command executes the `aggregate-average-counters.py` script to process a CSV report named `benchmark_average_counters_report.csv`. The script aggregates the average counters present in the report.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/aggregate-average-counters/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\naggregate-average-counters.py benchmark_average_counters_report.csv\n```\n\n----------------------------------------\n\nTITLE: StridedSlice Shrink Axis Mask Example in XML\nDESCRIPTION: Illustrates how to remove a dimension from the output tensor using the `shrink_axis_mask` attribute. This example shows the equivalent of array[0:1, 0, 0:384, 0:640, 0:8]. The mask collapses the specified dimension to size 1, effectively removing it from the shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/strided-slice-1.rst#_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StridedSlice\" ...>\n    <data begin_mask=\"0,0,0,0,0\" end_mask=\"0,0,0,0,0\" new_axis_mask=\"0,0,0,0,0\" shrink_axis_mask=\"0,1,0,0,0\" ellipsis_mask=\"0,0,0,0,0\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim> <!-- first dim -->\n            <dim>2</dim> <!-- second dim -->\n            <dim>384</dim>\n            <dim>640</dim>\n            <dim>8</dim>\n        </port>\n        <port id=\"1\">\n            <dim>5</dim> <!-- begin: [0, 0, 0, 0, 0] -->\n        </port>\n        <port id=\"2\">\n            <dim>5</dim> <!-- end: [1, 0, 384, 640, 8] -->\n        </port>\n        <port id=\"3\">\n            <dim>5</dim> <!-- stride: [1, 1, 1, 1, 1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>1</dim> <!-- first dim kept, as shrink_axis_mask is 0 -->\n            <dim>384</dim> <!-- second dim is missing as shrink_axis_mask is 1 -->\n            <dim>640</dim>\n            <dim>8</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing Directories\nDESCRIPTION: Specifies directories to be installed as part of the build process. This includes the test runner, automation scripts, utils, and related configurations. EXCLUDE_FROM_ALL prevents these directories from being included in the default all target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(DIRECTORY test_runner/ DESTINATION tests/memory_tests/test_runner COMPONENT tests EXCLUDE_FROM_ALL)\ninstall(DIRECTORY .automation/ DESTINATION tests/memory_tests/test_runner/.automation COMPONENT tests EXCLUDE_FROM_ALL)\ninstall(DIRECTORY scripts/ DESTINATION tests/memory_tests/scripts COMPONENT tests EXCLUDE_FROM_ALL)\ninstall(DIRECTORY ../utils/ DESTINATION tests/utils COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Setting Library and Target Names\nDESCRIPTION: This snippet sets the name of the OpenVINO frontend to 'mock_py', constructs the target name by concatenating prefixes, the frontend name, and suffixes. It is used for naming the library during the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/mock_py_frontend/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(OV_FRONTEND_NAME \"mock_py\")\nset(TARGET_NAME \"${FRONTEND_NAME_PREFIX}${OV_FRONTEND_NAME}${FRONTEND_NAME_SUFFIX}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for ONNX Frontend Tests\nDESCRIPTION: These commands install the required Python dependencies for running the ONNX Frontend tests. They use pip to install the packages listed in the requirements.txt and requirements_test.txt files located in the python bindings directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/docs/tests.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npip install -r <OV_REPO_DIR>/src/bindings/python/requirements.txt\npip install -r <OV_REPO_DIR>/src/bindings/python/requirements_test.txt\n```\n\n----------------------------------------\n\nTITLE: StringTensorPack Example 2 (Empty String)\nDESCRIPTION: This example demonstrates the StringTensorPack operation with an empty string in the output. The 'begins' and 'ends' tensors are configured to produce an empty string between \"OMZ\" and \"GenAI\" in the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/string-tensor-pack-15.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StringTensorPack\" ... >\n    <input>\n        <port id=\"0\" precision=\"I64\">\n            <dim>2</dim>     <!-- begins = [0, 3, 3, 8, 9] -->\n        </port>\n        <port id=\"1\" precision=\"I64\">\n            <dim>2</dim>     <!-- ends = [3, 3, 8, 9, 13] -->\n        </port>\n        <port id=\"2\" precision=\"U8\">\n            <dim>13</dim>    <!-- symbols = \"OMZGenAI 2024\" encoded in an utf-8 array -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\" precision=\"STRING\">\n            <dim>5</dim>     <!-- output = [\"OMZ\", \"\", \"GenAI\", \" \", \"2024\"] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining Target Using `ov_add_target` in CMake\nDESCRIPTION: This snippet utilizes the `ov_add_target` macro to define the 'protopipe' executable. It specifies the target type, name, root directory, source directories, include directories, and link libraries.  It also enables cpplint checks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(PROTOPIPE_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/src)\n\nov_add_target(ADD_CPPLINT\n              TYPE EXECUTABLE\n              NAME ${TARGET_NAME}\n              ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n              ADDITIONAL_SOURCE_DIRS ${PROTOPIPE_SOURCE_DIR}\n              INCLUDES ${PROTOPIPE_SOURCE_DIR}\n              LINK_LIBRARIES PRIVATE ${DEPENDENCIES})\n```\n\n----------------------------------------\n\nTITLE: Setting Public Headers Directory in CMake\nDESCRIPTION: This snippet defines the variable `PUBLIC_HEADERS_DIR` to point to the include directory within the current source directory. This directory will be used to specify the location of public header files for the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/ov_helpers/ov_snippets_models/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(PUBLIC_HEADERS_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: Check JavaScript API code style with ESLint\nDESCRIPTION: Checks the code style of the JavaScript API using ESLint. This command enforces a consistent coding style and identifies potential issues.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/test_examples.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpm run lint\n```\n\n----------------------------------------\n\nTITLE: Component Dependencies Definition in components.yml (YAML)\nDESCRIPTION: This YAML snippet defines the dependencies between components in the .github/components.yml file. It specifies which components need to be revalidated (build and test) or built if a particular component is changed. Dependencies are not transitive, and must be defined explicitly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nPyTorch_FE:       # Component name\n  revalidate:     # Defines the list of components to revalidate (build + test) if the component above was changed\n    - OVC         # This component depends on PyTorch_FE and requires full revalidation\n  build:          # Defines the list of components to build if the PyTorch_FE was changed (test runs for them are skipped)\n    - CPU         # This component and the component below must be built if PyTorch_FE was changed\n    - Python_API\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: Sets the include directories for the `${TARGET_NAME}` library. Specifically, it adds the current source directory to the public include directories, allowing other targets to include headers from this library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/common/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC ${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Create Python Package (CMake)\nDESCRIPTION: This section of the CMake file creates a custom command to build and install the OpenVINO Python API using setup.py. It removes the existing installation directory, then executes setup.py with specific arguments to build extensions, install them into a prefix directory, and record the installed files. It also creates a similar command for the telemetry component. The command depends on the files and targets defined in ov_setup_py_deps and telemetry_files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_PYTHON_PACKAGING)\n    # site-packages depending on package type\n    set(python_xy \"${Python3_VERSION_MAJOR}.${Python3_VERSION_MINOR}\")\n    if(CPACK_GENERATOR STREQUAL \"DEB\")\n        set(python_versioned_folder \"python${Python3_VERSION_MAJOR}\")\n        set(ov_site_packages \"dist-packages\")\n    elseif(CPACK_GENERATOR STREQUAL \"RPM\")\n        set(python_versioned_folder \"python${python_xy}\")\n        set(ov_site_packages \"site-packages\")\n    endif()\n\n    # install OpenVINO Python API\n\n    set(ov_python_package_prefix \"${CMAKE_CURRENT_BINARY_DIR}/openvino/install_${pyversion}\")\n    set(ov_install_lib \"${ov_python_package_prefix}/lib/${python_versioned_folder}/${ov_site_packages}\")\n    set(openvino_meta_info_subdir \"openvino-${OpenVINO_VERSION}-py${python_xy}.egg-info\")\n    set(openvino_meta_info_file \"${ov_install_lib}/${openvino_meta_info_subdir}/PKG-INFO\")\n\n    add_custom_command(OUTPUT ${openvino_meta_info_file}\n        COMMAND ${CMAKE_COMMAND} -E remove_directory\n            \"${ov_python_package_prefix}\"\n        COMMAND ${setup_py_env}\n                # variables to reflect options (extensions only or full wheel package)\n                PYTHON_EXTENSIONS_ONLY=ON\n                SKIP_RPATH=ON\n            \"${Python3_EXECUTABLE}\" \"${CMAKE_CURRENT_SOURCE_DIR}/wheel/setup.py\"\n                --no-user-cfg\n                --quiet\n                build\n                    --executable \"/usr/bin/python3\"\n                build_ext\n                install\n                    --no-compile\n                    --prefix \"${ov_python_package_prefix}\"\n                    --install-lib \"${ov_install_lib}\"\n                    --install-scripts \"${ov_python_package_prefix}/bin\"\n                    --single-version-externally-managed\n                    --record=installed.txt\n        WORKING_DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}\"\n        DEPENDS ${ov_setup_py_deps}\n        COMMENT \"Create python package with ${openvino_meta_info_subdir} folder\")\n\n    # Install OpenVINO Telemetry\n    set(OpenVINO_Telemetry_SOURCE_DIR \"${OpenVINO_SOURCE_DIR}/thirdparty/telemetry\")\n    file(GLOB_RECURSE telemetry_files ${OpenVINO_Telemetry_SOURCE_DIR}/*)\n\n    set(telemetry_python_package_prefix \"${CMAKE_CURRENT_BINARY_DIR}/telemetry/install_${pyversion}\")\n    set(telemetry_install_lib \"${telemetry_python_package_prefix}/lib/${python_versioned_folder}/${ov_site_packages}\")\n    set(openvino_telemetry_meta_info_subdir \"openvino-telemetry-${OpenVINO_VERSION}-py${python_xy}.egg-info\")\n    set(openvino_telemetry_meta_info_file \"${telemetry_install_lib}/${openvino_telemetry_meta_info_subdir}/PKG-INFO\")\n\n    add_custom_command(OUTPUT ${openvino_telemetry_meta_info_file}\n        COMMAND ${CMAKE_COMMAND} -E remove_directory\n        \"${telemetry_python_package_prefix}\"\n        COMMAND \"${Python3_EXECUTABLE}\" \"${OpenVINO_Telemetry_SOURCE_DIR}/setup.py\"\n                    --no-user-cfg\n                    --quiet\n                    build\n                        --executable \"/usr/bin/python3\"\n                    install\n                        --no-compile\n                        --prefix \"${telemetry_python_package_prefix}\"\n                        --install-lib \"${telemetry_install_lib}\"\n                        --install-scripts \"${telemetry_python_package_prefix}/bin\"\n                        --single-version-externally-managed\n                        --record=installed.txt\n        WORKING_DIRECTORY \"${OpenVINO_Telemetry_SOURCE_DIR}\"\n        DEPENDS ${telemetry_files}\n        COMMENT \"Create python package with ${openvino_telemetry_meta_info_subdir} folder\")\n\n    # create custom target\n\n    add_custom_target(_python_api_package ALL DEPENDS ${openvino_meta_info_file} ${openvino_telemetry_meta_info_file})\n\n    # install python package, which will be later packed into DEB | RPM\n    ov_cpack_add_component(${OV_CPACK_COMP_PYTHON_OPENVINO}_package_${pyversion} HIDDEN)\n\n    install(DIRECTORY ${ov_python_package_prefix}/ ${telemetry_python_package_prefix}/\n            DESTINATION .\n            COMPONENT ${OV_CPACK_COMP_PYTHON_OPENVINO_PACKAGE}_${pyversion}\n            ${OV_CPACK_COMP_PYTHON_OPENVINO_PACKAGE_EXCLUDE_ALL}\n            USE_SOURCE_PERMISSIONS)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Set Compile Options for Release Configuration\nDESCRIPTION: Sets compile options specific to the Release configuration. For MSVC, it uses /Os (favor small code size), and for other compilers, it uses -Os.  This optimizes the code size in release builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_options(${TARGET_NAME} PRIVATE\n    $<$<CONFIG:Release>:$<IF:$<CXX_COMPILER_ID:MSVC>,/Os,-Os>>)\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories (CMake)\nDESCRIPTION: This sets the include directories for the target. These include directories are used when compiling the library and are exposed to downstream projects using the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC $<BUILD_INTERFACE:${INCLUDE_DIR}>\n                                                 $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}>\n                                                 $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/kernels/>\n                                                 $<BUILD_INTERFACE:${CODEGEN_INCDIR}>)\n```\n\n----------------------------------------\n\nTITLE: Defining i16 data type in TypeScript\nDESCRIPTION: Defines the i16 data type as a number in TypeScript. This represents a 16-bit integer, used for storing integer values within a specific range.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/element.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ni16: number\n```\n\n----------------------------------------\n\nTITLE: Configuring Hello Classification Sample with CMake\nDESCRIPTION: This CMake snippet configures the build for the 'hello_classification' sample. It defines the sample's name, specifies the source file 'main.cpp', and lists the required dependencies 'format_reader' and 'ie_samples_utils'. This ensures that the sample is built correctly with the necessary libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_classification/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_sample(NAME hello_classification\n              SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/main.cpp\"\n              DEPENDENCIES format_reader ie_samples_utils)\n```\n\n----------------------------------------\n\nTITLE: Define Model output Method (TypeScript) - No Parameters\nDESCRIPTION: This code defines the `output` method of the `Model` interface without parameters. It gets the output of the model and throws an exception if the model has more than one output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\noutput(): Output\n```\n\n----------------------------------------\n\nTITLE: Defining Compile Definitions for Static Builds CMake\nDESCRIPTION: This snippet defines compile definitions when building a static library. The `target_compile_definitions` command adds the `OPENVINO_STATIC_LIBRARY` definition to the target if `BUILD_SHARED_LIBS` is not enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/src/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME} PUBLIC OPENVINO_STATIC_LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: ConvertLike Operation C++ Example\nDESCRIPTION: Illustrates how to represent the ConvertLike operation in a C++-like layer definition. The layer takes two input tensors, 'data' (int32) and 'like' (float32), and outputs a tensor with the same shape as 'data' but with the data type of 'like' (float32).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/convert-like-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"ConvertLike\">\n    <input>\n        <port id=\"0\">        <!-- type: int32 -->\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">        <!-- type: float32 -->\n            <dim>3</dim>     <!-- any data -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">        <!-- result type: float32 -->\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: EmbeddingBagOffsets NumPy Implementation\nDESCRIPTION: This Python snippet demonstrates the functionality of the EmbeddingBagOffsets operation using NumPy. It takes an embedding table, indices, offsets, default index, per-sample weights, and reduction mode as input, and returns the embeddings for each bag after applying the specified reduction.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sparse/embedding-bag-offsets-15.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef embedding_bag_offsets(\n        emb_table: np.ndarray,\n        indices: np.ndarray,\n        offsets: np.ndarray,\n        default_index: Optional[int] = None,\n        per_sample_weights: Optional[np.ndarray] = None,\n        reduction: Literal[\"sum\", \"mean\"] = \"sum\",\n    ):\n        assert (\n            reduction == \"sum\" or per_sample_weights is None\n        ), \"Attribute per_sample_weights is only supported in sum reduction.\"\n        if per_sample_weights is None:\n            per_sample_weights = np.ones_like(indices)\n        embeddings = []\n        for emb_idx, emb_weight in zip(indices, per_sample_weights):\n            embeddings.append(emb_table[emb_idx] * emb_weight)\n        previous_offset = offsets[0]\n        bags = []\n        offsets = np.append(offsets, len(indices))\n        for bag_offset in offsets[1:]:\n            bag_size = bag_offset - previous_offset\n            if bag_size != 0:\n                embedding_bag = embeddings[previous_offset:bag_offset]\n                reduced_bag = np.add.reduce(embedding_bag)\n                if reduction == \"mean\":\n                    reduced_bag = reduced_bag / bag_size\n                bags.append(reduced_bag)\n            else:\n                # Empty bag case\n                if default_index is not None and default_index != -1:\n                    bags.append(emb_table[default_index])\n                else:\n                    bags.append(np.zeros(emb_table.shape[1:]))\n            previous_offset = bag_offset\n        return np.stack(bags, axis=0)\n```\n\n----------------------------------------\n\nTITLE: CoreConstructor Constructor Definition in TypeScript\nDESCRIPTION: This TypeScript code snippet defines the constructor for the `CoreConstructor` interface. It shows that the constructor returns a `Core` object, which represents the OpenVINO runtime core. This is the standard way to instantiate a Core object within the OpenVINO JavaScript bindings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CoreConstructor.rst#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nnew CoreConstructor(): Core\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: This command installs the required Python libraries (numpy) for OpenVINO using pip. It reads the list of dependencies from the `requirements.txt` file located in the `python` subdirectory of the OpenVINO installation directory. It uses `python3 -m pip` to ensure that the pip version associated with the python3 interpreter is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\ncd /opt/intel/openvino_2025.1.0\npython3 -m pip install -r ./python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Ceiling Layer XML Definition in OpenVINO\nDESCRIPTION: This XML snippet demonstrates how to define a Ceiling layer in OpenVINO. It specifies the input and output ports, along with their dimensions. The layer performs an element-wise ceiling operation on the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/ceiling-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Ceiling\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Abs Layer XML Configuration in OpenVINO\nDESCRIPTION: Defines the XML configuration for an Abs layer in OpenVINO. It specifies the input and output ports with their dimensions. This example shows how to represent the Abs operation within an OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/abs-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Abs\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Accessing Output by name of CompiledModel (TypeScript)\nDESCRIPTION: This code shows the declaration of the `output` method within the `CompiledModel` interface to retrieve an output by name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\noutput(name): Output\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Options (Clang)\nDESCRIPTION: If the C++ compiler is Clang, this sets the `-Wno-error=register` compile option for the target library. This suppresses errors related to the deprecated 'register' specifier, ensuring compatibility with older codebases.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n    target_compile_options(${TARGET_NAME} PRIVATE \"-Wno-error=register\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenCL Variables CMake\nDESCRIPTION: This snippet retrieves the library path and name for OpenCL and sets the OpenCL_INCLUDE_DIR, OpenCL_LIBRARY, and opencl_root_hints variables for use with find_package(OpenCL).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ocl/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nget_lib_path(\"${CMAKE_RUNTIME_OUTPUT_DIRECTORY}\" OPENCL_LIB_DIR)\nget_lib_name(\"OpenCL\" OPENCL_LIB_NAME)\n\nset(OpenCL_INCLUDE_DIR \"${OPENCL_ICD_LOADER_HEADERS_DIR}\" PARENT_SCOPE)\nset(OpenCL_LIBRARY \"${OPENCL_LIB_DIR}/${OPENCL_LIB_NAME}\" PARENT_SCOPE)\nset(opencl_root_hints \"${OpenCL_INCLUDE_DIR}\" PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Greater-1 XML Example (Numpy Broadcast) - OpenVINO\nDESCRIPTION: This XML snippet illustrates the Greater-1 operation utilizing numpy broadcasting rules. The input tensors have different shapes, but broadcasting is enabled via 'auto_broadcast=\"numpy\"'. The output tensor's shape is the result of applying numpy broadcasting rules to the input shapes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/greater-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Greater\">\n    <data auto_broadcast=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ov_build_target_faster CMake\nDESCRIPTION: This snippet calls the `ov_build_target_faster` function to enable faster build options for the specified target. The `UNITY` option suggests building in unity mode, which can improve compilation times.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME}\n    UNITY\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Library Target\nDESCRIPTION: This snippet defines a static library target named '${TARGET_NAME}' using the collected source and header files.  It also creates an alias 'openvino::frontend::common_translators' for the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common_translators/src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${LIBRARY_SRC} ${LIBRARY_HEADERS})\nadd_library(openvino::frontend::common_translators ALIAS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Linking TensorFlow Frontend Library in CMake\nDESCRIPTION: This snippet uses the `target_link_libraries` command to link the static library defined earlier (`TARGET_NAME`) against the `openvino_tensorflow_frontend` library. The `PUBLIC` keyword makes the dependency visible to consumers of the static library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/tests/standalone_build/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino_tensorflow_frontend)\n```\n\n----------------------------------------\n\nTITLE: Shared Drive Cache: Mounting in Docker Container\nDESCRIPTION: This YAML snippet demonstrates how to mount a shared drive into a Docker container within a GitHub Actions workflow. It defines a `volumes` mapping under the `container` section, mapping the runner's `/mount` directory to the same path inside the container. It also passes environment variables required by `sccache` to the container.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/caches.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nBuild:\n  ...\n  runs-on: aks-linux-16-cores-32gb\n  container:\n    image: openvinogithubactions.azurecr.io/dockerhub/ubuntu:20.04\n    volumes:\n      - /mount:/mount\n    options: -e SCCACHE_AZURE_BLOB_CONTAINER -e SCCACHE_AZURE_CONNECTION_STRING\n```\n\n----------------------------------------\n\nTITLE: ScaledDotProductAttention Example 2 (XML)\nDESCRIPTION: This XML snippet showcases the ScaledDotProductAttention layer configuration with multiple batch dimensions (N1, N2, N3). The input and output ports specify the dimensions for query, key, value, and attention_mask. 'causal' attribute is set to false.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/scaled-dot-product-attention.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"286\" name=\"aten::scaled_dot_product_attention_0\" type=\"ScaledDotProductAttention\" version=\"opset13\">\n\t\t\t<data causal=\"false\" />\n\t\t\t<input>\n\t\t\t\t<!-- Multiple batch dimensions: N1 = 1, N2 = 2, N3 = 3-->\n\t\t\t\t<port id=\"0\" precision=\"FP32\"> <!-- query -->\n\t\t\t\t\t<dim>1</dim> <!-- N1 -->\n\t\t\t\t\t<dim>2</dim> <!-- N2 -->\n\t\t\t\t\t<dim>3</dim> <!-- N3 -->\n\t\t\t\t\t<dim>-1</dim> <!-- L -->\n\t\t\t\t\t<dim>80</dim> <!-- E -->\n\t\t\t\t</port>\n\t\t\t\t<port id=\"1\" precision=\"FP32\"> <!-- key -->\n\t\t\t\t\t<dim>1</dim> <!-- N1 -->\n\t\t\t\t\t<dim>2</dim> <!-- N2 -->\n\t\t\t\t\t<dim>3</dim> <!-- N3 -->\n\t\t\t\t\t<dim>-1</dim> <!-- S -->\n\t\t\t\t\t<dim>80</dim> <!-- E -->\n\t\t\t\t</port>\n\t\t\t\t<port id=\"2\" precision=\"FP32\"> <!-- value -->\n\t\t\t\t\t<dim>1</dim> <!-- N1 -->\n\t\t\t\t\t<dim>2</dim> <!-- N2 -->\n\t\t\t\t\t<dim>3</dim> <!-- N3 -->\n\t\t\t\t\t<dim>-1</dim> <!-- S -->\n\t\t\t\t\t<dim>80</dim> <!-- Ev -->\n\t\t\t\t</port>\n\t\t\t\t<port id=\"3\" precision=\"FP32\"> <!-- attention_mask -->\n\t\t\t\t\t<dim>1</dim> <!-- N1 -->\n\t\t\t\t\t<dim>2</dim> <!-- N2 -->\n\t\t\t\t\t<dim>3</dim> <!-- N3 -->\n\t\t\t\t\t<dim>-1</dim> <!-- L -->\n\t\t\t\t\t<dim>-1</dim> <!-- S -->\n\t\t\t\t</port>\n\t\t\t</input>\n\t\t\t<output>\n\t\t\t\t<port id=\"4\" precision=\"FP32\">\n\t\t\t\t\t<dim>1</dim> <!-- N1 -->\n\t\t\t\t\t<dim>2</dim> <!-- N2 -->\n\t\t\t\t\t<dim>3</dim> <!-- N3 -->\n\t\t\t\t\t<dim>-1</dim> <!-- L -->\n\t\t\t\t\t<dim>80</dim> <!-- Ev -->\n\t\t\t\t</port>\n\t\t\t</output>\n\t\t</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Library in CMake\nDESCRIPTION: Creates a static library named `${TARGET_NAME}` from the collected source and header files. This library will be built and linked into other projects that depend on it.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/format_reader/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${MAIN_SRC} ${LIBRARY_HEADERS} ${LIBRARY_PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Define Source Files for Tests\nDESCRIPTION: This snippet defines lists of source files for multi-backend tests (MULTI_TEST_SRC) and general tests (SRC). It then iterates through these lists, converting relative paths to absolute paths and appending them to the full_src_names list.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(MULTI_TEST_SRC\n    onnx_import.in.cpp\n    onnx_import_com_microsoft.in.cpp\n    onnx_import_controlflow.in.cpp\n    onnx_import_const_folding.in.cpp\n    onnx_import_convpool.in.cpp\n    onnx_import_deprecated.in.cpp\n    onnx_import_dyn_shapes.in.cpp\n    onnx_import_org_openvino.in.cpp\n    onnx_import_org_pytorch.in.cpp\n    onnx_import_reshape.in.cpp\n    onnx_import_rnn.in.cpp\n    onnx_import_signal.in.cpp\n    onnx_import_quant.in.cpp\n    onnx_import_with_editor.in.cpp)\nset(SRC\n    conversion.cpp\n    convert_tests.cpp\n    convert_partially_tests.cpp\n    library_extension.cpp\n    load_from.cpp\n    onnx_editor.cpp\n    onnx_editor_topological_sort.cpp\n    onnx_import_exceptions.cpp\n    onnx_importer_test.cpp\n    onnx_tensor_names.cpp\n    onnx_utils.cpp\n    onnx_transformations.cpp\n    op_extension.cpp\n    telemetry.cpp\n    lib_close.cpp\n    model_support_tests.cpp\n    onnx_ops_registration.cpp\n    onnx_reader_external_data.cpp\n    skip_tests_config.cpp)\n\nforeach(src IN LISTS SRC MULTI_TEST_SRC)\n    if(IS_ABSOLUTE \"${src}\")\n        list(APPEND full_src_names ${src})\n    else()\n        list(APPEND full_src_names \"${CMAKE_CURRENT_SOURCE_DIR}/${src}\")\n    endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Einsum Trace Computation Example C++\nDESCRIPTION: This example illustrates how Einsum computes the trace for each batch object using the Einstein summation convention.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/einsum-7.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nA = [[[1.0, 2.0, 3.0],\n      [4.0, 5.0, 6.0],\n      [7.0, 8.0, 9.0]],\n     [[2.0, 4.0, 6.0],\n      [8.0, 10.0, 12.0],\n      [14.0, 16.0, 18.0]]]\nequation = \"kii->k\"\noutput = [15.0, 30.0]\n```\n\n----------------------------------------\n\nTITLE: Add Tests Subdirectory\nDESCRIPTION: Adds the `tests` subdirectory to the build process. This ensures that all tests within that directory are compiled and linked into the project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/common_test_utils/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(tests)\n```\n\n----------------------------------------\n\nTITLE: Read ONNX model using read_model in Python\nDESCRIPTION: This snippet shows how to read an ONNX model using the `read_model()` method from the OpenVINO Core object in Python. The model is then compiled using `compile_model()` for execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-preparation/convert-model-to-ir.rst#_snippet_13\n\nLANGUAGE: py\nCODE:\n```\nimport openvino as ov\n\ncore = ov.Core()\nov_model = core.read_model(\"<INPUT_MODEL>.onnx\")\ncompiled_model = ov.compile_model(ov_model, \"AUTO\")\n```\n\n----------------------------------------\n\nTITLE: Configure Build with CMake (Bash)\nDESCRIPTION: This CMake command configures the build environment for the OpenVINO Node.js bindings. It sets build options such as build type, package generator, and various feature flags to customize the build process based on the user's environment and needs.  It disables features like TBB, tests, samples, Python, Wheel and GPU support.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncmake \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DCPACK_GENERATOR=NPM \\\n  -DENABLE_SYSTEM_TBB=OFF -UTBB* \\\n  -DENABLE_TESTS=OFF \\\n  -DENABLE_SAMPLES=OFF \\\n  -DENABLE_WHEEL=OFF \\\n  -DENABLE_PYTHON=OFF \\\n  -DENABLE_INTEL_GPU=OFF \\\n  -DCMAKE_INSTALL_PREFIX=\"../src/bindings/js/node/bin\" \\\n  ..\n```\n\n----------------------------------------\n\nTITLE: Verifying OpenVINO Packages\nDESCRIPTION: Verifies if OpenVINO packages have been built and included in the Yocto image by listing the packages using `oe-pkgdata-util` and filtering the output for packages containing \"openvino\". This confirms the successful integration of OpenVINO into the built image.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yocto.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\noe-pkgdata-util list-pkgs | grep openvino\n```\n\n----------------------------------------\n\nTITLE: List of Python Dependencies\nDESCRIPTION: This snippet lists the Python dependencies required for the project. It includes packages for numerical computation (numpy), image processing (opencv-python-headless), HTTP requests (requests), testing (pytest, pytest-xdist), and YAML parsing (pyyaml).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/samples_tests/smoke_tests/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-c ../../constraints.txt\nnumpy\nopencv-python-headless\nrequests\npytest\npytest-xdist  # Enable pytest --numprocesses\npyyaml\n```\n\n----------------------------------------\n\nTITLE: Set Target Name CMake\nDESCRIPTION: Sets the target name for the library to 'openvino_onnx_common'. This name is used to refer to the library target in subsequent CMake commands.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/onnx_common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_onnx_common\")\n```\n\n----------------------------------------\n\nTITLE: Dump IR with formats filter OpenVINO (sh)\nDESCRIPTION: This example shows how to use multiple options with OV_CPU_DUMP_IR, including filtering transformations and specifying the formats for IR dumping. It provides an example of dumping IR in SVG and XML formats after specific transformations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/graph_serialization.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_DUMP_IR=\"transformations=all,-common dir=path/dumpdir formats=svg,xml\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Sin Layer XML Configuration in OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of a Sin layer in OpenVINO.  It specifies the input and output ports with their respective dimensions, showcasing how the Sin operation is integrated within an OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/sin-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Sin\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags for MSVC\nDESCRIPTION: This snippet adds a compiler flag to suppress a specific warning (`/wd5051`) when using the MSVC compiler. This is a compiler-specific configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/vectorized/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    ov_add_compiler_flags(/wd5051)\nendif()\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Semicolons\nDESCRIPTION: This rule enforces the use of semicolons at the end of statements in JavaScript and TypeScript code. It is enforced by ESLint with the configuration `semi: ['error']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nsemi: ['error']\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for IR Frontend (CMake)\nDESCRIPTION: This snippet conditionally adds the 'ir' subdirectory to the build if the ENABLE_OV_IR_FRONTEND CMake option is enabled. This includes the functionality related to the OpenVINO Intermediate Representation (IR) frontend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_OV_IR_FRONTEND)\n    add_subdirectory(ir)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries\nDESCRIPTION: This snippet links the target ${TARGET_NAME} to the OpenVINO runtime library and conditionally links it to the ONNX, TensorFlow, and Paddle frontend libraries based on whether the corresponding frontend support is enabled. The openvino::runtime library is always linked.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/mock_py_frontend/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC\n    openvino::runtime\n    $<$<BOOL:${ENABLE_OV_ONNX_FRONTEND}>:openvino::frontend::onnx>\n    $<$<BOOL:${ENABLE_OV_TF_FRONTEND}>:openvino::frontend::tensorflow>\n    $<$<BOOL:${ENABLE_OV_PADDLE_FRONTEND}>:openvino::frontend::paddle>)\n```\n\n----------------------------------------\n\nTITLE: Install Python\nDESCRIPTION: Installs the newly built Python version using `sudo make altinstall`. The `altinstall` target is crucial to prevent overwriting the system's default Python installation, allowing the new version to coexist with the existing one.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/python_version_upgrade.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo make altinstall\n```\n\n----------------------------------------\n\nTITLE: BFloat16 Conversion\nDESCRIPTION: Converts a value to bfloat16 by applying a mask and divisor.  This snippet demonstrates the conversion process for bfloat16 data types, used within a random number generation context.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\nmantissa_digits = 8 //(mantissa / significand bits count of bfloat16 + 1, equal to 8)\nmask = uint32(uint64(1) << mantissa_digits - 1)\ndivisor = float(1) / (uint64(1) << mantissa_digits)\noutput = bfloat16((x & mask) * divisor)\n```\n\n----------------------------------------\n\nTITLE: Install Documentation Packages\nDESCRIPTION: Installs additional packages required for building the documentation, including Doxygen, Graphviz, and Texlive. Uses the `apt` package manager. Requires sudo privileges.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/documentation_build_instructions.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\n$ apt install -y doxygen graphviz texlive\n```\n\n----------------------------------------\n\nTITLE: Creating Static Library with CMake\nDESCRIPTION: This CMake snippet defines a static library named 'timetests_helper'. It gathers all .cpp files in the current directory, adds them as source files, and then specifies the include directories for the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/src/timetests_helper/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset (TARGET_NAME \"timetests_helper\")\n\nfile (GLOB SRC *.cpp)\nadd_library(${TARGET_NAME} STATIC ${SRC})\ntarget_include_directories(${TARGET_NAME} PUBLIC \"${CMAKE_SOURCE_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding Tests Subdirectory with CMake\nDESCRIPTION: This CMake code block conditionally adds the 'tests' subdirectory to the build process if the 'ENABLE_TESTS' variable is set to true. If 'ENABLE_TESTS' is enabled, CMake will process the CMakeLists.txt file within the 'tests' directory, including the tests in the build. The 'ENABLE_TESTS' variable is typically defined during the CMake configuration step.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/paddle/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_TESTS)\n   add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This snippet sets the target name for the unit tests to `ov_inference_unit_tests`. This name is used to identify the target during the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/tests/unit/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME ov_inference_unit_tests)\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files\nDESCRIPTION: This snippet uses the file(GLOB_RECURSE) command to find all .cpp and .h files in the current directory and its subdirectories.  These files are then stored in the SRC and HDR variables, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/thread_local/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile (GLOB_RECURSE SRC *.cpp)\nfile (GLOB_RECURSE HDR *.h)\n```\n\n----------------------------------------\n\nTITLE: Activate Conda Environment (Azure ML)\nDESCRIPTION: This command activates the Conda environment `openvino_env`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_19\n\nLANGUAGE: console\nCODE:\n```\nconda activate openvino_env\n```\n\n----------------------------------------\n\nTITLE: Define Common Utilities Library\nDESCRIPTION: Defines a CMake function `add_common_utils` that configures a library target with specified dependencies, include directories, and compile definitions. It handles conditional inclusion of PostgreSQL support based on the `ENABLE_CONFORMANCE_PGQL` flag and linking against static or shared OpenVINO runtime libraries based on the presence of `USE_STATIC_IE`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/common_test_utils/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(add_common_utils ADD_TARGET_NAME)\n    list(APPEND TARGET_EXCLUDED_SOURCE_PATHS\n        ${CMAKE_CURRENT_SOURCE_DIR}/tests\n    )\n    if(NOT ENABLE_CONFORMANCE_PGQL)\n        list(APPEND TARGET_EXCLUDED_SOURCE_PATHS\n            ${CMAKE_CURRENT_SOURCE_DIR}/include/common_test_utils/postgres_link.hpp\n            ${CMAKE_CURRENT_SOURCE_DIR}/include/common_test_utils/postgres_helpers.hpp\n            ${CMAKE_CURRENT_SOURCE_DIR}/src/postgres_link.cpp\n            ${CMAKE_CURRENT_SOURCE_DIR}/src/postgres_helpers.cpp\n        )\n    endif()\n\n    # create target\n    ov_add_target(\n            NAME ${ADD_TARGET_NAME}\n            TYPE STATIC\n            ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n            EXCLUDED_SOURCE_PATHS\n                ${TARGET_EXCLUDED_SOURCE_PATHS}\n            ADD_CLANG_FORMAT\n            DEPENDENCIES\n                openvino_template_plugin\n            LINK_LIBRARIES\n                PUBLIC\n                    gtest\n                    gtest_main\n                    gmock\n                    openvino::runtime\n                    openvino::runtime::dev\n                PRIVATE\n                    openvino::util\n                    openvino::shape_inference\n                INCLUDES\n                    PUBLIC\n                        \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\"\n                    PRIVATE\n                        \"${CMAKE_CURRENT_SOURCE_DIR}/src\"\n                        \"${OpenVINO_SOURCE_DIR}/src/plugins/template/include\"\n    )\n\n    ov_developer_package_export_targets(TARGET ${ADD_TARGET_NAME}\n                                        INSTALL_INCLUDE_DIRECTORIES \"${CMAKE_CURRENT_SOURCE_DIR}/include/\")\n\n    if(ENABLE_CONFORMANCE_PGQL)\n        target_compile_definitions(${ADD_TARGET_NAME} PUBLIC ENABLE_CONFORMANCE_PGQL)\n    endif()\n\n    # USE_STATIC_IE is passed\n    if(ARGN)\n        target_link_libraries(${ADD_TARGET_NAME} PRIVATE openvino_runtime_s)\n    endif()\n\n    ov_build_target_faster(${ADD_TARGET_NAME}\n        UNITY\n        PCH PRIVATE \"src/precomp.hpp\"\n    )\n\n    target_include_directories(${ADD_TARGET_NAME}\n        PUBLIC\n            $<TARGET_PROPERTY:openvino::runtime,INTERFACE_INCLUDE_DIRECTORIES>\n        PRIVATE\n            $<TARGET_PROPERTY:openvino::runtime::dev,INTERFACE_INCLUDE_DIRECTORIES>)\n    target_include_directories(${ADD_TARGET_NAME} SYSTEM PUBLIC \"$<BUILD_INTERFACE:${OV_TESTS_ROOT}/test_utils>\")\n\n    target_compile_definitions(${ADD_TARGET_NAME} PUBLIC ${ARGN})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: Adds the 'src' subdirectory to the CMake build process. This includes all CMakeLists.txt files within the 'src' directory and its subdirectories for compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Add API Validator Post Build Step\nDESCRIPTION: This snippet uses the custom CMake function `ov_add_api_validator_post_build_step` to add a post-build step for API validation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n# must be called after all target_link_libraries\nov_add_api_validator_post_build_step(TARGET ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Checking OpenCL Driver Path (Linux)\nDESCRIPTION: This command checks the path to the OpenCL driver by reading the contents of the `/etc/OpenCL/vendors/intel.icd` file. This file should contain the path to the OpenCL runtime library. The path to the runtime library may vary depending on the driver version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_driver_troubleshooting.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ cat /etc/OpenCL/vendors/intel.icd\n/usr/lib/x86_64-linux-gnu/intel-opencl/libigdrcl.so\n```\n\n----------------------------------------\n\nTITLE: Adding API Validator Post-Build Step\nDESCRIPTION: Adds a post-build step to the specified target to validate the API using the `ov_add_api_validator_post_build_step` macro. This ensures that the API of the target is consistent and correct after the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_42\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_api_validator_post_build_step(TARGET ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Grouping Source Files\nDESCRIPTION: This function groups source files in Visual Studio for better organization. It iterates through files in a directory and assigns them to a specific group using the UNITY_GROUP property. This improves project structure in IDEs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(group_source_file GROUP_NAME GROUP_DIR)\n    file(GLOB GROUP_FILES  ${GROUP_DIR}/*.cpp)\n    foreach(file ${GROUP_FILES})\n        set_source_files_properties(${file} PROPERTIES UNITY_GROUP ${GROUP_NAME})\n    endforeach()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files\nDESCRIPTION: This snippet uses file(GLOB_RECURSE) to collect source files ('.h', '.hpp', '.cpp') from the current source directory and specified subdirectories within the OpenVINO project.  These files are stored in the SOURCES_MAIN variable for later use in building the executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES_MAIN\n    \"${CMAKE_CURRENT_SOURCE_DIR}/*.h\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\"\n    # openvino graph transformation\n    \"${CMAKE_HOME_DIRECTORY}/src/plugins/intel_gpu/src/plugin/transformations/*.hpp\"\n    \"${CMAKE_HOME_DIRECTORY}/src/plugins/intel_gpu/src/plugin/transformations/*.cpp\"\n    \"${CMAKE_HOME_DIRECTORY}/src/plugins/intel_gpu/src/plugin/variable_state.cpp\"\n    \"${CMAKE_HOME_DIRECTORY}/src/plugins/intel_gpu/src/plugin/multi_tensor_variable_state.cpp\"\n    \"${CMAKE_HOME_DIRECTORY}/src/plugins/intel_gpu/src/plugin/remote_context.cpp\"\n    \"${CMAKE_HOME_DIRECTORY}/src/plugins/intel_gpu/src/plugin/remote_tensor.cpp\"\n    \"${CMAKE_HOME_DIRECTORY}/src/plugins/intel_gpu/src/plugin/usm_host_tensor.cpp\"\n    \"${CMAKE_HOME_DIRECTORY}/src/plugins/intel_gpu/src/plugin/common_utils.cpp\"\n  )\n```\n\n----------------------------------------\n\nTITLE: AvgPool Layer Configuration with same_upper auto_pad\nDESCRIPTION: This XML snippet shows the configuration of an AvgPool layer with `auto_pad` set to `same_upper`. The `kernel` size is 2x2, and the padding excludes padded values. The stride is set to 2x2.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/avg-pool-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"AvgPool\" ... >\n    <data auto_pad=\"same_upper\" exclude-pad=\"true\" kernel=\"2,2\" pads_begin=\"0,0\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Set HuggingFace Endpoint (Linux/macOS)\nDESCRIPTION: These commands install or upgrade `huggingface_hub` and set the `HF_ENDPOINT` environment variable. This is specifically for users in PRC who have trouble accessing Hugging Face directly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_17\n\nLANGUAGE: console\nCODE:\n```\npip install -U huggingface_hub\nset HF_ENDPOINT = https://hf-mirror.com\n```\n\n----------------------------------------\n\nTITLE: Squeeze 1D Tensor to Dynamic Shape using OpenVINO XML\nDESCRIPTION: This example demonstrates squeezing a 1D tensor with a dynamic shape (-1) to a fully dynamic shape. The 'allow_axis_skip' attribute is set to true, which allows for dynamic rank output. Input port 0 has a dynamic dimension of -1, and input port 1 specifies the axis to be squeezed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/squeeze-15.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Squeeze\" version=\"opset15\">\n    <data allow_axis_skip=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>-1</dim>\n        </port>\n    </input>\n    <input>\n        <port id=\"1\">\n            <dim>1</dim>  <!-- value is [0] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\"/>    <!-- output with dynamic rank -->\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Configuring PaddlePaddle Frontend Module with CMake\nDESCRIPTION: This CMake snippet uses the `frontend_module` function to configure the `py_paddle_frontend` module. It specifies the module name, the framework (`paddle`), and the component name which includes a python version. The function is assumed to handle setting up build rules, dependencies, and installation procedures for the frontend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/frontend/paddle/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(${pyopenvino_SOURCE_DIR}/frontend/frontend_module.cmake)\nfrontend_module(py_paddle_frontend paddle ${OV_CPACK_COMP_PYTHON_OPENVINO}_${pyversion})\n```\n\n----------------------------------------\n\nTITLE: Environment Setup (Windows)\nDESCRIPTION: This snippet sets up the environment for building OpenVINO on Windows using Visual Studio. It calls the `vcvar64.bat` script to configure the environment and sets the `OPENVINO_HOME` variable.  It then creates and navigates to a build directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\ncall C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvar64.bat\nset OPENVINO_HOME=D:\\work_path\\openvino\ncd %OPENVINO_HOME%\nmd build_cc\ncd build_cc\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This snippet sets the target name for the library to 'openvino_util'. This name is used throughout the CMake script to refer to the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME openvino_util)\n```\n\n----------------------------------------\n\nTITLE: 3-Clause BSD License\nDESCRIPTION: This code snippet represents the 3-Clause BSD License applied to Xbyak_riscv. It specifies the terms for redistribution, use, and modification of the software, including retaining copyright notices and disclaimers of warranty.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/third-party-programs.txt#_snippet_3\n\nLANGUAGE: Text\nCODE:\n```\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\nNeither the name of the copyright owner nor the names of its contributors may\nbe used to endorse or promote products derived from this software without\nspecific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF\nTHE POSSIBILITY OF SUCH DAMAGE.\n```\n\n----------------------------------------\n\nTITLE: Complex Conditional Job Execution with Boolean Operators (YAML)\nDESCRIPTION: This YAML snippet shows how to use boolean operators in the 'if' condition to create complex conditions for job or step execution, allowing for more flexible control based on multiple component changes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_12\n\nLANGUAGE: YAML\nCODE:\n```\n# The below condition will force the job/step to run when either COMPONENT_1 or COMPONENT_2 was changed\nif: fromJSON(needs.smart_ci.outputs.affected_components).COMPONENT_1.test ||\n    fromJSON(needs.smart_ci.outputs.affected_components).COMPONENT_2.test\n```\n\n----------------------------------------\n\nTITLE: Requiring C++17 for ONNX Compilation\nDESCRIPTION: This snippet sets the compile features for the `onnx_proto` target to require C++17. This is necessary because ONNX version 1.13.1 and later requires C++17 when compiling on Windows, and from ONNX 1.16.0 on Linux.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/onnx/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_features(onnx_proto PUBLIC cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Querying Available Devices using hello_query_device in Shell\nDESCRIPTION: This shell command executes the `hello_query_device` application to list the available devices on the system. The output is truncated to show only the device names, such as CPU and GPU instances. This is a tool from the OpenVINO samples.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./hello_query_device\nAvailable devices:\n    Device: CPU\n...\n    Device: GPU.0\n...\n    Device: GPU.1\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: Specifies the include directories for the `mock_engine` target.  The current source directory is added as a public include directory, meaning that other targets that link against this library will also have access to these headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/unit_test_utils/mocks/mock_engine/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC ${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Installing Test Data Directory\nDESCRIPTION: This CMake snippet installs the `layer_tests_summary` directory to the `tests/functional_test_utils` destination. The component is set to `tests` and it is excluded from the `ALL` target, implying it's only installed for specific test builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/functional_test_utils/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(DIRECTORY \"${CMAKE_CURRENT_SOURCE_DIR}/layer_tests_summary\"\n        DESTINATION tests/functional_test_utils\n        COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO Samples with Make (macOS)\nDESCRIPTION: This command uses the `make` utility to build the OpenVINO samples based on the generated Makefiles.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\nmake\n```\n\n----------------------------------------\n\nTITLE: Filter Blob Dumps by Node Name (Regex)\nDESCRIPTION: Filters the blobs to be dumped based on the name of the node matching a specified regular expression for OpenVINO CPU executions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_NODE_NAME=<regex> binary ...\n```\n\n----------------------------------------\n\nTITLE: Defining Compile Definitions for OpenVINO Core Object Library\nDESCRIPTION: This snippet defines compile definitions for the `openvino_core_obj` target. These definitions are used to control the behavior of the compiled code. `IMPLEMENT_OPENVINO_API` likely enables the implementation of the OpenVINO API, while `XBYAK_NO_OP_NAMES` and `XBYAK64` are likely related to the Xbyak library used for code generation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(openvino_core_obj PRIVATE IMPLEMENT_OPENVINO_API XBYAK_NO_OP_NAMES XBYAK64)\n```\n\n----------------------------------------\n\nTITLE: RandomUniform Example (i32)\nDESCRIPTION: Example of RandomUniform output with initial_seed = 80, output_type = i32, and alignment = PYTORCH. The input shape is [2, 3], minval = 50, and maxval = 100.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\ninput_shape = [ 2, 3 ]\nminval = 50\nmaxval = 100\noutput  = [[89 73 68] \\\n           [95 78 61]]\n```\n\n----------------------------------------\n\nTITLE: Add timetests_helper Subdirectory CMake\nDESCRIPTION: Adds the 'timetests_helper' subdirectory to the current CMake build process. This allows the CMakeLists.txt file within 'timetests_helper' to be incorporated into the overall build configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(timetests_helper)\n```\n\n----------------------------------------\n\nTITLE: Getting Tensor Size in TypeScript\nDESCRIPTION: Shows the `getSize` method for obtaining the total number of elements in the tensor. This is equivalent to the product of all dimensions in the tensor's shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Tensor.rst#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\ngetSize(): number[]\n```\n\n----------------------------------------\n\nTITLE: Running application with ITT collector (Python)\nDESCRIPTION: This snippet shows how to run an application (e.g., benchmark_app) under the ITT collector to collect code usage statistics. The python script 'sea_runtool.py' is used to interface with the ITT collector. The command requires that `OPENVINO_LIBRARY_DIR` and `MY_MODEL` are set. This step collects statistics in CSV format for the specified model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/conditional_compilation.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython ../thirdparty/itt_collector/runtool/sea_runtool.py --bindir ${OPENVINO_LIBRARY_DIR} \\ \\\n-o ${MY_MODEL_RESULT} ! ./benchmark_app -niter 1 -nireq 1 -m ${MY_MODEL}.xml\n...\n```\n\n----------------------------------------\n\nTITLE: Overriding Automatic Batching with benchmark_app and explicit batch size (sh)\nDESCRIPTION: This code snippet shows how to override the automatically deduced batch size when using automatic batching with the benchmark_app tool.  It utilizes the 'BATCH:GPU(16)' device specification and a model path. It notes the need for quotes around complex device names in some shells.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\n$benchmark_app -hint none -d BATCH:GPU(16) -m 'path to your favorite model'\n```\n\n----------------------------------------\n\nTITLE: Generating Primitive Database (CMake)\nDESCRIPTION: This snippet uses `add_custom_command` to generate the primitive database using a Python script. The command creates a directory and executes the Python script with specific arguments for output paths and kernel files. The output files are `ks_primitive_db.inc` and `ks_primitive_db_batch_headers.inc`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/kernel_selector/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(OUTPUT \"${CODEGEN_CACHE_DIR}/${PRIM_DB}\"\n  COMMAND \"${CMAKE_COMMAND}\" -E make_directory \"${CODEGEN_CACHE_DIR}\"\n  COMMAND \"${Python3_EXECUTABLE}\" \"${CODEGEN_SCRIPT}\" -out_path \"${CODEGEN_CACHE_DIR}\"\n                                                      -out_file_name_prim_db \"${PRIM_DB}\"\n                                                      -out_file_name_batch_headers \"${PRIM_DB_BATCH_HEADERS}\"\n                                                      -kernels \"${CMAKE_CURRENT_SOURCE_DIR}/cl_kernels/\"\n  DEPENDS ${KERNELS} \"${CODEGEN_SCRIPT}\"\n  COMMENT \"Generating ${CODEGEN_CACHE_DIR}/${PRIM_DB} ...\"\n)\n```\n\n----------------------------------------\n\nTITLE: Float16 Value Calculation (C++)\nDESCRIPTION: This code snippet calculates the float16 value from a uint32 random value. It truncates the upper 16 bits, shifts the exponent, and performs a bitwise AND operation to get the final float16 representation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nx_uint16 = x // Truncate the upper 16 bits.\nval = ((exponent << 10) | x_uint16 & 0x3ffu) - 1.0,\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: Sets the public include directories for the target, allowing other targets that link against this library to find its header files. The $<BUILD_INTERFACE:path> generator expression ensures that the include directory is only added when building the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/backend/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}\n    PUBLIC\n        $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\n)\n```\n\n----------------------------------------\n\nTITLE: Excluding MLAS Paths\nDESCRIPTION: This snippet excludes MLAS-related paths if `ENABLE_MLAS_FOR_CPU` is not enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_24\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT ENABLE_MLAS_FOR_CPU)\n    list(APPEND EXCLUDE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/nodes/executors/mlas/*)\n    list(APPEND EXCLUDE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/mlas/*)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories with CMake\nDESCRIPTION: This code snippet adds the `shared_test_classes` and `plugin` directories as subdirectories in the CMake build process. The `add_subdirectory` command tells CMake to process the `CMakeLists.txt` file in each of these directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(shared_test_classes)\nadd_subdirectory(plugin)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: This command adds the specified subdirectory to the build. The 'frontend/shared' directory should contain its own CMakeLists.txt file with build instructions. This makes the contents of that subdirectory available to the current project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(frontend/shared)\n```\n\n----------------------------------------\n\nTITLE: Adding Version File (Visual Studio)\nDESCRIPTION: Conditionally adds a Visual Studio version file using `ov_add_vs_version_file` if the command is available. This file contains version information for the library. The `FILEDESCRIPTION` is set to 'Test Utils Python library'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(COMMAND ov_add_vs_version_file)\n    ov_add_vs_version_file(NAME ${TARGET_NAME}\n            FILEDESCRIPTION \"Test Utils Python library\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Sample Output (Python)\nDESCRIPTION: This is sample output from the python model creation sample.  It shows the steps performed, and the output of inference. Requires the model to be built and loaded successfully.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/model-creation.rst#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n[ INFO ] Creating OpenVINO Runtime Core\n[ INFO ] Loading the model using openvino with weights from lenet.bin\n[ INFO ] Loading the model to the plugin\n[ INFO ] Starting inference in synchronous mode\n[ INFO ] Top 1 results:\n[ INFO ] Image 0\n[ INFO ]\n[ INFO ] classid probability label\n[ INFO ] -------------------------\n[ INFO ] 0       1.0000000   0\n[ INFO ]\n[ INFO ] Image 1\n[ INFO ]\n[ INFO ] classid probability label\n[ INFO ] -------------------------\n[ INFO ] 1       1.0000000   1\n[ INFO ]\n[ INFO ] Image 2\n[ INFO ]\n[ INFO ] classid probability label\n[ INFO ] -------------------------\n[ INFO ] 2       1.0000000   2\n[ INFO ]\n[ INFO ] Image 3\n[ INFO ]\n[ INFO ] classid probability label\n[ INFO ] -------------------------\n[ INFO ] 3       1.0000000   3\n[ INFO ]\n[ INFO ] Image 4\n[ INFO ]\n[ INFO ] classid probability label\n[ INFO ] -------------------------\n[ INFO ] 4       1.0000000   4\n[ INFO ]\n[ INFO ] Image 5\n[ INFO ]\n[ INFO ] classid probability label\n[ INFO ] -------------------------\n[ INFO ] 5       1.0000000   5\n[ INFO ]\n[ INFO ] Image 6\n[ INFO ]\n[ INFO ] classid probability label\n[ INFO ] -------------------------\n[ INFO ] 6       1.0000000   6\n[ INFO ]\n[ INFO ] Image 7\n[ INFO ]\n[ INFO ] classid probability label\n[ INFO ] -------------------------\n[ INFO ] 7       1.0000000   7\n[ INFO ]\n[ INFO ] Image 8\n[ INFO ]\n[ INFO ] classid probability label\n[ INFO ] -------------------------\n[ INFO ] 8       1.0000000   8\n[ INFO ]\n[ INFO ] Image 9\n[ INFO ]\n[ INFO ] classid probability label\n[ INFO ] -------------------------\n[ INFO ] 9       1.0000000   9\n[ INFO ]\n[ INFO ] This sample is an API example, for any performance measurements please use the dedicated benchmark_app tool\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: Specifies the include directories for the target. The public include directory is set for build interface, and the private include directory is set to the source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/format_reader/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\"\n                                          PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}/src\")\n```\n\n----------------------------------------\n\nTITLE: Setting MSVC Compiler Flags - CMake\nDESCRIPTION: This code block checks if the compiler is MSVC and adds a compiler flag `/wd5105` to suppress a specific warning related to macro definitions in NPU plugin tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (MSVC)\n    # NPU plugin tests don't have macros that have `defined`\n    # in their expansion\n    ov_add_compiler_flags(/wd5105)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Pull Request Description Template\nDESCRIPTION: This is a template to be used when creating a pull request for changes to the OpenVINO project. It includes a description of the purpose of the pull request and a detailed list of changes made.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n*Describe what is the purpose of this PR*\n\n### Details:\n- *Describe your changes.*\n- ...\n```\n\n----------------------------------------\n\nTITLE: Reshape Tensor with Multiple Dimension Handling in OpenVINO XML\nDESCRIPTION: This XML snippet demonstrates reshaping a tensor while preserving the first two dimensions, fixing the third, and calculating the fourth dimension. The 'special_zero' attribute is set to 'true', which enables the copying of dimensions where a zero value is provided in the 'shape' input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/shape/reshape-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Reshape\" ...>\n    <data special_zero=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>2</dim>\n            <dim>3</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>   <!--The tensor contains 4 elements: 0, 0, 1, -1 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>2</dim>\n            <dim>2</dim>\n            <dim>1</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Suppressing Override Suggestions in CMake\nDESCRIPTION: This snippet conditionally adds the `-Wno-suggest-override` flag to the C++ compiler flags if `SUGGEST_OVERRIDE_SUPPORTED` is enabled. This suppresses warnings related to suggesting 'override' keyword.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/tests/unit/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(SUGGEST_OVERRIDE_SUPPORTED)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-suggest-override\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for MSVC\nDESCRIPTION: This snippet configures compiler flags specifically for MSVC to suppress warnings related to data conversion, bit shifting, and unary minus operators, which might arise from oneDNN headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    # C4267, 4244 issues from oneDNN headers conversion from 'XXX' to 'YYY', possible loss of data\n    ov_add_compiler_flags(/wd4018)\n    ov_add_compiler_flags(/wd4267)\n    ov_add_compiler_flags(/wd4244)\n    # mkldnn headers: '<<': result of 32-bit shift implicitly converted to 64 bits\n    ov_add_compiler_flags(/wd4334)\n    # oneDNN arm64: unary minus operator applied to unsigned type, result still unsigned\n    ov_add_compiler_flags(/wd4146)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Set Input Tensor by Tensor Object TypeScript\nDESCRIPTION: Sets the input tensor for the InferRequest when the model has a single input.  The tensor's type and shape must match the model's input requirements. An exception is thrown if the model has multiple inputs. This method returns void.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nsetInputTensor(tensor): void\n```\n\n----------------------------------------\n\nTITLE: Adding Compile Definitions\nDESCRIPTION: This snippet defines compile-time definitions related to shared library prefixes and suffixes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME}\n    PRIVATE\n        SHARED_LIB_PREFIX=\"${CMAKE_SHARED_LIBRARY_PREFIX}\"\n        SHARED_LIB_SUFFIX=\"${OV_BUILD_POSTFIX}${CMAKE_SHARED_LIBRARY_SUFFIX}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Defining string data type in TypeScript\nDESCRIPTION: Defines the string data type as a string in TypeScript. This represents a sequence of characters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/element.rst#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nstring: string\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO with CMake for Linux packages\nDESCRIPTION: This snippet configures the OpenVINO build using CMake for a RISC-V target using installed Linux packages. It specifies the build type, installation prefix, and the toolchain file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ncmake .. \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DCMAKE_INSTALL_PREFIX=<openvino_install_path> \\\n  -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/riscv64.linux.toolchain.cmake\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This CMake snippet sets the target name for the library to be built.  The `TARGET_NAME` variable is used throughout the CMake configuration to refer to this library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/compiler_adapter/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME \"openvino_npu_driver_compiler_adapter\")\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files in CMake\nDESCRIPTION: This line defines the source files that will be compiled into the library.  It specifies the `.cpp` files that constitute the extension's source code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/template_extension/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(SRC identity.cpp ov_extension.cpp)\n```\n\n----------------------------------------\n\nTITLE: Removing File CMake\nDESCRIPTION: Removes the `VERSION` file from the build directory. This is likely done to prevent conflicts with other version information or to ensure a clean build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/level_zero/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nfile(REMOVE \"${CMAKE_BINARY_DIR}/VERSION\")\n```\n\n----------------------------------------\n\nTITLE: Creating OpenVINO C++ Integration Example (Python)\nDESCRIPTION: This snippet defines a CMake configuration for building a C++ OpenVINO integration example that interacts with Python. It finds the OpenVINO package using a Python script, creates an executable named `ov_integration_snippet_py`, and links it with the `openvino::runtime` library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/snippets/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME_PY \"ov_integration_snippet_py\")\n# [cmake:integration_example_cpp_py]\ncmake_minimum_required(VERSION 3.10)\nset(CMAKE_CXX_STANDARD 17)\n\nif(NOT CMAKE_CROSSCOMPILING)\n    find_package(Python3 QUIET COMPONENTS Interpreter)\n    if(Python3_Interpreter_FOUND)\n        execute_process(\n            COMMAND ${Python3_EXECUTABLE} -c \"from openvino.utils import get_cmake_path; print(get_cmake_path(), end='')\"\n            OUTPUT_VARIABLE OpenVINO_DIR_PY\n            ERROR_QUIET)\n    endif()\nendif()\n\nfind_package(OpenVINO REQUIRED PATHS \"${OpenVINO_DIR_PY}\")\n\nadd_executable(${TARGET_NAME_PY} src/main.cpp)\n\ntarget_link_libraries(${TARGET_NAME_PY} PRIVATE openvino::runtime)\n\n# [cmake:integration_example_cpp_py]\n```\n\n----------------------------------------\n\nTITLE: Add Pybind11 Module\nDESCRIPTION: This line uses the `pybind11_add_module` function to create the Pybind11 module. It specifies the module name, type (MODULE), and the source file. NO_EXTRAS indicates that no additional compiler flags are needed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/pyngraph_fe_mock_api/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\npybind11_add_module(${PYBIND_FE_NAME} MODULE NO_EXTRAS ${PYBIND_FE_SRC})\n```\n\n----------------------------------------\n\nTITLE: PartialShape Property Declaration\nDESCRIPTION: Declares the PartialShape property within the NodeAddon interface, representing the PartialShapeConstructor. This property enables the creation of shapes, defining tensor dimensions which can be partially defined.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/addon.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nPartialShape: PartialShapeConstructor\n```\n\n----------------------------------------\n\nTITLE: Slice: Slicing with Out-of-Bounds Indices in OpenVINO XML\nDESCRIPTION: This example showcases the behavior of the Slice operation when the `start` and `stop` indices are outside the bounds of the input tensor. OpenVINO silently clamps these indices to the valid range of the dimension being sliced.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-8.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Slice\" ...>\n       <input>\n           <port id=\"0\">       <!-- data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -->\n             <dim>10</dim>\n           </port>\n           <port id=\"1\">       <!-- start: [-100] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"2\">       <!-- stop: [100] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"3\">       <!-- step: [1] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"4\">       <!-- axes: [0] -->\n             <dim>1</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">       <!-- output: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -->\n               <dim>10</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Install Sphinx Theme\nDESCRIPTION: Installs the custom OpenVINO Sphinx theme.  This theme is located in the `docs/openvino_sphinx_theme` directory. It uses pip within the activated virtual environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/documentation_build_instructions.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n(env) $ python -m pip install docs/openvino_sphinx_theme\n```\n\n----------------------------------------\n\nTITLE: CMake Project Configuration\nDESCRIPTION: This CMake snippet sets the minimum required CMake version to 3.13, declares the project name as 'model_hub_tests', and installs the contents of the current source directory into the 'tests' directory. The installed files are designated as a component and excluded from the default 'all' build target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/model_hub_tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\n\nproject(model_hub_tests)\n\ninstall(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: ReduceLogicalAnd XML Example with keep_dims=false\nDESCRIPTION: XML configuration showcasing the ReduceLogicalAnd operation with 'keep_dims' set to 'false'. This removes the reduced axes from the output tensor. The provided example performs a reduction along axes 2 and 3, resulting in an output tensor with dimensions 6x12.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-logical-and-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceLogicalAnd\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Compiling Model Synchronously (Model Object)\nDESCRIPTION: Compiles a model synchronously from a Model object for a specified device. This is a synchronous version of the asynchronous compileModel method. The config parameter allows specifying properties relevant only for this load operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\ncompileModelSync(model, deviceName, config?): CompiledModel\n```\n\n----------------------------------------\n\nTITLE: Configuring Protobuf Include Directories and Target Properties\nDESCRIPTION: This snippet configures include directories and target properties for Protobuf. It sets the `Protobuf_INCLUDE_DIRS` variable and configures the `libprotobuf-lite` target to hide warnings and set interprocedural optimization settings. Also sets CXX and C visibility presets to default.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/protobuf/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n# to hide libprotobuf warnings\ntarget_include_directories(libprotobuf-lite SYSTEM PRIVATE \"${Protobuf_INCLUDE_DIRS}\")\n\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG OR OV_COMPILER_IS_INTEL_LLVM)\n    if(protobuf_BUILD_PROTOC_BINARIES)\n        list(APPEND _protoc_libs protoc libprotoc libprotobuf)\n        set_target_properties(${_protoc_libs} PROPERTIES\n            CXX_VISIBILITY_PRESET default\n            C_VISIBILITY_PRESET default\n            VISIBILITY_INLINES_HIDDEN OFF\n            INTERPROCEDURAL_OPTIMIZATION_RELEASE OFF)\n    endif()\n    ov_disable_all_warnings(${_protoc_libs} libprotobuf-lite)\n    set_target_properties(libprotobuf-lite PROPERTIES\n        INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Export Compiled Model with OpenVINO in C++\nDESCRIPTION: This C++ snippet shows how to export a compiled model for inference on a specific device using OpenVINO.  This replaces the functionality of the discontinued `Compile tool`. It relies on the OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/legacy-features.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nvoid exportCompiledModel(const std::string& modelPath, const std::string& device, const std::string& outputPath) {\n    ov::Core core;\n    // read the model\n    std::shared_ptr<ov::Model> model = core.read_model(modelPath);\n    // compile the model for the specified device\n    ov::CompiledModel compiledModel = core.compile_model(model, device);\n    // export the compiled model\n    compiledModel.export_model(outputPath);\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: This snippet sets the public and private include directories for the target. The public interface directory is the 'include' directory within the root directory. The private directory is the 'src' directory within the root directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common_translators/src/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}\n    PUBLIC $<BUILD_INTERFACE:${root_dir}/include>\n    PRIVATE ${root_dir}/src)\n```\n\n----------------------------------------\n\nTITLE: Run Time Test\nDESCRIPTION: This snippet runs a single time test executable. It executes the `timetest_infer` executable, passing in a model file (`model.xml`) and specifying the CPU as the target device. The path to the executable may need to be adjusted based on the build location.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/run_timetest.py ../../bin/intel64/Release/timetest_infer -m model.xml -d CPU\n```\n\n----------------------------------------\n\nTITLE: Get Inference Precision - C++\nDESCRIPTION: This C++ snippet demonstrates how to retrieve the inference precision of a compiled model using `ov::CompiledModel::get_property` and `ov::hint::inference_precision`. It retrieves the compiled model's inference precision to check whether it is set to f16, bf16 or f32. Dependency: OpenVINO C++ API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/cpu-device.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n// [part1]\nauto model = core.read_model(\"model.xml\");\nauto compiled_model = core.compile_model(model, \"CPU\");\n\nauto inference_precision = compiled_model.get_property(ov::hint::inference_precision);\nstd::cout << \"Inference precision: \" << inference_precision.as<ov::element::Type>() << \"\\n\";\n// [part1]\n```\n\n----------------------------------------\n\nTITLE: Add OpenVINO APT repository (Ubuntu 24)\nDESCRIPTION: Adds the OpenVINO APT repository for Ubuntu 24 to the system's software sources. This allows the system to find and install OpenVINO packages using the APT package manager.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\necho \"deb https://apt.repos.intel.com/openvino ubuntu24 main\" | sudo tee /etc/apt/sources.list.d/intel-openvino.list\n```\n\n----------------------------------------\n\nTITLE: Conditional flatc Build\nDESCRIPTION: This CMake snippet checks if cross-compilation is enabled, or if building for Apple architecture via Rosetta. If true, it disables building the `flatc` compiler directly. Otherwise, it enables the `flatc` compiler build. The variables `HOST_AARCH64`, `X86_64`, and `AARCH64` are implicitly defined based on the build environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/flatbuffers/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CROSSCOMPILING OR (APPLE AND (HOST_X86_64 AND AARCH64)) )\n    set(FLATBUFFERS_BUILD_FLATC OFF CACHE BOOL \"\" FORCE)\nelse()\n    set(FLATBUFFERS_BUILD_FLATC ON CACHE BOOL \"\" FORCE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Excluding Source Paths Based on Architecture (x86_64)\nDESCRIPTION: This block excludes source paths containing x64-specific code if the target architecture is not x86_64.  This avoids compilation errors on non-x64 platforms.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT X86_64)\n    list(APPEND EXCLUDED_SOURCE_PATHS_FOR_UNIT_TEST\n      ${CMAKE_CURRENT_SOURCE_DIR}/jit_kernel_test.cpp\n      ${CMAKE_CURRENT_SOURCE_DIR}/registers_pool.cpp\n      ${CMAKE_CURRENT_SOURCE_DIR}/transformations/x64\n      ${CMAKE_CURRENT_SOURCE_DIR}/snippets_transformations/x64\n      ${CMAKE_CURRENT_SOURCE_DIR}/nodes/eltwise_node_test.cpp\n      ${CMAKE_CURRENT_SOURCE_DIR}/brgemm_executor_test.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: This snippet sets the include directories for the zlib library, allowing other parts of the project to find the zlib header files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/zlib/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC \"${CMAKE_CURRENT_SOURCE_DIR}/zlib\")\n```\n\n----------------------------------------\n\nTITLE: Unique Operation with Non-Default Precision - XML Configuration\nDESCRIPTION: This XML snippet demonstrates the Unique operation without an axis input and non-default outputs precision.  The operation finds unique elements in the flattened input tensor. The `index_element_type` and `count_element_type` are set to i32.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/unique-10.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Unique\" ... >\n    <data sorted=\"false\" index_element_type=\"i32\" count_element_type=\"i32\"/>\n    <input>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"FP32\">\n            <dim>-1</dim>\n        </port>\n        <port id=\"2\" precision=\"I32\">\n            <dim>-1</dim>\n        </port>\n        <port id=\"3\" precision=\"I32\">\n            <dim>9</dim>\n        </port>\n        <port id=\"4\" precision=\"I32\">\n            <dim>-1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating a key store and certificate\nDESCRIPTION: Creates a keystore and generates a self-signed certificate using the ovsatool.  It generates a CSR, and then creates a self-signed certificate for demonstration purposes. Production environments require CA-signed certificates.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_39\n\nLANGUAGE: sh\nCODE:\n```\ncd $OVSA_DEV_ARTEFACTS\n/opt/ovsa/bin/ovsatool keygen -storekey -t ECDSA -n Intel -k isv_keystore -r  isv_keystore.csr -e \"/C=IN/CN=localhost\"\n```\n\n----------------------------------------\n\nTITLE: Adding Version File CMake\nDESCRIPTION: This snippet adds a version file for Visual Studio. The `ov_add_vs_version_file` macro creates a version file with the specified file description.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/src/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_vs_version_file(NAME ${TARGET_NAME}\n    FILEDESCRIPTION \"OpenVINO C API Core Runtime library\")\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation Definition for Intel CPU with CMake\nDESCRIPTION: This CMake snippet conditionally sets a compile definition for a specific source file based on the `ENABLE_INTEL_CPU` variable. This allows specific code paths to be enabled when building for Intel CPU, configuring compiler settings for targeted hardware.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/tests/functional/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_INTEL_CPU)\n    set_source_files_properties(\n        \"${CMAKE_CURRENT_SOURCE_DIR}/shared_tests_instances/behavior/executable_network/get_metric.cpp\"\n        PROPERTIES COMPILE_DEFINITIONS ENABLE_INTEL_CPU=1)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Gather Operation Example 5\nDESCRIPTION: Demonstrates the Gather operation with a negative batch_dims value. The normalized batch_dims value is calculated as indices.rank + batch_dims.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-8.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = -1  <-- normalized value will be indices.rank + batch_dims = 2 - 1 = 1\naxis = 1\n\nindices = [[0, 0, 4], <-- this is applied to the first batch\n           [4, 0, 0]]  <-- this is applied to the second batch\nindices_shape = (2, 3)\n\ndata    = [[1, 2, 3, 4, 5],  <-- the first batch\n           [6, 7, 8, 9, 10]]  <-- the second batch\ndata_shape = (2, 5)\n\noutput  = [[ 1, 1, 5],\n           [10, 6, 6]]\noutput_shape = (2, 3)\n```\n\n----------------------------------------\n\nTITLE: Restart Guest VM\nDESCRIPTION: This command restarts the QEMU system emulator after Ubuntu is installed, booting from the virtual disk image. It sets the memory, CPU, disk, and network settings, using a TAP network device and VNC for console access. The CDROM option is removed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\nsudo qemu-system-x86_64 -m 8192 -enable-kvm \\\n-cpu host \\\n-drive if=virtio,file=<path-to-disk-image>/ovsa_isv_dev_vm_disk.qcow2,cache=none \\\n-device e1000,netdev=hostnet1,mac=52:54:00:d1:66:5f \\\n-netdev tap,id=hostnet1,script=<path-to-scripts>/virbr0-qemu-ifup,downscript=<path-to-scripts>/virbr0-qemu-ifdown \\\n-vnc :1\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO™ with specific framework using PIP\nDESCRIPTION: This snippet demonstrates how to install OpenVINO with specific framework dependencies (e.g., TensorFlow2) using pip and a mirror source. It is helpful when a specific framework integration is required during the installation process. It also includes adding a trusted host parameter if the URL is http.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/troubleshooting-install-config.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install openvino-dev[tensorflow2] -i https://mirrors.aliyun.com/pypi/simple/\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Sum Reduction - C++\nDESCRIPTION: This code snippet demonstrates the 'sum' reduction mode for the ScatterElementsUpdate operation. The corresponding elements from the updates tensor are added to the elements in the data tensor at the specified indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\noutput[indices[i]] += updates[i], axis = 0\n```\n\n----------------------------------------\n\nTITLE: Tensor Property Declaration\nDESCRIPTION: Declares the Tensor property within the NodeAddon interface, which is a constructor for Tensor objects.  Tensor objects are used to hold input and output data for inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/addon.rst#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nTensor: TensorConstructor\n```\n\n----------------------------------------\n\nTITLE: Adding compiler flags\nDESCRIPTION: Adds a compiler flag to suppress missing declarations warnings, specifically for GCC and Clang compilers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG)\n    ov_add_compiler_flags(-Wno-missing-declarations)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Add Subdirectory\nDESCRIPTION: Adds a subdirectory named 'standalone_build' and sets up a dependency for the main test target on the standalone build target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/tests/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nget_target_property(TENSORFLOW_LITE_FRONTEND_SRC_DIR openvino_tensorflow_lite_frontend SOURCE_DIR)\n\nadd_subdirectory(standalone_build)\nadd_dependencies(${TARGET_NAME} tensorflow_lite_fe_standalone_build_test)\n```\n\n----------------------------------------\n\nTITLE: Using Reusable Workflow: Python Unit Tests in YAML\nDESCRIPTION: This YAML snippet demonstrates how to use a reusable workflow named `job_python_unit_tests.yml` as a job within a validation workflow like `ubuntu_22.yml`.  It defines the job's name, dependencies (needs), the path to the reusable workflow (uses), and input parameters (with), such as the runner and container configuration, as well as the affected components. The container parameter is represented as a JSON string.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/reusable_workflows.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  Python_Unit_Tests:\n    name: Python unit tests\n    needs: [ Build, Smart_CI ]\n    uses: ./.github/workflows/job_python_unit_tests.yml\n    with:\n      runner: 'aks-linux-4-cores-16gb'\n      container: '{\"image\": \"openvinogithubactions.azurecr.io/dockerhub/ubuntu:20.04\", \"volumes\": [\"/mount:/mount\"]}'\n      affected-components: ${{ needs.smart_ci.outputs.affected_components }}\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Target in CMake\nDESCRIPTION: This snippet creates an executable target named `${TARGET_NAME}` using the source and header files found in the `SRC` and `HDR` variables. The `add_executable` command tells CMake to build an executable from the listed source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/memleaks_tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(${TARGET_NAME} ${HDR} ${SRC})\n```\n\n----------------------------------------\n\nTITLE: Defining PrePostProcessorConstructor Interface in TypeScript\nDESCRIPTION: Defines the PrePostProcessorConstructor interface with a single constructor method. This constructor takes a Model object as a parameter and returns a PrePostProcessor object. It is part of the OpenVINO toolkit's TypeScript bindings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PrePostProcessorConstructor.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface PrePostProcessorConstructor {\n    new PrePostProcessorConstructor(model): PrePostProcessor;\n}\n```\n\n----------------------------------------\n\nTITLE: Clone OpenVINO Notebooks Repository\nDESCRIPTION: This command clones the OpenVINO notebooks repository from GitHub, using the `--depth=1` option to download only the latest commit, reducing the download size. It then navigates into the cloned directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\ngit clone --depth=1 https://github.com/openvinotoolkit/openvino_notebooks.git\ncd openvino_notebooks\n```\n\n----------------------------------------\n\nTITLE: IDFT Layer XML Configuration (4D input, with signal_size)\nDESCRIPTION: This XML snippet provides the configuration for an IDFT layer with a 4D input tensor and includes the signal_size input. The snippet details the dimensions of input ports for data, axes, signal size, and the output port, demonstrating how to specify signal size for the transformation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/idft-7.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>\t<!-- [1, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>2</dim>\t<!-- [512, 100] -->\n        </port>\n    <output>\n        <port id=\"3\">\n            <dim>1</dim>\n            <dim>512</dim>\n            <dim>100</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Convert Layer Configuration Example C++\nDESCRIPTION: This C++ code snippet illustrates the configuration of a Convert layer in OpenVINO. It defines the input and output ports with their dimensions and specifies the destination data type for the conversion.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/type/convert-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"Convert\">\n    <data destination_type=\"f32\"/>\n    <input>\n        <port id=\"0\">        <!-- type: i32 -->\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">        <!-- result type: f32 -->\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: RDFT Layer XML Configuration (With signal_size, 4D Input, unsorted axes, example 2)\nDESCRIPTION: Configures an RDFT layer in XML with a signal_size input and unsorted axes, processing a 4D input tensor. This example shows another configuration with a -1 in the signal_size and different axis arrangements.  Illustrates RDFT handling of various input tensor shapes and axis configurations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/rdft-9.rst#_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"RDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>580</dim>\n            <dim>320</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim> <!-- axes input contains  [3, 0, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim> <!-- signal_size input contains [258, -1, 2056] -->\n        </port>\n    <output>\n        <port id=\"3\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>1029</dim>\n            <dim>258</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for CMake Target\nDESCRIPTION: This CMake command adds the current source directory to the include directories for the 'gtest_main_manifest' target. The INTERFACE keyword ensures that dependent targets also have this directory in their include path. This allows the source files within the current directory to be included in dependent projects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/gtest_main_manifest/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} INTERFACE ${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Enabling SHL Integration\nDESCRIPTION: Conditionally enables the SHL library for the CPU plugin by linking it to the target. The `ENABLE_SHL_FOR_CPU` variable controls whether SHL is enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_34\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_SHL_FOR_CPU)\n    target_link_libraries(${TARGET_NAME} PRIVATE shl)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: Adds the specified subdirectories to the build process. This allows CMake to process the CMakeLists.txt files located within these subdirectories, enabling the build of the corresponding components (C++ and C in this case).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(cpp)\nadd_subdirectory(c)\n```\n\n----------------------------------------\n\nTITLE: Conditional Compile Definitions (CMake)\nDESCRIPTION: Adds a compile definition if shared libraries are not being built. OPENVINO_STATIC_LIBRARY is defined publicly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME} PUBLIC OPENVINO_STATIC_LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Interface Library in CMake\nDESCRIPTION: This snippet creates an INTERFACE library named `${TARGET_NAME}`. This library is used to provide an interface for other libraries to link against, and handles include directories and link dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} INTERFACE)\n\ntarget_include_directories(${TARGET_NAME} INTERFACE\n    $<BUILD_INTERFACE:${PUBLIC_HEADERS_DIR}>)\n\ntarget_link_libraries(${TARGET_NAME} INTERFACE openvino::runtime)\n```\n\n----------------------------------------\n\nTITLE: Add OpenVINO APT repository (Ubuntu 22)\nDESCRIPTION: Adds the OpenVINO APT repository for Ubuntu 22 to the system's software sources. This allows the system to find and install OpenVINO packages using the APT package manager.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\necho \"deb https://apt.repos.intel.com/openvino ubuntu22 main\" | sudo tee /etc/apt/sources.list.d/intel-openvino.list\n```\n\n----------------------------------------\n\nTITLE: MaxPool with 4D input, dilated 2D kernel, explicit padding (sh)\nDESCRIPTION: Shows MaxPool operation on a 4D input tensor using a dilated 2D kernel with explicit padding and floor rounding.  It presents the input tensor and operation parameters, without specifying outputs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-8.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[1, 2, 3],\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO LPT Transformations Test Target with CMake\nDESCRIPTION: This CMake snippet defines a test target named 'ov_lp_transformations_tests' for testing low precision transformations within OpenVINO. It sets the source directory, links required libraries such as gtest, gmock, openvino::runtime::dev, common_test_utils, and ov_lpt_models, and specifies include directories. The defined labels help categorize the test within OpenVINO's test suite.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_lp_transformations_tests)\n\nov_add_test_target(\n    NAME ${TARGET_NAME}\n    ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n    DEPENDENCIES\n    LINK_LIBRARIES\n        gtest\n        gtest_main\n        openvino::runtime::dev\n        common_test_utils\n        ov_lpt_models\n        gmock\n    INCLUDES ${CMAKE_CURRENT_SOURCE_DIR}\n    LABELS\n        OV UNIT LP_TRANSFORMATIONS\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Dependencies in CMake\nDESCRIPTION: This snippet defines a list of dependencies required to build the 'protopipe' target, including Threads, gflags, yaml-cpp, OpenVINO runtime, and OpenCV gapi.  These dependencies will be linked to the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(DEPENDENCIES\n        Threads::Threads\n        gflags\n        yaml-cpp\n        openvino::runtime\n        opencv_gapi\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Source Groups in CMake\nDESCRIPTION: Organizes the source and header files into named groups within the Visual Studio project. This improves the project's structure and makes it easier to navigate in the IDE.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/unit_test_utils/mocks/mock_engine/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${LIBRARY_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This snippet sets the target name for the OpenVINO API conformance tests using the `set` command. This variable is then used in subsequent commands to reference the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/api_conformance_runner/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME ov_api_conformance_tests)\n```\n\n----------------------------------------\n\nTITLE: Defining Snippets LIBXSMM TPP\nDESCRIPTION: This snippet conditionally defines `SNIPPETS_LIBXSMM_TPP` and `LIBXSMM_DEFAULT_CONFIG` if `ENABLE_SNIPPETS_LIBXSMM_TPP` is enabled.  This allows the snippets to be built using libxsmm's TPP (Template Pattern Programming).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_SNIPPETS_LIBXSMM_TPP)\n    # Note: LIBXSMM_DEFAULT_CONFIG needed so libxsmm_config can be included without issues\n    add_definitions(-DSNIPPETS_LIBXSMM_TPP -DLIBXSMM_DEFAULT_CONFIG)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Accessing Input by index of CompiledModel (TypeScript)\nDESCRIPTION: This code shows the declaration of the `input` method within the `CompiledModel` interface to retrieve an input by index.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\ninput(index): Output\n```\n\n----------------------------------------\n\nTITLE: Define ov_callback struct in C\nDESCRIPTION: This struct defines a callback function and its arguments.  It's used for asynchronous operations within the OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_9\n\nLANGUAGE: C\nCODE:\n```\ntypedef struct {\n\n    void(OPENVINO_C_API_CALLBACK* callback_func)(void* args);\n\n    void* args;\n\n} ov_callback_t;\n```\n\n----------------------------------------\n\nTITLE: Roll Layer Configuration Example 1\nDESCRIPTION: This XML configuration demonstrates a Roll layer where 'shift' and 'axes' are 1D tensors. It defines the input and output ports with their dimensions, specifying the tensor shapes and data flow.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/roll-7.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Roll\">\n        <input>\n            <port id=\"0\">\n                <dim>3</dim>\n                <dim>10</dim>\n                <dim>100</dim>\n                <dim>200</dim>\n            </port>\n            <port id=\"1\">\n                <dim>2</dim>\n            </port>\n            <port id=\"2\">\n                <dim>2</dim> <!-- shifting along specified axes with the corresponding shift values -->\n            </port>\n        </input>\n        <output>\n            <port id=\"0\">\n                <dim>3</dim>\n                <dim>10</dim>\n                <dim>100</dim>\n                <dim>200</dim>\n            </port>\n        </output>\n    </layer>\n```\n\n----------------------------------------\n\nTITLE: Download OpenVINO Archive\nDESCRIPTION: Downloads the OpenVINO Runtime archive file for Windows from the specified URL using curl and saves it to the Downloads folder.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-windows.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncd <user_home>/Downloads\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino/packages/2025.1/windows/openvino_toolkit_windows_2025.1.0.18503.6fec06580ab_x86_64.zip --output openvino_2025.1.0.zip\n```\n\n----------------------------------------\n\nTITLE: CMake Project Configuration\nDESCRIPTION: This snippet sets the minimum required CMake version, defines the project name as 'layer_tests'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/layer_tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\n\nproject(layer_tests)\n```\n\n----------------------------------------\n\nTITLE: Define Source and Header Files for OpenVINO Proxy Plugin\nDESCRIPTION: This snippet defines the source and header files for the OpenVINO proxy plugin.  It uses `file(GLOB)` to find all `.cpp` files in the `src` directory and all `.hpp` files in the `dev_api` and `src` directories. These lists are then stored in `LIBRARY_SRC` and `LIBRARY_HEADERS` variables, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(PUBLIC_HEADERS_DIR ${CMAKE_CURRENT_SOURCE_DIR}/dev_api)\nfile(GLOB LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB LIBRARY_HEADERS ${PUBLIC_HEADERS_DIR}/*.hpp ${CMAKE_CURRENT_SOURCE_DIR}/src/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Target for Code Generation\nDESCRIPTION: This CMake snippet creates a custom target named `run_cm_codegen` that depends on the generated kernel sources in the include directory. This target can be used to explicitly trigger the code generation process. The `ALL` keyword ensures that this target is built by default.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/cm/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(run_cm_codegen ALL DEPENDS \"${CODEGEN_INCDIR}/${KERNEL_SOURCES}\")\n```\n\n----------------------------------------\n\nTITLE: GatherND XML Layer Definition (Example 2)\nDESCRIPTION: This XML layer definition shows another GatherND configuration, specifying the batch_dims attribute as 2. It also outlines the input and output port dimensions, showcasing a different shape configuration for the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-5.rst#_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"GatherND\">\n    <data batch_dims=2 />\n    <input>\n        <port id=\"0\">\n            <dim>30</dim>\n            <dim>2</dim>\n            <dim>100</dim>\n            <dim>35</dim>\n        </port>\n        <port id=\"1\">\n            <dim>30</dim>\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>60</dim>\n            <dim>3</dim>\n            <dim>35</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Accessing Output without parameters of CompiledModel (TypeScript)\nDESCRIPTION: This code shows the declaration of the `output` method within the `CompiledModel` interface to retrieve a single output without specifying the index or name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\noutput(): Output\n```\n\n----------------------------------------\n\nTITLE: Sqrt Layer Definition with Float Input in OpenVINO (XML)\nDESCRIPTION: This XML snippet demonstrates the use of the Sqrt layer in OpenVINO with floating-point input values. It shows the input and output ports with corresponding dimensions and example values before and after the square root operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/sqrt-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Sqrt\">\n    <input>\n        <port id=\"0\">\n            <dim>4</dim> <!-- float input values: [4.0, 7.0, 9.0, 10.0] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>4</dim> <!-- float output values: [2.0, 2.6457512, 3.0, 3.1622777] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Testing the Extended Class\nDESCRIPTION: This code demonstrates how to use the extended `MyTensor` class in Python. It initializes an instance of the class and calls the overridden `say_hello` method with different argument types to test its functionality. The output shows the behavior of the method with various inputs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nimport openvino.runtime as ov\n\na = ov.MyTensor([1,2,3])             # Notice that initializers are preserved from pybind class\na.say_hello()\n>>> Hello there!\n>>> 1.0\n>>> 2.0\n>>> 3.0\n\na.say_hello(\"I know how to make new features!\")\n>>> I know how to make new features! # String takes effect!\n>>> 1.0\n>>> 2.0\n>>> 3.0\n>>> 0                                # Correct return!\n\na.say_hello(8)\n>>> 8                                # String conversion takes effect!\n>>> 1.0\n>>> 2.0\n>>> 3.0\n>>> 'Converted int to string!'       # Correct return!\n\na.say_hello(2)\n>>> 3\n\na.say_hello([1,2,3])\n>>> Traceback (most recent call last):\n>>>   File \"<stdin>\", line 1, in <module>\n>>>   File \".../openvino/runtime/mymodule_ext.py\", line 20, in say_hello\n>>>     raise TypeError(\"Unsupported argument type!\")\n>>> TypeError: Unsupported argument type!\n```\n\n----------------------------------------\n\nTITLE: Run Emscripten Docker Image\nDESCRIPTION: This snippet pulls the Emscripten docker image and runs it, mounting the OpenVINO source code directory as a volume. This allows building OpenVINO inside a controlled environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_webassembly.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n$ docker pull emscripten/emsdk\n$ docker run -it --rm -v `pwd`:/openvino emscripten/emsdk bash\n```\n\n----------------------------------------\n\nTITLE: Importing OpenVINO Node.js Addon\nDESCRIPTION: This code snippet imports the openvino-node package and extracts the addon object, which provides access to the OpenVINO functionality. The addon is aliased to `ov` for easier use.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/nodejs_api.rst#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst { addon: ov } = require('openvino-node');\n```\n\n----------------------------------------\n\nTITLE: Add Subdirectory CMake\nDESCRIPTION: Adds the 'tests_shared_lib' subdirectory (located at '${OpenVINO_SOURCE_DIR}/tests/lib') to the current CMake build process. The subdirectory is renamed to 'tests_shared_lib'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(\"${OpenVINO_SOURCE_DIR}/tests/lib\" tests_shared_lib)\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library in CMake\nDESCRIPTION: This CMake snippet installs the static library to a specific location based on the `NPU_INTERNAL_COMPONENT` variable, typically used for internal components within the OpenVINO project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/compiler_adapter/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${NPU_INTERNAL_COMPONENT})\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties\nDESCRIPTION: This snippet sets the `EXPORT_NAME` property for the target, which affects how it's exported.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME interpreter_backend)\n```\n\n----------------------------------------\n\nTITLE: Adding Definitions\nDESCRIPTION: This snippet adds a preprocessor definition to indicate that the code is part of an OpenVINO component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_definitions(-DIN_OV_COMPONENT)\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Component Structure\nDESCRIPTION: This code snippet shows the structure of an OpenVINO component. It includes directories for CMake scripts, developer API, documentation, public API, source code, tests, and third-party dependencies, along with a main CMake script and README file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/index.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nov_component/           // Component folder\n    cmake/              // (optional) CMake scripts that are related only to this component\n    dev_api/            // (optional) Developer API is used when the component provides API for internal developers\n    docs/               // (optional) Contains detailed component documentation\n    include/            // (optional) Public component API\n    src/                // Sources of the component\n    tests/              // Tests for the component\n    thirdparty/         // (optional) Third-party dependencies\n    CMakeLists.txt      // Main CMake script\n    README.md           // (optional) Entry point for the developer documentation\n```\n\n----------------------------------------\n\nTITLE: Setting LTO Properties in CMake\nDESCRIPTION: Sets the INTERPROCEDURAL_OPTIMIZATION_RELEASE property for the library target, enabling Link Time Optimization (LTO) if the ENABLE_LTO variable is set.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(${TARGET_NAME}_obj PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: SoftPlus Layer XML Configuration in OpenVINO\nDESCRIPTION: This XML snippet demonstrates how to configure a SoftPlus layer in an OpenVINO model. It specifies the input and output ports with their respective dimensions.  This layer performs the SoftPlus activation function on the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/softplus-4.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"SoftPlus\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target\nDESCRIPTION: This snippet adds a custom target for running clang-format on the source files of the `${PROJECT_NAME}` target, allowing for code formatting as part of the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${PROJECT_NAME}_clang FOR_TARGETS ${PROJECT_NAME})\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files\nDESCRIPTION: This snippet uses the `file(GLOB_RECURSE)` command to recursively find all C++ source files in the 'src' directory and header files in the 'include' directory within the root directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common_translators/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC ${root_dir}/src/*.cpp)\nfile(GLOB_RECURSE LIBRARY_HEADERS ${root_dir}/include/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: ExperimentalDetectronTopKROIs XML Layer Example\nDESCRIPTION: This XML snippet demonstrates the configuration of an ExperimentalDetectronTopKROIs layer. It shows how to define the layer's type, version, data attributes (max_rois), input ports with dimensions, and the output port with precision and dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/experimental-detectron-top-krois-6.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ExperimentalDetectronTopKROIs\" version=\"opset6\">\n    <data max_rois=\"1000\"/>\n    <input>\n        <port id=\"0\">\n            <dim>5000</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>5000</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>1000</dim>\n            <dim>4</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Compile Definitions for Debug Caps CMake\nDESCRIPTION: This snippet adds a compile definition (NPU_PLUGIN_DEVELOPER_BUILD) if the ENABLE_NPU_DEBUG_CAPS flag is enabled. This allows for conditional compilation of debug-related code within the NPU plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_NPU_DEBUG_CAPS)\n    add_compile_definitions(NPU_PLUGIN_DEVELOPER_BUILD)\nendif()\n```\n\n----------------------------------------\n\nTITLE: SpaceToBatch XML Layer Definition\nDESCRIPTION: This XML snippet defines a SpaceToBatch layer in OpenVINO. It specifies the input and output ports, including their dimensions. The input ports represent the data, block_shape, pads_begin, and pads_end tensors. The output port represents the resulting tensor after the SpaceToBatch operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/space-to-batch-2.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"SpaceToBatch\" ...>\n    <input>\n        <port id=\"0\">       <!-- data -->\n            <dim>2</dim>    <!-- batch -->\n            <dim>6</dim>    <!-- spatial dimension 1 -->\n            <dim>10</dim>   <!-- spatial dimension 2 -->\n            <dim>3</dim>    <!-- spatial dimension 3 -->\n            <dim>3</dim>    <!-- spatial dimension 4 -->\n        </port>\n        <port id=\"1\">       <!-- block_shape value: [1, 2, 4, 3, 1] -->\n            <dim>5</dim>\n        </port>\n        <port id=\"2\">       <!-- pads_begin value: [0, 0, 1, 0, 0] -->\n            <dim>5</dim>\n        </port>\n        <port id=\"3\">       <!-- pads_end value: [0, 0, 1, 0, 0] -->\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>48</dim>   <!-- data.shape[0] * block_shape.shape[0] * block_shape.shape[1] *... * block_shape.shape[4] -->\n            <dim>3</dim>    <!-- (data.shape[1] + pads_begin[1] + pads_end[1]) / block_shape.shape[1]  -->\n            <dim>3</dim>    <!-- (data.shape[2] + pads_begin[2] + pads_end[2]) / block_shape.shape[2] -->\n            <dim>1</dim>    <!-- (data.shape[3] + pads_begin[3] + pads_end[3]) / block_shape.shape[3] -->\n            <dim>3</dim>    <!-- (data.shape[4] + pads_begin[4] + pads_end[4]) / block_shape.shape[4] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining Target Name\nDESCRIPTION: This snippet defines the target name for the zlib library, which is used later when creating the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/zlib/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_zlib\")\n```\n\n----------------------------------------\n\nTITLE: Installing C Samples Build Scripts (Windows) in CMake\nDESCRIPTION: Installs the build_samples_msvc.bat and build_samples.ps1 scripts for C samples on Windows systems. These scripts facilitate building C samples using MSVC and PowerShell, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nelseif(WIN32)\n    install(PROGRAMS cpp/build_samples_msvc.bat cpp/build_samples.ps1\n            DESTINATION ${OV_CPACK_SAMPLESDIR}/c\n            COMPONENT ${OV_CPACK_COMP_C_SAMPLES}\n            ${OV_CPACK_COMP_C_SAMPLES_EXCLUDE_ALL})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual TPM Directory and Provisioning Certificates (sh)\nDESCRIPTION: Creates necessary directories for the virtual TPM device, sets the XDG_CONFIG_HOME environment variable, and uses swtpm_setup to generate certificates and configure the TPM state. This ensures that only root has read/write permissions to the directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nsudo mkdir -p /var/OVSA/\nsudo mkdir /var/OVSA/vtpm\nsudo mkdir /var/OVSA/vtpm/vtpm_isv_dev\n\nexport XDG_CONFIG_HOME=~/.config\n/usr/share/swtpm/swtpm-create-user-config-files\nswtpm_setup --tpmstate /var/OVSA/vtpm/vtpm_isv_dev --create-ek-cert --create-platform-cert --overwrite --tpm2 --pcr-banks -\n```\n\n----------------------------------------\n\nTITLE: Define Target Name\nDESCRIPTION: This snippet defines the target name for the OpenVINO Intel GPU SYCL object. This name will be used in subsequent CMake commands to configure and build the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/sycl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_intel_gpu_sycl_obj\")\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: This snippet sets the include directories for the target library. It adds the current source directory and the directory specified by `INCLUDE_DIR` as public include directories, allowing other libraries to include headers from these locations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC\n    $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}>\n    $<BUILD_INTERFACE:${INCLUDE_DIR}>)\n```\n\n----------------------------------------\n\nTITLE: Adding Pre-Build Command in CMake\nDESCRIPTION: This CMake snippet adds a pre-build command to the pyopenvino target. It checks if the configuration is Debug, and if so, it prints an error message and fails the build. This prevents building the Python API in Debug mode, likely due to known issues or incompatibilities.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(OV_GENERATOR_MULTI_CONFIG)\n    string(APPEND _cmd_echo\n        \"$<$<CONFIG:Debug>:\\\"\n            \"${CMAKE_COMMAND};-E;cmake_echo_color;--red;\\\"OpenVINO;Python;API;cannot;be;built;for;'Debug'\\\"\"\n        \">\")\n    string(APPEND cmd_error\n        \"$<$<CONFIG:Debug>:\\\"\n            \"${CMAKE_COMMAND};-E;false\"\n        \">\")\n\n    add_custom_command(TARGET ${PROJECT_NAME} PRE_BUILD\n        COMMAND \"${_cmd_echo}\"\n        COMMAND \"${cmd_error}\"\n        COMMAND_EXPAND_LISTS)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Build Docker Image\nDESCRIPTION: Builds a Docker image for OpenVINO notebooks using the Dockerfile in the current directory.  The `-t` flag tags the image as `openvino_notebooks`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_25\n\nLANGUAGE: console\nCODE:\n```\ndocker build -t openvino_notebooks .\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Components Relationships Legend Diagram\nDESCRIPTION: This diagram provides a legend for understanding the relationships between internal components within the OpenVINO dynamic library. It defines the different types of libraries (interface, object, static, and dynamic) and APIs (public and developer) and shows how they are linked or dependent on each other.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/docs/architecture.md#_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    subgraph legend [Legend]\n    interface_lib[Interface library]\n    style interface_lib fill:#af8401\n\n    object_lib[Object library]\n    style object_lib fill:#6e9f01\n\n    static_lib[(Static library)]\n    style static_lib fill:#01429f\n    dynamic_lib([Dynamic library])\n    style dynamic_lib fill:#cb6969\n\n    some_api{{Some library API}}\n    some_dev_api{{Some developer API}}\n\n    object_lib ===>|Link with| interface_lib\n\n    some_api --->|Depends on headers only| static_lib\n    some_api -.->|Include headers| some_dev_api\n    end\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: End of Line\nDESCRIPTION: This rule ensures that all files end with a newline character in JavaScript and TypeScript code. It's a common practice for ensuring file consistency and is enforced by ESLint with the configuration `eol-last: ['error']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\neol-last: ['error']\n```\n\n----------------------------------------\n\nTITLE: Install Python API Dependencies\nDESCRIPTION: This snippet updates `pip` and `setuptools`, and then installs the required Python dependencies specified in the `requirements.txt` file located in the OpenVINO source tree.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n% # update pip and setuptools to newer versions\n% python3 -m pip install -U pip\n% python3 -m pip install -r <openvino source tree>/src/bindings/python/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Select-1 XML Example\nDESCRIPTION: This XML code demonstrates a Select-1 operation. It defines the input ports for the condition (cond), the 'then' tensor, and the 'else' tensor, along with the output port. The example illustrates how the output tensor is generated based on the condition and the corresponding values from the 'then' and 'else' tensors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/condition/select-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Select\">\n    <input>\n        <port id=\"0\">     <!-- cond value is: [[false, false], [true, false], [true, true]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">     <!-- then value is: [[-1, 0], [1, 2], [3, 4]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"2\">     <!-- else value is: [[11, 10], [9, 8], [7, 6]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">     <!-- output value is: [[11, 10], [1, 8], [3, 4]] -->\n            <dim>3</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Generating Test Models CMake\nDESCRIPTION: Generates TensorFlow test models if TensorFlow is found. It locates generation scripts and models, then uses Python to execute the scripts and create the models.  A custom target `tensorflow_test_models` is created to manage this process. If tensorflow is not found, add custom command will produce warning during build time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (tensorflow_FOUND)\n    set(TEST_TENSORFLOW_MODELS ${TEST_MODEL_ZOO_OUTPUT_DIR}/tensorflow_test_models/)\n\n    file(GLOB_RECURSE TENSORFLOW_GEN_SCRIPTS ${CMAKE_CURRENT_SOURCE_DIR}/test_models/gen_scripts/generate_*.py)\n    file(GLOB_RECURSE TENSORFLOW_MODELS_PBTXT ${CMAKE_CURRENT_SOURCE_DIR}/test_models/models_pbtxt/*.pbtxt)\n    list (APPEND TENSORFLOW_GEN_SCRIPTS ${TENSORFLOW_MODELS_PBTXT})\n    file(GLOB_RECURSE TENSORFLOW_ALL_SCRIPTS ${CMAKE_CURRENT_SOURCE_DIR}/*.py)\n    set(OUT_FILES \"\")\n    foreach(GEN_SCRIPT ${TENSORFLOW_GEN_SCRIPTS})\n        get_filename_component(FILE_WE ${GEN_SCRIPT} NAME_WE)\n        set(OUT_DONE_FILE ${TEST_TENSORFLOW_MODELS}/${FILE_WE}_done.txt)\n        set(OUT_FILES ${OUT_DONE_FILE} ${OUT_FILES})\n        add_custom_command(OUTPUT ${OUT_DONE_FILE}\n                COMMAND ${Python3_EXECUTABLE}\n                    ${CMAKE_CURRENT_SOURCE_DIR}/test_models/gen_wrapper.py\n                    ${GEN_SCRIPT}\n                    ${TEST_TENSORFLOW_MODELS}\n                    ${OUT_DONE_FILE}\n                JOB_POOL four_jobs\n                DEPENDS ${TENSORFLOW_ALL_SCRIPTS})\n    endforeach()\n    add_custom_target(tensorflow_test_models DEPENDS ${OUT_FILES})\n\n    install(DIRECTORY ${TEST_TENSORFLOW_MODELS}\n            DESTINATION tests/${TEST_TENSORFLOW_MODELS_DIRNAME}\n            COMPONENT tests\n            EXCLUDE_FROM_ALL)\nelse()\n    # Produce warning message at build time as well\n    add_custom_command(OUTPUT unable_build_tensorflow_models.txt\n            COMMAND ${CMAKE_COMMAND}\n            -E cmake_echo_color --red \"Warning: Unable to generate tensorflow test models. Running '${TARGET_NAME}' will likely fail\"\n            )\n    add_custom_target(tensorflow_test_models DEPENDS unable_build_tensorflow_models.txt)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Checking library architecture\nDESCRIPTION: This command uses the `file` utility to determine the architecture of a given library. It helps verify if a library is compiled for arm64 or x86_64.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\nfile /opt/homebrew/Cellar/tbb/2021.5.0_2/lib/libtbb.12.5.dylib\n```\n\n----------------------------------------\n\nTITLE: Set PYTHONPATH Environment Variable\nDESCRIPTION: Sets the `PYTHONPATH` environment variable to include the path to the OpenVINO™ Python API binaries. This allows Python to find and import the OpenVINO™ modules.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport PYTHONPATH=PYTHONPATH:<openvino_repo>/bin/intel64/Release/python\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCL headers on Ubuntu\nDESCRIPTION: This command installs the necessary OpenCL headers on an Ubuntu system. This is a prerequisite for building OpenVINO with OpenCL support.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/use_device_mem.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\n# apt install opencl-c-headers opencl-clhpp-headers\n```\n\n----------------------------------------\n\nTITLE: ISTFT Example: 4D input, 2D output, center=true, default length (XML)\nDESCRIPTION: An example illustrating the ISTFT operation with a 4D input and a 2D output. Here, 'center' is set to 'true', which affects the default signal length calculation. This setup processes batched complex-valued data and reconstructs a real-valued signal.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/istft-16.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ISTFT\" ... >\n    <data center=\"true\" ... />\n    <input>\n        <port id=\"0\">\n            <dim>4</dim>\n            <dim>6</dim>\n            <dim>16</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n        </port>\n        <port id=\"2\"></port> <!-- frame_size value: 11 -->\n        <port id=\"3\"></port> <!-- frame_step value: 3 -->\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>4</dim>\n            <dim>45</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO build dependencies\nDESCRIPTION: This snippet executes the `install_build_dependencies.sh` script located in the project root folder, typically used to install necessary build-time dependencies for OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nsudo ./install_build_dependencies.sh\n```\n\n----------------------------------------\n\nTITLE: Broadcast with numpy mode in OpenVINO (XML)\nDESCRIPTION: This XML code snippet demonstrates the Broadcast operation with the 'numpy' mode. It shows how a tensor with shape [16, 1, 1] is broadcasted to [1, 16, 50, 50] based on the target shape [4] and numpy broadcasting rules. The third input 'axes_mapping' is not required when mode is set to 'numpy'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/broadcast-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Broadcast\" ...>\n    <data mode=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n       </port>\n        <port id=\"1\">\n            <dim>4</dim>   <!--The tensor contains 4 elements: [1, 16, 50, 50] -->\n        </port>\n        <!-- the 3rd input shouldn't be provided with mode=\"numpy\" -->\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>16</dim>\n            <dim>50</dim>\n            <dim>50</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Generating Kernel Sources and Headers with Python\nDESCRIPTION: This CMake snippet creates a custom command to run a Python script (`CODEGEN_SCRIPT`) to generate kernel sources and headers. The script takes the CM kernel files and headers as input and generates the `gpu_cm_kernel_sources.inc` and `gpu_cm_kernel_headers.inc` files in the code generation cache directory. These files contain the generated code that will be compiled into the target library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/cm/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(OUTPUT \"${CODEGEN_CACHE_DIR}/${KERNEL_SOURCES}\"\n  COMMAND \"${CMAKE_COMMAND}\" -E make_directory \"${CODEGEN_CACHE_DIR}\"\n  COMMAND \"${Python3_EXECUTABLE}\" \"${CODEGEN_SCRIPT}\" -out_sources \"${CODEGEN_CACHE_DIR}/${KERNEL_SOURCES}\"\n                                                      -out_headers \"${CODEGEN_CACHE_DIR}/${KERNEL_HEADERS}\"\n                                                      -in_kernels_dir \"${CODEGEN_CACHE_DIR}/cm_kernels\"\n                                                      -in_headers_dir \"${CODEGEN_CACHE_DIR}/cm_kernels/include\"\n                                                      -lang \"cm\"\n  DEPENDS ${KERNELS} \"${CODEGEN_SCRIPT}\" \"${XETLA_HEADER_FULL_PATH}\"\n  COMMENT \"Generating ${CODEGEN_CACHE_DIR}/${KERNEL_SOURCES} ...\"\n)\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Runtime Library Architecture\nDESCRIPTION: This snippet represents a Mermaid diagram illustrating the architecture of the OpenVINO Runtime library. It includes components such as core, inference, common transformations, low precision transformations and frontend_common.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/README.md#_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    subgraph openvino [openvino library]\n        core\n        inference\n        transformations[Common transformations]\n        lp_transformations[LP transformations]\n        frontend_common\n        style frontend_common fill:#7f9dc0,stroke:#333,stroke-width:4px\n        style transformations fill:#3d85c6,stroke:#333,stroke-width:4px\n        style lp_transformations fill:#0b5394,stroke:#333,stroke-width:4px\n        style core fill:#679f58,stroke:#333,stroke-width:4px\n        style inference fill:#d7a203,stroke:#333,stroke-width:4px\n    end\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files\nDESCRIPTION: This snippet uses the `file(GLOB)` command to collect all C++ source files (*.cpp) and header files (*.hpp) in the current source directory. These files will be used to build the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/test_builtin_extensions/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp)\nfile(GLOB LIBRARY_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Prepare the Python build\nDESCRIPTION: Configures the Python build using the `./configure` tool. The `--with-ensurepip=install` option ensures that `pip` is installed along with Python, enabling package management.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/python_version_upgrade.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd Python-3.8.13\n./configure --with-ensurepip=install\n```\n\n----------------------------------------\n\nTITLE: Target Link Libraries CMake\nDESCRIPTION: This snippet specifies the libraries that the `openvino_snippets` target should link against. It links publicly against `openvino::runtime` and privately against `openvino::reference` and `openvino::runtime::dev`. Public dependencies are visible to downstream projects, while private dependencies are only used within the library itself.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino::runtime\n                                     PRIVATE openvino::reference openvino::runtime::dev)\n```\n\n----------------------------------------\n\nTITLE: Creating OpenVINO Installation Directory\nDESCRIPTION: This command creates the `/opt/intel` directory, which will be used to install OpenVINO.  This is the recommended directory for administrators and root users.  The command uses `sudo` to elevate privileges, as creating directories in `/opt` requires root access.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nsudo mkdir /opt/intel\n```\n\n----------------------------------------\n\nTITLE: Defining the getShape Method in Output Interface\nDESCRIPTION: Defines the `getShape` method within the `Output` interface. This method returns the shape of the output tensor as an array of numbers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Output.rst#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\ngetShape(): number[]\n```\n\n----------------------------------------\n\nTITLE: Install Layer Test Requirements (sh)\nDESCRIPTION: This command installs the necessary requirements for running the layer tests.  It navigates to the 'tests/layer_tests' directory and installs the packages listed in the 'requirements.txt' file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/layer_tests/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncd tests/layer_tests\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Directory Structure\nDESCRIPTION: This code snippet represents the directory structure of the OpenVINO project. It shows the different components such as bindings, cmake scripts, common components, core, frontends, inference, plugins, and tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nbindings/           // OpenVINO bindings\ncmake/              // Common cmake scripts\ncommon/             // Common components\ncore/               // OpenVINO core component provides model representation, operations and other core functionality\nfrontends/          // OpenVINO frontends\ninference/          // Provides API for model inference\nplugins/            // OpenVINO plugins\ntests/              // A backed of tests binaries for core and plugins\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: This code snippet sets the include directories for the library target.  It adds the `${PUBLIC_HEADERS_DIR}` as a public include directory for build interfaces only.  It adds the source directory `${CMAKE_CURRENT_SOURCE_DIR}/src` and the pugixml interface include directories as private include directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/offline_transformations/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC $<BUILD_INTERFACE:${PUBLIC_HEADERS_DIR}>\n                                          PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}/src\"\n                                                  $<TARGET_PROPERTY:openvino::pugixml,INTERFACE_INCLUDE_DIRECTORIES>)\n```\n\n----------------------------------------\n\nTITLE: Define Model Outputs Property (TypeScript)\nDESCRIPTION: This code defines the `outputs` property of the `Model` interface as an array of `Output` objects. This property provides access to the output tensors of the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\noutputs: Output[]\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO Tests (Bash)\nDESCRIPTION: This script sets up the OpenVINO environment, creates a build directory, and builds the tests using cmake.  It requires OpenVINO to be installed and the `setupvars.sh` script to be sourced. It uses cmake to generate makefiles and then builds the project using multiple cores (-j8).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsource <OpenVINO_install_dir>/setupvars.sh\nmkdir build && cd build\ncmake .. && cmake --build . -j8\n```\n\n----------------------------------------\n\nTITLE: Add LPT Models Subdirectory CMake\nDESCRIPTION: Adds the `ov_lpt_models` subdirectory, which likely contains model definitions or scripts related to low precision transformation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/ov_helpers/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(ov_lpt_models)\n```\n\n----------------------------------------\n\nTITLE: DFT Layer XML Definition (No signal_size, 4D input)\nDESCRIPTION: Defines a DFT layer in XML format without the signal_size input for a 4-dimensional input tensor. It shows the input and output port dimensions, and the axes parameter specifies on which dimensions the DFT is applied.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/dft-7.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"DFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- axes input contains [1, 2] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>1</dim>\n            <dim>320</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing the Target\nDESCRIPTION: This snippet configures the installation settings for the target ${TARGET_NAME}. It specifies the runtime and library destinations as 'tests', marks the component as 'tests', and excludes the target from the 'all' build. This installs the built library into the 'tests' folder.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/mock_py_frontend/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n    RUNTIME DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL\n    LIBRARY DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL\n)\n```\n\n----------------------------------------\n\nTITLE: Including Subdirectories with CMake\nDESCRIPTION: This CMake snippet includes the 'unit' and 'functional' subdirectories into the current build process. It utilizes the `add_subdirectory` command to achieve this. Each of the mentioned subdirectories are expected to have their own CMakeLists.txt file to define their respective build configurations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(unit)\nadd_subdirectory(functional)\n```\n\n----------------------------------------\n\nTITLE: Setting Public Headers Directory in CMake\nDESCRIPTION: This code snippet defines the directory containing public headers for the library. It uses the `set` command to assign the path to the `PUBLIC_HEADERS_DIR` variable, which is constructed using `CMAKE_CURRENT_SOURCE_DIR` and appending `/include`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/offline_transformations/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(PUBLIC_HEADERS_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: Get OpenVINO Version in Python\nDESCRIPTION: This command executes a Python script to import the OpenVINO module and print its version. It can be used to programmatically determine the installed OpenVINO version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/release-notes-openvino/release-policy.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython3 -c \"import openvino; print(openvino.__version__)\"\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This CMake snippet sets the target name to `ov_transformations_tests`. This target name is then used throughout the rest of the configuration file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_transformations_tests)\n```\n\n----------------------------------------\n\nTITLE: Docker Run Command for Interactive Shell\nDESCRIPTION: This command shows how to run an interactive shell session inside the locally built Docker image. It provides a way to interact with the environment configured in the Dockerfile.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/docker_images.md#_snippet_5\n\nLANGUAGE: docker\nCODE:\n```\ndocker run --entrypoint bash -it my_local_ov_test_build\n```\n\n----------------------------------------\n\nTITLE: Setting Binary Folder Based on Architecture\nDESCRIPTION: This snippet determines the architecture and sets the BIN_FOLDER variable accordingly. If the architecture is x86_64, amd64, or CMAKE_OSX_ARCHITECTURES is x86_64, it sets the architecture to intel64. If the architecture is i386, it sets the architecture to ia32.  It appends the CMAKE_BUILD_TYPE for UNIX systems.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT BIN_FOLDER)\n    string(TOLOWER ${CMAKE_SYSTEM_PROCESSOR} ARCH)\n    if(ARCH STREQUAL \"x86_64\" OR ARCH STREQUAL \"amd64\" # Windows detects Intel's 64-bit CPU as AMD64\n        OR CMAKE_OSX_ARCHITECTURES STREQUAL \"x86_64\")\n        set(ARCH intel64)\n    elseif(ARCH STREQUAL \"i386\")\n        set(ARCH ia32)\n    endif()\n\n    set (BIN_FOLDER ${ARCH})\n\n    if(UNIX)\n        set(BIN_FOLDER \"${BIN_FOLDER}/${CMAKE_BUILD_TYPE}\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Subtract Layer Definition (Numpy Broadcast) - XML\nDESCRIPTION: This example illustrates a Subtract layer definition in XML utilizing NumPy-style broadcasting. Input tensors with different shapes are broadcasted according to NumPy rules. The example showcases the layer definition, input ports with their respective dimensions, and the output port defining the dimensions after broadcasting.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/subtract-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Subtract\">\n    <data auto_broadcast=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name\nDESCRIPTION: This snippet sets the target name for the OpenVINO interpreter backend library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME openvino_interpreter_backend)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for Selective Build (Second Build)\nDESCRIPTION: This snippet shows how to configure CMake for the second build, which performs the selective build based on the statistics collected in the first build. The `-DSELECTIVE_BUILD=ON` option enables selective building.  The `-DSELECTIVE_BUILD_STAT=<CSV_PATH>` option specifies the path to the CSV file containing the statistics. The selective build process includes only the components listed in the CSV file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/selective_build.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd ../../../build\n\ncmake .. \\\n-DCMAKE_BUILD_TYPE=Release \\\n-DENABLE_CPU_DEBUG_CAPS=ON \\\n-DENABLE_DEBUG_CAPS=ON \\\n-DSELECTIVE_BUILD=ON \\\n-DSELECTIVE_BUILD_STAT=<CSV_PATH> \\\n-DPython3_EXECUTABLE=/usr/bin/python3.7 \\\n-DCMAKE_INSTALL_PREFIX=`pwd`/install \\\n-DCMAKE_INSTALL_RPATH=`pwd`/install/runtime/3rdparty/tbb/lib:`pwd`/install/runtime/lib/intel64\n\ncmake --build . --config Release -j 8\n```\n\n----------------------------------------\n\nTITLE: Defining f64 data type in TypeScript\nDESCRIPTION: Defines the f64 data type as a number in TypeScript.  This represents a 64-bit floating-point number, providing higher precision than f32.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/element.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nf64: number\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for External Libraries in CMake\nDESCRIPTION: These lines add the 'cnpy' and 'zlib' subdirectories to the build, making the respective libraries available for linking. The `add_subdirectory` command specifies the source directory and the binary output directory. This step ensures that the dependent third-party libraries are built and linked correctly with the fuzz tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(../../../thirdparty/cnpy ${CMAKE_CURRENT_BINARY_DIR}/cnpy)\nadd_subdirectory(../../../thirdparty/zlib ${CMAKE_CURRENT_BINARY_DIR}/zlib)\n```\n\n----------------------------------------\n\nTITLE: Creating alias target\nDESCRIPTION: Creates an alias target `openvino::npu_logger_utils` that points to the original target `openvino_npu_logger_utils`. This allows for more convenient linking using the namespace qualified name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/logger/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(openvino::npu_logger_utils ALIAS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Type Independent Remote Tensor Implementation\nDESCRIPTION: This code snippet shows the implementation of a type-independent remote tensor class. It wraps a type-dependent tensor internally and overrides the methods of the ov::IRemoteTensor interface, delegating calls to the type-dependent implementation. This approach allows working with remote tensors without direct knowledge of the underlying data type.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/remote-tensor.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n/*! [vector_impl:implementation]\n*/\n#pragma once\n\n#include <openvino/runtime/remote_tensor.hpp>\n\n#include <memory>\n#include <vector>\n\n#include \"template/remote_tensor.hpp\"\n\nnamespace template_plugin {\n\n/**\n * \\brief Implementation of Remote Tensor which wraps memory allocated in STL vector.\n *        The class wraps type dependent tensor.\n */\nclass VectorTensorImpl : public RemoteTensor {\nprivate:\n    // The type dependent Remote Tensor\n    std::shared_ptr<ov::RemoteTensor> m_tensor;\n\npublic:\n    /**\n     * \\brief Wraps stl vector into Remote Tensor object\n     * \\param rt - Type dependent Remote Tensor\n     */\n    VectorTensorImpl(const std::shared_ptr<ov::RemoteTensor>& rt) : m_tensor(rt) {} \n\n    bool type_check(const ov::RemoteTensor& remote_tensor) override { return RemoteTensor::type_check(remote_tensor); }\n    const void* get_data() const override { return m_tensor->get_data(); }\n    void* get_data() override { return m_tensor->get_data(); }\n    std::shared_ptr<ov::RemoteContext> get_context() const override { return m_tensor->get_context(); }\n};\n\n}  // namespace template_plugin\n/*! [vector_impl:implementation] */\n```\n\n----------------------------------------\n\nTITLE: Compare Two Dumps and Analyze Differences\nDESCRIPTION: Compares two dumps of output tensors from the CPU plugin and analyzes the differences between them. Replace `/path/to/model` with the path to the model file, and `dump1` and `dump2` with the names of the dump files to compare.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/dump_check/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 dump_check.py -m=/path/to/model dump1 dump2\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library and Setting Compile Definitions in CMake\nDESCRIPTION: This code creates an object library, sets public and private include directories, defines compile definitions for private and public usage, and handles compile definitions based on whether it's building shared libraries. It also links to openvino::util and openvino::core::dev.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME}_obj OBJECT ${LIBRARY_SRC} ${LIBRARY_HEADERS} ${LIBRARY_PUBLIC_HEADERS})\n\ntarget_include_directories(${TARGET_NAME}_obj\n    PUBLIC\n        $<BUILD_INTERFACE:${FRONTEND_INCLUDE_DIR}>\n    PRIVATE\n        ${CMAKE_CURRENT_SOURCE_DIR}/src\n        $<TARGET_PROPERTY:openvino::frontend::common,INTERFACE_INCLUDE_DIRECTORIES>\n        # for ov_frontends.hpp in static build\n        ${CMAKE_CURRENT_BINARY_DIR}/src)\n\ntarget_compile_definitions(${TARGET_NAME}_obj PRIVATE IMPLEMENT_OPENVINO_API)\n\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME}_obj PUBLIC OPENVINO_STATIC_LIBRARY)\nendif()\n\ntarget_link_libraries(${TARGET_NAME}_obj PRIVATE openvino::util openvino::core::dev)\n```\n\n----------------------------------------\n\nTITLE: Slice: Slicing Backwards with Step -1 in OpenVINO XML\nDESCRIPTION: This example demonstrates slicing a tensor backwards using a negative step.  The `start` index is set to the end of the tensor, the `stop` is before the beginning, and the `step` is -1, resulting in a reversed copy of the input tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-8.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Slice\" ...>\n       <input>\n           <port id=\"0\">       <!-- data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -->\n             <dim>10</dim>\n           </port>\n           <port id=\"1\">       <!-- start: [9] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"2\">       <!-- stop: [-11] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"3\">       <!-- step: [-1] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"4\">       <!-- axes: [0] -->\n             <dim>1</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">       <!-- output: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] -->\n               <dim>10</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name for Unit Tests in CMake\nDESCRIPTION: This snippet sets the target name for the OpenVINO Hetero unit tests to 'ov_hetero_unit_tests'.  The target name is used in subsequent CMake commands to define the build process for these tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/tests/unit/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME ov_hetero_unit_tests)\n```\n\n----------------------------------------\n\nTITLE: Add Test Target with Exclusions\nDESCRIPTION: This snippet defines the 'ov_add_test_target' function, which adds a test target with specified name, root directory, include directories, excluded source paths, object files, defines, dependencies, and link libraries. It also enables cpplint and sets labels for the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/functional/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        INCLUDES ${INCLUDES}\n        EXCLUDED_SOURCE_PATHS ${EXCLUDED_SOURCE_PATHS}\n        OBJECT_FILES ${TMP_EXPLICITLY_ENABLED_TESTS}\n        DEFINES ${DEFINES}\n        DEPENDENCIES ${DEPENDENCIES}\n        LINK_LIBRARIES ${LINK_LIBRARIES}\n        ADD_CPPLINT\n        LABELS OV CPU\n)\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate 2D Tensor Update - C++ (axis=0)\nDESCRIPTION: This code snippet shows how the ScatterElementsUpdate operation updates a 2D tensor when axis is set to 0. It demonstrates the element update logic based on the indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\noutput[indices[i][j]][j] = reduction(updates[i][j], output[indices[i][j]][j]) if axis = 0\n```\n\n----------------------------------------\n\nTITLE: Set Link Flags (GCC - LTO Workaround)\nDESCRIPTION: Sets link flags for GCC to suppress \"maybe-uninitialized\" and \"stringop-overflow\" warnings during LTO builds. These warnings can arise from aggressive LTO optimizations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX)\n  set_target_properties(${TARGET_NAME} PROPERTIES LINK_FLAGS_RELEASE \"-Wno-error=maybe-uninitialized -Wno-maybe-uninitialized -Wno-stringop-overflow\"\n                                                  LINK_FLAGS_RELWITHDEBINFO \"-Wno-error=maybe-uninitialized -Wno-maybe-uninitialized -Wno-stringop-overflow\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: ScatterElementsUpdate Example 5 - XML\nDESCRIPTION: This XML snippet provides an example of the ScatterElementsUpdate operation with use_init_val set to 'true' and reduction set to 'prod'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-elements-update-12.rst#_snippet_15\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... use_init_val=\"true\" reduction=\"prod\" type=\"ScatterElementsUpdate\">\n        <input>\n            <port id=\"0\">>  <!-- data -->\n```\n\n----------------------------------------\n\nTITLE: Adding Shared Library\nDESCRIPTION: This snippet adds a shared library target named ${TARGET_NAME} using the specified source files and headers. It also creates an alias openvino::frontend::mock_py for the target.  The library is built as a shared library to prevent static member duplication issues.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/mock_py_frontend/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} SHARED ${LIBRARY_SRC} ${LIBRARY_HEADERS})\nadd_library(openvino::frontend::mock_py ALIAS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Define Project Name\nDESCRIPTION: Defines the name of the CMake project. This name is used for various purposes during the build process, such as naming the generated executables and libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/ovc/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nproject(OpenVINOConverter)\n```\n\n----------------------------------------\n\nTITLE: Setting Public Headers Directory in CMake\nDESCRIPTION: Defines the directory where the public header files for the library are located. This allows other projects to include these headers when using the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(PUBLIC_HEADERS_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: Acosh Layer XML Configuration in OpenVINO\nDESCRIPTION: This XML snippet demonstrates the configuration of an Acosh layer within an OpenVINO model. It defines the input and output ports, along with their dimensions, specifying how the Acosh operation will be applied to a tensor of shape (256, 56). This example illustrates the basic structure needed to include the Acosh operation in a model definition.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/acosh-3.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Acosh\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Reactivating a Virtual Environment (Windows)\nDESCRIPTION: This command reactivates a Python virtual environment named `openvino_env` on Windows. It uses the `activate` script located within the `Scripts` directory of the environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/run-notebooks.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsource openvino_env\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Definitions (CMake)\nDESCRIPTION: Adds a compiler definition for enabling autobatch unit test functionality. This definition might be used within the source code to conditionally compile test-specific code or features.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/tests/unit/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_definitions(-DAUTOBATCH_UNITTEST)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories\nDESCRIPTION: Adds the `thread_local` and `appverifier_tests` directories as subdirectories to the build. This includes the CMakeLists.txt files within those directories in the build process, allowing them to be built as part of the overall project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(thread_local)\nadd_subdirectory(appverifier_tests)\n```\n\n----------------------------------------\n\nTITLE: Set Target Name\nDESCRIPTION: This snippet sets the name of the target to be built as 'ov_cpu_func_tests'. This target represents the collection of CPU functional tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/functional/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_cpu_func_tests)\n```\n\n----------------------------------------\n\nTITLE: Maximum-1 XML Example - NumPy Broadcasting\nDESCRIPTION: This example illustrates the Maximum-1 operation with NumPy broadcasting enabled. The input tensors have different shapes, and the auto_broadcast attribute is set to \"numpy\".\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/maximum-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Maximum\">\n    <data auto_broadcast=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Move OpenVINO Repo File to YUM Config\nDESCRIPTION: Moves the OpenVINO repository file from the /tmp directory to the /etc/yum.repos.d directory. This requires sudo privileges to modify the system's package manager configuration. The purpose is to configure YUM to recognize the OpenVINO repository.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsudo mv /tmp/openvino.repo /etc/yum.repos.d\n```\n\n----------------------------------------\n\nTITLE: Exporting Targets for Developer Package (CMake)\nDESCRIPTION: This snippet exports the library target for use in the developer package. It specifies the include directories that should be included in the package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/shape_inference/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET ${TARGET_NAME}\n                                    INSTALL_INCLUDE_DIRECTORIES \"${SHAPE_INFER_INCLUDE_DIR}/\")\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name\nDESCRIPTION: This line sets the target name for the unit tests to 'ov_cpu_unit_tests'. This name is used to refer to the test executable in subsequent CMake commands.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_cpu_unit_tests)\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target in CMake\nDESCRIPTION: Optionally adds a Clang format target for code formatting if the `ov_add_clang_format_target` command is available. This command likely comes from a custom CMake module.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/format_reader/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(COMMAND ov_add_clang_format_target)\n    ov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Run JIT disassembly tool\nDESCRIPTION: Executes the Python script to disassemble JIT code using the provided trace file and binary dump. The tool extracts line number information and disassembles the binary code using `addr2line` and `objdump`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/dump_jit_disassm/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython ~/openvino/src/plugins/intel_cpu/tools/dump_jit_disassm/ dnnl_traces_cpu_jit_avx512_core_amx_compute_zp_pbuff_t.121.txt dnnl_dump_cpu_jit_avx512_core_amx_compute_zp_pbuff_t.121.bin\n```\n\n----------------------------------------\n\nTITLE: Enabling Snippet Parameter Dump\nDESCRIPTION: This snippet demonstrates how to enable the snippet properties dump feature using the OV_SNIPPETS_DUMP_BRGEMM_PARAMS environment variable. It specifies the path to the CSV file where the parameters will be dumped. This requires the OpenVINO environment to be properly configured.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/docs/debug_capabilities/parameters_dump.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOV_SNIPPETS_DUMP_BRGEMM_PARAMS=\"path=<path_to_csv_dump_file>\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Defining the Project\nDESCRIPTION: This snippet defines the CMake project name as 'stress_tests'.  This is the top-level project name used by CMake and is important for organization and dependency management.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nproject(stress_tests)\n```\n\n----------------------------------------\n\nTITLE: Set Target Properties for LTO (CMake)\nDESCRIPTION: This sets the INTERPROCEDURAL_OPTIMIZATION_RELEASE property for the target to enable LTO if it's enabled globally.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Setting Build Type\nDESCRIPTION: This snippet sets the CMake build type to 'Release'.  It uses a CACHE STRING to allow users to override the default build type through the CMake interface.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset (CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"Choose the build type\")\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: These commands add include directories for the target library. `target_include_directories` specifies the directories where the compiler should search for header files. PUBLIC visibility means that these include directories will also be available to other targets that link against this library. The include directories point to the library's own include directory and the parent directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include)\ntarget_include_directories(${TARGET_NAME} PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/../..)\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINO Package CMake\nDESCRIPTION: Uses the `find_package` command to locate the OpenVINO package, requiring the `Runtime` component. This step ensures that the OpenVINO runtime libraries are available for linking.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/utils/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(OpenVINO REQUIRED COMPONENTS Runtime)\n```\n\n----------------------------------------\n\nTITLE: Compile Converted Tokenizer Models\nDESCRIPTION: Compiles the converted OpenVINO tokenizer and detokenizer models for inference using OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom openvino import compile_model\n\ntokenizer, detokenizer = compile_model(ov_tokenizer), compile_model(ov_detokenizer)\n```\n\n----------------------------------------\n\nTITLE: Reactivating Conda Environment\nDESCRIPTION: This command reactivates the current Conda environment. Reactivating the environment refreshes the environment variables, ensuring that changes from previous installations (e.g., compiler installation) are properly reflected.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conda.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nconda activate py310\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for Implementations\nDESCRIPTION: Iterates through the list of available implementation types (`AVAILABLE_IMPL_TYPES`) and adds each implementation directory as a subdirectory to the build. Appends object files to OBJ_FILES list.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(IMPL_TYPE IN LISTS AVAILABLE_IMPL_TYPES)\n    add_subdirectory(impls/${IMPL_TYPE})\n    if (TARGET openvino_intel_gpu_${IMPL_TYPE}_obj)\n        list(APPEND OBJ_FILES $<TARGET_OBJECTS:openvino_intel_gpu_${IMPL_TYPE}_obj>)\n    endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Swish Layer Configuration with Beta Parameter in OpenVINO (XML)\nDESCRIPTION: This XML snippet demonstrates how to configure a Swish layer in OpenVINO, including the specification of an input tensor and a beta value.  The `port id=\"1\"` represents the beta value, which in this case is implicitly set within the model file based on the original framework. The layer takes a tensor of shape 256x56 as input and produces an output tensor of the same shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/swish-4.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Swish\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">  <!-- beta value: 2.0 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Wrap cl::Image2D (C)\nDESCRIPTION: This C snippet shows how to wrap an existing OpenCL 2D image (cl::Image2D) into an OpenVINO RemoteTensor using the GPU plugin. This is useful for processing image data directly from OpenCL. Requires OpenCL and OpenVINO libraries. The wrapped image can then be used as a RemoteTensor within an OpenVINO graph.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_23\n\nLANGUAGE: c\nCODE:\n```\n// example usage\n{\n    cl_mem mem;\n    size_t buffer_size;\n    // fill mem with data\n\n    auto remote_blob = context.create_tensor(buffer_size, mem);\n}\n```\n\n----------------------------------------\n\nTITLE: Update WSL and Shutdown\nDESCRIPTION: Updates the Windows Subsystem for Linux (WSL) to the latest version and then shuts it down. Requires WSL to be installed and accessible from PowerShell.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/configurations/configurations-intel-gpu.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nwsl --update\nwsl --shutdown\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files in CMake\nDESCRIPTION: Uses `file(GLOB)` to collect all C++ source files starting with `ov_` and `test_model_repo.cpp` and all header files with the `.hpp` extension in the current source directory. These files will be used as sources for the executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/ov_*.cpp test_model_repo.cpp)\nfile(GLOB HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries in CMake\nDESCRIPTION: Links the library target against the openvino::itt and openvino::core::dev libraries. These are likely dependencies that the openvino_lp_transformations library needs to function correctly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME}_obj PRIVATE openvino::itt openvino::core::dev)\n```\n\n----------------------------------------\n\nTITLE: Marking Target for ABI Free in CMake\nDESCRIPTION: This snippet marks the object library `${TARGET_NAME}_obj` as important for ABI (Application Binary Interface) stability. This is important for ensuring that the library remains compatible with other libraries over time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nov_abi_free_target(${TARGET_NAME}_obj)\n```\n\n----------------------------------------\n\nTITLE: Adding Test Command\nDESCRIPTION: Adds a test case named 'TestOCLCodePreprocessing' that executes the code generation test script using the Python interpreter. This runs as part of the testing framework.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nadd_test(NAME TestOCLCodePreprocessing\n         COMMAND ${Python3_EXECUTABLE} -B ${CODEGEN_TEST_SCRIPT}\n         WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR})\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Samples Release Directory on macOS\nDESCRIPTION: This command navigates to the OpenVINO C++ samples release directory on a macOS system. It assumes the build directory is located in the user's home directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_22\n\nLANGUAGE: sh\nCODE:\n```\ncd ~/openvino_cpp_samples_build/intel64/Release\n```\n\n----------------------------------------\n\nTITLE: Set CMake Options and Dependencies\nDESCRIPTION: This snippet disables interprocedural optimization for release builds, tries to use the gold linker, and disables deprecated errors. It also sets the ONNX frontend test status and defines the directory containing ONNX models and the manifest file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_INTERPROCEDURAL_OPTIMIZATION_RELEASE OFF)\n\nov_try_use_gold_linker()\n\nov_deprecated_no_errors()\n\nmessage(STATUS \"ONNX frontend test enabled\")\n\nadd_compile_definitions(\n    TEST_ONNX_MODELS_DIRNAME=\"${TEST_MODEL_ZOO}/onnx/\"\n    MANIFEST=\"${TEST_MODEL_ZOO}/onnx/unit_test.manifest\"\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Function to Build ONNX Static Library\nDESCRIPTION: This snippet defines a CMake function `ov_onnx_build_static` that sets the `BUILD_SHARED_LIBS` variable to `OFF` and then adds the `onnx` subdirectory to the build. This ensures that ONNX is built as a static library and excluded from the default build targets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/onnx/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ov_onnx_build_static)\n    set(BUILD_SHARED_LIBS OFF)\n    add_subdirectory(onnx EXCLUDE_FROM_ALL)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Compiler Options\nDESCRIPTION: This snippet sets compiler options to suppress the `-Wmissing-declarations` warning, which is specific to frontends implementation. The condition checks if the compiler is GCC or Clang.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common_translators/src/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG)\n    target_compile_options(${TARGET_NAME} PRIVATE -Wno-missing-declarations)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Source Directory\nDESCRIPTION: Defines the location of the OpenVINO source directory, which is used to locate necessary dependencies and resources. The path is relative to the current source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(OpenVINO_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../\")\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library\nDESCRIPTION: Installs the static library to the specified destination. The destination is determined by the NPU_INTERNAL_COMPONENT variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/backend/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${NPU_INTERNAL_COMPONENT})\n```\n\n----------------------------------------\n\nTITLE: Setting LTO Properties\nDESCRIPTION: This line sets the interprocedural optimization property for the target to the value of 'ENABLE_LTO'. This enables link-time optimization for release builds if 'ENABLE_LTO' is enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Protopipe Performance Measurement Execution\nDESCRIPTION: This command line execution runs Protopipe for performance measurement. It loads the configuration from 'config.yaml', drops frames exceeding the target FPS, and runs the inference for 30 seconds (-t 30).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n./protopipe --cfg config.yaml --drop_frames -t 30\n```\n\n----------------------------------------\n\nTITLE: Defining PartialShapeConstructor Interface in TypeScript\nDESCRIPTION: This code snippet defines the `PartialShapeConstructor` interface in TypeScript. It includes the constructor signature for creating `PartialShape` objects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PartialShapeConstructor.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface PartialShapeConstructor {\n    new PartialShapeConstructor(shape): PartialShape;\n}\n```\n\n----------------------------------------\n\nTITLE: Copying ovsa-developer.tar.gz to Guest VM\nDESCRIPTION: Copies the ovsa-developer.tar.gz file from the host machine to the /OVSA directory of the specified user's home directory on the guest VM using the scp command.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_32\n\nLANGUAGE: sh\nCODE:\n```\ncd $OVSA_RELEASE_PATH\nscp ovsa-developer.tar.gz username@<isv-developer-vm-ip-address>:/<username-home-directory>/OVSA\n```\n\n----------------------------------------\n\nTITLE: PrePostProcessor build method TypeScript\nDESCRIPTION: Defines the `build` method for the PrePostProcessor interface. This method is used to construct and finalize the PrePostProcessor object. It takes no arguments and returns the PrePostProcessor instance itself after construction.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PrePostProcessor.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nbuild(): PrePostProcessor\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files in CMake\nDESCRIPTION: This snippet uses the `file(GLOB_RECURSE)` command to recursively search for all `.cpp` source files and `.h` header files within the current directory and its subdirectories. The found files are stored in the SRC and HDR variables, respectively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/memleaks_tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile (GLOB_RECURSE SRC *.cpp)\nfile (GLOB_RECURSE HDR *.h)\n```\n\n----------------------------------------\n\nTITLE: Adding Version Defines (CMake)\nDESCRIPTION: Adds version defines to the target, using the specified source file. This allows the unit tests to access version information during runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/tests/unit/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_version_defines(${OpenVINO_SOURCE_DIR}/src/plugins/auto_batch/src/plugin.cpp ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Excluding ACL Paths\nDESCRIPTION: This snippet excludes ACL-related paths if `OV_CPU_WITH_ACL` is not enabled.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT OV_CPU_WITH_ACL)\n    list(APPEND EXCLUDE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/nodes/executors/acl/*\n                              ${CMAKE_CURRENT_SOURCE_DIR}/src/nodes/kernels/acl/*)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Define Model getOutputElementType Index Parameter (TypeScript)\nDESCRIPTION: This code defines the `index` parameter for `getOutputElementType` method. The index of the output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nindex: number\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries\nDESCRIPTION: This snippet links the target library (TARGET_NAME) with the tests_shared_lib library, making its symbols available to ov_thread_local. Also links against the openvino runtime libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/thread_local/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC tests_shared_lib)\n\n```\n\n----------------------------------------\n\nTITLE: Build Target Faster\nDESCRIPTION: This snippet uses the custom CMake function `ov_build_target_faster` to optimize the build process, using Unity builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME}\n    UNITY\n)\n```\n\n----------------------------------------\n\nTITLE: Third Correct TopK Output\nDESCRIPTION: Displays a third possible correct output for the TopK operation with the example input and configuration, showing the values and indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/top-k-11.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\noutput_values  = [3, 1, 2, 5]\noutput_indices = [1, 2, 3, 5]\n```\n\n----------------------------------------\n\nTITLE: Creating static library\nDESCRIPTION: Creates a static library with the specified target name using the collected source files. The static library will be linked into other targets at compile time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/logger/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target\nDESCRIPTION: This snippet adds a target for running clang-format on the source code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE all_backends_src \"${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\" \"${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\")\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_SOURCES ${all_backends_src})\n```\n\n----------------------------------------\n\nTITLE: Network Configuration Example\nDESCRIPTION: This example shows how to configure network settings in /etc/netplan/50-cloud-init.yaml to use a bridge interface (br0) for external network access, setting the interface eno1 to dhcp4: false and assigning it to the br0 bridge.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nnetwork:\n  ethernets:\n     eno1:\n       dhcp4: true\n       dhcp-identifier: mac\n  version: 2\n```\n\n----------------------------------------\n\nTITLE: Adding MSVC Compiler Flag\nDESCRIPTION: This snippet adds a compiler flag (`/wd4305`) when the compiler is MSVC. This flag suppresses a specific warning.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto/tests/functional/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    ov_add_compiler_flags(/wd4305)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This snippet sets the target name for the proxy plugin tests. The target name is used to identify the test target during the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_proxy_plugin_tests)\n```\n\n----------------------------------------\n\nTITLE: Define RESIZE_LINEAR Constant - Typescript\nDESCRIPTION: Defines the RESIZE_LINEAR constant used as an enumeration member within the OpenVINO library. This constant represents the linear interpolation resize algorithm. It's located in the addon.ts file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/resizeAlgorithm.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nRESIZE_LINEAR: number\n```\n\n----------------------------------------\n\nTITLE: If-8 Layer Definition in OpenVINO XML\nDESCRIPTION: This XML snippet defines an If-8 layer named \"if/cond\" within an OpenVINO model. It specifies the input and output ports of the If-8 operation, along with mappings to the input and output layers within the then_body and else_body subgraphs. The condition input at port 0 determines which subgraph to execute.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/condition/if-8.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"6\" name=\"if/cond\" type=\"If\" version=\"opset8\">\n    <input>\n        <port id=\"0\"/>\n        <port id=\"1\">\n            <dim>2</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"2\">\n            <dim>2</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"3\">\n            <dim>2</dim>\n            <dim>4</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"4\" names=\"if/cond/Identity:0,if/cond:0\" precision=\"FP32\">\n            <dim>2</dim>\n            <dim>4</dim>\n        </port>\n    </output>\n    <then_port_map>\n        <input external_port_id=\"1\" internal_layer_id=\"0\"/>\n        <input external_port_id=\"2\" internal_layer_id=\"1\"/>\n        <output external_port_id=\"0\" internal_layer_id=\"3\"/>\n    </then_port_map>\n    <else_port_map>\n        <input external_port_id=\"1\" internal_layer_id=\"0\"/>\n        <input external_port_id=\"3\" internal_layer_id=\"1\"/>\n        <output external_port_id=\"0\" internal_layer_id=\"3\"/>\n    </else_port_map>\n    <then_body>\n        <layers>\n            <layer id=\"0\" name=\"add_x\" type=\"Parameter\" version=\"opset1\">\n                <data element_type=\"f32\" shape=\"2,4\"/>\n                <output>\n                    <port id=\"0\" names=\"add_x:0\" precision=\"FP32\">\n                        <dim>2</dim>\n                        <dim>4</dim>\n                    </port>\n                </output>\n            </layer>\n            <layer id=\"1\" name=\"add_z\" type=\"Parameter\" version=\"opset1\">\n                <data element_type=\"f32\" shape=\"2,4\"/>\n                <output>\n                    <port id=\"0\" names=\"add_z:0\" precision=\"FP32\">\n                        <dim>2</dim>\n                        <dim>4</dim>\n                    </port>\n                </output>\n            </layer>\n            <layer id=\"2\" name=\"Add\" type=\"Add\" version=\"opset1\">\n                <data auto_broadcast=\"numpy\"/>\n                <input>\n                    <port id=\"0\">\n                        <dim>2</dim>\n                        <dim>4</dim>\n                    </port>\n                    <port id=\"1\">\n                        <dim>2</dim>\n                        <dim>4</dim>\n                    </port>\n                </input>\n                <output>\n                    <port id=\"2\" names=\"Add:0\" precision=\"FP32\">\n                        <dim>2</dim>\n                        <dim>4</dim>\n                    </port>\n                </output>\n            </layer>\n            <layer id=\"3\" name=\"Identity/sink_port_0\" type=\"Result\" version=\"opset1\">\n                <input>\n                    <port id=\"0\">\n                        <dim>2</dim>\n                        <dim>4</dim>\n                    </port>\n                </input>\n            </layer>\n        </layers>\n        <edges>\n            <edge from-layer=\"0\" from-port=\"0\" to-layer=\"2\" to-port=\"0\"/>\n            <edge from-layer=\"1\" from-port=\"0\" to-layer=\"2\" to-port=\"1\"/>\n            <edge from-layer=\"2\" from-port=\"2\" to-layer=\"3\" to-port=\"0\"/>\n        </edges>\n    </then_body>\n    <else_body>\n        <layers>\n            <layer id=\"0\" name=\"add_x\" type=\"Parameter\" version=\"opset1\">\n                <data element_type=\"f32\" shape=\"2,4\"/>\n                <output>\n                    <port id=\"0\" names=\"add_x:0\" precision=\"FP32\">\n                        <dim>2</dim>\n                        <dim>4</dim>\n                    </port>\n                </output>\n            </layer>\n            <layer id=\"1\" name=\"add_w\" type=\"Parameter\" version=\"opset1\">\n                <data element_type=\"f32\" shape=\"2,4\"/>\n                <output>\n                    <port id=\"0\" names=\"add_w:0\" precision=\"FP32\">\n                        <dim>2</dim>\n                        <dim>4</dim>\n                    </port>\n                </output>\n            </layer>\n            <layer id=\"2\" name=\"Add\" type=\"Add\" version=\"opset1\">\n                <data auto_broadcast=\"numpy\"/>\n                <input>\n                    <port id=\"0\">\n                        <dim>2</dim>\n                        <dim>4</dim>\n                    </port>\n                    <port id=\"1\">\n                        <dim>2</dim>\n                        <dim>4</dim>\n                    </port>\n                </input>\n                <output>\n                    <port id=\"2\" names=\"Add:0\" precision=\"FP32\">\n                        <dim>2</dim>\n                        <dim>4</dim>\n                    </port>\n                </output>\n            </layer>\n            <layer id=\"3\" name=\"Identity/sink_port_0\" type=\"Result\" version=\"opset1\">\n                <input>\n                    <port id=\"0\">\n                        <dim>2</dim>\n                        <dim>4</dim>\n                    </port>\n                </input>\n            </layer>\n        </layers>\n        <edges>\n            <edge from-layer=\"0\" from-port=\"0\" to-layer=\"2\" to-port=\"0\"/>\n            <edge from-layer=\"1\" from-port=\"0\" to-layer=\"2\" to-port=\"1\"/>\n            <edge from-layer=\"2\" from-port=\"2\" to-layer=\"3\" to-port=\"0\"/>\n        </edges>\n    </else_body>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Try Compile with stdc++fs (CMake)\nDESCRIPTION: This snippet attempts to compile the `main.cpp` program linking the `stdc++fs` library. The result is stored in `STD_FS_NEEDS_STDCXXFS`, indicating whether the compilation succeeded when linking against `stdc++fs`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\ntry_compile(STD_FS_NEEDS_STDCXXFS ${CMAKE_CURRENT_BINARY_DIR}\n            SOURCES ${CMAKE_CURRENT_BINARY_DIR}/main.cpp\n            COMPILE_DEFINITIONS -std=c++11\n            LINK_LIBRARIES stdc++fs)\n```\n\n----------------------------------------\n\nTITLE: Setting Common Include Directories\nDESCRIPTION: Defines the common include directories required for building the target. These directories are set as build interface properties, which means they are used when this library is included by other targets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(COMMON_INCLUDE_DIRS $<BUILD_INTERFACE:${MAIN_DIR}/src>\n                        $<BUILD_INTERFACE:${INCLUDE_DIR}>\n                        $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}>\n                        $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\n                        $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/impls>\n)\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Comma Spacing\nDESCRIPTION: This rule enforces consistent spacing around commas in JavaScript and TypeScript code. It improves code readability and is enforced by ESLint with the configuration `comma-spacing: ['error']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\ncomma-spacing: ['error']\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version\nDESCRIPTION: Sets the minimum required CMake version to 3.13. This ensures that the CMake version used to build the project is compatible with the CMake commands used in the file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This line sets the target name variable, which is used throughout the CMake configuration file to refer to the build target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/tests/standalone_build/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"tensorflow_lite_fe_standalone_build_test\")\n```\n\n----------------------------------------\n\nTITLE: Overriding Automatic Batching with benchmark_app and explicit device notion (sh)\nDESCRIPTION: This code snippet demonstrates how to use the benchmark_app tool with an explicit device notion (BATCH:GPU) to override the default automatic batching behavior. It uses the 'none' performance hint and specifies a model path.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark_app -hint none -d BATCH:GPU -m 'path to your favorite model'\n```\n\n----------------------------------------\n\nTITLE: Enable Hetero Plugin\nDESCRIPTION: This code snippet checks if the ENABLE_HETERO option is enabled and returns if it is not, preventing the Hetero plugin from being built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/hetero/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT ENABLE_HETERO)\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Add Custom Post-Build Command\nDESCRIPTION: This code adds a custom command to be executed after building the target. It copies the unit test manifest files from the source directory to the specified output directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(TARGET ov_onnx_frontend_tests POST_BUILD\n                   COMMAND ${CMAKE_COMMAND} -E copy \"${CMAKE_CURRENT_SOURCE_DIR}/unit_test.manifest\"\n                                                    \"${TEST_MODEL_ZOO_OUTPUT_DIR}/onnx/unit_test.manifest\"\n                   ${custom_commands}\n                   COMMENT \"Copy test manifest files to ${TEST_MODEL_ZOO}/onnx\")\n```\n\n----------------------------------------\n\nTITLE: Conditional Linking for OpenCL in CMake\nDESCRIPTION: Conditionally links the `OpenCL::OpenCL` library to the `ov_capi_test` target if the `OpenCL::OpenCL` target is defined. This enables OpenCL support for the tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/tests/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(TARGET OpenCL::OpenCL)\n    target_link_libraries(${TARGET_NAME} PRIVATE OpenCL::OpenCL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: OVSA Development Artifacts Directory Change\nDESCRIPTION: Navigates to the OVSA development artifacts directory, assumed to be set by the $OVSA_DEV_ARTEFACTS environment variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_44\n\nLANGUAGE: sh\nCODE:\n```\ncd $OVSA_DEV_ARTEFACTS\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard and Compiler Flags\nDESCRIPTION: Sets the C++ standard to C++17, disables compiler extensions, and requires the specified standard. It also adds a compiler flag for GNU compilers to ensure compatibility.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset (CMAKE_CXX_STANDARD 17)\nset (CMAKE_CXX_EXTENSIONS OFF)\nset (CMAKE_CXX_STANDARD_REQUIRED ON)\nif (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n    set (CMAKE_CXX_FLAGS \"-std=c++11 ${CMAKE_CXX_FLAGS}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Export Target for Developer Package and Install Includes\nDESCRIPTION: This CMake code exports the 'sharedTestClasses' target for use in developer packages using 'ov_developer_package_export_targets'. It also specifies the installation directory for the include files, ensuring that they are properly installed for downstream usage.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/shared_test_classes/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET ${TARGET_NAME}\n                                    INSTALL_INCLUDE_DIRECTORIES \"${CMAKE_CURRENT_SOURCE_DIR}/include/\")\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for Tests in CMake\nDESCRIPTION: Conditionally adds a subdirectory named tests to the build process if the ENABLE_TESTS variable is set. This allows the project to include unit tests or integration tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Build Type\nDESCRIPTION: This snippet sets the CMake build type to 'Release' and allows users to select a different build type (Debug, RelWithDebInfo, MinSizeRel). This allows different build configurations to be chosen at CMake configuration time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"CMake build type\")\nset_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Release\" \"Debug\" \"RelWithDebInfo\" \"MinSizeRel\")\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to CMake Target\nDESCRIPTION: This CMake command links the 'frontend_shared_test_classes' library to the 'gtest_main_manifest' target. The INTERFACE keyword ensures that any target linking against 'gtest_main_manifest' also links against 'frontend_shared_test_classes'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/gtest_main_manifest/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} INTERFACE frontend_shared_test_classes)\n```\n\n----------------------------------------\n\nTITLE: Emulate L2/L3 Cache Size\nDESCRIPTION: This C++ code snippet demonstrates how to emulate L2/L3 cache sizes by modifying the `dnnl::impl::cpu::platform::get_per_core_cache_size` function. It checks for environment variables (`OV_CC_L2_CACHE_SIZE`, `OV_CC_L3_CACHE_SIZE`, `OV_CC_L1_CACHE_SIZE`) and uses their values to override the reported cache sizes. This is only active during the analysis phase (`SELECTIVE_BUILD_ANALYZER` is defined).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/optimial-binary-size-conditional-compilation.rst#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n#if defined(SELECTIVE_BUILD_ANALYZER)\n    if (level == 2) {\n        const char* L2_cache_size = std::getenv(\"OV_CC_L2_CACHE_SIZE\");\n        if (L2_cache_size) {\n            int size = std::atoi(L2_cache_size);\n            if (size > 0) {\n                return size;\n            }\n        }\n    } else if (level == 3) {\n        const char* L3_cache_size = std::getenv(\"OV_CC_L3_CACHE_SIZE\");\n        if (L3_cache_size) {\n            int size = std::atoi(L3_cache_size);\n            if (size > 0) {\n                return size;\n            }\n        }\n    } else if (level == 1) {\n        const char* L1_cache_size = std::getenv(\"OV_CC_L1_CACHE_SIZE\");\n        if (L1_cache_size) {\n            int size = std::atoi(L1_cache_size);\n            if (size > 0) {\n                return size;\n            }\n        }\n    }\n#endif\n```\n\n----------------------------------------\n\nTITLE: Set environment variables for JIT dump\nDESCRIPTION: Sets the environment variables required for enabling JIT code dumping in the OpenVINO CPU plugin.  ONEDNN_JIT_DUMP enables JIT dumping, ONEDNN_VERBOSE controls verbosity, and OV_CPU_DEBUG_LOG specifies the output for debug logs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/dump_jit_disassm/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ONEDNN_JIT_DUMP=1\nexport ONEDNN_VERBOSE=2\nexport OV_CPU_DEBUG_LOG=-\n```\n\n----------------------------------------\n\nTITLE: Creating OpenVINO C Integration Example\nDESCRIPTION: This snippet defines a CMake configuration for building a C OpenVINO integration example. It finds the OpenVINO package, creates an executable named `ov_integration_snippet_c`, and links it with the `openvino::runtime::c` library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/snippets/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME_C \"ov_integration_snippet_c\")\n# [cmake:integration_example_c]\ncmake_minimum_required(VERSION 3.10)\nset(CMAKE_CXX_STANDARD 17)\n\nfind_package(OpenVINO REQUIRED)\n\nadd_executable(${TARGET_NAME_C} src/main.c)\n\ntarget_link_libraries(${TARGET_NAME_C} PRIVATE openvino::runtime::c)\n\n# [cmake:integration_example_c]\n```\n\n----------------------------------------\n\nTITLE: Conditional Check for Intel LLVM Compiler\nDESCRIPTION: This snippet checks if the OV_COMPILER_IS_INTEL_LLVM variable is not set. If it's not defined, it means the compiler is not Intel LLVM, so it will exit to prevent executing the below SYCL configurations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/sycl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT OV_COMPILER_IS_INTEL_LLVM)\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: CMake Script for Extension Library\nDESCRIPTION: This snippet shows a CMake script to configure the build of an OpenVINO extension library. It finds the OpenVINO package using the find_package command.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n# [cmake:extension]\ncmake_minimum_required(VERSION 3.13)\nproject(template_extension)\n\nfind_package(OpenVINO REQUIRED)\n\nadd_library(${PROJECT_NAME} SHARED ov_extension.cpp)\ntarget_link_libraries(${PROJECT_NAME} PUBLIC OpenVINO::runtime OpenVINO::extension)\nset_target_properties(${PROJECT_NAME} PROPERTIES\n    CXX_STANDARD 11\n    CXX_STANDARD_REQUIRED ON\n    POSITION_INDEPENDENT_CODE ON\n)\n# [cmake:extension]\n```\n\n----------------------------------------\n\nTITLE: Setting XeTLA Include Directory in CMake\nDESCRIPTION: Defines the include directory for XeTLA headers in the CMake build system.  The path is cached for later use in the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/thirdparty/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(XETLA_INCLUDE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/xetla/include/\" CACHE PATH \"Path to XeTLA headers\")\n```\n\n----------------------------------------\n\nTITLE: Mapping OCL Memory C++\nDESCRIPTION: When dealing with OCL memory buffers, you must map the memory into host memory to access its contents. This allows the host to read and write data to the OCL memory, which is essential for operations such as dumping layer in/out buffers for debugging. Memory mapping provides a mechanism to interact with device-specific memory from the host.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/execution_of_inference.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// If it is OCL memory, you map this into host memory.\n```\n\n----------------------------------------\n\nTITLE: Add OpenVINO Plugin (CMake)\nDESCRIPTION: This snippet uses the ov_add_plugin function to create the plugin with specified sources, device name, and version defines.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_plugin(NAME ${TARGET_NAME}\n              DEVICE_NAME \"BATCH\"\n              PSEUDO_DEVICE\n              SOURCES ${SOURCES} ${HEADERS}\n              VERSION_DEFINES_FOR src/plugin.cpp ADD_CLANG_FORMAT)\n```\n\n----------------------------------------\n\nTITLE: Generate CMake project\nDESCRIPTION: This command generates a CMake project for building the OpenVINO conformance tests. It enables both regular tests and functional tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/README.md#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake -DENABLE_TESTS=ON -DENABLE_FUNCTIONAL_TESTS=ON ..\n```\n\n----------------------------------------\n\nTITLE: Finding OpenVINODeveloperScripts Package\nDESCRIPTION: Finds the OpenVINODeveloperScripts package. It specifies the path to search for the package and indicates that the search should not include the CMake root path or default paths. This package likely contains helper scripts for development within the OpenVINO project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenVINODeveloperScripts REQUIRED\n             PATHS \"${OpenVINO_SOURCE_DIR}/cmake/developer_package\"\n             NO_CMAKE_FIND_ROOT_PATH\n             NO_DEFAULT_PATH)\n```\n\n----------------------------------------\n\nTITLE: Setting Dynamic VCXX Runtime CMake\nDESCRIPTION: This snippet sets the USE_DYNAMIC_VCXX_RUNTIME variable to ON if it is not already defined. This controls whether the dynamic version of the Visual C++ runtime is used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ocl/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED USE_DYNAMIC_VCXX_RUNTIME)\n    set(USE_DYNAMIC_VCXX_RUNTIME ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Displaying GPG Key Information\nDESCRIPTION: This snippet demonstrates how to display information about a GPG key file using the `gpg --show-keys` command.  This allows you to identify the key ID.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/integrate-openvino-with-ubuntu-snap.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ngpg --show-keys ./GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB\n```\n\n----------------------------------------\n\nTITLE: Installing the target\nDESCRIPTION: Configures the installation of the executable target, placing it in the 'tools/single-image-test' directory.  It also handles the installation of a README.md file if it exists.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n        RUNTIME DESTINATION \"tools/${TARGET_NAME}\"\n        COMPONENT ${NPU_INTERNAL_COMPONENT}\n        ${OV_CPACK_COMP_NPU_INTERNAL_EXCLUDE_ALL})\n\nif(EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/README.md\")\n    install(FILES \"${CMAKE_CURRENT_SOURCE_DIR}/README.md\"\n            DESTINATION \"tools/${TARGET_NAME}\"\n            COMPONENT ${NPU_INTERNAL_COMPONENT}\n            ${OV_CPACK_COMP_NPU_INTERNAL_EXCLUDE_ALL})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Include OpenVINO Op in C++\nDESCRIPTION: This snippet shows how to include the OpenVINO Op class in C++, which is required for creating custom operations. It includes the necessary header file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-openvino-operations.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"openvino/core/node.hpp\"\n#include \"openvino/op/op.hpp\"\n#include \"openvino/core/rt_info.hpp\"\n#include \"openvino/core/attribute_visitor.hpp\"\n#include \"openvino/op/util/shape_infer_interface.hpp\"\n\n#include <memory>\n#include <vector>\n```\n\n----------------------------------------\n\nTITLE: C++ Usage Example\nDESCRIPTION: Example of how to check the usage options for the C++ version of the async classification sample.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/image-classification-async.rst#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nclassification_sample_async -h\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO Runtime with CMake\nDESCRIPTION: This command installs the OpenVINO™ Runtime to a custom location specified by `<INSTALLDIR>`. The `<BUILDDIR>` placeholder should be replaced with the path to the build directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncmake --install <BUILDDIR> --prefix <INSTALLDIR>\n```\n\n----------------------------------------\n\nTITLE: Define Static Library Target in CMake\nDESCRIPTION: This CMake code snippet defines a static library target named 'sharedTestClasses' using the 'ov_add_target' macro. It specifies include directories, source directories, and links the 'func_test_utils' library. The 'ADD_CPPLINT' option enables cpplint checks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/shared_test_classes/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME sharedTestClasses)\n\nov_add_target(\n        NAME ${TARGET_NAME}\n        TYPE STATIC\n        ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/include\"\n        ADD_CPPLINT\n        INCLUDES\n            PUBLIC\n                \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\"\n            PRIVATE\n                \"${OpenVINO_SOURCE_DIR}/src/plugins/template/include\"\n        ADDITIONAL_SOURCE_DIRS\n            ${CMAKE_CURRENT_SOURCE_DIR}/src\n        LINK_LIBRARIES\n            PRIVATE\n                func_test_utils\n)\n```\n\n----------------------------------------\n\nTITLE: Running OpenVINO Tests (ctest)\nDESCRIPTION: This snippet demonstrates how to execute OpenVINO tests using `ctest`. The `-V` flag enables verbose output, providing detailed information about the test execution process. Running tests is a prerequisite for generating accurate coverage reports.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/test_coverage.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ ctest -V\n```\n\n----------------------------------------\n\nTITLE: Creating a build directory\nDESCRIPTION: This set of commands creates a `build` directory and changes the current working directory to it. This is a standard practice to keep the source directory clean and organized.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_linux.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nmkdir build && cd build\n```\n\n----------------------------------------\n\nTITLE: Define Input Parameter - Index (TypeScript)\nDESCRIPTION: Defines the 'index' parameter as a number for the input function, allowing selection of input by index.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nindex: number\n```\n\n----------------------------------------\n\nTITLE: ReduceMax Layer Configuration in XML (keep_dims=false)\nDESCRIPTION: This XML configuration defines a ReduceMax layer with `keep_dims` set to `false`. The input tensor has dimensions 6x12x10x24, and the reduction is performed along axes 2 and 3 (values [2, 3]). The output tensor removes the reduced dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-max-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceMax\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags for MSVC in CMake\nDESCRIPTION: This snippet adds a compiler flag (`/wd4244`) to suppress warning 4244 in MSVC, which relates to data loss during type conversion. This helps reduce noise in the build output when using the MSVC compiler.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/onnx/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    # 4244 conversion from 'XXX' to 'YYY', possible loss of data\n    ov_add_compiler_flags(/wd4244)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Example: Dump Blobs for Convolution and Reorder Nodes\nDESCRIPTION: Sets the environment variable to dump blobs only for nodes of type Convolution and Reorder during OpenVINO CPU execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_BLOB_DUMP_NODE_TYPE='Convolution Reorder' binary ...\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: This snippet sets the name of the target to 'protopipe'.  This variable is later used to define the executable name and installation directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME protopipe)\n```\n\n----------------------------------------\n\nTITLE: Getting Device Plugin Versions\nDESCRIPTION: Retrieves version information for the specified device plugin, including the build number and description. This method provides insight into the specific version of the plugin being used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\ngetVersions(deviceName): {\n    [deviceName: string]: {\n        buildNumber: string;\n        description: string;\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Absolute Install Destination Warning CMake\nDESCRIPTION: This snippet disables warnings about absolute paths in install destinations, which are common in OpenCL due to absolute paths to include directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ocl/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_WARN_ON_ABSOLUTE_INSTALL_DESTINATION OFF)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Modules\nDESCRIPTION: This command installs the required Python modules for the conditional compilation tests using pip. The requirements are listed in the requirements.txt file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/conditional_compilation/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Install Unit Tests\nDESCRIPTION: Installs the unit tests to a specific destination directory if testing is enabled. It uses variables like `ENABLE_TESTS` to conditionally include the tests and excludes the tests from the default installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/ovc/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n    install(DIRECTORY unit_tests\n            DESTINATION tests/ovc\n            COMPONENT tests\n            EXCLUDE_FROM_ALL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Exporting targets for developer package\nDESCRIPTION: Configures the export of the target for use in a developer package.  It defines what include directories are required to use the target when it's installed. `ov_developer_package_export_targets` is assumed to be a macro defined elsewhere.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/logger/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nov_developer_package_export_targets(TARGET openvino::npu_logger_utils\n                                    INSTALL_INCLUDE_DIRECTORIES\n                                        $<BUILD_INTERFACE:${NPU_UTILS_SOURCE_DIR}/include>)\n```\n\n----------------------------------------\n\nTITLE: Installing Latex and Graphviz on Ubuntu\nDESCRIPTION: This command installs the texlive-full and graphviz packages on Ubuntu systems using the apt-get package manager.  texlive-full provides a complete LaTeX environment, and graphviz is used for generating diagrams within the documentation. This is a necessary step to prepare the system for building the OpenVINO documentation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/building_documentation.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\napt-get install texlive-full graphviz\n```\n\n----------------------------------------\n\nTITLE: Add Tests Subdirectory\nDESCRIPTION: Conditionally adds the `tests` subdirectory to the build process if the `ENABLE_TESTS` option is enabled.  This allows for building and running tests for the Intel GPU plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/CMakeLists.txt#_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n  add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Shape Preallocation Check C++\nDESCRIPTION: This is the signature of the `can_preallocate` function in the ShapePredictor class. It takes the desired buffer size and returns if the ShapePredictor can preallocate desired size or not.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/dynamic_shape/memory_preallocation.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nShapePredictor::can_preallocate(size_t desired_buffer_size)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This CMake command adds the 'src' subdirectory to the build process. The source directory contains the C++ source files for the project, this command will make sure the target is built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Nodes' Output Buffer Preallocation C++\nDESCRIPTION: This code snippet demonstrates how to use the ShapePredictor to preallocate output buffers for nodes. It retrieves the current shape and data type, calls `predict_preallocation_shape` to get a predicted shape, and then checks if the predicted shape's memory can be allocated using `can_preallocate`. If successful, it updates the output layout with the predicted shape.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/dynamic_shape/memory_preallocation.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nauto current_shape = actual_layout.get_shape();\nauto& sp = *get_network().get_shape_predictor();\nauto dt_size = ov::element::Type(actual_layout.data_type).bitwidth();\n// Request prediction for `current_shape` and `actual_layout.data_type` data type\nauto prealloc_info = sp.predict_preallocation_shape(id(), current_shape, dt_size, can_reuse_buffer);\n// Check if shape was successfully predicted and there is enough free memory for preallocation\nif (prealloc_info.first && sp.can_preallocate(ov::shape_size(prealloc_info.second) * dt_size)) {\n    auto new_layout = actual_layout;\n    new_layout.set_partial_shape(prealloc_info.second);\n    // Update `updated_params` output layout which will be used for memory allocation\n    updated_params.output_layouts[0] = new_layout;\n}\n```\n\n----------------------------------------\n\nTITLE: OVSA Database Update\nDESCRIPTION: Updates the license server database with the generated license and customer certificate. It requires the ovsa.db database file, the model's license file, and the customer keystore certificate.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_46\n\nLANGUAGE: sh\nCODE:\n```\ncd /opt/ovsa/DB\npython3 ovsa_store_customer_lic_cert_db.py ovsa.db $OVSA_DEV_ARTEFACTS/<name-of-the-model>.lic $OVSA_DEV_ARTEFACTS/custkeystore.csr.crt\n```\n\n----------------------------------------\n\nTITLE: Querying Optimal Number of Requests in C++\nDESCRIPTION: This C++ snippet demonstrates how to query the `ov::optimal_number_of_infer_requests` property from a compiled model. This value is used to determine the optimal number of inference requests to run simultaneously for best performance with automatic batching.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/automatic-batching.rst#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nov::Core core;\nstd::shared_ptr<ov::Model> model = core.read_model(\"model.xml\");\nint num_requests = model->get_property(ov::properties::optimal_number_of_infer_requests);\nstd::cout << \"Optimal number of infer requests: \" << num_requests << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Define Model getOutputSize Method (TypeScript)\nDESCRIPTION: This code defines the `getOutputSize` method of the `Model` interface. It returns the number of model outputs as an array of numbers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\ngetOutputSize(): number[]\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory to Build\nDESCRIPTION: This line adds the 'src' subdirectory to the current CMake project, which allows CMake to find and process the CMakeLists.txt file within the src directory and include its build targets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/lib/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: LRN Square Sum Calculation (4D Data, Axes=[2,3])\nDESCRIPTION: Calculates the squared sum for local response normalization with a 4D input tensor and normalization along the second and third axes.  It shows how the normalization region is determined across multiple dimensions and how boundary conditions are handled using max/min operations to prevent out-of-bounds accesses.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/lrn-1.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsqr_sum[a, b, c, d] =\n    sum(data[a, b, max(0, c - size / 2) : min(data.shape[2], c + size / 2 + 1),  max(0, d - size / 2) : min(data.shape[3], d + size / 2 + 1)] ** 2)\noutput = data / (bias + (alpha / size ** len(axes)) * sqr_sum) ** beta\n```\n\n----------------------------------------\n\nTITLE: Equal-1 Operation with No Broadcast in OpenVINO XML\nDESCRIPTION: This example demonstrates the Equal-1 operation in OpenVINO with auto-broadcasting disabled. The input tensors must have matching shapes for element-wise comparison.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/equal-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Equal\">\n    <data auto_broadcast=\"none\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: InferRequest Class Header Declaration C++\nDESCRIPTION: This code snippet shows the declaration of a synchronous request class, inheriting from ov::ISyncInferRequest. It defines the class members for profiling, durations, backend input/output tensors, executable object, evaluation context, and variable states.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/synch-inference-request.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nclass InferRequest : public ov::ISyncInferRequest {\npublic:\n    InferRequest() : ov::ISyncInferRequest() {}\n\nprivate:\n    // Profiling related data\n    static constexpr size_t numOfStages = 4;\n    std::array<openvino::itt::handle_t, numOfStages> m_profiling_task;\n    std::array<std::chrono::duration<double, std::micro>, numOfStages> m_durations;\n\n    // Backend specific fields\n    std::vector<BackendInputTensor> m_backend_input_tensors;\n    std::vector<BackendOutputTensor> m_backend_output_tensors;\n    Executable m_executable;\n    EvaluationContext m_eval_context;\n    std::vector<VariableState> m_variable_states;\n};\n```\n\n----------------------------------------\n\nTITLE: Navigating to the release_files directory\nDESCRIPTION: Changes the current directory to the `release_files` directory where the packaged OVSA components are located.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_28\n\nLANGUAGE: sh\nCODE:\n```\ncd release_files\n```\n\n----------------------------------------\n\nTITLE: Clone OpenVINO Notebooks (Docker)\nDESCRIPTION: Clones the OpenVINO notebooks repository from GitHub and changes the current directory into the cloned repository.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_24\n\nLANGUAGE: console\nCODE:\n```\ngit clone https://github.com/openvinotoolkit/openvino_notebooks.git\ncd openvino_notebooks\n```\n\n----------------------------------------\n\nTITLE: Compiler Flags for MSVC CMake\nDESCRIPTION: Applies compiler-specific flags for MSVC. It disables treating warnings as errors (`/WX-`) and mitigates Spectre vulnerabilities (`/Qspectre-`) when using the MSVC compiler.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/level_zero/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    ov_add_compiler_flags(/WX-)\n    add_compile_options(\"/Qspectre-\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: NPU Enable Check CMake\nDESCRIPTION: This snippet checks if the ENABLE_INTEL_NPU flag is enabled. If not, it immediately returns, preventing the rest of the script from executing. This is a prerequisite for building the NPU plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT ENABLE_INTEL_NPU)\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Options with CMake\nDESCRIPTION: This snippet sets compiler options for the target library, disabling specific warnings when using GCC or Clang compilers. It disables `-Wno-undef`, `-Wno-deprecated-declarations`, and `-Wno-multichar` warnings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/itt_collector/sea_itt_lib/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCC OR OV_COMPILER_IS_CLANG)\n    target_compile_options(${TARGET_NAME} PRIVATE -Wno-undef -Wno-deprecated-declarations -Wno-multichar)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Navigating to Build Directory for OpenVINO Samples (macOS)\nDESCRIPTION: This command changes the current directory to the `build` directory previously created, preparing the environment for the CMake build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\ncd build\n```\n\n----------------------------------------\n\nTITLE: Get default RemoteContext from Core (C)\nDESCRIPTION: This snippet shows how to retrieve the default `ov::RemoteContext` from the OpenVINO `ov_core_t` object using the GPU plugin's C API. This is useful when you want to use the plugin's internal context. Requires the OpenVINO runtime and intel_gpu ocl headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_10\n\nLANGUAGE: c\nCODE:\n```\n//! [default_context_from_core]\n#include <openvino/runtime.h>\n#include <openvino/runtime/intel_gpu/ocl/ocl.h>\n\nvoid get_default_context_from_core() {\n    // default context from plugin\n    ov_core_t* core = ov_core_create();\n    ov_remote_context remote_context = ov_core_get_default_context(core, \"GPU\");\n    ov_remote_context_free(remote_context);\n    ov_core_free(core);\n}\n//! [default_context_from_core]\n```\n\n----------------------------------------\n\nTITLE: Apply Netplan Configuration\nDESCRIPTION: This command applies the network configuration generated by `netplan generate`. It activates the changes made to the network configuration files, creating the specified bridges and assigning IP addresses.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nsudo netplan apply\n```\n\n----------------------------------------\n\nTITLE: Defining Test Target with Dependencies in CMake\nDESCRIPTION: This CMake snippet defines a test target named `ov_subgraphs_dumper_tests` and specifies its source directory, include directories, and link libraries. It uses the `ov_add_test_target` macro to set up the test target with necessary dependencies such as `ov_subgraphs_dumper_util` and `func_test_utils`. The `ADD_CPPLINT` option enables cpplint checks, and the `LABELS` option adds the `OV UNIT` label.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/subgraphs_dumper/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME ov_subgraphs_dumper_tests)\n\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        INCLUDES\n            ${CMAKE_CURRENT_SOURCE_DIR}/\n        LINK_LIBRARIES\n            PRIVATE\n            ov_subgraphs_dumper_util\n            func_test_utils\n        ADD_CPPLINT\n        LABELS\n            OV UNIT\n)\n```\n\n----------------------------------------\n\nTITLE: isDynamic Method Definition (TypeScript)\nDESCRIPTION: Defines the `isDynamic` method of the `PartialShape` interface, which returns a boolean indicating whether the shape is dynamic. A dynamic shape has at least one dimension that is not fixed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PartialShape.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nisDynamic(): boolean\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name\nDESCRIPTION: Sets the name of the target to 'openvino_intel_gpu_graph'. This name is used to refer to the target throughout the CMake configuration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_intel_gpu_graph\")\n```\n\n----------------------------------------\n\nTITLE: List OpenVINO Packages\nDESCRIPTION: This command searches the ZYPPER repository for packages related to OpenVINO.  It lists available OpenVINO packages that can be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-zypper.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nzypper se openvino\n```\n\n----------------------------------------\n\nTITLE: Build GPU unit tests\nDESCRIPTION: This command builds the GPU unit tests after CMake configuration. It compiles the test sources and creates the executable for running the tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/gpu_plugin_unit_test.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake ov_gpu_unit_tests\n```\n\n----------------------------------------\n\nTITLE: Run TensorFlow Layer Test (sh)\nDESCRIPTION: This example demonstrates how to run a TensorFlow layer test for the `tf.raw_ops.Unique` operation on the CPU.  It sets the `TEST_DEVICE` environment variable to `CPU` and then executes the test using `pytest`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/layer_tests/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncd tests/layer_tests\nexport TEST_DEVICE=\"CPU\"\npytest tensorflow_tests/test_tf_Unique.py\n```\n\n----------------------------------------\n\nTITLE: Installing Directories\nDESCRIPTION: This snippet installs the 'scripts/' and '.automation/' directories to specific locations within the 'tests/stress_tests/' directory. The `COMPONENT tests` argument indicates that these files are part of the 'tests' component and the `EXCLUDE_FROM_ALL` option prevents them from being included in the default installation target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(DIRECTORY scripts/ DESTINATION tests/stress_tests/scripts COMPONENT tests EXCLUDE_FROM_ALL)\ninstall(DIRECTORY .automation/ DESTINATION tests/stress_tests/.automation COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Adding clang-format target\nDESCRIPTION: Adds a custom target for running clang-format on the source files of the library.  `ov_add_clang_format_target` is assumed to be a macro defined elsewhere.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/logger/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Protopipe Filtering Scenarios (YAML)\nDESCRIPTION: This configuration file shows how multiple inference scenarios can be defined, each running a different model. It provides a basis for filtering scenarios using the `-exec_filter` command-line option.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_dir:\n  local: /models/\ndevice_name: CPU\nmulti_inference:\n- input_stream_list:\n  - network:\n    - { name: A.xml }\n- input_stream_list:\n  - network:\n    - { name: B.xml }\n- input_stream_list:\n  - network:\n    - { name: C.xml }\n```\n\n----------------------------------------\n\nTITLE: Install Python Dependencies\nDESCRIPTION: This snippet specifies the Python dependencies required for the project. It includes constraints from a file, an extra index URL for PyTorch, and specific packages like optimum-intel and nncf. It also includes a direct installation from a GitHub repository for the whowhatbench tool.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/llm/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-c ../constraints.txt\n--extra-index-url https://download.pytorch.org/whl/cpu\noptimum-intel\nnncf\nwhowhatbench @ git+https://github.com/openvinotoolkit/openvino.genai.git#subdirectory=tools/who_what_benchmark\npytest\n```\n\n----------------------------------------\n\nTITLE: Setting Build Type\nDESCRIPTION: Defines the default build type as 'Release'. It also sets the available build types for selection via the CMake GUI or command line.  The build type influences optimization levels and debug information.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"CMake build type\")\nset_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Release\" \"Debug\" \"RelWithDebInfo\" \"MinSizeRel\")\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Intel GPU OpenCL Target in CMake\nDESCRIPTION: This snippet defines and adds a backend target named 'openvino_intel_gpu_ocl_obj' for OpenVINO, specifically tailored for Intel GPUs using OpenCL. It relies on the `ov_gpu_add_backend_target` CMake macro provided by the OpenVINO project to handle the target creation and integration into the build system. The macro likely sets necessary compile flags, links libraries, and defines dependencies for the OpenCL backend.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/ocl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_intel_gpu_ocl_obj\")\n\nov_gpu_add_backend_target(\n    NAME ${TARGET_NAME}\n)\n```\n\n----------------------------------------\n\nTITLE: Mark Target as C++\nDESCRIPTION: This snippet marks the target as a C++ target. This helps CMake to properly handle the C++ source files and dependencies.  `ov_mark_target_as_cc` is a custom CMake function (presumably defined elsewhere).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nov_mark_target_as_cc(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Adding Cpplint Target in CMake\nDESCRIPTION: This code snippet adds a cpplint target that runs cpplint on the source code of `TARGET_NAME`. The `add_cpplint_target` macro likely comes from a custom CMake module within the OpenVINO project. It ensures code style compliance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/offline_transformations/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nadd_cpplint_target(${TARGET_NAME}_cpplint FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Disable Shared Libraries and Google Test Installation\nDESCRIPTION: This code snippet disables the building of shared libraries and prevents Google Test from being installed as a separate component. This configuration is suitable when Google Test is used internally within the project and doesn't need to be exposed as a standalone library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/gtest/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(BUILD_SHARED_LIBS OFF)\nset(INSTALL_GTEST OFF CACHE BOOL \"\" FORCE)\n```\n\n----------------------------------------\n\nTITLE: EmbeddingBagOffsets Example 1 (XML)\nDESCRIPTION: This XML snippet provides an example configuration for the EmbeddingBagOffsets operation with the reduction attribute set to \"sum\", per_sample_weights provided, and default_index set to 0.  It demonstrates how to configure the layer with specific input tensors and attributes to achieve a desired output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sparse/embedding-bag-offsets-15.rst#_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"EmbeddingBagOffsets\" ... >\n       <data reduction=\"sum\"/>\n       <input>\n           <port id=\"0\">     <!-- emb_table value is: [[-0.2, -0.6], [-0.1, -0.4], [-1.9, -1.8], [-1.,  1.5], [ 0.8, -0.7]] -->\n               <dim>5</dim>\n               <dim>2</dim>\n           </port>\n           <port id=\"1\">     <!-- indices value is: [0, 2, 3, 4] -->\n               <dim>4</dim>\n           </port>\n           <port id=\"2\">     <!-- offsets value is: [0, 2, 2] - 3 \"bags\" containing [2,0,4-2] elements, second \"bag\" is empty -->\n               <dim>3</dim>\n           </port>\n           <port id=\"3\"/>    <!-- default_index value is: 0 -->\n           <port id=\"4\"/>    <!-- per_sample_weights value is: [0.5, 0.5, 0.5, 0.5] -->\n               <dim>4</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">     <!-- output value is: [[-1.05, -1.2], [-0.2, -0.6], [-0.1, 0.4]] -->\n               <dim>3</dim>\n               <dim>2</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Building and installing OpenVINO\nDESCRIPTION: This snippet uses the `make` command to build and install the OpenVINO project. The `-j$(nproc)` option enables parallel compilation using all available processors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_riscv64.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nmake install -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Install Python dependencies\nDESCRIPTION: This command installs the necessary Python dependencies for running the conformance tests summary scripts.  It uses pip3 to install the requirements listed in the requirements.txt file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/openvino/src/tests/test_utils/functional_test_utils/layer_tests_summary\npip3 install -r [requirements.txt](./../../../../../tests/test_utils/functional_test_utils/layer_tests_summary/requirements.txt)\n```\n\n----------------------------------------\n\nTITLE: Add Static Library CMake\nDESCRIPTION: Creates a static library target named `${TARGET_NAME}` using the discovered source and header files. A static library is a collection of compiled object files that are linked into other programs at compile time.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/onnx_common/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${LIBRARY_SRC} ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Build Wheel Command with pip or build - CMake\nDESCRIPTION: This CMake code defines the command used to build the Python wheel. It checks if the pip version is greater than or equal to 22.0 and uses `pip wheel` if it is; otherwise, it uses `python -m build`. It sets build options for build number and platform name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(pip_version VERSION_GREATER_EQUAL 22.0)\n    set(wheel_build_command\n        ${Python3_EXECUTABLE} -m pip wheel\n            --no-deps\n            --wheel-dir ${openvino_wheels_output_dir}\n            --verbose\n            --build-option --build-number=${WHEEL_BUILD}\n            --build-option --plat-name=${PLATFORM_TAG}\n            \"${CMAKE_CURRENT_SOURCE_DIR}\")\nelse()\n    # for --config-setting explanation see https://github.com/pypa/setuptools/issues/2491\n    set(wheel_build_command\n        ${Python3_EXECUTABLE} -m build \"${CMAKE_CURRENT_SOURCE_DIR}\"\n        --outdir ${openvino_wheels_output_dir}\n        --config-setting=--build-option=--build-number=${WHEEL_BUILD}\n        --config-setting=--build-option=--plat-name=${PLATFORM_TAG}\n        --config-setting=--quiet\n        --wheel)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Create Build Directory\nDESCRIPTION: Creates a `build` directory and navigates into it. This is where the CMake build files will be generated.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/documentation_build_instructions.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\n(env) $ mkdir build && cd build\n```\n\n----------------------------------------\n\nTITLE: Deleting OpenVINO Files - Shell\nDESCRIPTION: This snippet removes the extracted folder and archive file associated with the OpenVINO installation. The `<extracted_folder>` should be replaced with the actual path to the extracted folder, and `<path_to_archive>` should be replaced with the path to the archive file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\nrm -r <extracted_folder> && rm <path_to_archive>\n```\n\n----------------------------------------\n\nTITLE: Building Protoc Natively for Cross-Compilation\nDESCRIPTION: This snippet handles the native build of `protoc` when `protobuf_BUILD_PROTOC_BINARIES` is disabled (typically in cross-compilation scenarios). It uses `ov_native_compile_external_project` to build `protoc` and associated libraries in a separate native build. It then sets up an imported target `protobuf::protoc` to refer to the natively built executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/protobuf/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT protobuf_BUILD_PROTOC_BINARIES)\n    set(HOST_PROTOC_INSTALL_DIR \"${CMAKE_CURRENT_BINARY_DIR}/install\")\n\n    ov_native_compile_external_project(\n        TARGET_NAME host_protoc\n        NATIVE_INSTALL_DIR \"${HOST_PROTOC_INSTALL_DIR}\"\n        CMAKE_ARGS \"-Dprotobuf_VERBOSE=${protobuf_VERBOSE}\"\n                   \"-Dprotobuf_BUILD_TESTS=${protobuf_BUILD_TESTS}\"\n                   \"-Dprotobuf_WITH_ZLIB=${protobuf_WITH_ZLIB}\"\n        NATIVE_SOURCE_SUBDIR \"${protobuf_dir}\"\n        NATIVE_TARGETS protoc libprotobuf-lite)\n\n    set(PROTOC_EXECUTABLE \"${HOST_PROTOC_INSTALL_DIR}/bin/protoc\")\n    add_executable(protobuf::protoc IMPORTED GLOBAL)\n    set_property(TARGET protobuf::protoc APPEND PROPERTY IMPORTED_CONFIGURATIONS RELEASE)\n    set_target_properties(protobuf::protoc PROPERTIES\n        IMPORTED_LOCATION_RELEASE \"${PROTOC_EXECUTABLE}\")\n    set_target_properties(protobuf::protoc PROPERTIES\n        MAP_IMPORTED_CONFIG_DEBUG Release\n        MAP_IMPORTED_CONFIG_MINSIZEREL Release\n        MAP_IMPORTED_CONFIG_RELWITHDEBINFO Release)\n    add_dependencies(protobuf::protoc host_protoc)\n\n    set(PROTOC_DEPENDENCY host_protoc PARENT_SCOPE)\n    set(PROTOC_EXECUTABLE \"${PROTOC_EXECUTABLE}\" PARENT_SCOPE)\nelse()\n    set(PROTOC_EXECUTABLE $<TARGET_FILE:protoc> PARENT_SCOPE)\n    set(PROTOC_DEPENDENCY protoc PARENT_SCOPE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name CMake\nDESCRIPTION: This snippet sets the target name for the library being built. The target name is stored in the TARGET_NAME variable and is used in subsequent CMake commands to refer to the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/ov_helpers/ov_lpt_models/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME ov_lpt_models)\n```\n\n----------------------------------------\n\nTITLE: Slice: Slicing with Step 2 in OpenVINO XML\nDESCRIPTION: This example illustrates slicing a 1D tensor with a step value of 2. The `step` parameter controls the increment between each index used in slicing, effectively selecting every other element within the specified start and stop range.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-8.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Slice\" ...>\n       <input>\n           <port id=\"0\">       <!-- data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -->\n             <dim>10</dim>\n           </port>\n           <port id=\"1\">       <!-- start: [1] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"2\">       <!-- stop: [8] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"3\">       <!-- step: [2] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"4\">       <!-- axes: [0] -->\n             <dim>1</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">       <!-- output: [1, 3, 5, 7] -->\n               <dim>4</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries (CMake)\nDESCRIPTION: This snippet links the openvino_npu_zero_utils library to the openvino::runtime::dev library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/zero/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino::runtime::dev)\n```\n\n----------------------------------------\n\nTITLE: Copying ovsa-model-hosting.tar.gz to Guest VM\nDESCRIPTION: Copies the ovsa-model-hosting.tar.gz file from the host machine to the /OVSA directory of the specified user's home directory on the guest VM using the scp command.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_36\n\nLANGUAGE: sh\nCODE:\n```\ncd $OVSA_RELEASE_PATH\nscp ovsa-model-hosting.tar.gz username@<runtime-vm-ip-address>:/<username-home-directory>/OVSA\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Compiler Flags\nDESCRIPTION: This snippet conditionally sets the C++ compiler flag '-Wno-suggest-override' if 'SUGGEST_OVERRIDE_SUPPORTED' is enabled.  It suppresses warnings related to suggested overrides during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/unit/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(SUGGEST_OVERRIDE_SUPPORTED)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-suggest-override\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: ModelPass Template Transformation Implementation C++\nDESCRIPTION: This code snippet provides the implementation of the template ModelPass transformation in C++. It shows a basic structure for the run_on_model method and how to return a boolean value indicating whether the model has been modified. The transformation acts on an ov::Model and potentially alters its structure or attributes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/model-pass.rst#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nbool TemplateModelPass::run_on_model(const std::shared_ptr<ov::Model>& model) {\n    // Add transformation code here\n    return false;\n}\n```\n\n----------------------------------------\n\nTITLE: Packing PDB Files with CMake\nDESCRIPTION: This snippet uses CMake to pack PDB files into a dedicated archive after the build process.  It's crucial for debugging and requires specifying the build folder, configuration, installation path, and the 'pdb' component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_windows.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncmake --install <build folder> --config <Release | Debug | RelWithDebInfo> --prefix <installation path> --component pdb\n```\n\n----------------------------------------\n\nTITLE: Cloning vcpkg repository\nDESCRIPTION: This command clones the vcpkg repository from GitHub, which is a package manager for C++ libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/use_device_mem.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n> git clone https://github.com/microsoft/vcpkg\n```\n\n----------------------------------------\n\nTITLE: Configure Level Zero Extension Library (CMake)\nDESCRIPTION: This snippet creates an interface library called `level-zero-ext` when `ENABLE_NPU_PLUGIN_ENGINE` is enabled. It sets the interface include directories for the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/thirdparty/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_NPU_PLUGIN_ENGINE)\n    add_library(level-zero-ext INTERFACE)\n    set_property(TARGET level-zero-ext APPEND PROPERTY INTERFACE_INCLUDE_DIRECTORIES $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/level-zero-ext/>)\n    add_library(LevelZero::NPUExt ALIAS level-zero-ext)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Properties of CompiledModel (TypeScript)\nDESCRIPTION: This code shows the declaration of the `setProperty` method within the `CompiledModel` interface. This method allows setting properties on the compiled model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nsetProperty(properties: Record<string, OVAny>): void\n```\n\n----------------------------------------\n\nTITLE: Defining OutputTensorInfo Interface\nDESCRIPTION: Defines the `OutputTensorInfo` interface with methods for setting element type and layout.  `setElementType` configures the element data type and `setLayout` sets the tensor layout.  Both methods return an `InputTensorInfo` object, allowing method chaining.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/OutputTensorInfo.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface OutputTensorInfo {\n    setElementType(elementType): InputTensorInfo;\n    setLayout(layout): InputTensorInfo;\n}\n```\n\n----------------------------------------\n\nTITLE: Eye Operation Example 2 (XML)\nDESCRIPTION: Demonstrates the Eye operation with batch shape specified. It defines the layer to generate a batch of matrices with shape [2, 3, -1, -1] and output type f32.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/eye-9.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... name=\"Eye\" type=\"Eye\">\n    <data output_type=\"f32\"/>\n    <input>\n        <port id=\"0\" precision=\"I32\"/>  <!-- num rows -->\n        <port id=\"1\" precision=\"I32\"/>  <!-- num columns -->\n        <port id=\"2\" precision=\"I32\"/>  <!-- diagonal index -->\n        <port id=\"3\" precision=\"I32\"/>  <!-- batch_shape : [2, 3] -->\n    </input>\n    <output>\n        <port id=\"3\" precision=\"F32\" names=\"Eye:0\">\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>-1</dim>\n            <dim>-1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: SoftMax Layer XML Configuration\nDESCRIPTION: This XML snippet demonstrates the configuration of a SoftMax layer in OpenVINO. It shows how to specify the 'axis' attribute, which determines the dimension along which the SoftMax function is applied. The input and output layers are also indicated.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/softmax-8.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"SoftMax\" ... >\n    <data axis=\"1\" />\n    <input> ... </input>\n    <output> ... </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Adding a Test Target using CMake Macro\nDESCRIPTION: This CMake macro, 'ov_add_test_target', creates the test target.  It takes parameters such as the name of the target, the source directory, dependencies, link libraries, and whether to add cpplint checks. It also sets labels 'OV' and 'UNIT'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/common_test_utils/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_test_target(\n    NAME ${TARGET_NAME}\n    ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n    DEPENDENCIES\n    LINK_LIBRARIES\n      common_test_utils\n      func_test_utils\n      sharedTestClasses\n    ADD_CPPLINT\n    LABELS\n      OV UNIT\n)\n```\n\n----------------------------------------\n\nTITLE: AUGRU Formula in Python\nDESCRIPTION: This snippet presents the mathematical formulas that define the AUGRU cell's computations. It describes how the update, reset, and hidden gates are calculated, incorporating the attention score for modified update gate. The formulas use matrix multiplications and element-wise (Hadamard) products with activation functions to compute the next hidden state.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/internal/augru-sequence.rst#_snippet_0\n\nLANGUAGE: py\nCODE:\n```\nAUGRU formula:\n  *  - matrix multiplication\n (.) - Hadamard product (element-wise)\n\nf, g - activation functions\nz - update gate, r - reset gate, h - hidden gate\na - attention score\n\n rt = f(Xt*(Wr^T) + Ht-1*(Rr^T) + Wbr + Rbr)\n zt = f(Xt*(Wz^T) + Ht-1*(Rz^T) + Wbz + Rbz)\n ht = g(Xt*(Wh^T) + (rt (.) Ht-1)*(Rh^T) + Rbh + Wbh)  # 'linear_before_reset' is False\n\n zt' = (1 - at) (.) zt  # multiplication by attention score\n\n Ht = (1 - zt') (.) ht + zt' (.) Ht-1\n```\n\n----------------------------------------\n\nTITLE: Configuring Git User Information\nDESCRIPTION: This code shows how to configure the user name and email in Git. This information is used by the Git hook to create the 'Signed-off-by' line. Ensure these are set correctly before committing.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/commit_signoff_policy.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ngit config user.name 'FIRST_NAME LAST_NAME'\ngit config user.email 'MY_EMAIL@example.com'\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for Tests\nDESCRIPTION: Adds subdirectories containing test-related code to the build process. These subdirectories likely contain source code, CMakeLists.txt files, and other resources necessary for running tests.  Requires CMake.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(ov_helpers)\nadd_subdirectory(test_utils)\n\nif(ENABLE_FUNCTIONAL_TESTS)\n    add_subdirectory(functional)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name in CMake\nDESCRIPTION: Sets the target name for the library being built. This variable is later used to define the library and its properties.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME StressTestsCommon)\n```\n\n----------------------------------------\n\nTITLE: Setting Root Directory and Enabling Testing in CMake\nDESCRIPTION: Sets the root directory for OpenVINO tests and enables testing functionality in CMake.  This allows for the discovery and execution of tests defined within the project. Requires CMake.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(OV_TESTS_ROOT ${CMAKE_CURRENT_SOURCE_DIR})\n\nenable_testing()\n```\n\n----------------------------------------\n\nTITLE: Define Pybind11 Module Source File\nDESCRIPTION: This line sets the source file for the Pybind11 module.  It specifies the path to the C++ file that defines the Python bindings for the mock frontend API.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/pyngraph_fe_mock_api/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(PYBIND_FE_SRC ${CMAKE_CURRENT_SOURCE_DIR}/pyngraph_mock_frontend_api.cpp)\n```\n\n----------------------------------------\n\nTITLE: Dependency List\nDESCRIPTION: This text file lists Python package dependencies. In this case, the dependencies are numpy and tensorflow. These packages must be installed to ensure the project operates correctly.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/tests/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nnumpy\ntensorflow\n```\n\n----------------------------------------\n\nTITLE: Installing Header Files for OpenVINO Core Dev API\nDESCRIPTION: This snippet installs the header files for the OpenVINO Core Dev API to the specified destination directory. It only includes `.hpp` and `.h` files, and the component is `OV_CPACK_COMP_CORE_DEV`. Also, it uses `OV_CPACK_COMP_CORE_DEV_EXCLUDE_ALL` to exclude all files by default.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_20\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/include/\n        DESTINATION ${OV_CPACK_INCLUDEDIR}\n        COMPONENT ${OV_CPACK_COMP_CORE_DEV}\n        ${OV_CPACK_COMP_CORE_DEV_EXCLUDE_ALL}\n        FILES_MATCHING\n            PATTERN \"*.hpp\"\n            PATTERN \"*.h\")\n```\n\n----------------------------------------\n\nTITLE: GatherND Layer Configuration Example 1\nDESCRIPTION: Illustrates the XML configuration for a GatherND layer with batch_dims set to 0. This example demonstrates how the layer is defined in the model graph, including input and output port dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-8.rst#_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"GatherND\" version=\"opset8\">\n    <data batch_dims=\"0\" />\n    <input>\n        <port id=\"0\">\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>10</dim>\n            <dim>15</dim>\n        </port>\n        <port id=\"1\">\n            <dim>25</dim>\n            <dim>125</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>25</dim>\n            <dim>125</dim>\n            <dim>15</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing System Dependencies on Amazon SageMaker\nDESCRIPTION: This code snippet installs the necessary system dependencies for running OpenVINO notebooks on Amazon SageMaker. It includes updating the package list and installing build-essential, development libraries for Python 3.9, and OpenGL libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\napt update\napt install build-essential -y\napt install libpython3.9-dev -y\napt install libgl1-mesa-glx -y\n```\n\n----------------------------------------\n\nTITLE: Add Source Subdirectory with CMake\nDESCRIPTION: Adds the 'src' subdirectory to the project. This allows CMake to process the CMakeLists.txt file within the 'src' directory and include the source code in the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: This snippet sets the private include directories for the object library `${TARGET_NAME}_obj`. It includes the public headers directory and the source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}_obj PRIVATE \"${PUBLIC_HEADERS_DIR}\"\n                                                      \"${CMAKE_CURRENT_SOURCE_DIR}/src\")\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCL Static Library CMake\nDESCRIPTION: This snippet installs the OpenCL static library as part of the core component of the OpenVINO package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ocl/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(OpenCL ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: Define Input Parameter - Name (TypeScript)\nDESCRIPTION: Defines the 'name' parameter as a string for the input function, allowing selection of input by name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nname: string\n```\n\n----------------------------------------\n\nTITLE: Configure CMake with vcpkg toolchain file\nDESCRIPTION: This command configures the CMake build using the vcpkg toolchain file. It specifies the build directory, source directory, and the path to the vcpkg toolchain file, enabling CMake to use vcpkg for dependency management.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-vcpkg.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncmake -B <build dir> -S <source dir> -DCMAKE_TOOLCHAIN_FILE=<VCPKG_ROOT>/scripts/buildsystems/vcpkg.cmake\n```\n\n----------------------------------------\n\nTITLE: CMake Main Configuration\nDESCRIPTION: This snippet configures the CMake project for the OpenVINO Template Plugin. It sets the minimum required CMake version, finds the OpenVINODeveloperPackage, adds compiler flags, includes source directories, sets up testing if enabled, and configures packaging.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\n\nproject(OpenVINOTemplatePlugin)\n\nfind_package(OpenVINODeveloperPackage REQUIRED)\n\nov_option(ENABLE_TEMPLATE_REGISTRATION \"Enables registration of TEMPLATE plugin\" OFF)\n\nif(CMAKE_COMPILER_IS_GNUCXX)\n    ov_add_compiler_flags(-Wall)\nendif()\n\nadd_subdirectory(src)\n\nif(ENABLE_TESTS)\n    include(CTest)\n    enable_testing()\n\n    if(ENABLE_FUNCTIONAL_TESTS)\n        add_subdirectory(tests/functional)\n    endif()\nendif()\n\n# install\n\nif(OpenVINODeveloperPackage_FOUND)\n    ov_cpack(template)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Average Counters\nDESCRIPTION: This environment variable specifies the filename to store average counter data. This helps in profiling and analyzing the performance of the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nOV_CPU_AVERAGE_COUNTERS=filename\n```\n\n----------------------------------------\n\nTITLE: Jinja2 Template for Class Documentation\nDESCRIPTION: This Jinja2 template generates reStructuredText documentation for a Python class. It includes sections for class members, inheritance, methods (including __init__), and attributes, using reStructuredText directives for automatic summary and method generation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/openvino_sphinx_theme/openvino_sphinx_theme/templates/custom-class-template.rst#_snippet_0\n\nLANGUAGE: Jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n   :members:\n   :show-inheritance:\n   :inherited-members:\n\n   {% block methods %}\n   .. automethod:: __init__\n\n   {% if methods %}\n   .. rubric:: Methods\n\n   .. autosummary::\n   {% for met in methods %}\n      ~{{ name }}.{{ met }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block attributes %}\n   {% if attributes %}\n   .. rubric:: Attributes\n\n   .. autosummary::\n   {% for att in attributes %}\n      ~{{ name }}.{{ att }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Declaring the shape Property in Output Interface\nDESCRIPTION: Defines the `shape` property within the `Output` interface as an array of numbers. This property represents the dimensions or shape of the model output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Output.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nshape: number[]\n```\n\n----------------------------------------\n\nTITLE: Adding BitBake Layers\nDESCRIPTION: Adds the required layers to the BitBake configuration using the `bitbake-layers add-layer` command. These layers include meta-intel, meta-openembedded (with meta-oe and meta-python), and meta-clang, which provide additional recipes and configurations for the Yocto build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yocto.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nbitbake-layers add-layer ../meta-intel\nbitbake-layers add-layer ../meta-openembedded/meta-oe\nbitbake-layers add-layer ../meta-openembedded/meta-python\nbitbake-layers add-layer ../meta-clang\n```\n\n----------------------------------------\n\nTITLE: Navigate to npm Package Directory (Bash)\nDESCRIPTION: This command changes the current directory to the `openvino-node` npm package directory, preparing for subsequent npm commands like installing dependencies and running tests.  This puts the terminal in the correct location for further npm commands.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd ../src/bindings/js/node\n```\n\n----------------------------------------\n\nTITLE: Installing Rosetta\nDESCRIPTION: This command installs Rosetta, which is required to run x86_64 binaries on Apple Silicon. It must be executed before building OpenVINO for x86_64 architecture on an arm64 macOS system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\nsoftwareupdate --install-rosetta\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries CMake\nDESCRIPTION: This snippet links the 'openvino_cnpy' library against the 'openvino::zlib' library, indicating a dependency on the zlib compression library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/cnpy/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino::zlib)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name\nDESCRIPTION: Defines the name of the target library as 'test_utils_api'. This name is used throughout the CMake script to refer to the library being built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"test_utils_api\")\n```\n\n----------------------------------------\n\nTITLE: Disabling CPU Plugin features using OV_CPU_DISABLE\nDESCRIPTION: The OV_CPU_DISABLE environment variable allows disabling specific features in the OpenVINO CPU plugin. Options are space-separated and processed from left to right, with later options overriding earlier ones. This snippet shows the general syntax.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/feature_disabling.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nOV_CPU_DISABLE=<space_separated_options> binary ...\n```\n\n----------------------------------------\n\nTITLE: File Structure of OpenVINO GPU Plugin\nDESCRIPTION: This snippet shows the file structure of the OpenVINO GPU plugin, including directories for internal headers, graph representations, OpenCL kernels, plugin implementation, and runtime.  It also includes third-party dependencies such as oneDNN and RapidJSON.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/source_code_structure.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nsrc/plugins/intel_gpu                  - root GPU plugin folder\n             ├── include\n             │   ├── intel_gpu         - library internal headers\n             │   │   ├── graph         - headers for internal graph representations\n             │   │   ├── plugin        - definition of classes required for OpenVINO plugin API implementation\n             │   │   ├── primitives    - primitive definitions for all supported operations\n             │   │   └── runtime       - abstraction for execution runtime entities (memory, device, engine, etc)\n             │   └── va\n             ├── src\n             │   ├── graph - all sources related to internal graph representation\n             │   │    ├── graph_optimizer - passes for graph transformations\n             │   │    ├── impls - definition of primitive implementations\n             │   │    └── include - headers with graph nodes\n             │   │\n             │   ├── kernel_selector - OpenCL™ kernels (host+device parts) + utils for optimal kernels selection\n             │   │   ├── common      - definition of some generic classes/structures used in kernel_selector\n             │   │   └── core        - kernels, kernel selectors, and kernel parameters definitions\n             │   │       ├── actual_kernels  - host side part of OpenCL™ kernels including applicability checks, performance heuristics and Local/Global work-groups description\n             │   │       ├── cache  - cache.json - tuning cache of the kernels which is redistributed with the plugin to improve kernels and kernel parameters selection for better performance\n             │   │       ├── cl_kernels - templates of GPU kernels (device part) written on OpenCL™\n             │   │       └── common - utils for code generation and kernels selection\n             │   ├── plugin - implementation of OpenVINO plugin API\n             │   │    └── ops - factories for conversion of OpenVINO operations to internal primitives\n             │   └── runtime\n             │        └── ocl/ - implementation for OpenCL™ based runtime\n             ├── tests\n             │   ├── test_cases\n             │   └── test_utils\n             └── thirdparty\n                 ├── onednn_gpu - <a href=\"https://github.com/oneapi-src/oneDNN\">oneDNN</a> submodule which may be used to accelerate some primitives\n                 └── rapidjson  - thirdparty <a href=\"https://github.com/Tencent/rapidjson\">RapidJSON</a> lib for reading json files (cache.json)\n```\n\n----------------------------------------\n\nTITLE: OneHot Layer Definition Example 1 XML\nDESCRIPTION: This XML code defines a OneHot layer in OpenVINO. It specifies the axis attribute as -1, indicating the last dimension. The input ports define the indices, depth, on_value, and off_value. The output port defines the shape of the output tensor after applying the OneHot operation. This example demonstrates a basic usage scenario.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sequence/one-hot-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"OneHot\" ...>\n    <data axis=\"-1\"/>\n    <input>\n        <port id=\"0\">    <!-- indices value: [0, 3, 1, 2] -->\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">    <!-- depth value: 3 -->\n        </port>\n        <port id=\"2\">    <!-- on_value 1 -->\n        </port>\n        <port id=\"3\">    <!-- off_value 2 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">    <!-- output value # [[1, 2, 2], [2, 2, 2], [2, 1, 2], [2, 2, 1]] -->\n            <dim>4</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Importing Model Synchronously\nDESCRIPTION: Synchronously imports a previously exported compiled model from a model stream. This is a synchronous version of the asynchronous importModel method. The config parameter allows specifying properties relevant only for this load operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Core.rst#_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimportModelSync(modelStream, device, config?): CompiledModel\n```\n\n----------------------------------------\n\nTITLE: AvgPool Configuration: same_upper, exclude-pad true\nDESCRIPTION: This XML snippet configures an AvgPool layer with 'same_upper' auto_pad and 'exclude-pad' set to true. It defines the kernel size as 2x2, stride as 2x2, and shows the input/output port dimensions. This means that padding is added to the top and left of the input to match the output size and zero-values from padding are not included in averaging calculation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/avg-pool-14.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"AvgPool\" ... >\n    <data auto_pad=\"same_upper\" exclude-pad=\"true\" kernel=\"2,2\" pads_begin=\"0,0\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: TopK Configuration Example\nDESCRIPTION: Illustrates the TopK configuration parameters with mode set to min, sort to index, and k to 4, used in conjunction with the previous example to showcase TopK's behavior.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/top-k-11.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nmode = min\nsort = index\nk = 4\n```\n\n----------------------------------------\n\nTITLE: Defining Project Name\nDESCRIPTION: This snippet defines the project name as 'time_tests'. It is a fundamental step in any CMake project, setting the name that will be used for the generated build system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nproject(time_tests)\n```\n\n----------------------------------------\n\nTITLE: Set environment variables for building\nDESCRIPTION: Sets several environment variables required for building OpenVINO for Android. These variables specify the target Android ABI (Architecture Binary Interface), platform version, STL (Standard Template Library), and the CMake toolchain file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_android.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# If you have no android devices please set CURRENT_ANDROID_ABI according to your preferences e.g. export CURRENT_ANDROID_ABI=arm64-v8a\nexport CURRENT_ANDROID_ABI=`$ANDROID_TOOLS_PATH/adb shell getprop ro.product.cpu.abi`\nexport CURRENT_ANDROID_PLATFORM=30\nexport CURRENT_ANDROID_STL=c++_shared\nexport CURRENT_CMAKE_TOOLCHAIN_FILE=$ANDROID_NDK_PATH/build/cmake/android.toolchain.cmake\n```\n\n----------------------------------------\n\nTITLE: Add Post-Build Command for Cleanup - CMake\nDESCRIPTION: This CMake code adds a post-build command to the `ie_wheel` target to remove the build directory used for the Python wheel. This helps to keep the build directory clean.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(\n    TARGET ie_wheel\n    POST_BUILD\n    COMMAND ${CMAKE_COMMAND} -E remove_directory \"${CMAKE_CURRENT_SOURCE_DIR}/build_${pyversion}\" || ${CMAKE_COMMAND} -E true # w/a this ensures the command always succeeds\n    WORKING_DIRECTORY \"${CMAKE_CURRENT_SOURCE_DIR}\"\n    COMMENT \"Cleaning up Python wheel build for wheel ${openvino_wheel_name}\"\n    VERBATIM)\n```\n\n----------------------------------------\n\nTITLE: Add Snippets Models Subdirectory CMake\nDESCRIPTION: Conditionally adds the `ov_snippets_models` subdirectory if the `openvino::snippets` target is defined.  This ensures that snippets models are only included when the snippets functionality is available.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/ov_helpers/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(TARGET openvino::snippets)\n    add_subdirectory(ov_snippets_models)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Create Virtual Disk Image\nDESCRIPTION: This command creates an empty virtual disk image using qemu-img in qcow2 format. The image will be used as the disk for the Guest VM (ovsa_isv_dev_vm_disk.qcow2) and has a size of 20GB.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\nsudo qemu-img create -f qcow2 <path>/ovsa_isv_dev_vm_disk.qcow2 20G\n```\n\n----------------------------------------\n\nTITLE: Installing pre-production dependencies on Linux\nDESCRIPTION: This snippet installs a pre-production version of OpenVINO and related packages. It uses the `--pre` flag and specifies an extra index URL to access nightly builds.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/inference-with-genai/inference-with-genai-on-npu.rst#_snippet_1\n\nLANGUAGE: Console\nCODE:\n```\npip install --pre openvino openvino-tokenizers openvino-genai --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n```\n\n----------------------------------------\n\nTITLE: Conditional Object Library Setting\nDESCRIPTION: If 'BUILD_SHARED_LIBS' is enabled, this sets the 'OBJ_LIB' variable to the object files of the 'openvino_intel_cpu_plugin_obj' target. This is used for linking when building shared libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_SHARED_LIBS)\n    set (OBJ_LIB $<TARGET_OBJECTS:openvino_intel_cpu_plugin_obj>)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for OpenVINO Sample\nDESCRIPTION: This snippet configures the build process for the 'hello_nv12_input_classification' sample using the `ov_add_sample` CMake macro. It specifies the sample's name, source files, and the required dependencies, including 'format_reader' and 'ie_samples_utils'. This setup allows CMake to properly compile and link the sample with the OpenVINO libraries.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_nv12_input_classification/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_sample(NAME hello_nv12_input_classification\n              SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/main.cpp\"\n              DEPENDENCIES format_reader ie_samples_utils)\n```\n\n----------------------------------------\n\nTITLE: Adding Python Samples Component in CMake\nDESCRIPTION: Adds a component for Python samples to the OpenVINO packaging system. This component is marked as hidden.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nov_cpack_add_component(${OV_CPACK_COMP_PYTHON_SAMPLES} HIDDEN)\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library in CMake\nDESCRIPTION: This snippet creates an object library from the collected source files, headers, and public headers. It then adds compile definitions based on whether a shared library is being built and applies a unity build for faster compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME}_obj OBJECT\n            ${LIBRARY_SRC}\n            ${LIBRARY_HEADERS}\n            ${PUBLIC_HEADERS})\n\n\nif(NOT BUILD_SHARED_LIBS)\n    target_compile_definitions(${TARGET_NAME}_obj PUBLIC OPENVINO_STATIC_LIBRARY)\nendif()\n\nov_build_target_faster(${TARGET_NAME}_obj\n    UNITY PCH PRIVATE \"src/precomp.hpp\"\n)\n```\n\n----------------------------------------\n\nTITLE: Define Pybind11 Module Name\nDESCRIPTION: This line sets the name of the Pybind11 frontend module to 'pybind_mock_frontend'. This name is later used when creating the module and setting up related targets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/pyngraph_fe_mock_api/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(PYBIND_FE_NAME \"pybind_mock_frontend\")\n```\n\n----------------------------------------\n\nTITLE: Determine Platform Tag for macOS - CMake\nDESCRIPTION: This CMake code block determines the platform tag for macOS. It checks for `CMAKE_OSX_DEPLOYMENT_TARGET` or extracts version information from the existing `PLATFORM_TAG` and constructs a new `PLATFORM_TAG` in the format `macosx_<macos major>_<macos minor>_<arch>`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(APPLE)\n    _ov_platform_arch()\n\n    if(CMAKE_OSX_DEPLOYMENT_TARGET)\n        set(_macos_target_version \"${CMAKE_OSX_DEPLOYMENT_TARGET}\")\n        if(_macos_target_version MATCHES \"^1[0-9]$\")\n            set(_macos_target_version \"${CMAKE_OSX_DEPLOYMENT_TARGET}.0\")\n        endif()\n        string(REPLACE \".\" \"_\" _macos_target_version \"${_macos_target_version}\")\n    else()\n        string(REGEX MATCH \"1[0-9]_[0-9]+\" _macos_target_version ${PLATFORM_TAG})\n    endif()\n\n    # common platform tag looks like macosx_<macos major>_<macos minor>_<arch>\n    if(_arch AND _macos_target_version)\n        set(PLATFORM_TAG \"macosx_${_macos_target_version}_${_arch}\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Custom Docker Image Usage in Workflow (YAML)\nDESCRIPTION: This snippet shows how to use custom Docker images built with the `handle_docker` action in a workflow job. It depends on the Docker job and retrieves the image reference from the action's output.  It also specifies that an Azure-based runner (aks-***) is required.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/docker_images.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nBuild:\n    needs: [Smart_CI, Docker]\n    ...\n    runs-on: aks-linux-16-cores-32gb\n    container:\n      image: ${{ fromJSON(needs.docker.outputs.images).ov_build.ubuntu_22_04_x64 }}\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements Files in CMake\nDESCRIPTION: This CMake snippet installs the `requirements.txt` file from the OpenVINOPython source directory to the `${OV_CPACK_PYTHONDIR}` directory.  It is assigned to the `${OV_CPACK_COMP_OPENVINO_REQ_FILES}` component and potentially excluded from other components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES ${OpenVINOPython_SOURCE_DIR}/requirements.txt\n        DESTINATION ${OV_CPACK_PYTHONDIR}\n        COMPONENT ${OV_CPACK_COMP_OPENVINO_REQ_FILES}\n        ${OV_CPACK_COMP_OPENVINO_REQ_FILES_EXCLUDE_ALL})\n```\n\n----------------------------------------\n\nTITLE: Accessing Output by index of CompiledModel (TypeScript)\nDESCRIPTION: This code shows the declaration of the `output` method within the `CompiledModel` interface to retrieve an output by index.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\noutput(index): Output\n```\n\n----------------------------------------\n\nTITLE: LIR Dump Example 1 (Bash)\nDESCRIPTION: This command dumps LIR before and after the 'ExtractLoopInvariants' pass to the specified directory in all available formats.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/docs/debug_capabilities/linear_ir_passes_serialization.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nOV_SNIPPETS_DUMP_LIR=\"passes=ExtractLoopInvariants dir=path/dumpdir formats=all\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name\nDESCRIPTION: Defines the name of the target for the GPU functional tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/functional/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_gpu_func_tests)\n```\n\n----------------------------------------\n\nTITLE: OCR Node.js Script Arguments\nDESCRIPTION: This snippet describes the command-line arguments accepted by the optical-character-recognition.js script. It specifies that the script requires the paths to the detection model, recognition model, input image, and the target device as arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/js/node/optical_character_recognition/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnode optical-character-recognition.js *path_to_detection_model_file* *path_to_recognition_model_file* *path_to_img* *device*\n```\n\n----------------------------------------\n\nTITLE: Download GPG key for OpenVINO repository\nDESCRIPTION: Downloads the GPG public key required to authenticate the Intel software products repository. This is a prerequisite for adding the OpenVINO repository to the system's package manager.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nwget https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB\n```\n\n----------------------------------------\n\nTITLE: Uninstall latest OpenVINO Runtime\nDESCRIPTION: Uninstalls the latest version of the OpenVINO Runtime using the APT package manager.  Uses `autoremove` to remove dependencies that are no longer needed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt autoremove openvino\n```\n\n----------------------------------------\n\nTITLE: MaxPool Example 7 (dilated kernel, explicit padding)\nDESCRIPTION: This example showcases MaxPool operating on a 4D input with a dilated 2D kernel, `auto_pad` set to 'explicit', and `rounding_type` set to 'floor'. It demonstrates how dilation affects the receptive field and the resulting output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/pooling_shape_rules.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ninput = [[[[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]]]]   # shape: (1, 1, 3, 3)\n      strides = [1, 1]\n      kernel = [2, 2]\n      dilations = [2, 2]\n      rounding_type = \"floor\"\n      auto_pad = \"explicit\"\n      pads_begin = [1, 1]\n      pads_end = [1, 1]\n      output0 = [[[[5, 6, 5],\n                   [8, 9, 8],\n                   [5, 6, 5]]]]   # shape: (1, 1, 3, 3)\n      output1 = [[[[4, 5, 4],\n                   [7, 8, 7],\n                   [4, 5, 4]]]]   # shape: (1, 1, 3, 3)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO wheel package with pip\nDESCRIPTION: This snippet installs the OpenVINO wheel package using pip, after it has been built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_windows.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npip install build/wheel/openvino-2023.0.0-9612-cp11-cp11-win_arm64.whl\n```\n\n----------------------------------------\n\nTITLE: Setting Log Level Programmatically (C++)\nDESCRIPTION: This C++ code snippet shows how to set the OpenVINO log level using the `ov::log::Level` enum. This enables control over the level of debugging information produced by the OpenVINO runtime during execution. The `compile_model` method or `set_property` method can be used to overwrite the environment variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/auto-device-selection/debugging-auto-device.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// [part6]:\n#include <openvino/openvino.hpp>\n\nint main() {\n    ov::Core core;\n    core.set_property({{ov::log::level.name(), ov::log::Level::Debug}});\n    core.compile_model(\"path_to_model.xml\", \"CPU\");\n\n    core.set_property(\"CPU\", {{ov::log::level.name(), ov::log::Level::Debug}});\n}\n```\n\n----------------------------------------\n\nTITLE: Create Build Directory\nDESCRIPTION: This snippet creates a `build` directory and navigates into it.  This is where the OpenVINO build process will take place.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nmkdir build && cd build\n```\n\n----------------------------------------\n\nTITLE: Getting Default Context in OpenVINO Plugin (C++)\nDESCRIPTION: This code snippet demonstrates the implementation of the `Plugin::get_default_context()` method. Similar to create_context(), if remote contexts aren't supported, an exception should be thrown. This provides a default execution context for the plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nRemoteContext::Ptr TemplatePlugin::get_default_context(const ParamMap& params) const override {\n    // In the simplest case, we can just return a new instance of the context\n    return std::make_shared<TemplateContext>(*this, params);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Python Requirements for OpenVINO Samples (Windows)\nDESCRIPTION: This code snippet shows how to install Python requirements on Windows. It navigates to the python samples directory and then runs pip install using the requirements.txt file.  The sample dir placeholder needs to be replaced with the specific sample directory name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncd <INSTALL_DIR>\\samples\\python\\<SAMPLE_DIR>\npython -m pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Adding Compile Definitions in CMake\nDESCRIPTION: Defines a compilation flag (`IMPLEMENT_OPENVINO_RUNTIME_PLUGIN`) for the `mock_engine` target. This flag is likely used within the library's code to enable or disable certain features or implementations, probably related to implementing an OpenVINO runtime plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/unit_test_utils/mocks/mock_engine/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME} PRIVATE IMPLEMENT_OPENVINO_RUNTIME_PLUGIN)\n```\n\n----------------------------------------\n\nTITLE: Finding Fuzzer Source Files in CMake\nDESCRIPTION: This snippet uses the `FILE(GLOB)` command to locate all source files that match the pattern `*-fuzzer.cc`. The located files are stored in the `tests` variable, which is later iterated over to create individual fuzz test targets. This helps automate the process of adding new fuzz tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nFILE(GLOB tests \"*-fuzzer.cc\")\n```\n\n----------------------------------------\n\nTITLE: Updating All Packages in Conda\nDESCRIPTION: This command updates all packages within the current Conda environment to their latest versions. This ensures that all dependencies are up to date before installing OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conda.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nconda update --all\n```\n\n----------------------------------------\n\nTITLE: Adding OpenVINO Snap as Build Dependency\nDESCRIPTION: This snippet demonstrates how to add the OpenVINO snap to the build-snaps list in the application's snapcraft.yaml. This ensures that the OpenVINO snap is available during the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/deployment-locally/integrate-openvino-with-ubuntu-snap.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nparts:\n  app-build:\n    build-snaps:\n      - openvino-libs-test\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies in CMake\nDESCRIPTION: This snippet defines the dependencies required for the proxy plugin tests. These dependencies represent other components or libraries that the tests rely on.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(DEPENDENCIES\n    mock_engine\n)\n```\n\n----------------------------------------\n\nTITLE: Create Tensor Implementation C++\nDESCRIPTION: Implements the create_tensor() method, which creates a device-specific remote tensor. It takes the element type and shape as input parameters and returns a shared pointer to the created tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/remote-context.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nov::SoPtr<ov::ITensor> TemplateRemoteContext::create_tensor(const ov::element::Type& type, const ov::Shape& shape) const {\n    // Implementation for creating a device-specific remote tensor\n    return {}; // Placeholder for actual tensor creation logic\n}\n```\n\n----------------------------------------\n\nTITLE: Check GNA Conformance with Custom OV Build\nDESCRIPTION: This command checks GNA conformance using a custom OpenVINO build and pre-generated Conformance IRs.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npython3 run_conformance.py -m /path/to/conformance_irs -s=0 -ov /path/to/ov_repo_on_custom_branch/bin/intel64/Debug -d GNA\n```\n\n----------------------------------------\n\nTITLE: Creating Static Library in CMake\nDESCRIPTION: This snippet uses the add_library command to create a static library with the name defined in TARGET_NAME. The EXCLUDE_FROM_ALL option prevents the library from being built by default. The SOURCES variable contains the list of source files to compile.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/common/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC EXCLUDE_FROM_ALL ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Convolution Test Class Definition in C++\nDESCRIPTION: This code snippet shows the definition of a convolution test class. It is a parametrized GoogleTest based class with the ``convLayerTestParamsSet`` tuple of parameters. It declares a test fixture for convolution layers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin-testing.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nclass ConvolutionLayerTest : public testing::WithParamInterface<convLayerTestParamsSet> {\npublic:\n    static std::string getTestCaseName(const testing::ParamInfo<convLayerTestParamsSet>& obj);\nprotected:\n    std::shared_ptr<ov::Model> function;\n    ov::Core core;\n    InferenceEngine::ExecutableNetwork executableNetwork;\n    InferenceEngine::CNNNetwork cnnNetwork;\n\n    void SetUp() override;\n    void TearDown() override;\n};\n\n```\n\n----------------------------------------\n\nTITLE: Disable LTO in CMake\nDESCRIPTION: This snippet disables Link Time Optimization (LTO) by setting the CMake variable `ENABLE_LTO` to `OFF`. This is likely done to address an issue with adding symbols to partition 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(ENABLE_LTO OFF)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory\nDESCRIPTION: Adds a subdirectory named 'src' to the build. This directory contains the source code for the memory tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target\nDESCRIPTION: This snippet adds a clang-format target for the specified target to enforce code formatting standards.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common_translators/src/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Creating Source Groups (CMake)\nDESCRIPTION: Creates named folders for the sources within the Visual Studio project (.vcproj). This helps to organize the source code within the IDE.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Pull Request Description Template (Internal Contributors)\nDESCRIPTION: This is a template for internal contributors to use when creating a pull request for the OpenVINO™ Python API. It includes sections for details of the changes, requirements that have been introduced or changed, and any relevant ticket numbers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/contributing.md#_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\nDetails:\n...\n\nRequirements introduced/changed:       <-- only if applicable\n...\n\nTickets:\nXXXX, YYYY                             <-- only numbers from tickets\n```\n\n----------------------------------------\n\nTITLE: Installing Target and README in CMake\nDESCRIPTION: This snippet handles the installation of the 'protopipe' executable and its README file. It specifies the destination directory, component, and exclusion rules for the installation. The `install(TARGETS)` command installs the executable, while the `install(FILES)` command installs the README.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n        RUNTIME DESTINATION \"tools/${TARGET_NAME}\"\n        COMPONENT ${NPU_INTERNAL_COMPONENT}\n        ${OV_CPACK_COMP_NPU_INTERNAL_EXCLUDE_ALL})\n\nif(EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/README.md\")\n    install(FILES \"${CMAKE_CURRENT_SOURCE_DIR}/README.md\"\n            DESTINATION \"tools/${TARGET_NAME}\"\n            COMPONENT ${NPU_INTERNAL_COMPONENT}\n            ${OV_CPACK_COMP_NPU_INTERNAL_EXCLUDE_ALL})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Wheel Package (sh)\nDESCRIPTION: This command installs the OpenVINO wheel package. It is typically used when testing changes in OpenVINO and requires building a local wheel package for installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/layer_tests/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install openvino.whl\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for Memory Tests in CMake\nDESCRIPTION: This CMake command adds a subdirectory named 'memory_tests' to the current build process. This implies that the 'memory_tests' directory contains its own CMakeLists.txt file which will be processed as part of the overall build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(memory_tests)\n```\n\n----------------------------------------\n\nTITLE: Setting up Python version for OpenVINO workflow\nDESCRIPTION: This snippet configures the Python environment using a custom GitHub Action designed for OpenVINO. It specifies the Python version, pip cache path, and whether to set up pip paths for subsequent steps. It also indicates if the runner is self-hosted.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/custom_actions.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  - name: Setup Python ${{ env.PYTHON_VERSION }}\n    uses: ./openvino/.github/actions/setup_python\n    with:\n      version: '3.11'\n      pip-cache-path: ${{ env.PIP_CACHE_PATH }}\n      should-setup-pip-paths: 'true'\n      self-hosted-runner: 'true'\n```\n\n----------------------------------------\n\nTITLE: First Correct TopK Output\nDESCRIPTION: Displays one possible correct output for the TopK operation with the example input and configuration, showing the values and indices.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/top-k-11.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\noutput_values  = [5, 3, 1, 2]\noutput_indices = [0, 1, 2, 3]\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories CMake\nDESCRIPTION: This snippet sets the include directories for the 'openvino_cnpy' target, making the current source directory available for include files during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/cnpy/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC \"${CMAKE_CURRENT_SOURCE_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Activate a Python virtual environment\nDESCRIPTION: This command activates a previously created Python virtual environment. Activating the environment modifies the shell's environment variables to use the Python interpreter and installed packages within the virtual environment. All subsequent pip installs will be local to the environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/README.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nsource <directory_for_environment>/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Configure CPack Packaging\nDESCRIPTION: Configures CPack for creating distribution packages. It ensures that CPack is only run if the current source directory is the same as the OpenVINOConverter source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tools/ovc/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_SOURCE_DIR STREQUAL OpenVINOConverter_SOURCE_DIR)\n    ov_cpack(${OV_CPACK_COMPONENTS_ALL})\nendif()\n```\n\n----------------------------------------\n\nTITLE: HSwish Layer Definition in OpenVINO XML\nDESCRIPTION: This XML snippet defines an HSwish layer in OpenVINO. It specifies the input and output ports with their respective dimensions, showcasing how to integrate the HSwish activation function into an OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/hswish-4.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"HSwish\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Ineffective Cache Test C++\nDESCRIPTION: This code example shows an incorrect way to define target shapes for testing the cache. Having identical shapes in sequence will prevent the cache from triggering, as the shape infer and parameter preparation stages will be skipped if the shapes haven't changed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/runtime_parameters_cache.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n{\n    {{-1, -1, -1}, {{5, 5, 5}, {5, 5, 5}}}, // input 0\n    {{-1, -1, 5},  {{5, 5, 5}, {5, 5, 5}}}  // input 1\n},\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name\nDESCRIPTION: This snippet sets the target name for the library to be built. The target name is used in subsequent CMake commands to refer to the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/test_builtin_extensions/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME \"test_builtin_extensions\")\n```\n\n----------------------------------------\n\nTITLE: Update APT package list\nDESCRIPTION: Updates the APT package list to include the newly added OpenVINO repository. This ensures that the system is aware of the available OpenVINO packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt update\n```\n\n----------------------------------------\n\nTITLE: Define ov_preprocess_resize_algorithm enum in C\nDESCRIPTION: This enum defines the supported resize algorithms for image preprocessing in OpenVINO. It includes linear, cubic, and nearest neighbor interpolation methods.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_13\n\nLANGUAGE: C\nCODE:\n```\ntypedef enum {\n\n    RESIZE_LINEAR,  //!< linear algorithm\n\n    RESIZE_CUBIC,   //!< cubic algorithm\n\n    RESIZE_NEAREST  //!< nearest algorithm\n\n} ov_preprocess_resize_algorithm_e;\n```\n\n----------------------------------------\n\nTITLE: Shutdown Protobuf Library in CMake\nDESCRIPTION: This code snippet conditionally adds a subdirectory named shutdown_protobuf if Protobuf_IN_FRONTEND is enabled and BUILD_SHARED_LIBS is also enabled. This is used to handle protobuf library shutdown.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(Protobuf_IN_FRONTEND AND BUILD_SHARED_LIBS)\n    add_subdirectory(shutdown_protobuf)\nendif()\n```\n\n----------------------------------------\n\nTITLE: LIR Dump Example 2 (Bash)\nDESCRIPTION: This command dumps LIR around every pass to the specified directory, using the control flow format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/docs/debug_capabilities/linear_ir_passes_serialization.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nOV_SNIPPETS_DUMP_LIR=\"passes=all dir=path/dumpdir formats=control_flow\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Generating Statistics File using sea_runtool.py\nDESCRIPTION: This snippet generates the statistics file (in CSV format) by running the `sea_runtool.py` script along with the benchmark application.  The script analyzes the benchmark application's execution and identifies the used components.  It requires the path to the benchmark application, model, and output directory.  The generated CSV file contains the usage statistics for different components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/selective_build.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ../../../bin/intel64/Release/\n\npython ../../../thirdparty/itt_collector/runtool/sea_runtool.py \\\n--bindir ./lib -o models_statistics_dir ! \\\n./benchmark_app -niter 1 -nireq 1 \\\n-m <model_path>\n```\n\n----------------------------------------\n\nTITLE: Adding Compile Definitions CMake\nDESCRIPTION: Conditionally adds the `ENABLE_CONFORMANCE_PGQL` compile definition to the `conformance_shared` target if the `ENABLE_CONFORMANCE_PGQL` variable is set. This allows enabling or disabling features during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/test_runner/conformance_infra/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_CONFORMANCE_PGQL)\n    target_compile_definitions(${TARGET_NAME} PRIVATE ENABLE_CONFORMANCE_PGQL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Search OpenVINO packages in APT cache\nDESCRIPTION: Searches the APT package cache for packages related to OpenVINO.  This is used to verify that the repository has been properly configured and to see the available OpenVINO packages.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\napt-cache search openvino\n```\n\n----------------------------------------\n\nTITLE: Installing OVSA developer components on Guest VM\nDESCRIPTION: Navigates to the ~/OVSA directory on the guest VM, unpacks the ovsa-developer.tar.gz archive, navigates into the unpacked directory, and executes the install.sh script with sudo privileges to install the OVSA developer components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_34\n\nLANGUAGE: sh\nCODE:\n```\ncd ~/OVSA\ntar xvfz ovsa-developer.tar.gz\ncd ovsa-developer\nsudo ./install.sh\n```\n\n----------------------------------------\n\nTITLE: Release Allocated Objects in C\nDESCRIPTION: This C code snippet shows how to properly release allocated memory and objects when using the OpenVINO C API. Releasing the memory prevents memory leaks. The order of releasing the objects is critical.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference.rst#_snippet_36\n\nLANGUAGE: cpp\nCODE:\n```\n//! [part8]\nfree(input_data);\nfree(output_data);\nOPENVINO_ASSERT(ov_infer_request_free(infer_request));\nOPENVINO_ASSERT(ov_compiled_model_free(compiled_model));\nOPENVINO_ASSERT(ov_model_free(model));\nOPENVINO_ASSERT(ov_core_free(core));\n//! [part8]\n```\n\n----------------------------------------\n\nTITLE: Importing the Extended Class\nDESCRIPTION: This snippet shows how to import the extended Python class `MyTensor` from the `openvino.runtime.mymodule_ext` module. It assumes that the module is built from the pybind-generated code.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/code_examples.md#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nfrom openvino.runtime.mymodule_ext import MyTensor\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Runtime version 2025.1.0\nDESCRIPTION: Installs the OpenVINO Runtime version 2025.1.0 using the APT package manager. This is an example of installing a specific version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-apt.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt install openvino-2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Defining Test Target with ov_add_test_target CMake Function\nDESCRIPTION: This snippet uses the custom `ov_add_test_target` CMake function to define the test target. It specifies the target name, source directory, dependencies, link libraries, compile definitions, and include directories. It also sets labels for the test and configures clang-format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/tests/functional/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        DEPENDENCIES\n            ${DEPENDENCIES}\n        LINK_LIBRARIES\n            gmock\n            func_test_utils\n        DEFINES\n            ${COMPILE_DEFINITIONS}\n        INCLUDES\n            $<TARGET_PROPERTY:openvino_runtime_obj,SOURCE_DIR>/src\n            $<$<TARGET_EXISTS:openvino_proxy_plugin_obj>:$<TARGET_PROPERTY:openvino_proxy_plugin_obj,INTERFACE_INCLUDE_DIRECTORIES>>\n            ${CMAKE_CURRENT_SOURCE_DIR}\n        ADD_CLANG_FORMAT\n        LABELS\n            OV UNIT RUNTIME\n)\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to Target in CMake\nDESCRIPTION: This snippet links the `StressTestsCommon` library to the `StressMemLeaksTests` executable using the `target_link_libraries` command.  The `PRIVATE` keyword indicates that `StressTestsCommon` is only required for building the executable and not for any downstream targets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/memleaks_tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE StressTestsCommon)\n```\n\n----------------------------------------\n\nTITLE: GatherND Batch Dimensions Slice Example\nDESCRIPTION: Illustrates GatherND with batch_dims = 1, focusing on slice extraction. The indices are applied to each batch separately, selecting rows from each batch in the 'data' tensor to form the output. This shows the interplay between batch_dims and slice selection.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-8.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 1\nindices = [[1], <--- this is applied to the first batch\n              [0]] <--- this is applied to the second batch, shape = (2, 1)\ndata    = [[[1,   2,  3,  4], [ 5,  6,  7,  8], [ 9, 10, 11, 12]]  <--- the first batch\n              [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]] <--- the second batch, shape = (2, 3, 4)\noutput  = [[ 5,  6,  7,  8], [13, 14, 15, 16]], shape = (2, 4)\n```\n\n----------------------------------------\n\nTITLE: Gather Operation Example 7\nDESCRIPTION: Shows the Gather operation with indices out of the valid range. When an index is out of range, the corresponding output element is filled with zero.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-8.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nbatch_dims = 0\naxis = 0\n\nindices = [3, 10, -20]\ndata    = [1, 2, 3, 4, 5]\noutput  = [4, 0, 0]\n```\n\n----------------------------------------\n\nTITLE: ReduceLogicalAnd C++ Implementation\nDESCRIPTION: Illustrates the core logic of the ReduceLogicalAnd operation, performing a logical AND reduction along specified axes of an input tensor. This snippet represents the conceptual implementation, where each element in the output is the result of the AND operation across the specified dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-logical-and-1.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\noutput[i0, i1, ..., iN] = and[j0,c..., jN](x[j0, ..., jN]))\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name\nDESCRIPTION: This snippet sets the target name for the unit tests, which will be used in subsequent CMake commands. The target name is `ov_cpu_unit_tests_vectorized`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/vectorized/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME ov_cpu_unit_tests_vectorized)\n```\n\n----------------------------------------\n\nTITLE: Save Model Synchronously in TypeScript\nDESCRIPTION: The saveModelSync function saves an OpenVINO Model object to an Intermediate Representation (IR) file (XML and BIN format).  It compresses weights to FP16 by default and cleans up debug information. It takes a Model object, a file path string, and an optional boolean to disable FP16 compression as input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/addon.rst#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nsaveModelSync(model: Model, path: string, compressToFp16?: boolean): void;\n```\n\n----------------------------------------\n\nTITLE: Linking OpenCV Library and Include Directories (OpenVINO)\nDESCRIPTION: This snippet checks for OpenCV, and if found, includes the OpenCV include directories and links the `opencv_core` library to the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/snippets/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(OpenCV_FOUND)\n    target_include_directories(${TARGET_NAME} SYSTEM PRIVATE ${OpenCV_INCLUDE_DIRS})\n    target_link_libraries(${TARGET_NAME} PRIVATE opencv_core)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Export Blob Dump Environment Variables\nDESCRIPTION: Exports environment variables to control blob dumping in a shell session. This includes setting the directory, format, and node ports to dump blobs for CPU execution.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/blob_dumping.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport OV_CPU_BLOB_DUMP_DIR=dump_dir\nexport OV_CPU_BLOB_DUMP_FORMAT=TEXT\nexport OV_CPU_BLOB_DUMP_NODE_PORTS=OUT\nbinary ...\n```\n\n----------------------------------------\n\nTITLE: Identity Layer XML Configuration in OpenVINO\nDESCRIPTION: Defines an Identity layer in OpenVINO using XML configuration. It specifies the layer's type, input and output ports, data precision and dimensions. This configuration demonstrates how to instantiate and configure an Identity layer within an OpenVINO model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/identity-16.rst#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... name=\"Identity\" type=\"Identity\">\n    <data/>\n    <input>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"FP32\" names=\"Identity:16\">\n            <dim>3</dim>\n            <dim>3</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Output Directories\nDESCRIPTION: Sets the output directories for the library, archive, and PDB files. These directories are based on the `PYTHON_BRIDGE_OUTPUT_DIRECTORY` variable, ensuring the files are placed in the appropriate location for Python bridge integration.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${PYTHON_BRIDGE_OUTPUT_DIRECTORY}/test_utils)\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${PYTHON_BRIDGE_OUTPUT_DIRECTORY}/test_utils)\nset(CMAKE_PDB_OUTPUT_DIRECTORY ${PYTHON_BRIDGE_OUTPUT_DIRECTORY}/test_utils)\n```\n\n----------------------------------------\n\nTITLE: Set Target Properties CMake\nDESCRIPTION: This snippet sets the `EXPORT_NAME` property for the `openvino_snippets` target, which controls the name used when exporting the library. This ensures that the exported library has the correct name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME snippets)\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: This snippet adds the current source directory's 'include' subdirectory to the library's include directories. This makes the headers in that directory available to consumers of the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/thread_local/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: Exclude From All Target Conditionally\nDESCRIPTION: This snippet excludes the 'ov_cpu_func_tests' target from the 'all' target if a specific tests path is defined by 'ENABLE_CPU_SPECIFIC_TESTS_PATH'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/functional/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nif(DEFINED ENABLE_CPU_SPECIFIC_TESTS_PATH)\n    set_target_properties(${TARGET_NAME} PROPERTIES EXCLUDE_FROM_ALL ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: IRDFT Layer Definition (3D Input, No Signal Size) XML\nDESCRIPTION: Defines an IRDFT layer in XML for OpenVINO with a 3D input tensor and no signal_size. The input tensor has dimensions 161x161x2, and the axes tensor specifies dimensions 0 and 1 for the IRDFT operation. The output tensor will have dimensions 161x320.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/irdft-9.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IRDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>161</dim>\n            <dim>161</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim> <!-- [0, 1] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>161</dim>\n            <dim>320</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version\nDESCRIPTION: This snippet sets the minimum required CMake version, conditionally adjusting it based on the operating system. It requires CMake to be installed and sets a different version for Windows (WIN32) versus other systems.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/time_tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n    cmake_minimum_required(VERSION 3.16)\nelse()\n    cmake_minimum_required(VERSION 3.13)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation Definitions (Windows)\nDESCRIPTION: If the operating system is Windows, this adds the 'NOMINMAX' definition to prevent min/max from being defined as macros. This avoids conflicts with the standard library's min/max functions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nif (WIN32)\n    # Prevents defining min/max as macros\n    target_compile_definitions(${TARGET_NAME} PRIVATE NOMINMAX)\nendif()\n```\n\n----------------------------------------\n\nTITLE: SpaceToDepth Layer Example in XML\nDESCRIPTION: This XML code snippet provides an example of how to define a SpaceToDepth layer within an OpenVINO model. It includes the layer type, data attributes such as block_size and mode, and the input and output port dimensions. The dimensions are updated based on the specified block_size.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/space-to-depth-1.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"SpaceToDepth\" ...>\n    <data block_size=\"2\" mode=\"blocks_first\"/>\n    <input>\n        <port id=\"0\">\n            <dim>5</dim>\n            <dim>7</dim>\n            <dim>4</dim>\n            <dim>6</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>5</dim>    <!-- data.shape[0] -->\n            <dim>28</dim>   <!-- data.shape[1] * (block_size ^ 2) -->\n            <dim>2</dim>    <!-- data.shape[2] / block_size -->\n            <dim>3</dim>    <!-- data.shape[3] / block_size -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files in CMake\nDESCRIPTION: Uses `file(GLOB)` to collect C++ source files and header files from the 'src' and 'include' directories. The collected files are stored in variables for later use in defining the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/format_reader/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile (GLOB MAIN_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile (GLOB LIBRARY_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/src/*.h)\nfile (GLOB LIBRARY_PUBLIC_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/include/*.h)\n```\n\n----------------------------------------\n\nTITLE: Convolution Test Instantiation in C++\nDESCRIPTION: This snippet instantiates the convolution test using the GoogleTest macro ``INSTANTIATE_TEST_SUITE_P``. It creates multiple instances of the test with different parameter sets, allowing for comprehensive testing of the convolution layer with varying configurations and device targets.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin-testing.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nINSTANTIATE_TEST_SUITE_P(smoke_ConvolutionLayer_ExplicitPadding_FP32, ConvolutionLayerTest, ::testing::ValuesIn(convParams_ExplicitPadding_FP32),\n                            ConvolutionLayerTest::getTestCaseName);\n\n```\n\n----------------------------------------\n\nTITLE: Installing Test Target in CMake\nDESCRIPTION: This CMake snippet installs the built test target to the `tests` directory under the runtime destination. It also marks the component as `tests` and excludes it from the `ALL` target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/unit/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${TARGET_NAME}\n        RUNTIME DESTINATION tests\n        COMPONENT tests\n        EXCLUDE_FROM_ALL\n)\n```\n\n----------------------------------------\n\nTITLE: Mish Layer Definition in XML - OpenVINO\nDESCRIPTION: This XML snippet demonstrates how to define a Mish layer within an OpenVINO model. It specifies the input and output ports with their dimensions. The type attribute is set to \"Mish\" to indicate the activation function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/activation/mish-4.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Mish\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Setting Shared Headers Directory CMake\nDESCRIPTION: Defines the directory where shared header files for functional tests are located. This directory is then included in the target's include paths.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/auto_batch/tests/functional/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(SHARED_HEADERS_DIR \"${OpenVINO_SOURCE_DIR}/src/tests/functional/plugin/shared/include\")\n```\n\n----------------------------------------\n\nTITLE: Adding op_conformance_utils Target in CMake\nDESCRIPTION: This snippet uses the custom CMake function `ov_add_target` to add the `op_conformance_utils` library as a static library. It specifies the source directory, include directories, link libraries, and dependencies. It also enables `cpplint` for code style checking.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/op_conformance_utils/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_target(\n        NAME \"${TARGET_NAME}\"\n        TYPE STATIC\n        ROOT \"${CMAKE_CURRENT_SOURCE_DIR}/src\"\n        INCLUDES\n            PUBLIC\n                \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\"\n        LINK_LIBRARIES\n            PUBLIC\n                ${LIBRARIES}\n        DEPENDENCIES\n            ${LIBRARIES}\n        ADD_CPPLINT\n)\n```\n\n----------------------------------------\n\nTITLE: Find headers and apply clang format\nDESCRIPTION: This snippet finds all header files (.h and .hpp) recursively within the 'include' directory and then applies clang format to them.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE hdrs ${CMAKE_CURRENT_SOURCE_DIR}/include/*.h ${CMAKE_CURRENT_SOURCE_DIR}/include/*.hpp)\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_SOURCES ${hdrs})\n```\n\n----------------------------------------\n\nTITLE: Generate Netplan Configuration\nDESCRIPTION: This command generates the network configuration based on the netplan YAML files in /etc/netplan.  It is used after modifying network configuration files to apply the changes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nsudo netplan generate\n```\n\n----------------------------------------\n\nTITLE: Single Job Overview in GitHub Actions YAML\nDESCRIPTION: This YAML snippet provides an example of a single job definition within a GitHub Actions workflow, showcasing how to configure aspects such as dependencies (`needs`), machine type (`runs-on`), Docker image (`container`), environment variables (`env`), and execution steps.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/overview.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nPython_Unit_Tests:\n    name: Python unit tests\n    needs: Build\n    timeout-minutes: 40\n    defaults:\n      run:\n        shell: bash\n    runs-on: aks-linux-4-cores-16gb\n    container:\n      image: openvinogithubactions.azurecr.io/dockerhub/ubuntu:20.04\n      volumes:\n        - /mount/caches:/mount/caches\n    env:\n      OPENVINO_REPO: /__w/openvino/openvino/openvino\n      INSTALL_DIR: /__w/openvino/openvino/install\n      INSTALL_TEST_DIR: /__w/openvino/openvino/install/tests\n      LAYER_TESTS_INSTALL_DIR: /__w/openvino/openvino/install/tests/layer_tests\n\n    steps: ...\n```\n\n----------------------------------------\n\nTITLE: Generating and Collecting Analyzed Data\nDESCRIPTION: This set of commands generates and collects data analyzed in the SELECTIVE_BUILD_ANALYZER mode.  It uses the sea_runtool.py script to collect ITT profiling data by running benchmark_app.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/docs/develop_cc_for_new_component.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncmake --build build --target sea_itt_lib\nmkdir -p cc_data\npython thirdparty/itt_collector/runtool/sea_runtool.py --bindir ./bin/intel64/Release -o ./cc_data ! ./bin/intel64/Release/benchmark_app -niter 1 -nireq 1 -m <your_model.xml> -d CPU\n```\n\n----------------------------------------\n\nTITLE: LIR Dump Example 3 (Bash)\nDESCRIPTION: This command dumps LIR around 'FuseLoops', 'InsertLoops', and 'InsertLoadStore' passes using data flow format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/docs/debug_capabilities/linear_ir_passes_serialization.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nOV_SNIPPETS_DUMP_LIR=\"passes=FuseLoops,InsertLoops,InsertLoadStore formats=data_flow\" binary ...\n```\n\n----------------------------------------\n\nTITLE: Creating Interface Library with CMake\nDESCRIPTION: This snippet creates an interface library named 'openvino_conditional_compilation' and an alias 'openvino::conditional_compilation'. It also sets properties and links against 'openvino::itt'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME openvino_conditional_compilation)\n\nadd_library(${TARGET_NAME} INTERFACE)\n\nadd_library(openvino::conditional_compilation ALIAS ${TARGET_NAME})\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME conditional_compilation)\n\ntarget_link_libraries(${TARGET_NAME} INTERFACE openvino::itt)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name\nDESCRIPTION: Defines the target name for the library, which will be used in subsequent CMake commands to refer to this specific target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME openvino_npu_common)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Installing OpenVINO (ARM 64-bit)\nDESCRIPTION: These commands download the OpenVINO Runtime archive for ARM 64-bit systems, extract it, and move the extracted directory to `/opt/intel`. It uses `curl` to download the archive, `tar` to extract it, and `sudo mv` to move the extracted folder with root privileges.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino/packages/2025.1/linux/openvino_toolkit_ubuntu20_2025.1.0.18503.6fec06580ab_arm64.tgz -O openvino_2025.1.0.tgz\ntar -xf openvino_2025.1.0.tgz\nsudo mv openvino_toolkit_ubuntu20_2025.1.0.18503.6fec06580ab_arm64 /opt/intel/openvino_2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries\nDESCRIPTION: This snippet links the target with other OpenVINO libraries, specifying the required dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/backend/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PRIVATE openvino::reference openvino::util openvino::runtime::dev openvino::shape_inference)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO GenAI from Archive - Windows\nDESCRIPTION: These commands download the OpenVINO GenAI archive for Windows. The `curl` command downloads the zip file and saves it to the Downloads directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-genai.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ncd <user_home>/Downloads\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino_genai/packages/2025.1/windows/openvino_genai_windows_2025.1.0.0_x86_64.zip --output openvino_genai_2025.1.0.0.zip\n```\n\n----------------------------------------\n\nTITLE: Creating Headings with reStructuredText\nDESCRIPTION: Demonstrates how to create headings in reStructuredText using different punctuation marks for different heading levels (H1 to H5). The number of punctuation marks should match the length of the header text.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/CONTRIBUTING_DOCS.md#_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n   H1\n   ==================== \n    \n   H2\n   ####################  \n    \n   H3\n   ++++++++++++++++++++ \n    \n   H4\n   --------------------\n    \n   H5\n   ....................\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target CMake\nDESCRIPTION: This snippet adds a clang-format target for the source files.  The `ov_add_clang_format_target` macro creates a target to format the header and source files using clang-format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/src/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_SOURCES ${HEADERS} ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Package\nDESCRIPTION: This command installs the OpenVINO package using pip. The package includes the OpenVINO Runtime and other necessary components for deep learning inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/pypi_publish/pypi-openvino-rt.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npip install openvino\n```\n\n----------------------------------------\n\nTITLE: Adding Google Test Subdirectory\nDESCRIPTION: This command adds the Google Test source directory as a subdirectory within the CMake project. The EXCLUDE_FROM_ALL option ensures that gtest is not built by default when building the entire project, unless explicitly targeted.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/gtest/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(gtest EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: ExtractImagePatches Output Example 5 C++\nDESCRIPTION: This C++ snippet shows the output of the ExtractImagePatches operation with sizes=\"2,2\", strides=\"3,3\", rates=\"1,1\", and auto_pad=\"valid\" when the input is a 1x2x5x5 array. The output shape is [1, 8, 2, 2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/extract-image-patches-3.rst#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n[[[[ 1  4]\n   [16 19]]\n\n  [[26 29]\n   [41 44]]\n\n  [[ 2  5]\n   [17 20]]\n\n  [[27 30]\n   [42 45]]\n\n  [[ 6  9]\n   [21 24]]\n\n  [[31 34]\n   [46 49]]\n\n  [[ 7 10]\n   [22 25]]\n\n  [[32 35]\n   [47 50]]]]\n```\n\n----------------------------------------\n\nTITLE: Installing C++ Samples Directory (Windows) in CMake\nDESCRIPTION: Installs the C++ samples directory to the specified destination under Windows systems.  It excludes certain file patterns (e.g., *.sh, .clang-format).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nelseif(WIN32)\n    install(DIRECTORY cpp/\n            DESTINATION ${OV_CPACK_SAMPLESDIR}/cpp\n            COMPONENT ${OV_CPACK_COMP_CPP_SAMPLES}\n            ${OV_CPACK_COMP_CPP_SAMPLES_EXCLUDE_ALL}\n            PATTERN *.sh EXCLUDE\n            PATTERN .clang-format EXCLUDE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: RMS Layer Configuration in XML\nDESCRIPTION: This XML configuration defines an RMS normalization layer in OpenVINO. It specifies the epsilon value for numerical stability and defines the input and output tensor shapes. The input ports define the shape of the data and scale tensors, while the output port defines the shape of the resulting normalized tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/internal/rms.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"RMS\"> <!-- normalization always over the last dimension [-1] -->\n    <data eps=\"1e-6\"/>\n    <input>\n        <port id=\"0\">\n            <dim>12</dim>\n            <dim>25</dim>\n            <dim>512</dim>\n        </port>\n        <port id=\"1\">\n            <dim>512</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>12</dim>\n            <dim>25</dim>\n            <dim>512</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Parametrized MyTensor Test Output Shell\nDESCRIPTION: Example output of running the parametrized `MyTensor` creation test using pytest. It shows the test names and their execution status (PASSED).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\ntests/test_runtime/test_mytensor.py::test_mytensor_creation[source0] PASSED                                                                                                                                    [ 50%]\ntests/test_runtime/test_mytensor.py::test_mytensor_creation[source1] PASSED                                                                                                                                    [100%]\n```\n\n----------------------------------------\n\nTITLE: ReduceLogicalOr Example 1 (keep_dims=true)\nDESCRIPTION: This XML example demonstrates the ReduceLogicalOr operation with the `keep_dims` attribute set to `true`. The input tensor has dimensions 6x12x10x24, and the reduction is performed along axes 2 and 3. The output tensor retains these axes, but their size is reduced to 1.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-logical-or-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceLogicalOr\" ...>\n    <data keep_dims=\"true\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>2</dim>         <!-- value is [2, 3] that means independent reduction in each channel and batch -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>1</dim>\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating Static Library for OpenVINO Documentation Snippets\nDESCRIPTION: This snippet creates a static library named `openvino_docs_snippets` and specifies source files. It also sets include directories for the library to find OpenVINO headers and common utilities.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/snippets/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\"\n                  \"${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp\"\n                  \"${CMAKE_CURRENT_SOURCE_DIR}/src/*.c\"\n                  \"${CMAKE_CURRENT_SOURCE_DIR}/../articles_en/assets/snippets/*.cpp\"\n                  \"${CMAKE_CURRENT_SOURCE_DIR}/../articles_en/assets/snippets/*.c\")\n\nadd_library(${TARGET_NAME} STATIC ${SOURCES})\ntarget_include_directories(${TARGET_NAME} PRIVATE \"${OpenVINO_SOURCE_DIR}/src/inference/include\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/inference/dev_api\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/core/dev_api\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/core/include\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/bindings/c/include\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/common/transformations/include\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/common/preprocessing/src\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/common/util/include\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/common/low_precision_transformations/include\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/frontends/common/include\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/core/template_extension\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/frontends/onnx/frontend/include\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/frontends/tensorflow/include\"\n                                                  \"${OpenVINO_SOURCE_DIR}/src/frontends/paddle/include\")\n```\n\n----------------------------------------\n\nTITLE: Enabling MLAS for CPU\nDESCRIPTION: This snippet determines whether to enable MLAS (Microsoft Linear Algebra Subprograms) for the CPU plugin based on the target architecture (X86, X86_64, AARCH64), OS (Windows), and compiler. It disables MLAS for WebAssembly, AArch64 on Windows, MinGW, older GCC versions, and Intel LLVM on Windows.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(X86 OR X86_64 OR AARCH64)\n    # disable mlas with webassembly and intel compiler on windows\n    if(EMSCRIPTEN OR (WIN32 AND AARCH64) OR MINGW OR\n       (CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7) OR (OV_COMPILER_IS_INTEL_LLVM AND WIN32))\n        set(ENABLE_MLAS_FOR_CPU_DEFAULT OFF)\n    else()\n        set(ENABLE_MLAS_FOR_CPU_DEFAULT ON)\n    endif()\nelse()\n    set(ENABLE_MLAS_FOR_CPU_DEFAULT OFF)\nendif()\nov_option(ENABLE_MLAS_FOR_CPU \"Enable MLAS for OpenVINO CPU Plugin\" ${ENABLE_MLAS_FOR_CPU_DEFAULT})\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory for Tests (CMake)\nDESCRIPTION: This snippet conditionally adds the 'tests' subdirectory to the build if the ENABLE_TESTS CMake option is enabled. This allows for building and running tests related to the frontends.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating OpenVINO Coverage Report (make)\nDESCRIPTION: This snippet shows how to generate the OpenVINO coverage report using the `make` command. The `ov_coverage` target triggers the process of collecting coverage data and generating HTML reports, which are stored in the `coverage` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/test_coverage.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ make ov_coverage\n```\n\n----------------------------------------\n\nTITLE: Set Compiler Options\nDESCRIPTION: This adds compiler options specifically for Clang to suppress warnings about undefined macros and reserved identifiers. These warnings are suppressed for the target `ov_onnx_frontend_tests`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(OV_COMPILER_IS_CLANG)\n    target_compile_options(ov_onnx_frontend_tests PRIVATE -Wno-undef -Wno-reserved-id-macro)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Using Optional in the middle of a pattern\nDESCRIPTION: This C++ snippet demonstrates how to use the Optional operator to specify that a node in the middle of a pattern may or may not be present in the model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/patterns-python-api.rst#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\ntuple<shared_ptr<ov::Node>, shared_ptr<ov::Node>> pattern_optional_middle() {\n    // Creating nodes\n    auto relu_node = make_shared<WrapType<opset13::Relu>>();\n    auto sigmoid_node = make_shared<WrapType<opset13::Sigmoid>>();\n    auto optional_node = make_shared<Optional>(relu_node);\n\n    return {optional_node, sigmoid_node};\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Output Root Directory\nDESCRIPTION: This snippet defines the output directory for build artifacts. The `OUTPUT_ROOT` variable is set to the current source directory (`CMAKE_CURRENT_SOURCE_DIR`), meaning that the output files will be placed in the same directory as the source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(OUTPUT_ROOT ${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: StridedSlice Masking Example (begin/end) in XML\nDESCRIPTION: Demonstrates the usage of `begin_mask` and `end_mask` attributes in the StridedSlice layer. The masks allow ignoring the provided begin and end values for specific dimensions. This example is equivalent to array[1:, :, ::-1].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/strided-slice-1.rst#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"StridedSlice\" ...>\n    <data begin_mask=\"0,1,1\" end_mask=\"1,1,1\" new_axis_mask=\"0,0,0,0,0\" shrink_axis_mask=\"0,0\" ellipsis_mask=\"0\" />\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim> <!-- begin: [1, 1, 123] begin_mask ignores provided values -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim> <!-- end: [0, 0, 2] end_mask ignores provided values -->\n        </port>\n        <port id=\"3\">\n            <dim>3</dim> <!-- stride: [1, 1, -1] - last slicing is in reverse, masks' behavior changes -->\n        </port>\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>1</dim> <!-- begin = 1, end = 2 (end_mask), element ids: [1] -->\n            <dim>3</dim> <!-- begin = 0 (begin_mask), end = 3 (end_mask), element ids: [0, 1, 2] -->\n            <dim>3</dim> <!-- begin = 3 (begin_mask), end = 0 (end_mask), element ids: [3, 2, 1] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Clone OpenVINO Notebooks (Azure ML)\nDESCRIPTION: This command clones the OpenVINO notebooks repository from GitHub.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_20\n\nLANGUAGE: console\nCODE:\n```\ngit clone https://github.com/openvinotoolkit/openvino_notebooks.git\n```\n\n----------------------------------------\n\nTITLE: Get Property Implementation C++\nDESCRIPTION: Implements the get_property() method, which retrieves the device-specific properties associated with the remote context. This function enables access to device-specific context information.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/remote-context.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nov::Any TemplateRemoteContext::get_property(const std::string& name) const {\n    return m_property;\n}\n```\n\n----------------------------------------\n\nTITLE: ScatterNDUpdate Layer in XML\nDESCRIPTION: This XML code snippet shows how to define a ScatterNDUpdate layer in an OpenVINO model. It specifies the input and output ports with their dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/scatter-nd-update-3.rst#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ScatterNDUpdate\">\n    <input>\n        <port id=\"0\">\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>10</dim>\n            <dim>15</dim>\n        </port>\n        <port id=\"1\">\n            <dim>25</dim>\n            <dim>125</dim>\n            <dim>3</dim>\n        </port>\n        <port id=\"2\">\n            <dim>25</dim>\n            <dim>125</dim>\n            <dim>15</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>1000</dim>\n            <dim>256</dim>\n            <dim>10</dim>\n            <dim>15</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Try Compile with c++fs (CMake)\nDESCRIPTION: This snippet attempts to compile the `main.cpp` program linking the `c++fs` library. The result is stored in `STD_FS_NEEDS_CXXFS`, indicating whether the compilation succeeded when linking against `c++fs`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\ntry_compile(STD_FS_NEEDS_CXXFS ${CMAKE_CURRENT_BINARY_DIR}\n            SOURCES ${CMAKE_CURRENT_BINARY_DIR}/main.cpp\n            COMPILE_DEFINITIONS -std=c++11\n            LINK_LIBRARIES c++fs)\n```\n\n----------------------------------------\n\nTITLE: Defining Environment Variables (step level preferred)\nDESCRIPTION: This snippet illustrates defining environment variables at different levels (workflow, job, and step) in a GitHub Actions workflow. It emphasizes the recommendation to declare variables at the step level whenever possible to limit their scope and potential exposure. Prefer step level variables.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/security.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: Greeting on variable day\n\non:\n  workflow_dispatch\n\n# Workflow level variables. Avoid using these.\nenv:\n  DAY_OF_WEEK: Monday\n\njobs:\n  greeting_job:\n    runs-on: ubuntu-latest\n    # Job level variables\n    env:\n      Greeting: Hello\n    steps:\n      - name: \"Say Hello Mona it's Monday\"\n        run: echo \"$Greeting $First_Name. Today is $DAY_OF_WEEK!\"\n        # Step level variables. Prefer this approach\n        env:\n          First_Name: Mona\n```\n\n----------------------------------------\n\nTITLE: Create Python Virtual Environment and Install Libraries\nDESCRIPTION: Creates a Python virtual environment, activates it, and installs the necessary Python libraries from `docs/requirements.txt`. This isolates the dependencies for documentation building. Requires `python3` and `pip`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/documentation_build_instructions.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\n$ python3 -m venv env\n$ source env/bin/activate\n(env) $ pip install --upgrade setuptools pip\n(env) $ pip install -r docs/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Defining elementTypeString Type Alias in TypeScript\nDESCRIPTION: This code snippet defines the `elementTypeString` type alias in TypeScript.  It specifies that a variable of this type can hold one of several string literals, each representing a different data type such as unsigned and signed integers (u8, u32, u16, u64, i8, i64, i32, i16), floating-point numbers (f64, f32), or a string.  This type is likely used to constrain the allowed data types in other parts of the OpenVINO project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/types/elementTypeString.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nelementTypeString: \"u8\" | \"u32\" | \"u16\" | \"u64\" | \"i8\" | \"i64\" | \"i32\" | \"i16\" | \"f64\" | \"f32\" | \"string\"\n```\n\n----------------------------------------\n\nTITLE: Generating Header File for Conditional Compilation\nDESCRIPTION: This snippet defines a custom command to generate a header file ('conditional_compilation_gen.h') using a Python script ('ccheader.py') based on statistics files.  It then creates a custom target that depends on the generated header and adds a dependency to the main target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/conditional_compilation/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(GENERATED_HEADER ${CMAKE_CURRENT_BINARY_DIR}/conditional_compilation_gen.h CACHE FILEPATH \"\")\n    set(GENERATOR ${CMAKE_CURRENT_SOURCE_DIR}/scripts/ccheader.py)\n\n    add_custom_command(OUTPUT ${GENERATED_HEADER}\n                       COMMAND ${Python3_EXECUTABLE} ${GENERATOR} --stat ${SELECTIVE_BUILD_STAT} --out ${GENERATED_HEADER}\n                       DEPENDS ${STAT_FILES})\n    add_custom_target(conditional_compilation_gen DEPENDS ${GENERATED_HEADER})\n    add_dependencies(${TARGET_NAME} conditional_compilation_gen)\n\n    ov_force_include(${TARGET_NAME} INTERFACE ${GENERATED_HEADER})\n```\n\n----------------------------------------\n\nTITLE: Gflags Build Commands\nDESCRIPTION: These commands build the gflags library. They clone the gflags repository, create a build directory, configure CMake with an install prefix, and then build and install the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_24\n\nLANGUAGE: cmake\nCODE:\n```\ngit clone https://github.com/gflags/gflags\ncd gflags\nmkdir \"gflags_build\" && cd \"gflags_build\"\ncmake ../ -DCMAKE_INSTALL_PREFIX=install\ncmake --build . --config Release --target install --parallel\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files\nDESCRIPTION: Uses `file(GLOB)` to collect all C++ source files (`.cpp`) within the current source directory. These files are then used to build the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/test_utils/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_utils.cpp)\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target with CMake\nDESCRIPTION: This CMake code adds a clang format target for the library, enabling code formatting checks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/itt_collector/sea_itt_lib/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Binaries (Bash)\nDESCRIPTION: This command installs the compiled OpenVINO Node.js binaries to the specified installation prefix, making them available for use by the `openvino-node` package.  The installation path is defined during the cmake configuration stage.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncmake --install .\n```\n\n----------------------------------------\n\nTITLE: Set Defines for ONNX Frontend\nDESCRIPTION: This snippet defines the TEST_MODELS variable if the ENABLE_OV_ONNX_FRONTEND variable is enabled. Otherwise, it sets the EXCLUDED_SOURCE_PATHS variable to exclude the custom extension and ONNX shared test instances.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/functional/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_OV_ONNX_FRONTEND)\n    list(APPEND DEFINES TEST_MODELS=\"${TEST_MODEL_ZOO}\")\nelse()\n    set(EXCLUDED_SOURCE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/custom/extension ${CMAKE_CURRENT_SOURCE_DIR}/shared_tests_instances/onnx)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Globbing Source and Header Files (CMake)\nDESCRIPTION: This snippet uses the `file(GLOB_RECURSE)` command to find all source files (`.cpp`) in the `src` directory and all header files (`.hpp`) in the `include` directory. The results are stored in variables for later use.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/shape_inference/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE LIBRARY_SRC ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE PUBLIC_HEADERS ${SHAPE_INFER_INCLUDE_DIR}/*.hpp)\n```\n\n----------------------------------------\n\nTITLE: Accessing Tensor Input Information in TypeScript\nDESCRIPTION: This code snippet defines the `tensor()` method within the `InputInfo` interface.  It retrieves the `InputTensorInfo` object, which contains information about the input tensor, such as shape, data type, and layout. The method's return type is explicitly defined as `InputTensorInfo`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InputInfo.rst#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ntensor(): InputTensorInfo\n```\n\n----------------------------------------\n\nTITLE: Graph Rewrite Example C++\nDESCRIPTION: This C++ code snippet demonstrates how to use `ov::pass::GraphRewrite` to run multiple matcher passes on `ov::Model` in a single graph traversal. It shows a basic example within the context of template pattern transformation, showcasing the registration and execution of matcher passes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/transformation-api/graph-rewrite-pass.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n//! [matcher_pass:graph_rewrite]\n#include <openvino/pass/manager.hpp>\n#include <openvino/pass/graph_rewrite.hpp>\n\nvoid template_pattern_transformation() {\n    ov::pass::Manager manager;\n    manager.register_pass<ov::pass::GraphRewrite>();\n}\n//! [matcher_pass:graph_rewrite]\n```\n\n----------------------------------------\n\nTITLE: Python Usage Example\nDESCRIPTION: Example of how to check the usage options for the Python version of the async classification sample.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/image-classification-async.rst#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\npython classification_sample_async.py -h\n```\n\n----------------------------------------\n\nTITLE: Add Source Subdirectory CMake\nDESCRIPTION: This command adds the 'src' subdirectory to the build process. It tells CMake to look for a CMakeLists.txt file within the 'src' directory and process it.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Install CPU Checker\nDESCRIPTION: This command installs the cpu-checker package, which includes the kvm-ok utility used to verify hardware virtualization support.  The -y flag automatically confirms installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt install -y cpu-checker\n```\n\n----------------------------------------\n\nTITLE: Setting target name\nDESCRIPTION: Defines the target name for the library being built. This name is used internally by CMake to refer to the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/logger/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME openvino_npu_logger_utils)\n```\n\n----------------------------------------\n\nTITLE: Creating ov::Core instance in C++\nDESCRIPTION: This C++ code snippet shows the interface of `ov::Core` creation in OpenVINO. It demonstrates the constructor which optionally takes an `xml_config_file` parameter.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/how_to_wrap_openvino_interfaces_with_c.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nhttps://github.com/openvinotoolkit/openvino/blob/d96c25844d6cfd5ad131539c8a0928266127b05a/src/inference/include/openvino/runtime/core.hpp#L46-L58\n```\n\n----------------------------------------\n\nTITLE: Example Component Definition with Empty Dependencies (YAML)\nDESCRIPTION: This YAML snippet shows how to define a component in components.yml with no dependencies.  Empty lists under `revalidate` and `build` indicate that changes to `your_component` do not impact any other components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nyour_component:\n  revalidate: []\n  build: []\n```\n\n----------------------------------------\n\nTITLE: PartialShape Constructor with Shape Parameter in TypeScript\nDESCRIPTION: This TypeScript snippet shows the constructor for `PartialShapeConstructor` that accepts an optional shape string. It allows creating a `PartialShape` object with a defined shape, or an empty shape if no string is specified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PartialShapeConstructor.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nnew PartialShapeConstructor(shape): PartialShape\n```\n\n----------------------------------------\n\nTITLE: Defining u16 data type in TypeScript\nDESCRIPTION: Defines the u16 data type as a number in TypeScript.  This represents an unsigned 16-bit integer, suitable for storing non-negative integer values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/element.rst#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nu16: number\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version\nDESCRIPTION: This snippet sets the minimum required CMake version based on the operating system. For Windows, it requires version 3.15 and sets the CMP0091 policy to enable the use of MSVC_RUNTIME_LIBRARY. For other systems, it requires version 3.10.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n    cmake_minimum_required (VERSION 3.15)\n    if(POLICY CMP0091)\n        cmake_policy(SET CMP0091 NEW) # Enables use of MSVC_RUNTIME_LIBRARY\n    endif()\nelse()\n    cmake_minimum_required (VERSION 3.10)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding CPack Component\nDESCRIPTION: This snippet adds a hidden component for NPM packages to the CPack configuration. This is used to control packaging options for the Node.js addon.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/node/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nov_cpack_add_component(${OV_CPACK_COMP_NPM} HIDDEN)\n```\n\n----------------------------------------\n\nTITLE: Install Xcode Command Line Tools on macOS\nDESCRIPTION: This command installs Xcode Command Line Tools on macOS, providing essential tools for software development. It's a prerequisite for installing other development dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nxcode-select --install\n```\n\n----------------------------------------\n\nTITLE: Install Homebrew on macOS\nDESCRIPTION: This command installs Homebrew, a package manager for macOS, using a script downloaded from GitHub.  Homebrew simplifies the installation of other dependencies such as Python and FFMPEG.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/notebooks-installation.rst#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n```\n\n----------------------------------------\n\nTITLE: Creating a build directory\nDESCRIPTION: These commands create a `build` directory and navigate into it. This directory will contain the build artifacts generated by CMake.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nmkdir build && cd build\n```\n\n----------------------------------------\n\nTITLE: README.md Template\nDESCRIPTION: This template provides a basic structure for a README.md file in an OpenVINO component. It includes sections for component description, key contacts, components, architecture, tutorials, and related pages.  It's designed to provide a clear and concise entry point to the component's documentation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/dev_doc_guide.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Component name\n\nShort information about the component.\n * Responsibility\n * Supported features\n * Code style\n\n## Key contacts\n\nThe section provides information about groups of developers who can help in case of questions, and also review and merge PRs.\n\n## Components\n\nThe section contains basic information about the included components: API, sources, tests, etc.\nDetailed information can be located in the `docs/` folder. This section can contain links to these pages.\n\n## Architecture\n\nIs an optional section which provides main ideas about the component architecture.\nIt can contain references to the pages with additional information.\n\n## Tutorials\n\nThe section contains a list of component tutorials.\nExample:\n * [How to support new operation](./docs/operation_support.md)\n * [How to implement new feature](./docs/new_feature.md)\n * [How to debug the component](./docs/debug_capabilities.md)\n * ...\n\n## See also\n\nThe section contains a list of related pages.\nExample:\n * [OpenVINO™ README](../../README.md)\n * [Developer documentation](../../docs/dev/index.md)\n```\n\n----------------------------------------\n\nTITLE: Install Node.js Dependencies\nDESCRIPTION: This command installs the necessary dependencies for running the OpenVINO Node.js samples. It should be executed in the root directory of the examples.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/js/node/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: PrePostProcessorConstructor Constructor Definition in TypeScript\nDESCRIPTION: Defines the constructor for the PrePostProcessorConstructor interface. It accepts a Model object as input and returns a PrePostProcessor object. This constructor is used to instantiate PrePostProcessor objects with a given model.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PrePostProcessorConstructor.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nnew PrePostProcessorConstructor(model): PrePostProcessor\n```\n\n----------------------------------------\n\nTITLE: Running the Subgraphs Dumper tool\nDESCRIPTION: This is an example command for running the `ov_subgraphs_dumper` tool. It specifies the input folders containing the models and the output folder where the extracted subgraphs will be serialized.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/conformance/subgraphs_dumper/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nov_subgraphs_dumper --input_folders /dir_0/to/models,/dir_1/to/models --output_folder /path/to/dir\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Flags for MSVC\nDESCRIPTION: Adds specific compiler flags to suppress warnings related to data conversion and truncation when using the MSVC compiler. This addresses potential issues arising from oneDNN headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/tests/functional/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    # C4267, 4244 issues from oneDNN headers conversion from 'XXX' to 'YYY', possible loss of data\n    ov_add_compiler_flags(/wd4267)\n    ov_add_compiler_flags(/wd4244)\n    # 'initializing': truncation from 'XXX' to 'YYY'\n    ov_add_compiler_flags(/wd4305)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Amending the Last Commit with Sign-Off\nDESCRIPTION: This command amends the last commit to include the 'Signed-off-by' line. This is useful if you forgot to sign off the commit initially.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/commit_signoff_policy.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ngit commit --amend --signoff\n```\n\n----------------------------------------\n\nTITLE: Mersenne-Twister State Generation (C++)\nDESCRIPTION: This code shows how the next state array is generated recursively in the Mersenne-Twister algorithm. It calculates the twisted_state based on current and next states, and updates the current state.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\ncurrent_state = state[i]\nnext_state    = state[i+1] if i+1 <= 623 else state[0]\nnext_m_state  = state[i+m] if i+m <= 623 else state[i+m-623]\ntwisted_state = (((current_state & 0x80000000) | (next_state & 0x7fffffff)) >> 1) ^ (next_state & 1 ? 0x9908b0df : 0)\nstate[i] = next_m_state ^ twisted_state\n```\n\n----------------------------------------\n\nTITLE: Handle Docker Action Configuration (YAML)\nDESCRIPTION: This snippet shows how to configure the `handle_docker` action in a GitHub Actions workflow to build custom Docker images. It specifies the Dockerfiles to use, the registry, the root directory for the Dockerfiles, and the changed components.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/docker_images.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nDocker:\n    needs: Smart_CI\n    runs-on: aks-linux-4-cores-16gb-docker-build\n    container:\n      image: openvinogithubactions.azurecr.io/docker_build:0.2\n      volumes:\n        - /mount:/mount\n    outputs:\n      images: \"${{ steps.handle_docker.outputs.images }}\"\n    steps:\n      - name: Checkout\n        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # v4.1.7\n\n      - uses: ./.github/actions/handle_docker\n        id: handle_docker\n        with:\n          images: |\n            ov_build/ubuntu_22_04_x64\n            ov_build/ubuntu_22_04_x64_nvidia\n            ov_test/ubuntu_22_04_x64\n          registry: 'openvinogithubactions.azurecr.io'\n          dockerfiles_root_dir: '.github/dockerfiles'\n          changed_components: ${{ needs.smart_ci.outputs.changed_components }}\n```\n\n----------------------------------------\n\nTITLE: Setting Variables and Paths in CMake\nDESCRIPTION: This CMake snippet sets variables for the target name, code generation directories, kernel source and header filenames, and preprocessor options. These variables are used throughout the script to define paths and options for code generation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/cm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_intel_gpu_cm_obj\")\n\n# Path which points to root directory where code generated elements are created\n# (specific to build configuration).\nset(CODEGEN_DIR \"${CMAKE_CURRENT_BINARY_DIR}/codegen\")\nset(CODEGEN_CACHE_DIR \"${CODEGEN_DIR}/cache\")\n\n# Path which points to automatically included directory with code generated elements\n# (to support \"copy-if-different\" optimization).\nset(CODEGEN_INCDIR  \"${CODEGEN_DIR}/include\")\n\nset(KERNEL_SOURCES \"gpu_cm_kernel_sources.inc\")\nset(KERNEL_HEADERS \"gpu_cm_kernel_headers.inc\")\n\nset(CODEGEN_CACHE_SOURCES \"${CODEGEN_INCDIR}/${KERNEL_SOURCES}\"\n                          \"${CODEGEN_INCDIR}/${KERNEL_HEADERS}\")\n\nset(XETLA_HEADER \"cm_xetla.h\")\nset(XETLA_HEADER_FULL_PATH \"${CODEGEN_CACHE_DIR}/cm_kernels/include/batch_headers/${XETLA_HEADER}\")\nif(WIN32)\n    set(PREPROCESSOR_OPTIONS -D _WIN32 -EP)\nelse()\n    set(PREPROCESSOR_OPTIONS  -E -P)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Static Library Target in CMake\nDESCRIPTION: This command adds a static library target named `tensorflow_lite_fe_standalone_build_test` to the build process. It specifies that the source file for the target is `standalone_build_test.cpp`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/tests/standalone_build/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC standalone_build_test.cpp)\n```\n\n----------------------------------------\n\nTITLE: AsyncInferRequest Class Definition (C++)\nDESCRIPTION: This code snippet defines the AsyncInferRequest class, which inherits from ov::IAsyncInferRequest. It includes necessary members like a cancel callback and a wait executor for device tasks completion. This header file sets the stage for implementing asynchronous inference.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/asynch-inference-request.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n/**\n * @brief AsyncInferRequest class.\n * Implements asynchronous inference request.\n */\nclass AsyncInferRequest : public ov::IAsyncInferRequest {\npublic:\n    /**\n     * @brief Constructor.\n     * @param compiled_model Compiled model.\n     * @param inputs_preprocessor Preprocessor.\n     * @param wait_executor Task executor to wait for device tasks completion.\n     */\n    AsyncInferRequest(\n        const std::shared_ptr<CompiledModel>& compiled_model,\n        const PrePostProcessor& inputs_preprocessor,\n        const std::shared_ptr<ov::threading::ITaskExecutor>& wait_executor);\n\n    /**\n     * @brief Destructor.\n     */\n    ~AsyncInferRequest() override;\n\n    /**\n     * @brief Infer method.\n     */\n    void infer() override;\n\n    /**\n     * @brief Start asynchronous inference.\n     * @param inputs Input data.\n     * @param completion_callback Completion callback.\n     */\n    void start_async() override;\n\n    /**\n     * @brief Wait for the result.\n     * @param timeout Timeout.\n     */\n    void wait() override;\n\n    /**\n     * @brief Wait for the result with timeout.\n     * @param timeout Timeout.\n     * @return Status of the request.\n     */\n    ov::infer_request::Result wait_for(const std::chrono::milliseconds& timeout) override;\n\n    /**\n     * @brief Set callback to be called after inference is done.\n     * @param callback Callback.\n     */\n    void set_callback(ov::CompletionCallback callback) override;\n\n    /**\n     * @brief Get tensor.\n     * @param port Port.\n     * @return Tensor.\n     */\n    ov::Tensor get_tensor(const ov::Output<const ov::Node>& port) const override;\n\n    /**\n     * @brief Set tensor.\n     * @param port Port.\n     * @param tensor Tensor.\n     */\n    void set_tensor(const ov::Output<const ov::Node>& port, const ov::Tensor& tensor) override;\n\n    /**\n     * @brief Get input tensor.\n     * @param index Input index.\n     * @return Input tensor.\n     */\n    ov::Tensor get_input_tensor(size_t index) const override;\n\n    /**\n     * @brief Get input tensor.\n     * @param name Input name.\n     * @return Input tensor.\n     */\n    ov::Tensor get_input_tensor(const std::string& name) const override;\n\n    /**\n     * @brief Get output tensor.\n     * @param index Output index.\n     * @return Output tensor.\n     */\n    ov::Tensor get_output_tensor(size_t index) const override;\n\n    /**\n     * @brief Get output tensor.\n     * @param name Output name.\n     * @return Output tensor.\n     */\n    ov::Tensor get_output_tensor(const std::string& name) const override;\n\n    /**\n     * @brief Get preprocessor.\n     * @return Preprocessor.\n     */\n    const PrePostProcessor& get_preprocessor() const { return m_inputs_preprocessor; }\n\n    /**\n     * @brief Get compiled model.\n     * @return Compiled model.\n     */\n    std::shared_ptr<CompiledModel> get_compiled_model() const { return m_compiled_model.lock(); }\n\n    /**\n     * @brief Set cancel callback.\n     * @param callback Callback.\n     */\n    void set_cancel_callback(std::function<bool()> callback) override;\n\nprotected:\n    /**\n     * @brief Set blob.\n     * @param port Port.\n     * @param blob Blob.\n     */\n    void set_blob(const ov::Output<const ov::Node>& port, const ov::Tensor& blob);\n\nprivate:\n    /**\n     * @brief Preprocessor.\n     */\n    PrePostProcessor m_inputs_preprocessor;\n\n    /**\n     * @brief Compiled model.\n     */\n    std::weak_ptr<CompiledModel> m_compiled_model;\n\n    /**\n     * @brief Cancel callback.\n     */\n    std::function<bool()> m_cancel_callback;\n\n    /**\n     * @brief Task executor to wait for device tasks completion.\n     */\n    std::shared_ptr<ov::threading::ITaskExecutor> m_wait_executor;\n};\n```\n\n----------------------------------------\n\nTITLE: Dependencies and Constraints\nDESCRIPTION: This snippet lists the dependencies and constraints for the project. It includes setuptools, wheel, build, and patchelf. Patchelf is conditionally included based on the platform.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/requirements-dev.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n-c ../constraints.txt\nsetuptools\nwheel\nbuild\npatchelf; sys_platform == 'linux' and platform_machine == 'x86_64'\n```\n\n----------------------------------------\n\nTITLE: Define Model output Method (TypeScript) - Name Parameter\nDESCRIPTION: This code defines the `output` method of the `Model` interface with a name parameter. It gets the output of the model identified by the tensor name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\noutput(name): Output\n```\n\n----------------------------------------\n\nTITLE: Searching for Python versions using Brew\nDESCRIPTION: This command uses the `brew search` command to find available Python versions that can be installed using the Homebrew package manager.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_arm.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n% brew search python\n```\n\n----------------------------------------\n\nTITLE: Installing static library\nDESCRIPTION: Installs the static library to the installation directory. `ov_install_static_lib` is assumed to be a macro defined elsewhere. `NPU_PLUGIN_COMPONENT` likely defines the subdirectory for installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/logger/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${NPU_PLUGIN_COMPONENT})\n```\n\n----------------------------------------\n\nTITLE: toString Method Definition (TypeScript)\nDESCRIPTION: Defines the `toString` method of the `PartialShape` interface, which returns a string representation of the shape. This method can be used for debugging and logging purposes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PartialShape.rst#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ntoString(): string\n```\n\n----------------------------------------\n\nTITLE: Define Wheel Name and Output Directory - CMake\nDESCRIPTION: This CMake code defines the full wheel name using variables like `WHEEL_VERSION`, `WHEEL_BUILD`, `PYTHON_TAG`, `ABI_TAG`, and `PLATFORM_TAG`. It also sets the output directory for the generated wheels.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/wheel/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset(openvino_wheel_name \"openvino-${WHEEL_VERSION}-${WHEEL_BUILD}-${PYTHON_TAG}-${ABI_TAG}-${PLATFORM_TAG}.whl\")\nset(openvino_wheels_output_dir \"${CMAKE_BINARY_DIR}/wheels\")\nset(openvino_wheel_path \"${openvino_wheels_output_dir}/${openvino_wheel_name}\")\n```\n\n----------------------------------------\n\nTITLE: Finding and Linking Libraries (CMake)\nDESCRIPTION: Finds the Threads package and links the necessary libraries to the target. It links both `Threads::Threads` and `openvino::core::dev`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(Threads REQUIRED)\ntarget_link_libraries(${TARGET_NAME} PRIVATE Threads::Threads openvino::core::dev)\n```\n\n----------------------------------------\n\nTITLE: Slice: Slicing Backwards with Step -1 and Stop -10 in OpenVINO XML\nDESCRIPTION: This example demonstrates slicing a tensor backwards from the end, stopping before index -10 (relative to the end).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-8.rst#_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Slice\" ...>\n       <input>\n           <port id=\"0\">       <!-- data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -->\n             <dim>10</dim>\n           </port>\n           <port id=\"1\">       <!-- start: [9] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"2\">       <!-- stop: [-10] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"3\">       <!-- step: [-1] -->\n             <dim>1</dim>\n           </port>\n           <port id=\"4\">       <!-- axes: [0] -->\n             <dim>1</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">       <!-- output: [9, 8, 7, 6, 5, 4, 3, 2, 1] -->\n               <dim>9</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Dependency List\nDESCRIPTION: This snippet lists the Python dependencies for the OpenVINO project. These packages are required to run the project. They can be installed using a package manager like pip (e.g., `pip install -r requirements.txt` assuming this list is in a `requirements.txt` file).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/scripts/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-c ../../constraints.txt\npymongo\nJinja2\nPyYAML\npandas\nh5py\nscipy\ndefusedxml\n```\n\n----------------------------------------\n\nTITLE: Bootstrapping vcpkg\nDESCRIPTION: This command bootstraps the vcpkg environment, preparing it for use.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/docs/use_device_mem.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n> .\\bootstrap-vcpkg.bat\n```\n\n----------------------------------------\n\nTITLE: OpenVINO IR Frontend Architecture Diagram\nDESCRIPTION: This diagram illustrates the OpenVINO IR Frontend architecture, highlighting the interaction between pugixml, the frontend, the input model, the XML deserializer, and the OpenVINO model. It shows how the frontend loads the IR model as a stream, parses it using pugixml, and converts it to an OpenVINO model via the InputModel and XmlDeserializer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/ir/docs/architecture.md#_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TB\n    fw_model[(IR)]\n    style fw_model fill:#427cb0\n    \n    pugixml([pugixml])\n    subgraph frontend [ov::frontend::ir::FrontEnd]\n        load_impl[\"load_impl()\"]\n    end\n    fw_model--as stream-->load_impl\n    load_impl--load stream-->pugixml\n    pugixml--parsed object-->load_impl\n    \n    \n    subgraph input_model [ov::frontend::ir::InputModel]\n        convert[\"convert()\"]\n    end\n    \n    load_impl--create-->input_model\n    \n    xml_deserializer(ov::XmlDeserializer)\n    ov_model[ov::Model]\n    \n    convert--create visitor-->xml_deserializer\n    \n    xml_deserializer--recursively parse all operations from the model-->ov_model\n```\n\n----------------------------------------\n\nTITLE: Setting threading interface\nDESCRIPTION: Sets the threading interface for the target, likely using a custom OpenVINO macro.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Setting Output Root Directory\nDESCRIPTION: Defines the root directory where build artifacts will be placed. This variable is set to the current source directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(OUTPUT_ROOT ${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Setting Threading Interface for Test Target in CMake\nDESCRIPTION: This snippet uses the custom `ov_set_threading_interface_for` CMake function to set the threading interface for the defined test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/tests/functional/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Enabling MLAS Integration\nDESCRIPTION: Conditionally enables the MLAS (Microsoft Linear Algebra Subprograms) library for the CPU plugin.  It links the `mlas` library, adds its include directories, and defines the `OV_CPU_WITH_MLAS` macro.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_32\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_MLAS_FOR_CPU)\n    target_link_libraries(${TARGET_NAME} PRIVATE mlas)\n    target_include_directories(${TARGET_NAME} SYSTEM PRIVATE $<TARGET_PROPERTY:mlas,INCLUDE_DIRECTORIES>)\n    add_definitions(-DOV_CPU_WITH_MLAS)\nendif()\n```\n\n----------------------------------------\n\nTITLE: OpenCV Repository Cloning\nDESCRIPTION: This command clones the OpenCV repository from GitHub and checks out a specific commit (3919f33e21). This ensures a consistent version of OpenCV is used for building Protopipe.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/opencv/opencv\ncd opencv && git checkout 3919f33e21\n```\n\n----------------------------------------\n\nTITLE: Uninstalling OpenVINO Runtime\nDESCRIPTION: This command uninstalls the OpenVINO Runtime package with the specified version (2025.1.0) from the current Conda environment. It removes the core OpenVINO Runtime package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-conda.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nconda remove openvino=2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Including CMake Configuration File\nDESCRIPTION: This snippet includes the specified CMake configuration file, which likely defines the build process and dependencies for C++ samples in the OpenVINO toolkit. The included file contains necessary configurations for compiling and linking the samples.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/c/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(\"${OpenVINO_SOURCE_DIR}/samples/cpp/CMakeLists.txt\")\n```\n\n----------------------------------------\n\nTITLE: Installing Fuzz Test Targets in CMake\nDESCRIPTION: This snippet installs the fuzz test target to the `tests` directory.  The `RUNTIME` destination specifies where the executable files should be installed. The `COMPONENT tests` setting associates the installed target with the 'tests' component, and `EXCLUDE_FROM_ALL` prevents the tests from being included in the default install target. This ensures the fuzz tests are correctly packaged and available for execution after installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/src/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${test_name}\n            RUNTIME DESTINATION tests COMPONENT tests EXCLUDE_FROM_ALL)\n```\n\n----------------------------------------\n\nTITLE: Navigating to Downloads Directory\nDESCRIPTION: This command navigates the user to their Downloads directory, where the OpenVINO archive file will be downloaded. `<user_home>` represents the user's home directory.  This is a prerequisite to downloading and extracting the archive.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncd <user_home>/Downloads\n```\n\n----------------------------------------\n\nTITLE: IsFinite Layer XML Definition\nDESCRIPTION: Defines an IsFinite layer in OpenVINO's XML format. This example shows a layer taking a FP32 tensor as input and producing a boolean tensor of the same shape as output.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/isfinite-10.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IsFinite\">\n    <input>\n        <port id=\"0\" precision=\"FP32\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\" precision=\"BOOL\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining Resize Method in TypeScript\nDESCRIPTION: Defines the resize method, which takes an algorithm as a parameter and returns a PreProcessSteps object. The algorithm parameter can be a string or a resizeAlgorithm enum.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/PreProcessSteps.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nresize(algorithm): PreProcessSteps\n```\n\n----------------------------------------\n\nTITLE: Setting the Target Name in CMake\nDESCRIPTION: This CMake command sets the name of the target to 'ov_util_tests'.  This target name is used later when defining the test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/common_test_utils/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME ov_util_tests)\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Git Pre-Commit Hook\nDESCRIPTION: This bash command uninstalls the pre-commit hook from the local Git repository. This removes the automated stub file generation process, allowing developers to commit changes without triggering the hook.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/stubs.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\npre-commit uninstall\n```\n\n----------------------------------------\n\nTITLE: Running Image Classification Sample (Python, macOS)\nDESCRIPTION: This command runs the `classification_sample_async.py` sample with a specified model (`googlenet-v1.xml`), input image (`dog.bmp`), and target device (`CPU`) on a macOS system.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_31\n\nLANGUAGE: sh\nCODE:\n```\npython classification_sample_async.py -m ~/ir/googlenet-v1.xml -i ~/Downloads/dog.bmp -d CPU\n```\n\n----------------------------------------\n\nTITLE: Setting JSON_SystemInclude Variable in CMake\nDESCRIPTION: This CMake snippet sets the JSON_SystemInclude variable to ON, which configures the include paths for the nlohmann_json library. The CACHE BOOL and FORCE arguments ensure that the variable is persistently stored in the CMake cache and that its value is always overridden.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/json/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(JSON_SystemInclude ON CACHE BOOL \"\" FORCE)\n```\n\n----------------------------------------\n\nTITLE: Wrap cl_mem (C++)\nDESCRIPTION: This snippet shows how to create an `ov::RemoteTensor` by wrapping an existing OpenCL memory object (`cl_mem`) using the OpenVINO C++ API. It retrieves the `ClContext` and creates a tensor from the memory object. Requires OpenVINO runtime, intel_gpu ocl, and CL/cl.hpp.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n//! [wrap_cl_mem]\n#include <openvino/runtime/core.hpp>\n#include <openvino/runtime/intel_gpu/ocl/ocl.hpp>\n\n#if defined(OPENVINO_USE_OPENCL)\n#include <CL/cl.hpp>\n#endif\n\nvoid wrap_cl_memory(cl_mem mem_obj, size_t size) {\n    // Wrap cl_mem to remote tensor\n    ov::Core core;\n    auto context = core.get_default_context(\"GPU\").as<ov::intel_gpu::ocl::ClContext>();\n    ov::Tensor remote_tensor = context.create_tensor(ov::element::f32, {ov::Dimension(size)}, mem_obj);\n    (void)remote_tensor;\n}\n//! [wrap_cl_mem]\n```\n\n----------------------------------------\n\nTITLE: Creating Static Library in CMake\nDESCRIPTION: This snippet creates a static library named `TARGET_NAME` (which is `openvino_util`) from the source files listed in `LIBRARY_SRC` and the header files in `PUBLIC_HEADERS`. The `STATIC` keyword specifies that a static library should be created.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/util/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC ${LIBRARY_SRC} ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target in CMake\nDESCRIPTION: This snippet adds a custom target for running clang-format on the code.  It uses the ov_add_clang_format_target macro (defined elsewhere) to create a target named ov_appverifier_tests_clang that will format the source code associated with the ov_appverifier_tests target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/appverifier_tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Defining Target Name in CMake\nDESCRIPTION: This snippet sets the target name for the library to 'npu_tools_utils'. This variable is later used in the add_library command to define the library name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"npu_tools_utils\")\n```\n\n----------------------------------------\n\nTITLE: Type Dependent Remote Tensor Implementation\nDESCRIPTION: This code snippet demonstrates the implementation of a type-dependent remote tensor. It includes class fields for tensor element type, shape, strides, wrapped vector data, device name, and properties. The constructor initializes these fields. It also overrides methods like get_element_type, get_shape, get_strides, set_shape, get_properties, and get_device_name to provide access to tensor metadata.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/remote-tensor.rst#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n/*! [vector_impl_t:implementation]\n*/\n#include <template/remote_tensor.hpp>\n\n#include <openvino/runtime/remote_context.hpp>\n\n#include <vector>\n\nnamespace template_plugin {\n\ntemplate <typename T>\nclass VectorTensorImplT : public ov::RemoteTensor {\nprivate:\n    ov::element::Type m_element_type;\n    ov::Shape m_shape;\n    ov::Strides m_strides;\n    std::vector<T> m_data;\n    std::string m_dev_name;\n    ov::RemoteTensor::Properties m_properties;\n    std::shared_ptr<ov::RemoteContext> m_context;\n\npublic:\n    VectorTensorImplT(const ov::element::Type& element_type, const ov::Shape& shape, const std::shared_ptr<ov::RemoteContext>& context) : \n        m_element_type(element_type), m_shape(shape), m_context(context) {\n        m_dev_name = context->get_device_name();\n        ov::Tensor::set_shape(shape);\n        m_strides = ov::row_major_strides(shape);\n\n        size_t tensor_size = ov::shape_size(shape);\n        m_data.resize(tensor_size);\n    }\n\n    const void* get_data() const override { return m_data.data(); }\n    void* get_data() override { return m_data.data(); }\n    ov::element::Type get_element_type() const override { return m_element_type; }\n    ov::Shape get_shape() const override { return m_shape; }\n    ov::Strides get_strides() const override { return m_strides; }\n    void set_shape(const ov::Shape& shape) override {\n        ov::Tensor::set_shape(shape);\n        m_shape = shape;\n        m_strides = ov::row_major_strides(shape);\n        size_t tensor_size = ov::shape_size(shape);\n        m_data.resize(tensor_size);\n    }\n    const ov::RemoteTensor::Properties& get_properties() const override { return m_properties; }\n    std::string get_device_name() const override { return m_dev_name; }\n    std::shared_ptr<ov::RemoteContext> get_context() const override { return m_context; }\n};\n\n}  // namespace template_plugin\n/*! [vector_impl_t:implementation] */\n```\n\n----------------------------------------\n\nTITLE: Terminal Interaction Notice\nDESCRIPTION: This code snippet demonstrates a short notice to be displayed when a program starts in interactive mode. It includes a copyright notice, a disclaimer of warranty, and information about the software being free and redistributable.  The hypothetical commands `show w` and `show c` should show the appropriate parts of the General Public License.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/onetbb_third-party-programs.txt#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\n<program>  Copyright (C) <year>  <name of author>\nThis program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; type `show c' for details.\n```\n\n----------------------------------------\n\nTITLE: Define OpenVINO Common Properties (C)\nDESCRIPTION: These macros define common property keys used for configuring OpenVINO. They are used to set various performance and device-related parameters.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_14\n\nLANGUAGE: C\nCODE:\n```\nOPENVINO_C_VAR(const char*) ov_property_key_supported_properties;\n\nOPENVINO_C_VAR(const char*) ov_property_key_available_devices;\n\nOPENVINO_C_VAR(const char*) ov_property_key_optimal_number_of_infer_requests;\n\nOPENVINO_C_VAR(const char*) ov_property_key_range_for_async_infer_requests;\n\nOPENVINO_C_VAR(const char*) ov_property_key_range_for_streams;\n\nOPENVINO_C_VAR(const char*) ov_property_key_device_full_name;\n\nOPENVINO_C_VAR(const char*) ov_property_key_device_capabilities;\n\nOPENVINO_C_VAR(const char*) ov_property_key_model_name;\n\nOPENVINO_C_VAR(const char*) ov_property_key_optimal_batch_size;\n\nOPENVINO_C_VAR(const char*) ov_property_key_max_batch_size;\n\nOPENVINO_C_VAR(const char*) ov_property_key_cache_dir;\n\nOPENVINO_C_VAR(const char*) ov_property_key_num_streams;\n\nOPENVINO_C_VAR(const char*) ov_property_key_affinity;\n\nOPENVINO_C_VAR(const char*) ov_property_key_inference_num_threads;\n\nOPENVINO_C_VAR(const char*) ov_property_key_hint_enable_cpu_pinning;\n\nOPENVINO_C_VAR(const char*) ov_property_key_hint_enable_hyper_threading;\n\nOPENVINO_C_VAR(const char*) ov_property_key_hint_performance_mode;\n\nOPENVINO_C_VAR(const char*) ov_property_key_hint_scheduling_core_type;\n\nOPENVINO_C_VAR(const char*) ov_property_key_hint_inference_precision;\n\nOPENVINO_C_VAR(const char*) ov_property_key_hint_num_requests;\n\nOPENVINO_C_VAR(const char*) ov_property_key_log_level;\n\nOPENVINO_C_VAR(const char*) ov_property_key_hint_model_priority;\n\nOPENVINO_C_VAR(const char*) ov_property_key_enable_profiling;\n\nOPENVINO_C_VAR(const char*) ov_property_key_device_priorities;\n\nOPENVINO_C_VAR(const char*) ov_property_key_hint_execution_mode;\n\nOPENVINO_C_VAR(const char*) ov_property_key_force_tbb_terminate;\n\nOPENVINO_C_VAR(const char*) ov_property_key_enable_mmap;\n\nOPENVINO_C_VAR(const char*) ov_property_key_auto_batch_timeout;\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Version File Path\nDESCRIPTION: This shows the path to the version.txt file within the unzipped OpenVINO archive. This file contains the OpenVINO version information.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/release-notes-openvino/release-policy.rst#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n<UNZIPPED_ARCHIVE_ROOT>/runtime/version.txt\n```\n\n----------------------------------------\n\nTITLE: Navigate to Python API Folder Shell\nDESCRIPTION: Navigates the current directory to the OpenVINO™ Python API folder to make subsequent commands simpler.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/docs/test_examples.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd .../openvino/src/bindings/python/\n```\n\n----------------------------------------\n\nTITLE: Inference with OpenVINO - TensorFlow\nDESCRIPTION: Performs inference on a TensorFlow model converted to OpenVINO Intermediate Representation (IR).  Demonstrates how to run the quantized TensorFlow model with OpenVINO for optimized performance.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/model-optimization-guide/compressing-models-during-training/quantization-aware-training.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nie = Core()\n        net = ie.read_model(model=path_to_xml)\n        compiled_model = ie.compile_model(model=net, device_name=\"CPU\")\n        output_layer = next(iter(compiled_model.outputs))\n        req = compiled_model.create_infer_request()\n        results = req.infer({input_tensor_name: image})\n        return results[output_layer]\n```\n\n----------------------------------------\n\nTITLE: Install Python dependencies\nDESCRIPTION: Installs the required Python packages: argparse and colorama. These packages are necessary for running the JIT dump disassembler tool.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tools/dump_jit_disassm/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install argparse\npython3 -m pip install colorama\n```\n\n----------------------------------------\n\nTITLE: Add OpenVINO Tokenizers Extension (Linux)\nDESCRIPTION: Adds the OpenVINO Tokenizers extension to the OpenVINO Core object in C++ for Linux, enabling the use of the tokenizers in inference pipelines.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow-generative/ov-tokenizers.rst#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ncore.add_extension(\"libopenvino_tokenizers.so\")\n```\n\n----------------------------------------\n\nTITLE: Setting Python Version Variable in CMake\nDESCRIPTION: This CMake code snippet sets a variable `pyversion` to represent the Python version, combining the major and minor version numbers obtained from `FindPython3`. This variable can be used later to specify Python-version-specific paths or dependencies.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n# Python3_VERSION_MAJOR and Python3_VERSION_MINOR are defined by FindPython3\nset(pyversion python${Python3_VERSION_MAJOR}.${Python3_VERSION_MINOR})\n```\n\n----------------------------------------\n\nTITLE: ITT Library Alias and Installation CMake\nDESCRIPTION: This snippet creates an alias for the `ittnotify` target named `ittapi::ittnotify`. It exports the target for developer packages and installs the static library using `ov_install_static_lib` within the core component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ittapi/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\n# create alias ittapi::ittnotify\nadd_library(ittapi::ittnotify ALIAS ittnotify)\n\nov_developer_package_export_targets(TARGET ittapi::ittnotify)\nov_install_static_lib(ittnotify ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Samples Release Directory on Windows\nDESCRIPTION: This command navigates to the OpenVINO C++ samples release directory on a Windows system. It uses the `%USERPROFILE%` environment variable to locate the user's documents folder.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_21\n\nLANGUAGE: bat\nCODE:\n```\ncd  %USERPROFILE%\\Documents\\Intel\\OpenVINO\\openvino_samples_build\\intel64\\Release\n```\n\n----------------------------------------\n\nTITLE: ITT Package Finding or Subdirectory Inclusion CMake\nDESCRIPTION: This snippet attempts to find an existing ITT installation using `find_package`. If ITT is not found, it adds the ITT source directory as a subdirectory to be built. It sets a warning message if profiling is enabled but ITT is not found.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/ittapi/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(DEFINED INTEL_VTUNE_DIR OR DEFINED ENV{INTEL_VTUNE_DIR})\n    find_package(ITT\n                 PATHS \"${OpenVINO_SOURCE_DIR}/src/common/itt/cmake\"\n                 NO_DEFAULT_PATH)\n    if(NOT ITT_FOUND)\n        message(WARNING \"Profiling option enabled, but no ITT library was found under INTEL_VTUNE_DIR\")\n    endif()\nelse()\n    add_subdirectory(ittapi EXCLUDE_FROM_ALL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Define Input Dimensions in C for OpenVINO\nDESCRIPTION: This code snippet shows how to define the dimensions of an input tensor (INPUT0) in C for use with OpenVINO.  The `INPUT0_DIMS_SIZE` macro specifies the number of dimensions, and `INPUT0_DIMS` is an array holding the size of each dimension. This is used to define the shape of the input tensor passed to the kernel.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/custom-gpu-operations.rst#_snippet_4\n\nLANGUAGE: c\nCODE:\n```\n#define INPUT0_DIMS_SIZE 4\n#define INPUT0_DIMS (int []){ 1,96,55,55, }\n```\n\n----------------------------------------\n\nTITLE: Compiler Flags for GCC/Clang CMake\nDESCRIPTION: Applies compiler-specific flags for GCC or Clang. It disables treating warnings as errors (`-Wno-error`), undef warnings (`-Wno-undef`), missing declaration warnings (`-Wno-missing-declarations`), and unused-but-set-variable warnings. These flags are useful to silence known warnings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/thirdparty/level_zero/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nelseif(CMAKE_COMPILER_IS_GNUCXX OR OV_COMPILER_IS_CLANG)\n    ov_add_compiler_flags(-Wno-error)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} \\\n        -Wno-undef \\\n        -Wno-missing-declarations\")\n    if(UNUSED_BUT_SET_VARIABLE_SUPPORTED)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-unused-but-set-variable\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: ONNX Frontend Architecture Diagram\nDESCRIPTION: This diagram shows the architecture of the ONNX Frontend, illustrating the flow of data from an ONNX model to an OpenVINO model. It highlights the key components involved in the import process, including the `ov::frontend::onnx::InputModel` and `ov::frontend::onnx::FrontEnd` classes, as well as the `load_impl()` and `convert()` methods. The diagram is created using Mermaid syntax.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/README.md#_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    onnx[(\"ONNX (*.onnx)\")]\n    ov_model[(\"OV Model\")]\n\n    subgraph InputModel[\"ov::frontend::onnx::InputModel\"]\n    end\n\n    subgraph Frontend[\"ov::frontend::onnx::FrontEnd\"]\n        fe_load[\"load_impl()\"]\n        fe_convert[\"convert()\"]\n    end\n\n    style onnx fill:#6c9f7f\n    style ov_model fill:#6c9f7f\n\n    onnx-->|protobuf|fe_load\n    fe_load-->InputModel\n    InputModel-->fe_convert\n    fe_convert-->ov_model\n```\n\n----------------------------------------\n\nTITLE: Setting target properties\nDESCRIPTION: Sets the folder and C++ standard for the target. The folder is set to the current source directory, and the C++ standard is set to 17.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES\n                          FOLDER ${CMAKE_CURRENT_SOURCE_DIR}\n                          CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: Install OpenVINO Pre-Release Wheels with pip\nDESCRIPTION: This command installs a pre-release version of OpenVINO using pip, specifying the extra index URL to the OpenVINO pre-release wheels repository on AWS S3. The `--pre` flag is used to allow the installation of development builds and release candidates.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/release-notes-openvino/release-policy.rst#_snippet_1\n\nLANGUAGE: py\nCODE:\n```\npip install --pre openvino --extra-index-url\nhttps://storage.openvinotoolkit.org/simple/wheels/pre-release\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 for Xbyak_aarch64\nDESCRIPTION: This snippet presents the Apache License, Version 2.0, governing the use, reproduction, and distribution of the Xbyak_aarch64 component within the OpenVINO project. It outlines the terms and conditions related to copyright, patent licenses, redistribution, submission of contributions, disclaimer of warranty, and limitation of liability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/onednn_third-party-programs.txt#_snippet_0\n\nLANGUAGE: None\nCODE:\n```\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n```\n\n----------------------------------------\n\nTITLE: Adding Setupvars Component with CPack CMake\nDESCRIPTION: This snippet configures the installation of setupvars scripts. It conditionally includes shell scripts for Unix-like systems (.sh) and batch/powershell scripts for Windows (.bat, .ps1). The scripts are installed to the root directory and assigned to the `OV_CPACK_COMP_SETUPVARS` component.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/scripts/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nov_cpack_add_component(${OV_CPACK_COMP_SETUPVARS} HIDDEN)\n\nif(UNIX)\n    set(_setupvars_files \"${CMAKE_CURRENT_SOURCE_DIR}/setupvars/setupvars.sh\")\nelseif(WIN32)\n    # setupvars.bat\n    set(_setupvars_file \"setupvars/setupvars.bat\")\n    set(_setupvars_files \"${CMAKE_CURRENT_SOURCE_DIR}/${_setupvars_file}\")\n\n    # setupvars.ps1\n    set(_setupvars_file_pwsh \"setupvars/setupvars.ps1\")\n    list(APPEND _setupvars_files \"${CMAKE_CURRENT_SOURCE_DIR}/${_setupvars_file_pwsh}\")\nendif()\n\ninstall(PROGRAMS ${_setupvars_files}\n        DESTINATION .\n        COMPONENT ${OV_CPACK_COMP_SETUPVARS}\n        ${OV_CPACK_COMP_SETUPVARS_EXCLUDE_ALL})\n```\n\n----------------------------------------\n\nTITLE: Navigate to JavaScript API folder\nDESCRIPTION: Navigates to the JavaScript API folder within the OpenVINO repository. This is a prerequisite for running tests.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/test_examples.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd <openvino_repo>/src/bindings/js/node\n```\n\n----------------------------------------\n\nTITLE: Add Subdirectories for Tests CMake\nDESCRIPTION: These snippets conditionally add subdirectories for the core and general tests based on the value of the ENABLE_TESTS variable. If ENABLE_TESTS is true, the tests will be included in the build process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_TESTS)\n    add_subdirectory(core/tests)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: NV12 Preprocessing with Two Planes (Multiple Batches) in OpenVINO (C++)\nDESCRIPTION: Demonstrates how to handle NV12 format preprocessing in a multiple batches scenario within OpenVINO, utilizing two planes. This involves setting up the input tensors correctly to process multiple NV12 images in a batched manner. Requires OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_38\n\nLANGUAGE: cpp\nCODE:\n```\n//! [batched_case]\nstd::vector<ov::Tensor> input_tensors;\nfor (size_t i = 0; i < batch_size; ++i) {\n    input_tensors.emplace_back(ov::element::u8, {height + height / 2, width}, input_data + i * height * width * 3 / 2);\n}\ninfer_request.set_tensor(input_port, input_tensors);\n//! [batched_case]\n```\n\n----------------------------------------\n\nTITLE: Counting AVX/SSE Transitions with SDE\nDESCRIPTION: This snippet demonstrates how to use Intel SDE to count the number of AVX/SSE transitions for the current host while running a benchmark application. It requires the SDE binary and the benchmark_app executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/cpu_emulation.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n/path/to/sde -ast -- ./benchmark_app -m path/to/model.xml\n```\n\n----------------------------------------\n\nTITLE: List Available OpenVINO Packages\nDESCRIPTION: This command lists all available OpenVINO packages in the configured YUM repositories. It uses the `yum list` command with a wildcard to find packages starting with 'openvino'. It is used to check what OpenVINO packages are available for installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-yum.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nyum list 'openvino*'\n```\n\n----------------------------------------\n\nTITLE: Setting Threading Interface in CMake\nDESCRIPTION: This snippet uses the `ov_set_threading_interface_for` macro to configure the threading interface for the target `${TARGET_NAME}`. This likely sets up the appropriate threading library based on the build environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/tests/unit/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Finding OpenCV\nDESCRIPTION: Uses the `find_package` command to locate the OpenCV library with specified components (core, imgproc, imgcodecs). The QUIET option suppresses messages if OpenCV is not found.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenCV QUIET COMPONENTS core imgproc imgcodecs)\n```\n\n----------------------------------------\n\nTITLE: Build Documentation with CMake\nDESCRIPTION: Configures and builds the documentation using CMake.  `ENABLE_DOCS` must be set to `ON`.  Additional CMake variables can be used to enable building specific API documentation or notebooks.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/documentation_build_instructions.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\n(env) $ cmake .. -DENABLE_DOCS=ON\n(env) $ cmake --build . --target sphinx_docs\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard\nDESCRIPTION: This snippet sets the C++ standard to C++17 if CMAKE_CXX_STANDARD is not already defined. It also disables compiler extensions and requires the specified standard.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED CMAKE_CXX_STANDARD)\n    set (CMAKE_CXX_STANDARD 17)\n    set (CMAKE_CXX_EXTENSIONS OFF)\n    set (CMAKE_CXX_STANDARD_REQUIRED ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Install Static Library CMake\nDESCRIPTION: This snippet calls the `ov_install_static_lib` function to install the `openvino_snippets` static library. The `OV_CPACK_COMP_CORE` variable likely specifies the component under which the library should be installed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: Adding Standalone Build Subdirectory CMake\nDESCRIPTION: Adds the `standalone_build` subdirectory to the build process and creates a dependency between the main test target and the standalone build test target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/tests/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(standalone_build)\nadd_dependencies(${TARGET_NAME} tensorflow_fe_standalone_build_test)\n```\n\n----------------------------------------\n\nTITLE: Define CI Build Number\nDESCRIPTION: Defines a compile definition for the CI build number, setting it to \"mock_version\".\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tests/functional/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_definitions(${TARGET_NAME} PRIVATE CI_BUILD_NUMBER=\"mock_version\")\n```\n\n----------------------------------------\n\nTITLE: Setting Layout\nDESCRIPTION: The `setLayout` method sets the layout of the output tensor.  It accepts a `layout` string parameter and returns an `InputTensorInfo` object for method chaining.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/OutputTensorInfo.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nsetLayout(layout): InputTensorInfo\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Line Length\nDESCRIPTION: This rule enforces a maximum line length of 80 characters in JavaScript and TypeScript code. It helps maintain code readability and is enforced by ESLint with the configuration `max-len: ['error']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nmax-len: ['error']\n```\n\n----------------------------------------\n\nTITLE: Defining i8 data type in TypeScript\nDESCRIPTION: Defines the i8 data type as a number in TypeScript. This represents an 8-bit integer, typically used for smaller integer values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/element.rst#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\ni8: number\n```\n\n----------------------------------------\n\nTITLE: Defining OpenVINO JAX Frontend with CMake\nDESCRIPTION: This CMake code snippet defines an OpenVINO frontend named 'jax' for loading and converting JAX/Flax models. It specifies that the frontend is linkable, disables C++ installation, provides a file description, and links against the 'openvino::util' and 'openvino::core::dev' libraries. It uses `ov_add_frontend` CMake function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/jax/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nov_add_frontend(NAME jax\n                LINKABLE_FRONTEND\n                DISABLE_CPP_INSTALL\n                FILEDESCRIPTION \"FrontEnd to load and convert JAX/Flax models\"\n                LINK_LIBRARIES openvino::util openvino::core::dev)\n```\n\n----------------------------------------\n\nTITLE: Defining Test Target with CMake\nDESCRIPTION: This snippet defines the test target, specifies its dependencies, includes directories, defines, and labels. It utilizes the `ov_add_test_target` macro to configure the target with required dependencies, libraries, and settings, ensuring proper test setup.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/template/tests/functional/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_test_target(\n        NAME ${TARGET_NAME}\n        ROOT ${CMAKE_CURRENT_SOURCE_DIR}\n        DEPENDENCIES\n            openvino_template_plugin\n        LINK_LIBRARIES\n            openvino::funcSharedTests\n            openvino::runtime::dev\n        INCLUDES\n            \"${OpenVINOTemplatePlugin_SOURCE_DIR}/include\"\n            \"${CMAKE_CURRENT_SOURCE_DIR}/op_reference\"\n        DEFINES\n            IN_OV_COMPONENT            \n        ADD_CLANG_FORMAT\n        LABELS\n            OV UNIT TEMPLATE\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Makefiles with CMake for OpenVINO Samples (macOS)\nDESCRIPTION: This command uses CMake to generate Makefiles for building the OpenVINO C++ samples in release configuration. It specifies the build type and the path to the samples directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_BUILD_TYPE=Release <INSTALL_DIR>/samples/cpp\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name\nDESCRIPTION: This snippet sets the name of the target library to `openvino_intel_gpu_runtime`. This name is used in subsequent commands to refer to the library being built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/runtime/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_intel_gpu_runtime\")\n```\n\n----------------------------------------\n\nTITLE: Setting GITHUB_TOKEN Permissions (job level)\nDESCRIPTION: This snippet shows how to declare specific permissions for the `GITHUB_TOKEN` at the job level in a GitHub Actions workflow. This ensures that the token has only the necessary permissions for a particular job, following the principle of least privilege.  It defines write permissions for issues and pull requests for the `stale` job.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/security.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\njobs:\n  stale:\n    runs-on: ubuntu-latest\n\n    # GITHUB_TOKEN will have only these permissions for\n    # `stale` job\n    permissions:\n      issues: write\n      pull-requests: write\n\n    steps:\n      - uses: actions/stale@f7176fd3007623b69d27091f9b9d4ab7995f0a06\n```\n\n----------------------------------------\n\nTITLE: Define Model input Method (TypeScript) - No Parameters\nDESCRIPTION: This code defines the `input` method of the `Model` interface without parameters. It gets the input of the model and throws an exception if the model has more than one input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\ninput(): Output\n```\n\n----------------------------------------\n\nTITLE: Setting Memory Statistics Path to Standard Output\nDESCRIPTION: This environment variable dumps memory usage statistics to the standard output when the compiled model is destructed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nOV_CPU_MEMORY_STATISTICS_PATH=cout\n```\n\n----------------------------------------\n\nTITLE: Embedding Notebook Selector (HTML)\nDESCRIPTION: This snippet embeds an iframe that displays a notebook selector from the openvino_notebooks repository. It allows users to browse and access available Jupyter notebooks directly from the current page.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python.rst#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<script type=\"module\" crossorigin src=\"https://openvinotoolkit.github.io/openvino_notebooks/assets/embedded.js\"></script>\n<iframe id=\"notebooks-selector\" src=\"https://openvinotoolkit.github.io/openvino_notebooks/\" style=\"width: 100%; border: none;\" title=\"OpenVINO™ Notebooks - Jupyter notebook tutorials for OpenVINO™\"></iframe>\n```\n\n----------------------------------------\n\nTITLE: Create RemoteContext from VADisplay (C)\nDESCRIPTION: This snippet demonstrates how to create an `ov::RemoteContext` from an existing VA Display (`VADisplay`) using the GPU plugin's C API. It initializes the OpenVINO core and retrieves the extension. Requires the OpenVINO runtime and intel_gpu ocl headers.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/running-inference/inference-devices-and-modes/gpu-device/remote-tensor-api-gpu-plugin.rst#_snippet_7\n\nLANGUAGE: c\nCODE:\n```\n//! [context_from_va_display]\n#include <openvino/runtime.h>\n#include <openvino/runtime/intel_gpu/ocl/va.h>\n\n#if defined(OPENVINO_USE_VAAPI)\n#include <va/va.h>\n#endif\n\nvoid create_context_from_va_display(VADisplay display) {\n    // context from user-defined VA display\n    ov_core_t* core = ov_core_create();\n    ov_remote_context remote_context = ov_core_get_extension_va(core, display);\n    ov_remote_context_free(remote_context);\n    ov_core_free(core);\n}\n//! [context_from_va_display]\n```\n\n----------------------------------------\n\nTITLE: Finding OpenCV Package and Checking Version in CMake\nDESCRIPTION: This snippet uses `find_package` to locate the OpenCV library, specifically requiring the `gapi` component.  It then checks if the OpenCV version is less than 4.9. If so, it disables the NPU protopipe tool and exits, as the required dependencies are not met.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenCV QUIET COMPONENTS gapi)\nif(OpenCV_VERSION VERSION_LESS 4.9)\n    message(STATUS \"NPU ${TARGET_NAME} tool is disabled due to missing dependencies: gapi from OpenCV >= 4.9.\")\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining OVAny Type Alias in TypeScript\nDESCRIPTION: This snippet defines the `OVAny` type alias in TypeScript. It specifies that `OVAny` can hold a string, number, or boolean value. This is used to provide flexibility in function parameters or object properties where different data types may be expected.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/types/OVAny.rst#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nOVAny: string | number | boolean\n```\n\n----------------------------------------\n\nTITLE: Setting GNU C++ Compiler Flags\nDESCRIPTION: This snippet sets compiler flags specifically for the GNU compiler.  If the compiler is GNU, it adds the `-std=c++11` flag to the C++ flags. This addresses a potential compatibility issue or requirement for C++11 support with the GNU compiler.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/stress_tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n    set (CMAKE_CXX_FLAGS \"-std=c++11 ${CMAKE_CXX_FLAGS}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Install RPATH CMake\nDESCRIPTION: This snippet sets the install RPATH for the target. The `ov_set_install_rpath` macro sets the RPATH to ensure that the library can find its dependencies at runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/src/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_install_rpath(${TARGET_NAME}\n                     # openvino_c installed in the same directory as openvino\n                     ${OV_CPACK_RUNTIMEDIR} ${OV_CPACK_RUNTIMEDIR})\n```\n\n----------------------------------------\n\nTITLE: Defining CMake Project\nDESCRIPTION: This command defines the name of the CMake project as 'samples_tests'. It initializes the project and sets up the necessary build environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/samples_tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nproject(samples_tests)\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target in CMake\nDESCRIPTION: This command adds a clang-format target for the specified target library. `ov_add_clang_format_target` is a custom function (likely defined elsewhere) that configures clang-format for the target. This adds a target to format the source code of the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tests/frontend/shared/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Getting TensorFlow Frontend Source Directory CMake\nDESCRIPTION: Retrieves the source directory of the `openvino_tensorflow_frontend` target using `get_target_property` and stores it in the `TENSORFLOW_FRONTEND_SRC_DIR` variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nget_target_property(TENSORFLOW_FRONTEND_SRC_DIR openvino_tensorflow_frontend SOURCE_DIR)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name and Public Headers Directory\nDESCRIPTION: This snippet sets the target name to 'funcSharedTests' and defines the public headers directory. These variables are used later in the configuration process to define the target and manage header files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/shared/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME funcSharedTests)\n\nset(PUBLIC_HEADERS_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: Add Clang Format Target\nDESCRIPTION: This adds a clang-format target for the ONNX frontend tests, using the previously compiled list of source files.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(ov_onnx_frontend_tests_clang FOR_SOURCES ${full_src_names})\n```\n\n----------------------------------------\n\nTITLE: Setting Build Type\nDESCRIPTION: Sets the CMake build type to 'Release'. It also allows the user to select different build types such as Debug, RelWithDebInfo, and MinSizeRel. This affects the optimization level and debugging information included in the build.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"CMake build type\")\nset_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Release\" \"Debug\" \"RelWithDebInfo\" \"MinSizeRel\")\n```\n\n----------------------------------------\n\nTITLE: Defining the Tensor Interface in TypeScript\nDESCRIPTION: Defines the `Tensor` interface, providing a structure for interacting with tensor data in OpenVINO. It includes properties for accessing the data and methods for retrieving information about the tensor's element type, shape, size, and contiguity. The `SupportedTypedArray` type is assumed to be defined elsewhere.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Tensor.rst#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface Tensor {\n    data: SupportedTypedArray;\n    getElementType(): element;\n    getData(): SupportedTypedArray;\n    getShape(): number[];\n    getSize(): number;\n    isContinuous(): boolean;\n\n   }\n```\n\n----------------------------------------\n\nTITLE: Start Swap\nDESCRIPTION: Starts the swap file after it has been created or modified.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_raspbian.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsudo dphys-swapfile swapon\n```\n\n----------------------------------------\n\nTITLE: br0 Interface Down Script\nDESCRIPTION: This script, `br0-qemu-ifdown`, brings down the specified network interface (`$nic`) and removes it from the `br0` bridge. It reads configuration from `/etc/default/qemu-kvm` if the file exists.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\n#!/bin/sh\nnic=$1\nif [ -f /etc/default/qemu-kvm ]; then\n\t. /etc/default/qemu-kvm\nfi\nswitch=br0\nbrctl delif $switch $nic\nifconfig $nic 0.0.0.0 down\n```\n\n----------------------------------------\n\nTITLE: Run with Multiple Model Specifications (Python)\nDESCRIPTION: This command demonstrates that when multiple model files are given, the last one specified will be used.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/benchmark-tool.rst#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nbenchmark_app -m model.xml -m model2.xml\n```\n\n----------------------------------------\n\nTITLE: Building Fuzz Tests (Bash)\nDESCRIPTION: This script builds the fuzz tests with the `ENABLE_FUZZING` and `ENABLE_SANITIZER` options enabled.  It assumes that OpenVINO has already been built and that the same compiler is used for both builds. The path to the OpenVINO build directory must be provided.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n(\\nmkdir -p tests/fuzz/build && cd tests/fuzz/build && \\\nCC=clang CXX=clang++ cmake .. -DENABLE_FUZZING=ON -DENABLE_SANITIZER=ON -DOpenVINO_DIR=$(pwd)/../../../build && \\\ncmake --build . \\\n)\n```\n\n----------------------------------------\n\nTITLE: Check Pip Packages\nDESCRIPTION: Checks if the required pip packages from 'requirements.txt' are installed. If not, it issues a warning message indicating that some tests may fail due to missing models.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow_lite/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nov_check_pip_packages(REQUIREMENTS_FILE \"${CMAKE_CURRENT_SOURCE_DIR}/requirements.txt\"\n                      MESSAGE_MODE WARNING\n                      WARNING_MESSAGE \"TensorFlow Lite testing models weren't generated, some tests will fail due models not found\"\n                      RESULT_VAR tensorflow_FOUND)\n```\n\n----------------------------------------\n\nTITLE: Defining u32 data type in TypeScript\nDESCRIPTION: Defines the u32 data type as a number in TypeScript. This represents an unsigned 32-bit integer, capable of holding larger non-negative integer values than u16.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/element.rst#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nu32: number\n```\n\n----------------------------------------\n\nTITLE: Cross Compiling Files for Different Architectures\nDESCRIPTION: This snippet uses a custom function `cross_compiled_file` to conditionally compile specific files based on the target architecture.  Different architectures (AVX2, AVX512F, SVE, NEON_FP16) can have optimized implementations of functions, and this mechanism selects the appropriate source file and compiles it into the plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_39\n\nLANGUAGE: cmake\nCODE:\n```\ncross_compiled_file(${TARGET_NAME}\n        ARCH AVX2 ANY\n                    src/nodes/proposal_imp.cpp\n        API         src/nodes/proposal_imp.hpp\n        NAME        proposal_exec\n        NAMESPACE   ov::Extensions::Cpu::XARCH\n)\ncross_compiled_file(${TARGET_NAME}\n        ARCH AVX512F AVX2 SVE NEON_FP16 ANY\n                    src/nodes/kernels/scaled_attn/softmax.cpp\n        API         src/nodes/kernels/scaled_attn/softmax.hpp\n        NAME        attn_softmax\n        NAMESPACE   ov::Extensions::Cpu::XARCH\n)\ncross_compiled_file(${TARGET_NAME}\n        ARCH AVX512F AVX2 SVE NEON_FP16 ANY\n                    src/nodes/kernels/scaled_attn/mha_single_token.cpp\n        API         src/nodes/kernels/scaled_attn/mha_single_token.hpp\n        NAME        mha_single_token\n        NAMESPACE   ov::Extensions::Cpu::XARCH\n)\ncross_compiled_file(${TARGET_NAME}\n        ARCH AVX512F AVX2 SVE ANY\n                    src/nodes/kernels/scaled_attn/executor_pa.cpp\n        API         src/nodes/kernels/scaled_attn/executor_pa.hpp\n        NAME        make_pa_executor\n        NAMESPACE   ov::Extensions::Cpu::XARCH\n)\ncross_compiled_file(${TARGET_NAME}\n        ARCH AVX512F AVX2 SVE ANY\n                    src/nodes/kernels/scaled_attn/attn_memcpy.cpp\n        API         src/nodes/kernels/scaled_attn/attn_memcpy.hpp\n        NAME        attn_memcpy paged_attn_memcpy attn_memcpy2d_kernel\n        NAMESPACE   ov::Extensions::Cpu::XARCH\n)\ncross_compiled_file(${TARGET_NAME}\n        ARCH AVX512F AVX2 SVE ANY\n                    src/nodes/kernels/scaled_attn/attn_quant.cpp\n        API         src/nodes/kernels/scaled_attn/attn_quant.hpp\n        NAME        attn_quantkv paged_attn_quantkv attn_quant_u8 attn_dequant_u8 attn_quant_by_channel_u8 attn_dequant_by_channel_u8\n        NAMESPACE   ov::Extensions::Cpu::XARCH\n)\n\ncross_compiled_file(${TARGET_NAME}\n        ARCH AVX512F ANY\n                    src/nodes/kernels/x64/mlp_utils.cpp\n        API         src/nodes/kernels/x64/mlp_utils.hpp\n        NAME        llm_mlp_transpose_epi32_16x16  llm_mlp_quantize_bf16_i8 llm_mlp_quantize_f16_i8 llm_mlp_dequantize_i32_f32\n        NAMESPACE   ov::Extensions::Cpu::XARCH\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Global Parameters with YAML\nDESCRIPTION: This code snippet demonstrates how to configure global parameters such as model directory, device name, compiler type, and log level using YAML format in Protopipe. These parameters are optional and provide a way to customize the execution environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_dir:\n  local: C:\\\\workspace\\\\models\ndevice_name: NPU\ncompiler_type: MLIR\nlog_level: INFO\n```\n\n----------------------------------------\n\nTITLE: Setting Threading Interface (CMake)\nDESCRIPTION: Sets the threading interface for the target using the `ov_set_threading_interface_for` function.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nov_set_threading_interface_for(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Building C/C++ OpenVINO Samples (macOS)\nDESCRIPTION: This script builds the C and C++ sample applications for macOS. It navigates to the samples directory and executes the `build_samples.sh` script which handles the compilation process.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nbuild_samples.sh\n```\n\n----------------------------------------\n\nTITLE: Cache Usage in prepareParams C++\nDESCRIPTION: This code demonstrates how to integrate the cache into the `prepareParams()` method. It involves preparing the parameters to create a cache key, querying the cache using `getOrCreate` with the key and builder, and then retrieving the cached value (in this example, a pointer to an executor).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/runtime_parameters_cache.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nvoid preapareParams() override {\n     ... //code that prepares parameters for the key\n\n     //key instantiation\n     KeyType key = {param1, param2, ...};\n     // get a reference to the cache\n     auto cache = getRuntimeCache();\n     //query cahce, buildExecutor is the builder descibed in 3\n     auto result = cache->getOrCreate(key, buildExecutor); \n     // get the the cached value, in this example it is a pointer to an executor\n     execPtr = result.first; \n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Command to Copy Generated Files\nDESCRIPTION: Defines a custom command that copies the generated kernel source and header files from the cache directory to the include directory if they have changed. This ensures that the latest generated files are used in the build process, optimizing the build by only copying when necessary. It depends on the generated kernel sources and the kernel files and code generation script.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/ocl_v2/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(OUTPUT \"${CODEGEN_INCDIR}/${KERNEL_SOURCES}\"\n  COMMAND \"${CMAKE_COMMAND}\" -E copy_if_different \"${CODEGEN_CACHE_DIR}/${KERNEL_SOURCES}\" \"${CODEGEN_INCDIR}/${KERNEL_SOURCES}\"\n  COMMAND \"${CMAKE_COMMAND}\" -E copy_if_different \"${CODEGEN_CACHE_DIR}/${KERNEL_HEADERS}\" \"${CODEGEN_INCDIR}/${KERNEL_HEADERS}\"\n  DEPENDS \"${CODEGEN_CACHE_DIR}/${KERNEL_SOURCES}\" \"${KERNELS}\" \"${CODEGEN_SCRIPT}\"\n  COMMENT \"Updating file if the file changed (${CODEGEN_INCDIR}/${KERNEL_SOURCES}) ...\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running AppVerifier Tests\nDESCRIPTION: This snippet shows how to execute the compiled AppVerifier tests. It assumes the tests have been built and are located in the `test\\Release` directory. Before executing, the test executable should be configured with AppVerifier to detect potential memory leaks. Running the executable will trigger the tests, which can be filtered via command-line arguments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n.\\test\\Release\\ov_appverifier_tests.exe\n```\n\n----------------------------------------\n\nTITLE: Applying SSE4.2 Optimization Flags in CMake\nDESCRIPTION: This CMake code snippet conditionally applies SSE4.2 optimization flags to the detection_output.cpp source file. It first checks if ENABLE_SSE42 is set. If so, it uses ov_sse42_optimization_flags to obtain the flags and then sets the COMPILE_FLAGS and COMPILE_DEFINITIONS properties for the specified source file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/impls/cpu/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_SSE42)\n  ov_sse42_optimization_flags(sse4_2_flags)\n  set_source_files_properties(detection_output.cpp PROPERTIES\n    COMPILE_FLAGS \"${sse4_2_flags}\"\n    COMPILE_DEFINITIONS \"HAVE_SSE\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Using GitHub Actions Version with Tag (Not Recommended)\nDESCRIPTION: This snippet demonstrates using GitHub Action with tag. The example is provided to contrast it with a better approach, which involves pinning a specific commit hash instead of using a tag. This tag-based approach is NOT recommended due to the potential security risks associated with changing code under the same tag.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/security.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nuses: actions/checkout@v4.2.2\n```\n\n----------------------------------------\n\nTITLE: Protopipe Named Scenarios (YAML)\nDESCRIPTION: This YAML configuration demonstrates custom naming for inference scenarios within the `multi_inference` section. The `name` attribute allows for more descriptive identification of each scenario which can be utilized in filtering.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/README.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nmodel_dir:\n  local: /models/\ndevice_name: CPU\nmulti_inference:\n- name: Model-A-Scenario\n  input_stream_list:\n  - network:\n    - { name: A.xml }\n- name: Model-B-Scenario\n  input_stream_list:\n  - network:\n    - { name: B.xml }\n- name: Model-C-Scenario\n  input_stream_list:\n  - network:\n    - { name: C.xml }\n```\n\n----------------------------------------\n\nTITLE: Set Target Name CMake\nDESCRIPTION: This snippet sets the target name for the library being built.  It defines a variable `TARGET_NAME` which is later used to define the library name.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset (TARGET_NAME \"openvino_snippets\")\n```\n\n----------------------------------------\n\nTITLE: Product Component Definition in labeler.yml (YAML)\nDESCRIPTION: This YAML snippet shows how product components are defined in the .github/labeler.yml file. It maps source code paths (using minimatch glob patterns) to component names. If a PR changes a file matching one of these patterns, the corresponding component is considered changed by the Smart CI workflow.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/ci/github_actions/smart_ci.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n'category: CPU':\n- 'src/plugins/intel_cpu/**/*'\n- 'src/common/snippets/**/*'\n- 'thirdparty/xbyak/**/*'\n```\n\n----------------------------------------\n\nTITLE: Marking Target as C++ in CMake\nDESCRIPTION: Marks the target as a C++ target, likely setting compiler flags or other build configurations specific to C++ projects.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/low_precision_transformations/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nov_mark_target_as_cc(${TARGET_NAME}_obj)\n```\n\n----------------------------------------\n\nTITLE: Install Optional Dependencies (x86_64)\nDESCRIPTION: This snippet installs optional dependencies like TBB, pugixml, flatbuffers, snappy, and protobuf using `brew`. These dependencies are needed for native compilation on x86_64 systems.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n% brew install tbb pugixml flatbuffers snappy protobuf\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories\nDESCRIPTION: This snippet adds two subdirectories, `fuzz-testhelper` and `src`, to the build process. These subdirectories likely contain the source code for the fuzz tests and related helper functions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/fuzz/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(fuzz-testhelper)\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Adding Clang Format Target for OpenVINO Core\nDESCRIPTION: This snippet adds a Clang format target for the OpenVINO core library. This allows developers to easily format the code using Clang format, ensuring a consistent code style. The `ov_add_clang_format_target` function is a custom function that sets up the Clang format target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nov_add_clang_format_target(openvino_core_clang FOR_SOURCES ${LIBRARY_SRC} ${PUBLIC_HEADERS} ${DEV_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring Static Library with CMake\nDESCRIPTION: This CMake snippet defines a static library named `memory_tests_helper`, specifies the source files to be compiled, sets include directories, adds a subdirectory for the `gflags` library, and links the necessary libraries. The target library uses the `gflags` and `tests_shared_lib` for its functionality.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/memory_tests/src/memory_tests_helper/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset (TARGET_NAME \"memory_tests_helper\")\n\nfile (GLOB SRC *.cpp)\nadd_library(${TARGET_NAME} STATIC ${SRC})\ntarget_include_directories(${TARGET_NAME} PUBLIC \"${CMAKE_SOURCE_DIR}/include\")\n\nadd_subdirectory(${OpenVINO_SOURCE_DIR}/thirdparty/gflags\n                 ${CMAKE_CURRENT_BINARY_DIR}/gflags_build\n                 EXCLUDE_FROM_ALL)\n\ntarget_link_libraries(${TARGET_NAME} PUBLIC gflags tests_shared_lib)\n```\n\n----------------------------------------\n\nTITLE: Creating Interface Library and Linking in CMake\nDESCRIPTION: This snippet creates an INTERFACE library, sets up include directories for the library based on build interface, and links it with the openvino::runtime library. It then creates an alias for the target and installs the static library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} INTERFACE)\n\ntarget_include_directories(${TARGET_NAME} INTERFACE\n    $<BUILD_INTERFACE:${FRONTEND_DEV_INCLUDE_DIR}>\n    $<BUILD_INTERFACE:${FRONTEND_INCLUDE_DIR}>)\n\ntarget_link_libraries(${TARGET_NAME} INTERFACE openvino::runtime)\n\nadd_library(openvino::frontend::common ALIAS ${TARGET_NAME})\n\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: Set RISCV64 CPU ISA Traits\nDESCRIPTION: This snippet defines the CPU_ISA_TRAITS_RV64 variable, which specifies the source files for RISC-V 64-bit CPU ISA traits. It uses target properties to obtain the source directory of the openvino_intel_cpu_plugin.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/functional/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(RISCV64)\n    set(CPU_ISA_TRAITS_RV64 $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/src/nodes/kernels/riscv64/cpu_isa_traits.hpp\n                            $<TARGET_PROPERTY:openvino_intel_cpu_plugin,SOURCE_DIR>/src/nodes/kernels/riscv64/cpu_isa_traits.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Grouping Sources and Headers in CMake\nDESCRIPTION: These snippets use `source_group` to organize the source and header files into named groups within the Visual Studio project. This improves project organization and readability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${LIBRARY_SRC})\nsource_group(\"include\" FILES ${PUBLIC_HEADERS})\n```\n\n----------------------------------------\n\nTITLE: Nearest Neighbor Interpolation\nDESCRIPTION: Performs nearest neighbor interpolation to resize a tensor.  For each output coordinate, it calculates the corresponding input coordinate based on scaling factors and selects the nearest pixel in the input tensor. This method avoids blurring but can lead to pixelation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n       def nearest_interpolation(self, input_data):\n           result = np.zeros(self.output_shape)\n\n           num_of_axes = len(self.axes)\n           for coordinates in np.ndindex(tuple(self.output_shape)):\n               input_coords = np.array(coordinates, dtype=np.int64)\n               for axis, scale in enumerate(self.all_scales):\n                   in_coord = self.get_original_coordinate(coordinates[axis], scale, self.output_shape[axis], self.input_shape[axis])\n                   nearest_pixel = self.get_nearest_pixel(in_coord, scale < 1)\n                   input_coords[axis] = max(0, min(nearest_pixel, self.input_shape[axis] - 1))\n               result[coordinates] = input_data[tuple(input_coords)]\n\n           return result\n```\n\n----------------------------------------\n\nTITLE: Mark Target as CC CMake\nDESCRIPTION: This snippet calls the `ov_mark_target_as_cc` function to mark the `openvino_snippets` target as a C++ target. This may affect how certain build tools handle the target.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nov_mark_target_as_cc(${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Defining Public Headers Directory in CMake\nDESCRIPTION: This snippet sets the variable `PUBLIC_HEADERS_DIR` to the directory containing the public header files. This variable is later used to specify include directories for the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/transformations/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(PUBLIC_HEADERS_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/include\")\n```\n\n----------------------------------------\n\nTITLE: Building Target Faster with Precompiled Header\nDESCRIPTION: This CMake snippet configures the build process to use a precompiled header for faster compilation of the `func_test_utils` library. It specifies `src/precomp.hpp` as the precompiled header file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/test_utils/functional_test_utils/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME}\n    PCH PRIVATE \"src/precomp.hpp\"\n)\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO™ Security Add-on\nDESCRIPTION: Navigates to the security_addon directory and builds the OpenVINO™ Security Add-on software.  The `make package` command creates the compressed tar files for different roles within the `release_files` directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_27\n\nLANGUAGE: sh\nCODE:\n```\nmake clean all\nsudo -s make package\n```\n\n----------------------------------------\n\nTITLE: IRDFT Layer Definition (5D Input, Signal Size, Unsorted Axes) XML\nDESCRIPTION: Defines an IRDFT layer in XML for OpenVINO with a 5D input tensor, signal_size, and unsorted axes. The input tensor has dimensions 16x768x580x320x2, the axes tensor contains [3, 1, 2], and the signal_size tensor contains [170, -1, 1024]. The output tensor dimensions are 16x768x1024x170.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/irdft-9.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IRDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>580</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim> <!-- axes input contains  [3, 1, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim> <!-- signal_size input contains [170, -1, 1024] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>1024</dim>\n            <dim>170</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Installing Header Files in CMake\nDESCRIPTION: This snippet installs header files from the frontend include directory to the destination specified by `FRONTEND_INSTALL_INCLUDE`. It specifies the component and exclusion rules for the installation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY ${FRONTEND_INCLUDE_DIR}/openvino\n        DESTINATION ${FRONTEND_INSTALL_INCLUDE}\n        COMPONENT ${OV_CPACK_COMP_CORE_DEV}\n        ${OV_CPACK_COMP_CORE_DEV_EXCLUDE_ALL})\n```\n\n----------------------------------------\n\nTITLE: Add Subdirectory for Tests CMake\nDESCRIPTION: This snippet conditionally adds the `tests` subdirectory to the build process if `ENABLE_FUNCTIONAL_TESTS` is enabled. This allows building and running tests for the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/common/snippets/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_FUNCTIONAL_TESTS)\n    add_subdirectory(tests)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Target Faster (CMake)\nDESCRIPTION: Configures the target to build faster using unity builds and precompiled headers.  The precompiled header uses 'src/precomp.hpp'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/reference/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nov_build_target_faster(${TARGET_NAME}\n    UNITY\n    PCH PRIVATE \"src/precomp.hpp\")\n```\n\n----------------------------------------\n\nTITLE: Mermaid Diagram Example\nDESCRIPTION: This snippet demonstrates how to create a diagram using the Mermaid framework within Markdown. The code defines a simple directed graph with nodes A, B, C, and D, and specifies the connections between them. This allows for the inclusion of visual representations within the documentation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/dev_doc_guide.md#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\ngraph LR;\n    A-->B;\n    A-->C;\n    B-->D;\n    C-->D;\n```\n\n----------------------------------------\n\nTITLE: Upgrade PIP\nDESCRIPTION: This command upgrades the pip package installer to the latest version within the activated virtual environment. Upgrading pip ensures access to the latest features and bug fixes.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/pypi_publish/pypi-openvino-rt.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython -m pip install --upgrade pip\n```\n\n----------------------------------------\n\nTITLE: Setting Target Name CMake\nDESCRIPTION: Defines the target name for the library as `ie_samples_utils`. This variable is used throughout the CMake script to reference the library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/common/utils/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME \"ie_samples_utils\")\n```\n\n----------------------------------------\n\nTITLE: Build OpenVINO Runtime\nDESCRIPTION: Configures and builds the OpenVINO Runtime using CMake.  It specifies the build type as Release and sets the ARM compute SCONS jobs based on the number of processors.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_raspbian.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncmake -DCMAKE_BUILD_TYPE=Release \\\n        -DARM_COMPUTE_SCONS_JOBS=$(nproc --all) \\\n  .. && cmake --build . --parallel \n```\n\n----------------------------------------\n\nTITLE: Adding libxsmm Subdirectory\nDESCRIPTION: This CMake code conditionally includes the libxsmm subdirectory based on the `ENABLE_SNIPPETS_LIBXSMM_TPP` flag. It disables the \"warning as error\" flag during libxsmm compilation to avoid issues like missing declarations.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/thirdparty/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_SNIPPETS_LIBXSMM_TPP)\n    # This flag is to suppress \"warning as error\" in libxsmm compilation, such as\n    # \"generator_common_aarch64.c:60:6: error: no previous declaration for ‘libxsmm_generator_vcvt_f32i8_aarch64_sve’ [-Werror=missing-declarations]\"\n    ov_add_compiler_flags(-Wno-missing-declarations)\n    add_subdirectory(libxsmm)\n    ov_install_static_lib(libxsmm ${OV_CPACK_COMP_CORE})\nendif()\n```\n\n----------------------------------------\n\nTITLE: IRDFT Layer Definition (5D Input, Signal Size, Unsorted Axes) XML - Second Example\nDESCRIPTION: Defines an IRDFT layer in XML for OpenVINO with a 5D input tensor, signal_size, and unsorted axes (second example). The input tensor has dimensions 16x768x580x320x2, the axes tensor contains [3, 0, 2], and the signal_size tensor contains [258, -1, 2056]. The output tensor dimensions are 16x768x2056x258.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/irdft-9.rst#_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IRDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>580</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim> <!-- axes input contains  [3, 0, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim> <!-- signal_size input contains [258, -1, 2056] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>2056</dim>\n            <dim>258</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Group Source Files\nDESCRIPTION: This command groups the source file within the IDE for better organization. It organizes the 'pybind_mock_frontend_api.cpp' file under the 'src' source group.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/tests/mock/pyngraph_fe_mock_api/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(\"src\" FILES ${PYBIND_FE_SRC})\n```\n\n----------------------------------------\n\nTITLE: Checking for missing dependencies\nDESCRIPTION: Iterates through a list of OpenCV libraries to check if they are found. If any dependencies are missing, a message is displayed, and the script returns, disabling the tool.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(MISSING_DEPENDENCIES \"\")\nforeach(LIB opencv_core opencv_imgproc opencv_imgcodecs)\n    if(NOT TARGET ${LIB})\n        list(APPEND MISSING_DEPENDENCIES ${LIB})\n    endif()\nendforeach()\n\nif(NOT MISSING_DEPENDENCIES STREQUAL \"\")\n    message(STATUS \"NPU ${TARGET_NAME} tool is disabled due to missing dependencies: ${MISSING_DEPENDENCIES}\")\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Naming Convention Check\nDESCRIPTION: This snippet calls a custom function, `ov_ncc_naming_style`, to check the naming style of the source code, specifying the include and source directories and additional include directories from the target's properties.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common_translators/src/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nov_ncc_naming_style(FOR_TARGET ${TARGET_NAME}\n                    SOURCE_DIRECTORIES \"${root_dir}/include\"\n                                       \"${root_dir}/src\"\n                    ADDITIONAL_INCLUDE_DIRECTORIES\n                        $<TARGET_PROPERTY:${TARGET_NAME},INTERFACE_INCLUDE_DIRECTORIES>\n                        $<TARGET_PROPERTY:${TARGET_NAME},INCLUDE_DIRECTORIES>)\n```\n\n----------------------------------------\n\nTITLE: Deformable Convolution Example (deformable_group=1) in XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a DeformableConvolution layer with `deformable_group` set to 1. It specifies the layer's attributes, input port dimensions, and output port dimensions, including precision. The dilations, pads, and strides are set to default values and auto_pad is set to explicit. The input and output dimensions show a specific case.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/convolution/deformable-convolution-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer type=\"DeformableConvolution\" ...>\n    <data dilations=\"1,1\" pads_begin=\"0,0\" pads_end=\"0,0\" strides=\"1,1\" auto_pad=\"explicit\"  group=\"1\" deformable_group=\"1\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>224</dim>\n            <dim>224</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>50</dim>\n            <dim>220</dim>\n            <dim>220</dim>\n        </port>\n        <port id=\"2\">\n            <dim>64</dim>\n            <dim>4</dim>\n            <dim>5</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\" precision=\"FP32\">\n            <dim>1</dim>\n            <dim>64</dim>\n            <dim>220</dim>\n            <dim>220</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Set Target Name for OpenVINO Proxy Plugin\nDESCRIPTION: This snippet sets the target name for the OpenVINO proxy plugin object library to `openvino_proxy_plugin_obj`. This name is used to identify the target in subsequent CMake commands.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/proxy/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME \"openvino_proxy_plugin_obj\")\n```\n\n----------------------------------------\n\nTITLE: IR Format Filter Specification (Bash)\nDESCRIPTION: This code snippet shows how to specify the output IR formats using the 'formats' filter.  The tokens define the desired IR output format, such as XML, XML with binary, DOT, or SVG.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/debug_caps_filters.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nformats=<comma_separated_tokens>\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License Copyright Notice\nDESCRIPTION: This snippet provides an example of how to apply the Apache License to your work.  It instructs the user to replace the bracketed fields with their identifying information and enclose the text in appropriate comment syntax. It recommends including a file or class name and description of purpose for easier identification.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/runtime-third-party-programs.txt#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: IDFT Layer XML (5D, signal_size, unsorted axes, -1 in signal_size)\nDESCRIPTION: XML configuration for IDFT layer with 5D input. It includes signal_size with a -1 value indicating full axis size, and unsorted axes. Demonstrates flexible configuration with various dimensions and signal size adjustments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/idft-7.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"IDFT\" ... >\n    <input>\n        <port id=\"0\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>580</dim>\n            <dim>320</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>\t<!-- axes input contains  [3, 1, 2] -->\n        </port>\n        <port id=\"2\">\n            <dim>3</dim>\t<!-- signal_size input contains [170, -1, 1024] -->\n        </port>\n    <output>\n        <port id=\"3\">\n            <dim>16</dim>\n            <dim>768</dim>\n            <dim>1024</dim>\n            <dim>170</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating Static Library and Alias (CMake)\nDESCRIPTION: This snippet defines a static library named `openvino_protobuf_shutdown` built from `shutdown_protobuf.cpp`. It also creates an alias `openvino::protobuf_shutdown` and sets the `EXPORT_NAME` property to `protobuf_shutdown`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common/shutdown_protobuf/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME openvino_protobuf_shutdown)\n\nadd_library(${TARGET_NAME} STATIC shutdown_protobuf.cpp)\n\nadd_library(openvino::protobuf_shutdown ALIAS ${TARGET_NAME})\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME protobuf_shutdown)\n```\n\n----------------------------------------\n\nTITLE: Einsum Transpose Example C++\nDESCRIPTION: This example shows how Einsum transposes an input tensor using the Einstein summation convention.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/matrix/einsum-7.rst#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nA = [[[1.0, 2.0, 3.0],\n      [4.0, 5.0, 6.0],\n      [7.0, 8.0, 9.0]]]\nequation = \"ijk->kij\"\noutput = [[[1.0, 4.0, 7.0]],\n            [[2.0, 5.0, 8.0]],\n            [[3.0, 6.0, 9.0]]]\n```\n\n----------------------------------------\n\nTITLE: Defining NPU AL Library Target in CMake\nDESCRIPTION: This snippet defines a static library target named `openvino_npu_al` using source files found in the current source directory.  It sets properties for export, include directories, compile definitions based on `ENABLE_MLIR_COMPILER`, and links against `openvino::npu_logger_utils` and `openvino::runtime::dev`. The library's interprocedural optimization is configured based on the `ENABLE_LTO` variable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/al/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET_NAME openvino_npu_al)\n\nfile(GLOB_RECURSE SOURCES *.cpp *.hpp *.h)\nsource_group(TREE ${CMAKE_CURRENT_SOURCE_DIR} FILES ${SOURCES})\n\nadd_library(${TARGET_NAME} STATIC ${SOURCES})\nadd_library(openvino::npu_al ALIAS ${TARGET_NAME})\nset_target_properties(${TARGET_NAME} PROPERTIES EXPORT_NAME npu_al)\n\ntarget_include_directories(${TARGET_NAME}\n    PUBLIC\n        $<{BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include}>\n)\n\nif(ENABLE_MLIR_COMPILER)\n    target_compile_definitions(${TARGET_NAME} PRIVATE ENABLE_MLIR_COMPILER)\nendif()\n\ntarget_link_libraries(${TARGET_NAME}\n    PUBLIC\n        openvino::npu_logger_utils\n        openvino::runtime::dev\n)\n\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\nov_add_clang_format_target(${TARGET_NAME}_clang FOR_TARGETS ${TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Options for Clang Compiler in CMake\nDESCRIPTION: This snippet adds compile options to the target if the OV_COMPILER_IS_CLANG variable is set to TRUE. This disables -Wundef and -Wreserved-id-macro warnings.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/core/tests/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif (OV_COMPILER_IS_CLANG)\n    target_compile_options(${TARGET_NAME} PRIVATE -Wno-undef -Wno-reserved-id-macro)\nendif()\n```\n\n----------------------------------------\n\nTITLE: GatherND Layer Configuration Example 2\nDESCRIPTION: Illustrates the XML configuration for a GatherND layer with batch_dims set to 2. This example shows the layer configuration, including the batch_dims attribute and the dimensions of the input and output ports. The 'batch_dims' attribute influences how the indices are applied.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/gather-nd-8.rst#_snippet_8\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"GatherND\" version=\"opset8\">\n    <data batch_dims=\"2\" />\n    <input>\n        <port id=\"0\">\n            <dim>30</dim>\n            <dim>2</dim>\n            <dim>100</dim>\n            <dim>35</dim>\n        </port>\n        <port id=\"1\">\n            <dim>30</dim>\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"3\">\n            <dim>30</dim>\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>35</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Edge Mode Example - XML\nDESCRIPTION: XML example illustrating the Pad operation in edge mode. It showcases how the tensor is padded by replicating edge values based on pads_begin and pads_end.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_12\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Pad\" ...>\n    <data pad_mode=\"edge\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>40</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>     <!-- pads_begin = [0, 5, 2, 1]  -->\n        </port>\n        <port id=\"2\">\n            <dim>4</dim>     <!-- pads_end = [1, 0, 3, 7] -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>2</dim>     <!-- 2 = 0 + 1 + 1 = pads_begin[0] + input.shape[0] + pads_end[0] -->\n            <dim>8</dim>     <!-- 8 = 5 + 3 + 0 = pads_begin[1] + input.shape[1] + pads_end[1] -->\n            <dim>37</dim>    <!-- 37 = 2 + 32 + 3 = pads_begin[2] + input.shape[2] + pads_end[2] -->\n            <dim>48</dim>    <!-- 48 = 1 + 40 + 7 = pads_begin[3] + input.shape[3] + pads_end[3] -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ReduceSum XML Example (axes=[-2], keep_dims=false)\nDESCRIPTION: Illustrates the ReduceSum operation with `keep_dims` set to `false` and axes set to `[-2]`. The input tensor is 6x12x10x24, and the reduction happens along the second to last dimension. The output shape is 6x12x24, as the second to last dimension is removed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/reduction/reduce-sum-1.rst#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"ReduceSum\" ...>\n    <data keep_dims=\"false\" />\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>         <!-- value is [-2] that means independent reduction in each channel, batch and second spatial dimension -->\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: AUGRUSequence Layer Definition in XML\nDESCRIPTION: This XML code defines an example layer configuration for the AUGRUSequence operation within a neural network model. It specifies the layer's type as AUGRUSequence, defines the input and output port dimensions, and sets the 'hidden_size' attribute. This example illustrates how to integrate AUGRUSequence into an OpenVINO model using XML.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/internal/augru-sequence.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"AUGRUSequence\" ...>\n    <data hidden_size=\"128\"/>\n    <input>\n        <port id=\"0\"> <!-- `X` input data -->\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>16</dim>\n        </port>\n        <port id=\"1\"> <!-- `H_t` input -->\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"2\"> <!-- `sequence_lengths` input -->\n            <dim>1</dim>\n        </port>\n         <port id=\"3\"> <!-- `W` weights input -->\n            <dim>1</dim>\n            <dim>384</dim>\n            <dim>16</dim>\n        </port>\n         <port id=\"4\"> <!-- `R` recurrence weights input -->\n            <dim>1</dim>\n            <dim>384</dim>\n            <dim>128</dim>\n        </port>\n         <port id=\"5\"> <!-- `B` bias input -->\n            <dim>1</dim>\n            <dim>384</dim>\n        </port>\n        <port id=\"6\"> <!-- `A` attention score input -->\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>1</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"7\"> <!-- `Y` output -->\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>4</dim>\n            <dim>128</dim>\n        </port>\n        <port id=\"8\"> <!-- `Ho` output -->\n            <dim>1</dim>\n            <dim>1</dim>\n            <dim>128</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: GridSample Layer XML Example\nDESCRIPTION: This XML snippet demonstrates how to define a GridSample layer in an OpenVINO model. It specifies the attributes of the layer, such as align_corners, mode, and padding_mode, along with the input and output port dimensions. The input ports define the shape of the data and grid tensors, while the output port defines the shape of the resulting interpolated tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/grid-sample-9.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"GridSample\" ...>\n    <data align_corners=\"true\" mode=\"nearest\" padding_mode=\"border\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>100</dim>\n            <dim>100</dim>\n        </port>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n            <dim>2</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>10</dim>\n            <dim>10</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Specifying Extra Index URL for PyTorch\nDESCRIPTION: This snippet specifies an extra index URL to download PyTorch. It is used to resolve PyTorch dependencies from a specific CPU-optimized wheel.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/python/benchmark/bert_benchmark/requirements.txt#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cpu\n```\n\n----------------------------------------\n\nTITLE: MaxPool Layer with Same Upper Padding (XML)\nDESCRIPTION: This XML example defines a MaxPool layer with 'same_upper' padding. The configuration includes attributes like auto_pad, kernel, pads_begin, pads_end, and strides. It also specifies input and output port dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/pooling/max-pool-1.rst#_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MaxPool\" ... >\n    <data auto_pad=\"same_upper\" kernel=\"2,2\" pads_begin=\"1,1\" pads_end=\"1,1\" strides=\"2,2\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>1</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>32</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: BitwiseOr Layer Configuration Example (No Broadcast) in XML\nDESCRIPTION: This XML example configures a BitwiseOr layer with two input ports and one output port. It showcases a scenario where no broadcasting is required as the input tensors have matching dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-or-13.rst#_snippet_2\n\nLANGUAGE: XML\nCODE:\n```\n<layer ... type=\"BitwiseOr\">\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual TPM Directory for Runtime VM (sh)\nDESCRIPTION: Creates a directory to support the virtual TPM device for the Runtime VM and provisions its certificates. Only root should have read/write permissions to this directory.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\nsudo mkdir /var/OVSA/vtpm/vtpm_runtime\n\nexport XDG_CONFIG_HOME=~/.config\n/usr/share/swtpm/swtpm-create-user-config-files\nswtpm_setup --tpmstate /var/OVSA/vtpm/vtpm_runtime --create-ek-cert --create-platform-cert --overwrite --tpm2 --pcr-banks -\n```\n\n----------------------------------------\n\nTITLE: Building OpenVINO Samples with CMD (Windows)\nDESCRIPTION: This command executes the `build_samples_msvc.bat` batch file to build the OpenVINO samples for C or C++ on Windows using the Command Prompt. It automatically detects and utilizes the appropriate Visual Studio version.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/get-started-demos.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nbuild_samples_msvc.bat\n```\n\n----------------------------------------\n\nTITLE: STFT Configuration for 1D Signal, transpose_frames=true (XML)\nDESCRIPTION: This XML snippet demonstrates the configuration of the STFT operation for a 1D signal input where the transpose_frames attribute is set to true. The input signal has a dimension of 56, the window has a dimension of 7, frame_size is 11 and frame_step is 3. The output shape is [6, 16, 2].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/stft-15.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"STFT\" ... >\n    <data transpose_frames=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>56</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n        </port>\n        <port id=\"2\"></port> <!-- value: 11 -->\n        <port id=\"3\"></port> <!-- value: 3 -->\n    <output>\n        <port id=\"4\">\n            <dim>6</dim>\n            <dim>16</dim>\n            <dim>2</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: OVSA License Generation\nDESCRIPTION: Uses the ovsatool to generate a license for a model. It requires the model's master license, ISV keystore, license configuration file, TCB file, customer keystore certificate, and the model's license file name as input.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/about-openvino/openvino-ecosystem/openvino-project/openvino-security-add-on.rst#_snippet_45\n\nLANGUAGE: sh\nCODE:\n```\n/opt/ovsa/bin/ovsatool sale -m <name-of-the-model>.masterlic -k isv_keystore -l 30daylicense.config -t detect_runtime_vm.tcb -p custkeystore.csr.crt -c <name-of-the-model>.lic\n```\n\n----------------------------------------\n\nTITLE: ISTFT Example: 4D input, 2D output, center=false, default length (XML)\nDESCRIPTION: Configuration example for the ISTFT operation with a 4D data input, resulting in a 2D output signal. The 'center' attribute is set to 'false', and the signal length is calculated by default. This demonstrates the use of ISTFT with batched input data, showing the required input and output dimensions for this scenario.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/signals/istft-16.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"ISTFT\" ... >\n    <data center=\"false\" ... />\n    <input>\n        <port id=\"0\">\n            <dim>4</dim>\n            <dim>6</dim>\n            <dim>16</dim>\n            <dim>2</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n        </port>\n        <port id=\"2\"></port> <!-- frame_size value: 11 -->\n        <port id=\"3\"></port> <!-- frame_step value: 3 -->\n    </input>\n    <output>\n        <port id=\"4\">\n            <dim>4</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: NonMaxSuppression layer configuration in C++\nDESCRIPTION: This code snippet demonstrates how to configure a NonMaxSuppression layer within an OpenVINO model using C++. It showcases the XML layer definition, specifying attributes like box encoding, sorting order, and output type, along with input and output port configurations. This configuration defines how the NonMaxSuppression operation should be performed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/sort/non-max-suppression-9.rst#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<layer ... type=\"NonMaxSuppression\" ... >\n    <data box_encoding=\"corner\" sort_result_descending=\"1\" output_type=\"i64\"/>\n    <input>\n        <port id=\"0\">\n            <dim>3</dim>\n            <dim>100</dim>\n            <dim>4</dim>\n        </port>\n        <port id=\"1\">\n            <dim>3</dim>\n            <dim>5</dim>\n            <dim>100</dim>\n        </port>\n        <port id=\"2\"/> <!-- 10 -->\n        <port id=\"3\"/>\n        <port id=\"4\"/>\n        <port id=\"5\"/>\n    </input>\n    <output>\n        <port id=\"6\" precision=\"I64\">\n            <dim>150</dim> <!-- min(100, 10) * 3 * 5 -->\n            <dim>3</dim>\n        </port>\n        <port id=\"7\" precision=\"FP32\">\n            <dim>150</dim> <!-- min(100, 10) * 3 * 5 -->\n            <dim>3</dim>\n        </port>\n        <port id=\"8\" precision=\"I64\">\n            <dim>1</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: ONNX Linear Interpolation\nDESCRIPTION: This function acts as a wrapper for linear interpolation, deciding whether to call onnx_linear_interpolation4D or onnx_linear_interpolation5D depending on the rank of the input tensor.  It asserts that the rank is within the supported range (2D, 3D, 4D, 5D).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/image/interpolate-4.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n       def onnx_linear_interpolation(self, input_data):\n           rank = len(self.input_shape)\n           assert rank in [2, 3, 4, 5], \"mode 'linear_onnx' supports only 2D, 3D, 4D, or 5D tensors\"\n\n           if rank in [2, 4]:\n               self.onnx_linear_interpolation4D(input_data)\n           else:\n               self.onnx_linear_interpolation5D(input_data)\n```\n\n----------------------------------------\n\nTITLE: Tan Operation Example 2 XML\nDESCRIPTION: This example provides another demonstration of the Tan operation, showcasing the transformation of a different input tensor. It calculates the element-wise tangent for integer input values. The configuration is represented in XML format.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/tan-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\ninput = [-2, -1, 0, 1, 2]\noutput = [2, -2, 0, 2, -2]\n```\n\n----------------------------------------\n\nTITLE: Defining SHL Usage\nDESCRIPTION: This snippet defines `-DOV_CPU_WITH_SHL` and sets `OV_CPU_WITH_SHL` to ON if `ENABLE_SHL_FOR_CPU` is enabled, indicating that the CPU plugin will be built with SHL support.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nif (ENABLE_SHL_FOR_CPU)\n    add_definitions(-DOV_CPU_WITH_SHL)\n    set(OV_CPU_WITH_SHL ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: NPU Configuration File\nDESCRIPTION: This snippet shows the content of the `mobilenet-v2.conf` file, which is used to configure the NPU device. It specifies the compiler type and the platform.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/single-image-test/README.md#_snippet_5\n\nLANGUAGE: Text\nCODE:\n```\nNPU_COMPILER_TYPE DRIVER\nNPU_PLATFORM VPU3720\n```\n\n----------------------------------------\n\nTITLE: BitwiseAnd output for boolean tensor in Python\nDESCRIPTION: Demonstrates the BitwiseAnd operation for boolean tensors.  It showcases the logical AND operation equivalent when applied to boolean values. The input tensors `a` and `b` are boolean lists, and the output shows the result of applying a logical AND element-wise.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/bitwise/bitwise-and-13.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# For given boolean inputs:\na = [True, False, False]\nb = [True, True, False]\n# Perform logical AND operation same as in LogicalAnd operator:\noutput = [True, False, False]\n```\n\n----------------------------------------\n\nTITLE: Mersenne-Twister State Initialization (C++)\nDESCRIPTION: This code snippet shows how the state array is initialized in the Mersenne-Twister algorithm using a global seed. The state[0] is initialized with the global_seed, and the remaining elements are calculated recursively.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nstate[0] = global_seed & 0xffffffff;\nstate[i] = 1812433253 * state[i-1] ^ (state[i-1] >> 30) + i\n```\n\n----------------------------------------\n\nTITLE: Deactivating a Virtual Environment\nDESCRIPTION: This command deactivates the currently active Python virtual environment. This is useful for switching between different project environments.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/interactive-tutorials-python/run-notebooks.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndeactivate\n```\n\n----------------------------------------\n\nTITLE: Integer Conversion\nDESCRIPTION: Converts a value to an integer type (int32 or int64), handling special cases where min or max exceeds the maximum uint32 value. The int64 case optimizes for PyTorch output, concatenating two uint32s if necessary. Values are standardized via modulo operation and offset.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/generation/random-uniform-8.rst#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\nif output is of int32 dtype:\n   output = int32(x)\nelse if output is of int64 dtype and (min <= max(uint32) and max <= max(uint32)):\n   output = int64(x)\nelse:\n   output = int64(x << 32 + y) (uses 2 uint32s instead of one)\noutput = output % (max - min) + min\n```\n\n----------------------------------------\n\nTITLE: Activate Virtual Environment (Linux/macOS)\nDESCRIPTION: This command activates the Python virtual environment 'openvino_env' on Linux or macOS. After activation, the system will use the Python interpreter and packages installed within the virtual environment.  Uses the 'source' command to modify the current shell environment.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/pypi_publish/pypi-openvino-rt.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nsource openvino_env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Setting OpenVINO Source Directory\nDESCRIPTION: Sets the OpenVINO source directory. The ENABLE_CLANG_FORMAT variable is also set to ON. This is used to find the OpenVINODeveloperScripts package.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/tests/appverifier_tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(OpenVINO_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../\")\nset(ENABLE_CLANG_FORMAT ON)\n```\n\n----------------------------------------\n\nTITLE: Pad-12 Constant Mode Example (Mixed Pads) - XML\nDESCRIPTION: XML example showing the Pad operation in constant mode with both positive and negative padding values. It defines the input dimensions, pads_begin, pads_end (including negative values for cropping), and a pad_value of 15.0.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/pad-12.rst#_snippet_11\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Pad\" ...>\n    <data pad_mode=\"constant\"/>\n    <input>\n        <port id=\"0\">\n            <dim>2</dim>\n            <dim>3</dim>\n            <dim>32</dim>\n            <dim>40</dim>\n        </port>\n        <port id=\"1\">\n            <dim>4</dim>     <!-- pads_begin = [0, -2, -8, 1]  -->\n        </port>\n        <port id=\"2\">\n            <dim>4</dim>     <!-- pads_end = [-1, 4, -6, 7] -->\n        </port>\n        <port id=\"3\">\n                            <!-- pad_value = 15.0 -->\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>1</dim>     <!-- 2 = 0 + 2 + (-1) = pads_begin[0] + input.shape[0] + pads_end[0] -->\n            <dim>5</dim>     <!-- 5 = (-2) + 3 + 4 = pads_begin[1] + input.shape[1] + pads_end[1] -->\n            <dim>18</dim>    <!-- 18 = (-8) + 32 (-6) = pads_begin[2] + input.shape[2] + pads_end[2] -->\n            <dim>48</dim>    <!-- 48 = 1 + 40 + 7 = pads_begin[3] + input.shape[3] + pads_end[3] -->\n                            <!-- all new elements are filled with 15.0 value -->\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Round Operation Example XML - Half to Even\nDESCRIPTION: This XML snippet demonstrates the Round operation with the 'half_to_even' mode in OpenVINO. It defines a layer that rounds the input tensor, specifying input and output port dimensions.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/round-5.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Round\">\n    <data mode=\"half_to_even\"/>\n    <input>\n        <port id=\"0\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"1\">\n            <dim>256</dim>\n            <dim>56</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Defining ov_add_sample Macro\nDESCRIPTION: This macro simplifies the process of adding a sample application to the build. It takes the sample name, source files, header files, include directories, and dependencies as input. It creates an executable, sets target properties, links against the OpenVINO Runtime and Threads libraries, and installs the executable.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nmacro(ov_add_sample)\n    set(options EXCLUDE_CLANG_FORMAT)\n    set(oneValueArgs NAME)\n    set(multiValueArgs SOURCES HEADERS DEPENDENCIES INCLUDE_DIRECTORIES)\n    cmake_parse_arguments(SAMPLE \"${options}\" \"${oneValueArgs}\"\n                          \"${multiValueArgs}\" ${ARGN} )\n\n    # Create named folders for the sources within the .vcproj\n    # Empty name lists them directly under the .vcproj\n    source_group(\"src\" FILES ${SAMPLE_SOURCES})\n    if(SAMPLE_HEADERS)\n        source_group(\"include\" FILES ${SAMPLE_HEADERS})\n    endif()\n\n    # Create executable file from sources\n    add_executable(${SAMPLE_NAME} ${SAMPLE_SOURCES} ${SAMPLE_HEADERS})\n\n    set(folder_name cpp_samples)\n    if(SAMPLE_NAME MATCHES \".*_c$\")\n        set(c_sample ON)\n        set(folder_name c_samples)\n    endif()\n\n    # for cross-compilation with gflags\n    find_package(Threads REQUIRED)\n\n    find_package(OpenVINO REQUIRED COMPONENTS Runtime)\n    if(c_sample)\n        set(ov_link_libraries openvino::runtime::c)\n    else()\n        set(ov_link_libraries openvino::runtime)\n    endif()\n\n    set_target_properties(${SAMPLE_NAME} PROPERTIES FOLDER ${folder_name}\n                                                    # to ensure out of box LC_RPATH on macOS with SIP\n                                                    INSTALL_RPATH_USE_LINK_PATH ON)\n\n    if(SAMPLE_INCLUDE_DIRECTORIES)\n        target_include_directories(${SAMPLE_NAME} PRIVATE ${SAMPLE_INCLUDE_DIRECTORIES})\n    endif()\n    target_include_directories(${SAMPLE_NAME} PRIVATE \"${CMAKE_CURRENT_SOURCE_DIR}/../common\")\n\n    target_link_libraries(${SAMPLE_NAME} PRIVATE ${ov_link_libraries} Threads::Threads ${SAMPLE_DEPENDENCIES})\n\n    install(TARGETS ${SAMPLE_NAME}\n            RUNTIME DESTINATION samples_bin/\n            COMPONENT samples_bin\n            EXCLUDE_FROM_ALL)\n\n    # create global target with all samples / demo apps\n    if(NOT TARGET ov_samples)\n        add_custom_target(ov_samples ALL)\n    endif()\n    add_dependencies(ov_samples ${SAMPLE_NAME})\n\n    if(COMMAND ov_add_clang_format_target AND NOT SAMPLE_EXCLUDE_CLANG_FORMAT)\n        ov_add_clang_format_target(${SAMPLE_NAME}_clang FOR_SOURCES ${SAMPLE_SOURCES} ${SAMPLE_HEADERS})\n    endif()\n    if(COMMAND ov_ncc_naming_style AND NOT c_sample)\n        ov_ncc_naming_style(FOR_TARGET \"${SAMPLE_NAME}\"\n                            SOURCE_DIRECTORIES \"${CMAKE_CURRENT_SOURCE_DIR}\")\n    endif()\n\n    unset(options)\n    unset(oneValueArgs)\n    unset(multiValueArgs)\n    unset(c_sample)\n    unset(folder_name)\n    unset(ov_link_libraries)\n    unset(SAMPLE_NAME)\n    unset(SAMPLE_HEADERS)\n    unset(SAMPLE_DEPENDENCIES)\n    unset(SAMPLE_EXCLUDE_CLANG_FORMAT)\n    unset(SAMPLE_INCLUDE_DIRECTORIES)\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Define Model getName Method (TypeScript)\nDESCRIPTION: This code defines the `getName` method of the `Model` interface. It returns the unique name of the model as a string.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\ngetName(): string\n```\n\n----------------------------------------\n\nTITLE: CumSum Example 1: Inclusive Summation in OpenVINO (XML)\nDESCRIPTION: This XML example demonstrates the CumSum operation with inclusive summation (exclusive=\"0\") and forward direction (reverse=\"0\"). It calculates the cumulative sum of the input tensor [1., 2., 3., 4., 5.] along axis 0, resulting in the output tensor [1., 3., 6., 10., 15.].\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/arithmetic/cumsum-3.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"CumSum\" exclusive=\"0\" reverse=\"0\">\n    <input>\n        <port id=\"0\">     <!-- input value is: [1., 2., 3., 4., 5.] -->\n            <dim>5</dim>\n        </port>\n        <port id=\"1\"/>     <!-- axis value is: 0 -->\n    </input>\n    <output>\n        <port id=\"2\">     <!-- output value is: [1., 3., 6., 10., 15.] -->\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Create Symbolic Link\nDESCRIPTION: Creates a symbolic link to the OpenVINO installation directory for easier access. Requires administrator privileges.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-windows.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncd C:\\Program Files (x86)\\Intel\nmklink /D openvino_2025 openvino_2025.1.0\n```\n\n----------------------------------------\n\nTITLE: RegionYolo Layer Configuration for YOLO V3 in XML\nDESCRIPTION: This XML snippet demonstrates the configuration of a RegionYolo layer for YOLO V3. It specifies the anchors, axis, classes, coords, and mask attributes, as well as the input and output port dimensions. The `do_softmax` attribute is set to 0.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/detection/region-yolo-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<!-- YOLO V3 example -->\n<layer type=\"RegionYolo\" ... >\n    <data anchors=\"10,14,23,27,37,58,81,82,135,169,344,319\" axis=\"1\" classes=\"80\" coords=\"4\" do_softmax=\"0\" end_axis=\"3\" mask=\"0,1,2\" num=\"6\"/>\n    <input>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>255</dim>\n            <dim>26</dim>\n            <dim>26</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"0\">\n            <dim>1</dim>\n            <dim>255</dim>\n            <dim>26</dim>\n            <dim>26</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Run OpenVINO JavaScript API tests\nDESCRIPTION: Runs all the OpenVINO JavaScript API tests using npm. This command executes all test files in the project.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/test_examples.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm run test\n```\n\n----------------------------------------\n\nTITLE: Recreate Swap File\nDESCRIPTION: Recreates the swap file after it has been modified.  This command is needed after changing the swap size in /etc/dphys-swapfile.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_raspbian.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo dphys-swapfile setup\n```\n\n----------------------------------------\n\nTITLE: Installing Static Library\nDESCRIPTION: Installs the static library to the designated installation directory using the custom command `ov_install_static_lib`. The component associated with the library is `OV_CPACK_COMP_CORE`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nov_install_static_lib(${TARGET_NAME} ${OV_CPACK_COMP_CORE})\n```\n\n----------------------------------------\n\nTITLE: OutputInfo tensor() Method TypeScript\nDESCRIPTION: Defines the `tensor()` method of the `OutputInfo` interface. It returns an `OutputTensorInfo` object, providing information about the output tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/OutputInfo.rst#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ntensor(): OutputTensorInfo\n```\n\n----------------------------------------\n\nTITLE: MVN Layer with across_channels Attribute XML\nDESCRIPTION: This XML snippet defines an MVN layer in OpenVINO with the `across_channels` attribute set to `true`. This configuration performs Layer Normalization, sharing mean values across channels. It includes the input and output port dimensions and specifies the epsilon value for numerical stability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/normalization/mvn-1.rst#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"MVN\">\n    <data across_channels=\"true\" eps=\"1e-9\" normalize_variance=\"true\"/>\n    <input>\n        <port id=\"0\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>6</dim>\n            <dim>12</dim>\n            <dim>10</dim>\n            <dim>24</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Equal-1 Operation with NumPy Broadcast in OpenVINO XML\nDESCRIPTION: This example demonstrates the Equal-1 operation in OpenVINO with NumPy broadcasting enabled. The input tensors are broadcasted according to NumPy rules before element-wise comparison.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/comparison/equal-1.rst#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<layer ... type=\"Equal\">\n    <data auto_broadcast=\"numpy\"/>\n    <input>\n        <port id=\"0\">\n            <dim>8</dim>\n            <dim>1</dim>\n            <dim>6</dim>\n            <dim>1</dim>\n        </port>\n        <port id=\"1\">\n            <dim>7</dim>\n            <dim>1</dim>\n            <dim>5</dim>\n        </port>\n    </input>\n    <output>\n        <port id=\"2\">\n            <dim>8</dim>\n            <dim>7</dim>\n            <dim>6</dim>\n            <dim>5</dim>\n        </port>\n    </output>\n</layer>\n```\n\n----------------------------------------\n\nTITLE: Define Model getOutputElementType Method (TypeScript)\nDESCRIPTION: This code defines the `getOutputElementType` method of the `Model` interface. It gets the element type of a specific output of the model using provided index.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/Model.rst#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\ngetOutputElementType(index): string;\n```\n\n----------------------------------------\n\nTITLE: InputModelInfo Interface Definition TypeScript\nDESCRIPTION: Defines the InputModelInfo interface which includes the setLayout method. This method allows setting the layout of the input model. The interface itself does not specify any implementation details, only the contract for its methods.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InputModelInfo.rst#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\ninterface InputModelInfo {\n    setLayout(layout): InputModelInfo;\n}\n```\n\n----------------------------------------\n\nTITLE: Organizing source files in IDE\nDESCRIPTION: Organizes the source files in the IDE under a tree structure based on their location relative to the current source directory. This improves code organization and navigation within the IDE.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/logger/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nsource_group(TREE ${CMAKE_CURRENT_SOURCE_DIR} FILES ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties\nDESCRIPTION: Sets target properties such as interprocedural optimization for the release build configuration based on the value of `ENABLE_LTO`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_gpu/src/graph/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES INTERPROCEDURAL_OPTIMIZATION_RELEASE ${ENABLE_LTO})\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Example\nDESCRIPTION: Illustrates the file organization for CPU plugin single layer tests.  The directory structure includes `classes` and `instances` directories, with instances further categorized by architecture (`arm`, `common`, `x64`) and backend (`acl`, `onednn`).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsingle_layer_tests/\n├── classes # test classes\n│   ├── activation.cpp # test class with common parameters\n│   ├── activation.h\n│   ├── convolution.cpp\n│   └── convolution.h\n└── instances # test instances\n    ├── arm # arch specific instances if any\n    │   ├── acl # backend specific instances if any\n    │   │   └── actication.cpp \n    │   └── onednn\n    │       └── convolution.cpp\n    ├── common # common instances across all the architecture\n    │   ├── activation.cpp\n    │   └── onednn\n    │       └── convolution.cpp\n    ├── _some_new_arch\n    │   ├── activation.cpp\n    │   └── convolution.cpp\n    └── x64 # arch specific instances if any\n        ├── activation.cpp # native instances for the arch (no backend involved)\n        ├── onednn # backend specific instances if any\n        │   └── convolution.cpp\n        └── _some_new_backend\n            └── convolution.cpp\n```\n\n----------------------------------------\n\nTITLE: Define RESIZE_NEAREST Constant - Typescript\nDESCRIPTION: Defines the RESIZE_NEAREST constant used as an enumeration member within the OpenVINO library. This constant represents the nearest neighbor resize algorithm. It's located in the addon.ts file.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/enums/resizeAlgorithm.rst#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nRESIZE_NEAREST: number\n```\n\n----------------------------------------\n\nTITLE: Conditional Snippets Configuration\nDESCRIPTION: This snippet conditionally appends 'ov_snippets_models' to 'LINK_LIBRARIES_PRIVATE' if the target 'openvino::snippets' exists. If 'ENABLE_SNIPPETS_LIBXSMM_TPP' is enabled, it adds the definition '-DSNIPPETS_LIBXSMM_TPP'. Otherwise, it appends the 'src/snippets' directory to the 'EXCLUDED_SOURCE_PATHS'.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/tests/functional/plugin/shared/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif (TARGET openvino::snippets)\n    list(APPEND LINK_LIBRARIES_PRIVATE ov_snippets_models)\n    if (ENABLE_SNIPPETS_LIBXSMM_TPP)\n        add_definitions(-DSNIPPETS_LIBXSMM_TPP)\n    endif()\nelse()\n    list(APPEND EXCLUDED_SOURCE_PATHS ${CMAKE_CURRENT_SOURCE_DIR}/src/snippets)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Install Xcode Command Line Tools\nDESCRIPTION: This snippet installs the Clang compiler and other command-line tools from Xcode. Xcode 10.1 or higher is required for building OpenVINO.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_mac_intel_cpu.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n% xcode-select --install\n```\n\n----------------------------------------\n\nTITLE: Get Input Port (Single Input Model) in OpenVINO (C)\nDESCRIPTION: This function retrieves an input port from an OpenVINO model, designed for models with a single input. It accepts a pointer to a `ov_model_t` and returns a pointer to the `ov_output_port_t` representing the input port. A status code is returned to indicate the outcome of the operation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/docs/api_overview.md#_snippet_25\n\nLANGUAGE: C\nCODE:\n```\nov_status_e ov_model_input(const ov_model_t* model, ov_output_port_t** input_port);\n```\n\n----------------------------------------\n\nTITLE: Setting Test Model Directory CMake\nDESCRIPTION: Sets the directory where the TensorFlow test models are located.  It defines a compile definition to make the directory path available during compilation.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/tensorflow/tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_TENSORFLOW_MODELS_DIRNAME ${TEST_MODEL_ZOO}/tensorflow_test_models)\ntarget_compile_definitions(${TARGET_NAME} PRIVATE -D TEST_TENSORFLOW_MODELS_DIRNAME=\\\"${TEST_TENSORFLOW_MODELS_DIRNAME}/\\\")\n```\n\n----------------------------------------\n\nTITLE: Creating Static Library in CMake\nDESCRIPTION: This snippet creates a static library, which includes objects from the `openvino_proxy_plugin_obj` (if it exists) and the `TARGET_NAME_obj`. It configures the threading interface, links necessary libraries, and defines compile definitions for static library usage. It also conditionally enables debug capabilities.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/inference/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME}_s STATIC\n            $<$<TARGET_EXISTS:openvino_proxy_plugin_obj>:$<TARGET_OBJECTS:openvino_proxy_plugin_obj>>\n            $<TARGET_OBJECTS:${TARGET_NAME}_obj>)\n\nov_set_threading_interface_for(${TARGET_NAME}_s)\nif (TBBBIND_2_5_FOUND)\n    target_link_libraries(${TARGET_NAME}_s PRIVATE ${TBBBIND_2_5_IMPORTED_TARGETS})\nendif()\n\ntarget_include_directories(${TARGET_NAME}_s PUBLIC\n    $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src>)\n\ntarget_link_libraries(${TARGET_NAME}_s PRIVATE openvino::itt ${CMAKE_DL_LIBS}\n    openvino::runtime::dev openvino::pugixml)\n\ntarget_compile_definitions(${TARGET_NAME}_s PUBLIC USE_STATIC_IE)\n\nif(ENABLE_DEBUG_CAPS)\n    target_compile_definitions(${TARGET_NAME}_s PUBLIC ENABLE_DEBUG_CAPS)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Sample Output (C++)\nDESCRIPTION: This is sample output from the C++ model creation sample. It shows the steps performed, and the output of inference. Requires the model to be built and loaded successfully.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/learn-openvino/openvino-samples/model-creation.rst#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\n[ INFO ] OpenVINO Runtime version ......... <version>\n[ INFO ] Build ........... <build>\n[ INFO ]\n[ INFO ] Device info:\n[ INFO ] GPU\n[ INFO ] Intel GPU plugin version ......... <version>\n[ INFO ] Build ........... <build>\n[ INFO ]\n[ INFO ]\n[ INFO ] Create model from weights: lenet.bin\n[ INFO ] model name: lenet\n[ INFO ]     inputs\n[ INFO ]         input name: NONE\n[ INFO ]         input type: f32\n[ INFO ]         input shape: {64, 1, 28, 28}\n[ INFO ]     outputs\n[ INFO ]         output name: output_tensor\n[ INFO ]         output type: f32\n[ INFO ]         output shape: {64, 10}\n[ INFO ] Batch size is 10\n[ INFO ] model name: lenet\n[ INFO ]     inputs\n[ INFO ]         input name: NONE\n[ INFO ]         input type: u8\n[ INFO ]         input shape: {10, 28, 28, 1}\n[ INFO ]     outputs\n[ INFO ]         output name: output_tensor\n[ INFO ]         output type: f32\n[ INFO ]         output shape: {10, 10}\n[ INFO ] Compiling a model for the GPU device\n[ INFO ] Create infer request\n[ INFO ] Combine images in batch and set to input tensor\n[ INFO ] Start sync inference\n[ INFO ] Processing output tensor\n\nTop 1 results:\n\nImage 0\n\nclassid probability label\n------- ----------- -----\n0       1.0000000   0\n\nImage 1\n\nclassid probability label\n------- ----------- -----\n1       1.0000000   1\n\nImage 2\n\nclassid probability label\n------- ----------- -----\n2       1.0000000   2\n\nImage 3\n\nclassid probability label\n------- ----------- -----\n3       1.0000000   3\n\nImage 4\n\nclassid probability label\n------- ----------- -----\n4       1.0000000   4\n\nImage 5\n\nclassid probability label\n------- ----------- -----\n5       1.0000000   5\n\nImage 6\n\nclassid probability label\n------- ----------- -----\n6       1.0000000   6\n\nImage 7\n\nclassid probability label\n------- ----------- -----\n7       1.0000000   7\n\nImage 8\n\nclassid probability label\n------- ----------- -----\n8       1.0000000   8\n\nImage 9\n\nclassid probability label\n------- ----------- -----\n9       1.0000000   9\n```\n\n----------------------------------------\n\nTITLE: Slicing 2D Tensor with All Axes Specified XML\nDESCRIPTION: This XML configuration describes a Slice layer that operates on a 2D tensor. The snippet explicitly defines the start, stop, step, and axes for both dimensions of the input tensor. The input tensor is of size 2x5, and the output after slicing is a 2x2 tensor.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-ir-format/operation-sets/operation-specs/movement/slice-8.rst#_snippet_9\n\nLANGUAGE: xml\nCODE:\n```\n<layer id=\"1\" type=\"Slice\" ...>\n       <input>\n           <port id=\"0\">       <!-- data: data: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]] -->\n             <dim>2</dim>\n             <dim>5</dim>\n           </port>\n           <port id=\"1\">       <!-- start: [0, 1] -->\n             <dim>2</dim>\n           </port>\n           <port id=\"2\">       <!-- stop: [2, 4] -->\n             <dim>2</dim>\n           </port>\n           <port id=\"3\">       <!-- step: [1, 2] -->\n             <dim>2</dim>\n           </port>\n           <port id=\"4\">       <!-- axes: [0, 1] -->\n             <dim>2</dim>\n           </port>\n       </input>\n       <output>\n           <port id=\"5\">      <!-- output: [1, 3, 6, 8] -->\n               <dim>2</dim>\n               <dim>2</dim>\n           </port>\n       </output>\n   </layer>\n```\n\n----------------------------------------\n\nTITLE: Defining CoreConstructor Interface in TypeScript\nDESCRIPTION: This TypeScript code snippet defines the `CoreConstructor` interface. It specifies that the interface has a constructor that returns an instance of the `Core` class. This interface is used for creating `Core` objects, which are central to the OpenVINO runtime.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CoreConstructor.rst#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\ninterface CoreConstructor {\n    new (): Core;\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: This line adds the specified subdirectory ('src') to the build. CMake will look for a CMakeLists.txt file within that directory and process it accordingly. This is a fundamental way to structure CMake projects by organizing source code into different directories.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/common_translators/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Compiling a model with OpenVINO backend in PyTorch\nDESCRIPTION: This code snippet shows how to compile a PyTorch model using the OpenVINO backend with specified device, model caching, and cache directory options using the torch.compile function. This compiles the model for execution on the CPU and enables model caching.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/openvino-workflow/torch-compile.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel = torch.compile(model, backend=\"openvino\", options = {\"device\" : \"CPU\", \"model_caching\" : True, \"cache_dir\": \"./model_cache\"})\n```\n\n----------------------------------------\n\nTITLE: Linking Against OpenVINO Runtime in CMake\nDESCRIPTION: This snippet uses target_link_libraries to link the target library against the OpenVINO runtime library. The PUBLIC keyword makes the OpenVINO runtime library available to other targets that link against this library.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/common/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET_NAME} PUBLIC openvino::runtime)\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Dependencies with Version Constraints\nDESCRIPTION: This code snippet defines the Python package dependencies required for the OpenVINO project, including version constraints based on the Python version. The constraints ensure compatibility between OpenVINO and its dependencies, such as networkx, torch, torchvision, and pillow.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/openvino/preprocess/torchvision/requirements.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n--extra-index-url https://download.pytorch.org/whl/cpu\nnetworkx<=3.2.1; python_version <= '3.9'\ntorch<=2.5.1; python_version <= '3.9'\ntorch>=1.13; python_version > '3.9'\ntorchvision<=0.20.1; python_version <= '3.9'\ntorchvision; python_version > '3.9'\npillow>=9.0\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: This snippet configures the include directories for the target. It adds the `${CMAKE_CURRENT_SOURCE_DIR}/include` directory as a public include directory, but only for the build interface.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/compiler_adapter/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME}\n    PUBLIC\n        $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\n)\n```\n\n----------------------------------------\n\nTITLE: Add Static Library CMake\nDESCRIPTION: Adds a static library target named 'onnx_fe_standalone_build_test' using 'standalone_build_test.cpp' as the source file. This defines how the library is built.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/frontends/onnx/tests/standalone_build/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${TARGET_NAME} STATIC standalone_build_test.cpp)\n```\n\n----------------------------------------\n\nTITLE: Adding MLAS Subdirectory\nDESCRIPTION: This snippet conditionally includes the MLAS subdirectory and installs its static library if the `ENABLE_MLAS_FOR_CPU` flag is set.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/thirdparty/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(ENABLE_MLAS_FOR_CPU)\n    add_subdirectory(mlas)\n    ov_install_static_lib(mlas ${OV_CPACK_COMP_CORE})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Plugin Destructor C++\nDESCRIPTION: This snippet illustrates the plugin destructor implementation. It highlights the need to stop all plugin activities and clean up any allocated resources to prevent memory leaks or other issues.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/documentation/openvino-extensibility/openvino-plugin-library/plugin.rst#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nPlugin::~Plugin() {\n    // Wait for all callbacks to finish and remove them from the callback list\n    m_is_busy = true;\n    {   // protect callbacks_mutex\n        std::unique_lock<std::mutex> lock(m_callback_mutex);\n        m_callback_cv.wait(lock, [&] { return m_pending_callbacks.empty(); });\n    }\n    m_is_busy = false;\n}\n\n```\n\n----------------------------------------\n\nTITLE: ESLint Rule: Newline Before Return\nDESCRIPTION: This rule requires a newline before return statements in JavaScript and TypeScript code. It improves code readability and is enforced by ESLint with the configuration `newline-before-return: ['error']`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/js/docs/CODESTYLE.md#_snippet_13\n\nLANGUAGE: JavaScript\nCODE:\n```\nnewline-before-return: ['error']\n```\n\n----------------------------------------\n\nTITLE: Adding Dependency to ie_wheel Target in CMake\nDESCRIPTION: This CMake snippet adds a dependency to the `ie_wheel` target, meaning that building the `ie_wheel` target will trigger the build of the `pyopenvino` target first. This is conditional and only executes if the `ie_wheel` target exists.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/python/src/pyopenvino/CMakeLists.txt#_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\nif(TARGET ie_wheel)\n    add_dependencies(ie_wheel ${PROJECT_NAME})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Inference with InferRequest TypeScript\nDESCRIPTION: Performs inference asynchronously using the provided input data.  Input data can be an object with input names as keys and Tensors as values, or an array of Tensors. Returns a Promise that resolves to an object with output names and Tensor values.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/InferRequest.rst#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\ninferAsync(inputData): Promise<{ [outputName: string]: Tensor }>\n```\n\n----------------------------------------\n\nTITLE: Downloading and Installing OpenVINO (ARM 32-bit)\nDESCRIPTION: These commands download the OpenVINO Runtime archive for ARM 32-bit systems, extract it, and move the extracted directory to `/opt/intel`. It uses `curl` to download the archive, `tar` to extract it, and `sudo mv` to move the extracted folder with root privileges.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/articles_en/get-started/install-openvino/install-openvino-archive-linux.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://storage.openvinotoolkit.org/repositories/openvino/packages/2025.1/linux/openvino_toolkit_debian10_2025.1.0.18503.6fec06580ab_armhf.tgz -O openvino_2025.1.0.tgz\ntar -xf openvino_2025.1.0.tgz\nsudo mv openvino_toolkit_debian10_2025.1.0.18503.6fec06580ab_armhf /opt/intel/openvino_2025.1.0\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory\nDESCRIPTION: This CMake command adds the 'vectorized' subdirectory to the build process, allowing its CMakeLists.txt to be processed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/tests/unit/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(vectorized)\n```\n\n----------------------------------------\n\nTITLE: MIT License Text\nDESCRIPTION: This snippet shows the full text of the MIT License. It grants broad permissions to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software, subject to including the copyright notice and permission notice in all copies or substantial portions of the software. It also includes a disclaimer of warranty and limitation of liability.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/licensing/runtime-third-party-programs.txt#_snippet_3\n\nLANGUAGE: Text\nCODE:\n```\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n\n----------------------------------------\n\nTITLE: Average Counters Aggregation Script (Python)\nDESCRIPTION: This entry refers to the `aggregate-average-counters.py` script used to aggregate the average counter data collected during OpenVINO application execution. The script is located in the `tools/aggregate-average-counters/` directory within the OpenVINO repository.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_cpu/docs/debug_capabilities/average_counters.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n../../tools/aggregate-average-counters/aggregate-average-counters.py\n```\n\n----------------------------------------\n\nTITLE: Adding Logger Subdirectory in CMake\nDESCRIPTION: This command adds the 'logger' subdirectory to the current CMake project build. It indicates that the 'logger' directory contains its own CMakeLists.txt file that should be processed.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/src/utils/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(logger)\n```\n\n----------------------------------------\n\nTITLE: Setting Target Properties in CMake\nDESCRIPTION: This snippet sets the target properties for 'protopipe', including the folder where it should be located in the IDE and the C++ standard version to use (C++17).\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/plugins/intel_npu/tools/protopipe/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET_NAME} PROPERTIES\n                          FOLDER ${CMAKE_CURRENT_SOURCE_DIR}\n                          CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: Exporting CompiledModel to Buffer (TypeScript)\nDESCRIPTION: This code shows the declaration of the `exportModelSync` method within the `CompiledModel` interface. This method exports the model and returns it as a Buffer.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/docs/sphinx_setup/api/nodejs_api/openvino-node/interfaces/CompiledModel.rst#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nexportModelSync(): Buffer\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for Target in CMake\nDESCRIPTION: Sets the include directories for the `ov_capi_test` target.  This allows the compiler to find the necessary header files during compilation. The interface is set using the `OPENVINO_API_SOURCE_DIR`.\nSOURCE: https://github.com/openvinotoolkit/openvino/blob/master/src/bindings/c/tests/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${TARGET_NAME} PUBLIC\n    $<BUILD_INTERFACE:${OPENVINO_API_SOURCE_DIR}/include>)\n```"
  }
]